[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.16375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16375v1",
                "updated": "2024-11-25T13:33:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T13:33:41Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "title": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing"
                },
                "summary": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available at\nhttps://github.com/Dawn-LX/CausalCache-VDM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available at\nhttps://github.com/Dawn-LX/CausalCache-VDM"
                },
                "authors": [
                    {
                        "name": "Kaifeng Gao"
                    },
                    {
                        "name": "Jiaxin Shi"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Chunping Wang"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Long Chen"
                    }
                ],
                "author_detail": {
                    "name": "Long Chen"
                },
                "author": "Long Chen",
                "arxiv_comment": "Technical Report. Code is available at\n  https://github.com/Dawn-LX/CausalCache-VDM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19315v2",
                "updated": "2024-11-25T12:14:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    14,
                    33,
                    0,
                    330,
                    0
                ],
                "published": "2024-09-28T11:00:11Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "title": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models"
                },
                "summary": "Transformer networks, driven by self-attention, are central to Large Language\nModels. In generative Transformers, self-attention uses cache memory to store\ntoken projections, avoiding recomputation at each time step. However,\nGPU-stored projections must be loaded into SRAM for each new generation step,\ncausing latency and energy bottlenecks.\n  We present a custom self-attention in-memory computing architecture based on\nemerging charge-based memories called gain cells, which can be efficiently\nwritten to store new tokens during sequence generation and enable parallel\nanalog dot-product computation required for self-attention. However, the analog\ngain cell circuits introduce non-idealities and constraints preventing the\ndirect mapping of pre-trained models. To circumvent this problem, we design an\ninitialization algorithm achieving text processing performance comparable to\nGPT-2 without training from scratch. Our architecture respectively reduces\nattention latency and energy consumption by up to two and five orders of\nmagnitude compared to GPUs, marking a significant step toward ultra-fast,\nlow-power generative Transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer networks, driven by self-attention, are central to Large Language\nModels. In generative Transformers, self-attention uses cache memory to store\ntoken projections, avoiding recomputation at each time step. However,\nGPU-stored projections must be loaded into SRAM for each new generation step,\ncausing latency and energy bottlenecks.\n  We present a custom self-attention in-memory computing architecture based on\nemerging charge-based memories called gain cells, which can be efficiently\nwritten to store new tokens during sequence generation and enable parallel\nanalog dot-product computation required for self-attention. However, the analog\ngain cell circuits introduce non-idealities and constraints preventing the\ndirect mapping of pre-trained models. To circumvent this problem, we design an\ninitialization algorithm achieving text processing performance comparable to\nGPT-2 without training from scratch. Our architecture respectively reduces\nattention latency and energy consumption by up to two and five orders of\nmagnitude compared to GPUs, marking a significant step toward ultra-fast,\nlow-power generative Transformers."
                },
                "authors": [
                    {
                        "name": "Nathan Leroux"
                    },
                    {
                        "name": "Paul-Philipp Manea"
                    },
                    {
                        "name": "Chirag Sudarshan"
                    },
                    {
                        "name": "Jan Finkbeiner"
                    },
                    {
                        "name": "Sebastian Siegel"
                    },
                    {
                        "name": "John Paul Strachan"
                    },
                    {
                        "name": "Emre Neftci"
                    }
                ],
                "author_detail": {
                    "name": "Emre Neftci"
                },
                "author": "Emre Neftci",
                "arxiv_comment": "25 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11469v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11469v2",
                "updated": "2024-11-24T21:57:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    21,
                    57,
                    29,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-18T11:12:57Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    12,
                    57,
                    0,
                    323,
                    0
                ],
                "title": "Deegen: A JIT-Capable VM Generator for Dynamic Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deegen: A JIT-Capable VM Generator for Dynamic Languages"
                },
                "summary": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT."
                },
                "authors": [
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Fredrik Kjolstad"
                    }
                ],
                "author_detail": {
                    "name": "Fredrik Kjolstad"
                },
                "author": "Fredrik Kjolstad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11469v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11469v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15785v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15785v1",
                "updated": "2024-11-24T11:30:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    11,
                    30,
                    0,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T11:30:00Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    11,
                    30,
                    0,
                    6,
                    329,
                    0
                ],
                "title": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity"
                },
                "summary": "This paper proposes a method for building large language models with\npredefined Key-Value (KV) cache capacity, particularly suitable for the\nattention layers in Transformer decode-only architectures. This method\nintroduces fixed-length KV caches to address the issue of excessive memory\nconsumption in traditional KV caches when handling infinite contexts. By\ndynamically updating the key-value vector sequences, it achieves efficient\ninference within limited cache capacity, significantly reducing memory usage\nwhile maintaining model performance and system throughput. Experimental results\nshow that this method significantly reduces memory usage while maintaining the\nmodel's inference quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a method for building large language models with\npredefined Key-Value (KV) cache capacity, particularly suitable for the\nattention layers in Transformer decode-only architectures. This method\nintroduces fixed-length KV caches to address the issue of excessive memory\nconsumption in traditional KV caches when handling infinite contexts. By\ndynamically updating the key-value vector sequences, it achieves efficient\ninference within limited cache capacity, significantly reducing memory usage\nwhile maintaining model performance and system throughput. Experimental results\nshow that this method significantly reduces memory usage while maintaining the\nmodel's inference quality."
                },
                "authors": [
                    {
                        "name": "Zhonghua Yi"
                    },
                    {
                        "name": "Ge Niu"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Liqiu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liqiu Zhang"
                },
                "author": "Liqiu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15785v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15735v1",
                "updated": "2024-11-24T06:43:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    6,
                    43,
                    38,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T06:43:38Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    6,
                    43,
                    38,
                    6,
                    329,
                    0
                ],
                "title": "Test-time Alignment-Enhanced Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time Alignment-Enhanced Adapter for Vision-Language Models"
                },
                "summary": "Test-time adaptation with pre-trained vision-language models (VLMs) has\nattracted increasing attention for tackling the issue of distribution shift\nduring the test phase. While prior methods have shown effectiveness in\naddressing distribution shift by adjusting classification logits, they are not\noptimal due to keeping text features unchanged. To address this issue, we\nintroduce a new approach called Test-time Alignment-Enhanced Adapter (TAEA),\nwhich trains an adapter with test samples to adjust text features during the\ntest phase. We can enhance the text-to-image alignment prediction by utilizing\nan adapter to adapt text features. Furthermore, we also propose to adopt the\nnegative cache from TDA as enhancement module, which further improves the\nperformance of TAEA. Our approach outperforms the state-of-the-art TTA method\nof pre-trained VLMs by an average of 0.75% on the out-of-distribution benchmark\nand 2.5% on the cross-domain benchmark, with an acceptable training time. Code\nwill be available at https://github.com/BaoshunWq/clip-TAEA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation with pre-trained vision-language models (VLMs) has\nattracted increasing attention for tackling the issue of distribution shift\nduring the test phase. While prior methods have shown effectiveness in\naddressing distribution shift by adjusting classification logits, they are not\noptimal due to keeping text features unchanged. To address this issue, we\nintroduce a new approach called Test-time Alignment-Enhanced Adapter (TAEA),\nwhich trains an adapter with test samples to adjust text features during the\ntest phase. We can enhance the text-to-image alignment prediction by utilizing\nan adapter to adapt text features. Furthermore, we also propose to adopt the\nnegative cache from TDA as enhancement module, which further improves the\nperformance of TAEA. Our approach outperforms the state-of-the-art TTA method\nof pre-trained VLMs by an average of 0.75% on the out-of-distribution benchmark\nand 2.5% on the cross-domain benchmark, with an acceptable training time. Code\nwill be available at https://github.com/BaoshunWq/clip-TAEA."
                },
                "authors": [
                    {
                        "name": "Baoshun Tong"
                    },
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09688v2",
                "updated": "2024-11-23T22:11:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    22,
                    11,
                    42,
                    5,
                    328,
                    0
                ],
                "published": "2024-11-14T18:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Attention: Accelerating Long Context Length LLM Inference"
                },
                "summary": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "June Paik"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05396v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05396v3",
                "updated": "2024-11-23T10:42:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    10,
                    42,
                    11,
                    5,
                    328,
                    0
                ],
                "published": "2024-02-08T04:16:35Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    4,
                    16,
                    35,
                    3,
                    39,
                    0
                ],
                "title": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph\n  Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph\n  Representation Learning"
                },
                "summary": "Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated\nstate-of-the-art performance in various high-impact applications, including\nfraud detection and content recommendation. Despite the success of TGNNs, they\nare prone to the prevalent noise found in real-world dynamic graphs like\ntime-deprecated links and skewed interaction distribution. The noise causes two\ncritical issues that significantly compromise the accuracy of TGNNs: (1) models\nare supervised by inferior interactions, and (2) noisy input induces high\nvariance in the aggregated messages. However, current TGNN denoising techniques\ndo not consider the diverse and dynamic noise pattern of each node. In\naddition, they also suffer from the excessive mini-batch generation overheads\ncaused by traversing more neighbors. We believe the remedy for fast and\naccurate TGNNs lies in temporal adaptive sampling. In this work, we propose\nTASER, the first adaptive sampling method for TGNNs optimized for accuracy,\nefficiency, and scalability. TASER adapts its mini-batch selection based on\ntraining dynamics and temporal neighbor selection based on the contextual,\nstructural, and temporal properties of past interactions. To alleviate the\nbottleneck in mini-batch generation, TASER implements a pure GPU-based temporal\nneighbor finder and a dedicated GPU feature cache. We evaluate the performance\nof TASER using two state-of-the-art backbone TGNNs. On five popular datasets,\nTASER outperforms the corresponding baselines by an average of 2.3% in Mean\nReciprocal Rank (MRR) while achieving an average of 5.1x speedup in training\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated\nstate-of-the-art performance in various high-impact applications, including\nfraud detection and content recommendation. Despite the success of TGNNs, they\nare prone to the prevalent noise found in real-world dynamic graphs like\ntime-deprecated links and skewed interaction distribution. The noise causes two\ncritical issues that significantly compromise the accuracy of TGNNs: (1) models\nare supervised by inferior interactions, and (2) noisy input induces high\nvariance in the aggregated messages. However, current TGNN denoising techniques\ndo not consider the diverse and dynamic noise pattern of each node. In\naddition, they also suffer from the excessive mini-batch generation overheads\ncaused by traversing more neighbors. We believe the remedy for fast and\naccurate TGNNs lies in temporal adaptive sampling. In this work, we propose\nTASER, the first adaptive sampling method for TGNNs optimized for accuracy,\nefficiency, and scalability. TASER adapts its mini-batch selection based on\ntraining dynamics and temporal neighbor selection based on the contextual,\nstructural, and temporal properties of past interactions. To alleviate the\nbottleneck in mini-batch generation, TASER implements a pure GPU-based temporal\nneighbor finder and a dedicated GPU feature cache. We evaluate the performance\nof TASER using two state-of-the-art backbone TGNNs. On five popular datasets,\nTASER outperforms the corresponding baselines by an average of 2.3% in Mean\nReciprocal Rank (MRR) while achieving an average of 5.1x speedup in training\ntime."
                },
                "authors": [
                    {
                        "name": "Gangda Deng"
                    },
                    {
                        "name": "Hongkuan Zhou"
                    },
                    {
                        "name": "Hanqing Zeng"
                    },
                    {
                        "name": "Yinglong Xia"
                    },
                    {
                        "name": "Christopher Leung"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Rajgopal Kannan"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "arxiv_comment": "IPDPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05396v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05396v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02109v2",
                "updated": "2024-11-23T01:44:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    1,
                    44,
                    0,
                    5,
                    328,
                    0
                ],
                "published": "2024-07-02T09:51:56Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    9,
                    51,
                    56,
                    1,
                    184,
                    0
                ],
                "title": "HRSAM: Efficient Interactive Segmentation in High-Resolution Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HRSAM: Efficient Interactive Segmentation in High-Resolution Images"
                },
                "summary": "The Segment Anything Model (SAM) has advanced interactive segmentation but is\nlimited by the high computational cost on high-resolution images. This requires\ndownsampling to meet GPU constraints, sacrificing the fine-grained details\nneeded for high-precision interactive segmentation. To address SAM's\nlimitations, we focus on visual length extrapolation and propose a lightweight\nmodel named HRSAM. The extrapolation enables HRSAM trained on low resolutions\nto generalize to high resolutions. We begin by finding the link between the\nextrapolation and attention scores, which leads us to base HRSAM on Swin\nattention. We then introduce the Flexible Local Attention (FLA) framework,\nusing CUDA-optimized Efficient Memory Attention to accelerate HRSAM. Within\nFLA, we implement Flash Swin attention, achieving over a 35% speedup compared\nto traditional Swin attention, and propose a KV-only padding mechanism to\nenhance extrapolation. We also develop the Cycle-scan module that uses State\nSpace models to efficiently expand HRSAM's receptive field. We further develop\nthe HRSAM++ within FLA by adding an anchor map, providing multi-scale data\naugmentation for the extrapolation and a larger receptive field at slight\ncomputational cost. Experiments show that, under standard training, HRSAMs\nsurpass the previous SOTA with only 38% of the latency. With SAM-distillation,\nthe extrapolation enables HRSAMs to outperform the teacher model at lower\nlatency. Further finetuning achieves performance significantly exceeding the\nprevious SOTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Segment Anything Model (SAM) has advanced interactive segmentation but is\nlimited by the high computational cost on high-resolution images. This requires\ndownsampling to meet GPU constraints, sacrificing the fine-grained details\nneeded for high-precision interactive segmentation. To address SAM's\nlimitations, we focus on visual length extrapolation and propose a lightweight\nmodel named HRSAM. The extrapolation enables HRSAM trained on low resolutions\nto generalize to high resolutions. We begin by finding the link between the\nextrapolation and attention scores, which leads us to base HRSAM on Swin\nattention. We then introduce the Flexible Local Attention (FLA) framework,\nusing CUDA-optimized Efficient Memory Attention to accelerate HRSAM. Within\nFLA, we implement Flash Swin attention, achieving over a 35% speedup compared\nto traditional Swin attention, and propose a KV-only padding mechanism to\nenhance extrapolation. We also develop the Cycle-scan module that uses State\nSpace models to efficiently expand HRSAM's receptive field. We further develop\nthe HRSAM++ within FLA by adding an anchor map, providing multi-scale data\naugmentation for the extrapolation and a larger receptive field at slight\ncomputational cost. Experiments show that, under standard training, HRSAMs\nsurpass the previous SOTA with only 38% of the latency. With SAM-distillation,\nthe extrapolation enables HRSAMs to outperform the teacher model at lower\nlatency. Further finetuning achieves performance significantly exceeding the\nprevious SOTA."
                },
                "authors": [
                    {
                        "name": "You Huang"
                    },
                    {
                        "name": "Wenbin Lai"
                    },
                    {
                        "name": "Jiayi Ji"
                    },
                    {
                        "name": "Liujuan Cao"
                    },
                    {
                        "name": "Shengchuan Zhang"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15322v1",
                "updated": "2024-11-22T19:30:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    19,
                    30,
                    40,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T19:30:40Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    19,
                    30,
                    40,
                    4,
                    327,
                    0
                ],
                "title": "Deep Learning-Based Automatic Delineation of Liver Domes in kV Triggered\n  Images for Online Breath-hold Reproducibility Verification of Liver\n  Stereotactic Body Radiation Therapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning-Based Automatic Delineation of Liver Domes in kV Triggered\n  Images for Online Breath-hold Reproducibility Verification of Liver\n  Stereotactic Body Radiation Therapy"
                },
                "summary": "Stereotactic Body Radiation Therapy (SBRT) can be a precise, minimally\ninvasive treatment method for liver cancer and liver metastases. However, the\neffectiveness of SBRT relies on the accurate delivery of the dose to the tumor\nwhile sparing healthy tissue. Challenges persist in ensuring breath-hold\nreproducibility, with current methods often requiring manual verification of\nliver dome positions from kV-triggered images. To address this, we propose a\nproof-of-principle study of a deep learning-based pipeline to automatically\ndelineate the liver dome from kV-planar images. From 24 patients who received\nSBRT for liver cancer or metastasis inside liver, 711 KV-triggered images\nacquired for online breath-hold verification were included in the current\nstudy. We developed a pipeline comprising a trained U-Net for automatic liver\ndome region segmentation from the triggered images followed by extraction of\nthe liver dome via thresholding, edge detection, and morphological operations.\nThe performance and generalizability of the pipeline was evaluated using 2-fold\ncross validation. The training of the U-Net model for liver region segmentation\ntook under 30 minutes and the automatic delineation of a liver dome for any\ntriggered image took less than one second. The RMSE and rate of detection for\nFold1 with 366 images was (6.4 +/- 1.6) mm and 91.7%, respectively. For Fold2\nwith 345 images, the RMSE and rate of detection was (7.7 +/- 2.3) mm and 76.3%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stereotactic Body Radiation Therapy (SBRT) can be a precise, minimally\ninvasive treatment method for liver cancer and liver metastases. However, the\neffectiveness of SBRT relies on the accurate delivery of the dose to the tumor\nwhile sparing healthy tissue. Challenges persist in ensuring breath-hold\nreproducibility, with current methods often requiring manual verification of\nliver dome positions from kV-triggered images. To address this, we propose a\nproof-of-principle study of a deep learning-based pipeline to automatically\ndelineate the liver dome from kV-planar images. From 24 patients who received\nSBRT for liver cancer or metastasis inside liver, 711 KV-triggered images\nacquired for online breath-hold verification were included in the current\nstudy. We developed a pipeline comprising a trained U-Net for automatic liver\ndome region segmentation from the triggered images followed by extraction of\nthe liver dome via thresholding, edge detection, and morphological operations.\nThe performance and generalizability of the pipeline was evaluated using 2-fold\ncross validation. The training of the U-Net model for liver region segmentation\ntook under 30 minutes and the automatic delineation of a liver dome for any\ntriggered image took less than one second. The RMSE and rate of detection for\nFold1 with 366 images was (6.4 +/- 1.6) mm and 91.7%, respectively. For Fold2\nwith 345 images, the RMSE and rate of detection was (7.7 +/- 2.3) mm and 76.3%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Sugandima Weragoda"
                    },
                    {
                        "name": "Ping Xia"
                    },
                    {
                        "name": "Kevin Stephans"
                    },
                    {
                        "name": "Neil Woody"
                    },
                    {
                        "name": "Michael Martens"
                    },
                    {
                        "name": "Robert Brown"
                    },
                    {
                        "name": "Bingqi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Bingqi Guo"
                },
                "author": "Bingqi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v1",
                "updated": "2024-11-22T18:06:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v1",
                "updated": "2024-11-22T15:55:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04032v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04032v5",
                "updated": "2024-11-21T05:55:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    55,
                    43,
                    3,
                    326,
                    0
                ],
                "published": "2024-02-06T14:26:22Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    14,
                    26,
                    22,
                    1,
                    37,
                    0
                ],
                "title": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System"
                },
                "summary": "The model size growth of personalized recommendation systems poses new\nchallenges for inference. Weight-sharing algorithms have been proposed for size\nreduction, but they increase memory access. Recent advancements in\nprocessing-in-memory (PIM) enhanced the model throughput by exploiting memory\nparallelism, but such algorithms introduce massive CPU-PIM communication into\nprior PIM systems. We propose ProactivePIM, a PIM system for weight-sharing\nrecommendation system acceleration. ProactivePIM integrates a cache within the\nPIM with a prefetching scheme to leverage a unique locality of the algorithm\nand eliminate communication overhead through a subtable mapping strategy.\nProactivePIM achieves a 4.8x speedup compared to prior works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The model size growth of personalized recommendation systems poses new\nchallenges for inference. Weight-sharing algorithms have been proposed for size\nreduction, but they increase memory access. Recent advancements in\nprocessing-in-memory (PIM) enhanced the model throughput by exploiting memory\nparallelism, but such algorithms introduce massive CPU-PIM communication into\nprior PIM systems. We propose ProactivePIM, a PIM system for weight-sharing\nrecommendation system acceleration. ProactivePIM integrates a cache within the\nPIM with a prefetching scheme to leverage a unique locality of the algorithm\nand eliminate communication overhead through a subtable mapping strategy.\nProactivePIM achieves a 4.8x speedup compared to prior works."
                },
                "authors": [
                    {
                        "name": "Youngsuk Kim"
                    },
                    {
                        "name": "Junghwan Lim"
                    },
                    {
                        "name": "Hyuk-Jae Lee"
                    },
                    {
                        "name": "Chae Eun Rhee"
                    }
                ],
                "author_detail": {
                    "name": "Chae Eun Rhee"
                },
                "author": "Chae Eun Rhee",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04032v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04032v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13854v1",
                "updated": "2024-11-21T05:26:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    26,
                    57,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T05:26:57Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    26,
                    57,
                    3,
                    326,
                    0
                ],
                "title": "Static Reuse Profile Estimation for Array Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Reuse Profile Estimation for Array Applications"
                },
                "summary": "Reuse distance analysis is a widely recognized method for application\ncharacterization that illustrates cache locality. Although there are various\ntechniques to calculate the reuse profile from dynamic memory traces, it is\nboth time and space-consuming due to the requirement to collect dynamic memory\ntraces at runtime. In contrast, static analysis reuse profile estimation is a\npromisingly faster approach since it is calculated at compile time without\nrunning the program or collecting memory traces. This work presents a static\nanalysis technique to estimate the reuse profile of loop-based programs. For an\ninput program, we generate a basic block-level control flow graph and the\nexecution count by analyzing the LLVM IR of the program. We present the memory\naccesses of the application kernel in a compact bracketed format and use a\nrecursive algorithm to predict the reuse distance histogram. We deploy a\nseparate predictor that unrolls the loop(s) for smaller bounds and generates a\ntemporary reuse distance profile for those small cases. Using these smaller\nprofiles, the reuse profile is extrapolated for the actual loop bound(s). We\nuse this reuse profile to predict the cache hit rate. Results show that our\nmodel can predict cache hit rates with an average accuracy of 95% relative to\nthe dynamic reuse profile methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reuse distance analysis is a widely recognized method for application\ncharacterization that illustrates cache locality. Although there are various\ntechniques to calculate the reuse profile from dynamic memory traces, it is\nboth time and space-consuming due to the requirement to collect dynamic memory\ntraces at runtime. In contrast, static analysis reuse profile estimation is a\npromisingly faster approach since it is calculated at compile time without\nrunning the program or collecting memory traces. This work presents a static\nanalysis technique to estimate the reuse profile of loop-based programs. For an\ninput program, we generate a basic block-level control flow graph and the\nexecution count by analyzing the LLVM IR of the program. We present the memory\naccesses of the application kernel in a compact bracketed format and use a\nrecursive algorithm to predict the reuse distance histogram. We deploy a\nseparate predictor that unrolls the loop(s) for smaller bounds and generates a\ntemporary reuse distance profile for those small cases. Using these smaller\nprofiles, the reuse profile is extrapolated for the actual loop bound(s). We\nuse this reuse profile to predict the cache hit rate. Results show that our\nmodel can predict cache hit rates with an average accuracy of 95% relative to\nthe dynamic reuse profile methods."
                },
                "authors": [
                    {
                        "name": "Abdur Razzak"
                    },
                    {
                        "name": "Atanu Barai"
                    },
                    {
                        "name": "Nandakishore Santhi"
                    },
                    {
                        "name": "Abdel-Hameed A. Badawy"
                    }
                ],
                "author_detail": {
                    "name": "Abdel-Hameed A. Badawy"
                },
                "author": "Abdel-Hameed A. Badawy",
                "arxiv_comment": "Accepted in The International Symposium on Memory Systems (MEMSYS\n  24), September 30 to October 03, 2024, Washington, DC, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.02243v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.02243v3",
                "updated": "2024-11-21T04:12:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    4,
                    12,
                    53,
                    3,
                    326,
                    0
                ],
                "published": "2023-06-04T03:06:37Z",
                "published_parsed": [
                    2023,
                    6,
                    4,
                    3,
                    6,
                    37,
                    6,
                    155,
                    0
                ],
                "title": "Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification"
                },
                "summary": "The Contrastive Language-Image Pretraining (CLIP) model has been widely used\nin various downstream vision tasks. The few-shot learning paradigm has been\nwidely adopted to augment its capacity for these tasks. However, current\nparadigms may struggle with fine-grained classification, such as satellite\nimage recognition, due to widening domain gaps. To address this limitation, we\npropose retrieval-enhanced visual prompt learning (RePrompt), which introduces\nretrieval mechanisms to cache and reuse the knowledge of downstream tasks.\nRePrompt constructs a retrieval database from either training examples or\nexternal data if available, and uses a retrieval mechanism to enhance multiple\nstages of a simple prompt learning baseline, thus narrowing the domain gap.\nDuring inference, our enhanced model can reference similar samples brought by\nretrieval to make more accurate predictions. A detailed analysis reveals that\nretrieval helps to improve the distribution of late features, thus, improving\ngeneralization for downstream tasks. Reprompt attains state-of-the-art\nperformance on a wide range of vision datasets, including 11 image datasets, 3\nvideo datasets, 1 multi-view dataset, and 4 domain generalization benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Contrastive Language-Image Pretraining (CLIP) model has been widely used\nin various downstream vision tasks. The few-shot learning paradigm has been\nwidely adopted to augment its capacity for these tasks. However, current\nparadigms may struggle with fine-grained classification, such as satellite\nimage recognition, due to widening domain gaps. To address this limitation, we\npropose retrieval-enhanced visual prompt learning (RePrompt), which introduces\nretrieval mechanisms to cache and reuse the knowledge of downstream tasks.\nRePrompt constructs a retrieval database from either training examples or\nexternal data if available, and uses a retrieval mechanism to enhance multiple\nstages of a simple prompt learning baseline, thus narrowing the domain gap.\nDuring inference, our enhanced model can reference similar samples brought by\nretrieval to make more accurate predictions. A detailed analysis reveals that\nretrieval helps to improve the distribution of late features, thus, improving\ngeneralization for downstream tasks. Reprompt attains state-of-the-art\nperformance on a wide range of vision datasets, including 11 image datasets, 3\nvideo datasets, 1 multi-view dataset, and 4 domain generalization benchmarks."
                },
                "authors": [
                    {
                        "name": "Jintao Rong"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Linlin Ou"
                    },
                    {
                        "name": "Tianxiao Chen"
                    },
                    {
                        "name": "Xinyi Yu"
                    },
                    {
                        "name": "Yifan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Liu"
                },
                "author": "Yifan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.02243v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.02243v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13820v1",
                "updated": "2024-11-21T03:52:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T03:52:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "InstCache: A Predictive Cache for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstCache: A Predictive Cache for LLM Serving"
                },
                "summary": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB."
                },
                "authors": [
                    {
                        "name": "Longwei Zou"
                    },
                    {
                        "name": "Tingfeng Liu"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Jiangang Kong"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22649v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22649v2",
                "updated": "2024-11-21T03:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    34,
                    44,
                    3,
                    326,
                    0
                ],
                "published": "2024-10-30T02:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting"
                },
                "summary": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs. Our code is available at\nhttps://github.com/Leopold2333/WaveRoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs. Our code is available at\nhttps://github.com/Leopold2333/WaveRoRA."
                },
                "authors": [
                    {
                        "name": "Aobo Liang"
                    },
                    {
                        "name": "Yan Sun"
                    },
                    {
                        "name": "Nadra Guizani"
                    }
                ],
                "author_detail": {
                    "name": "Nadra Guizani"
                },
                "author": "Nadra Guizani",
                "arxiv_comment": "Model architecture changed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22649v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22649v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13786v1",
                "updated": "2024-11-21T02:15:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    15,
                    52,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T02:15:52Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    15,
                    52,
                    3,
                    326,
                    0
                ],
                "title": "Adaptable Embeddings Network (AEN)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptable Embeddings Network (AEN)"
                },
                "summary": "Modern day Language Models see extensive use in text classification, yet this\ncomes at significant computational cost. Compute-effective classification\nmodels are needed for low-resource environments, most notably on edge devices.\nWe introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder\narchitecture using Kernel Density Estimation (KDE). This architecture allows\nfor runtime adaptation of classification criteria without retraining and is\nnon-autoregressive. Through thorough synthetic data experimentation, we\ndemonstrate our model outputs comparable and in certain cases superior results\nto that of autoregressive models an order of magnitude larger than AEN's size.\nThe architecture's ability to preprocess and cache condition embeddings makes\nit ideal for edge computing applications and real-time monitoring systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern day Language Models see extensive use in text classification, yet this\ncomes at significant computational cost. Compute-effective classification\nmodels are needed for low-resource environments, most notably on edge devices.\nWe introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder\narchitecture using Kernel Density Estimation (KDE). This architecture allows\nfor runtime adaptation of classification criteria without retraining and is\nnon-autoregressive. Through thorough synthetic data experimentation, we\ndemonstrate our model outputs comparable and in certain cases superior results\nto that of autoregressive models an order of magnitude larger than AEN's size.\nThe architecture's ability to preprocess and cache condition embeddings makes\nit ideal for edge computing applications and real-time monitoring systems."
                },
                "authors": [
                    {
                        "name": "Stan Loosmore"
                    },
                    {
                        "name": "Alexander Titus"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Titus"
                },
                "author": "Alexander Titus",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13676v1",
                "updated": "2024-11-20T19:51:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    51,
                    25,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T19:51:25Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    51,
                    25,
                    2,
                    325,
                    0
                ],
                "title": "Hymba: A Hybrid-head Architecture for Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hymba: A Hybrid-head Architecture for Small Language Models"
                },
                "summary": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput."
                },
                "authors": [
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Wonmin Byeon"
                    },
                    {
                        "name": "Zijia Chen"
                    },
                    {
                        "name": "Ameya Sunil Mahabaleshwarkar"
                    },
                    {
                        "name": "Shih-Yang Liu"
                    },
                    {
                        "name": "Matthijs Van Keirsbilck"
                    },
                    {
                        "name": "Min-Hung Chen"
                    },
                    {
                        "name": "Yoshi Suhara"
                    },
                    {
                        "name": "Yingyan Lin"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "arxiv_comment": "20 pages, models are available on huggingface",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13532v1",
                "updated": "2024-11-20T18:31:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    31,
                    39,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T18:31:39Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    31,
                    39,
                    2,
                    325,
                    0
                ],
                "title": "A Distributed-memory Tridiagonal Solver Based on a Specialised Data\n  Structure Optimised for CPU and GPU Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Distributed-memory Tridiagonal Solver Based on a Specialised Data\n  Structure Optimised for CPU and GPU Architectures"
                },
                "summary": "Various numerical methods used for solving partial differential equations\n(PDE) result in tridiagonal systems. Solving tridiagonal systems on\ndistributed-memory environments is not straightforward, and often requires\nsignificant amount of communication. In this article, we present a novel\ndistributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a\nspecialised data structure. DistD2-TDS algorithm takes advantage of the\ndiagonal dominance in tridiagonal systems to reduce the communications in\ndistributed-memory environments. The underlying data structure plays a crucial\nrole for the performance of the algorithm. First, the data structure improves\ndata localities and makes it possible to minimise data movements via cache\nblocking and kernel fusion strategies. Second, data continuity enables a\ncontiguous data access pattern and results in efficient utilisation of the\navailable memory bandwidth. Finally, the data layout supports vectorisation on\nCPUs and thread level parallelisation on GPUs for improved performance. In\norder to demonstrate the robustness of the algorithm, we implemented and\nbenchmarked the algorithm on CPUs and GPUs. We investigated the single rank\nperformance and compared against existing algorithms. Furthermore, we analysed\nthe strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to\n8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the\nalgorithm by using compact finite difference schemes to solve a 3D non-linear\nPDE. The results demonstrate that DistD2 algorithm can sustain around 66% of\nthe theoretical peak bandwidth at scale on CPU and GPU based supercomputers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various numerical methods used for solving partial differential equations\n(PDE) result in tridiagonal systems. Solving tridiagonal systems on\ndistributed-memory environments is not straightforward, and often requires\nsignificant amount of communication. In this article, we present a novel\ndistributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a\nspecialised data structure. DistD2-TDS algorithm takes advantage of the\ndiagonal dominance in tridiagonal systems to reduce the communications in\ndistributed-memory environments. The underlying data structure plays a crucial\nrole for the performance of the algorithm. First, the data structure improves\ndata localities and makes it possible to minimise data movements via cache\nblocking and kernel fusion strategies. Second, data continuity enables a\ncontiguous data access pattern and results in efficient utilisation of the\navailable memory bandwidth. Finally, the data layout supports vectorisation on\nCPUs and thread level parallelisation on GPUs for improved performance. In\norder to demonstrate the robustness of the algorithm, we implemented and\nbenchmarked the algorithm on CPUs and GPUs. We investigated the single rank\nperformance and compared against existing algorithms. Furthermore, we analysed\nthe strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to\n8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the\nalgorithm by using compact finite difference schemes to solve a 3D non-linear\nPDE. The results demonstrate that DistD2 algorithm can sustain around 66% of\nthe theoretical peak bandwidth at scale on CPU and GPU based supercomputers."
                },
                "authors": [
                    {
                        "name": "Semih Akkurt"
                    },
                    {
                        "name": "Sbastien Lemaire"
                    },
                    {
                        "name": "Paul Bartholomew"
                    },
                    {
                        "name": "Sylvain Laizet"
                    }
                ],
                "author_detail": {
                    "name": "Sylvain Laizet"
                },
                "author": "Sylvain Laizet",
                "arxiv_comment": "42 pages, 13 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13373v1",
                "updated": "2024-11-20T14:52:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T14:52:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment"
                },
                "summary": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Plasma\nPhysics, was based on a source of positive hydrogen ions, accelerated to 50 keV\nand for an equivalent neutral beam current of about 5 A at the source. The beam\ncould be modulated and the maximum overall duration was 50 ms. With the upgrade\nof RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to\nsolve several plant faults and improve the overall reliability of the system.\nThe 50 kV power supply is being improved, as well as the power supplies in the\nhigh voltage deck and its insulation transformer. The control system,\noriginally based on CAMAC technology, was redesigned to be fully replaced. This\ncontribution reviews the technical criticalities emerged in the DNBI check-up\nand the new solutions adopted to make the DNBI operative and more reliable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Plasma\nPhysics, was based on a source of positive hydrogen ions, accelerated to 50 keV\nand for an equivalent neutral beam current of about 5 A at the source. The beam\ncould be modulated and the maximum overall duration was 50 ms. With the upgrade\nof RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to\nsolve several plant faults and improve the overall reliability of the system.\nThe 50 kV power supply is being improved, as well as the power supplies in the\nhigh voltage deck and its insulation transformer. The control system,\noriginally based on CAMAC technology, was redesigned to be fully replaced. This\ncontribution reviews the technical criticalities emerged in the DNBI check-up\nand the new solutions adopted to make the DNBI operative and more reliable."
                },
                "authors": [
                    {
                        "name": "Marco Barbisan"
                    },
                    {
                        "name": "Marco Boldrin"
                    },
                    {
                        "name": "Luca Cinnirella"
                    },
                    {
                        "name": "Bruno Laterza"
                    },
                    {
                        "name": "Alberto Maistrello"
                    },
                    {
                        "name": "Lionello Marrelli"
                    },
                    {
                        "name": "Federico Molon"
                    },
                    {
                        "name": "Simone Peruzzo"
                    },
                    {
                        "name": "Cesare Taliercio"
                    },
                    {
                        "name": "Marco Valisa"
                    },
                    {
                        "name": "Enrico Zampiva"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Zampiva"
                },
                "author": "Enrico Zampiva",
                "arxiv_comment": "6 pages (excl. highlights), 8 figures. Contribution to the 33rd\n  Symposium on Fusion Technology (SOFT), 22-27 September 2024. This is a\n  preprint for the \"Fusion Engineering and Design\" journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v4",
                "updated": "2024-11-20T02:04:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    4,
                    10,
                    2,
                    325,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "Published on the First Conference on Language Modeling (COLM 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17918v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17918v3",
                "updated": "2024-11-19T18:24:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    24,
                    3,
                    1,
                    324,
                    0
                ],
                "published": "2024-06-25T20:00:32Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    0,
                    32,
                    1,
                    177,
                    0
                ],
                "title": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and\n  Retrieval"
                },
                "summary": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17918v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17918v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12430v1",
                "updated": "2024-11-19T11:40:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    40,
                    56,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T11:40:56Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    40,
                    56,
                    1,
                    324,
                    0
                ],
                "title": "An Eulerian approach to regularized JKO scheme with low-rank tensor\n  decompositions for Bayesian inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Eulerian approach to regularized JKO scheme with low-rank tensor\n  decompositions for Bayesian inversion"
                },
                "summary": "The possibility of using the Eulerian discretization for the problem of\nmodelling high-dimensional distributions and sampling, is studied. The problem\nis posed as a minimization problem over the space of probability measures with\nrespect to the Wasserstein distance and solved with entropy-regularized JKO\nscheme. Each proximal step can be formulated as a fixed-point equation and\nsolved with accelerated methods, such as Anderson's. The usage of low-rank\nTensor Train format allows to overcome the \\emph{curse of dimensionality}, i.e.\nthe exponential growth of degrees of freedom with dimension, inherent to\nEulerian approaches. The resulting method requires only pointwise computations\nof the unnormalized posterior and is, in particular, gradient-free. Fixed\nEulerian grid allows to employ a caching strategy, significally reducing the\nexpensive evaluations of the posterior. When the Eulerian model of the target\ndistribution is fitted, the passage back to the Lagrangian perspective can also\nbe made, allowing to approximately sample from it. We test our method both for\nsynthetic target distributions and particular Bayesian inverse problems and\nreport comparable or better performance than the baseline Metropolis-Hastings\nMCMC with same amount of resources. Finally, the fitted model can be modified\nto facilitate the solution of certain associated problems, which we demonstrate\nby fitting an importance distribution for a particular quantity of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of using the Eulerian discretization for the problem of\nmodelling high-dimensional distributions and sampling, is studied. The problem\nis posed as a minimization problem over the space of probability measures with\nrespect to the Wasserstein distance and solved with entropy-regularized JKO\nscheme. Each proximal step can be formulated as a fixed-point equation and\nsolved with accelerated methods, such as Anderson's. The usage of low-rank\nTensor Train format allows to overcome the \\emph{curse of dimensionality}, i.e.\nthe exponential growth of degrees of freedom with dimension, inherent to\nEulerian approaches. The resulting method requires only pointwise computations\nof the unnormalized posterior and is, in particular, gradient-free. Fixed\nEulerian grid allows to employ a caching strategy, significally reducing the\nexpensive evaluations of the posterior. When the Eulerian model of the target\ndistribution is fitted, the passage back to the Lagrangian perspective can also\nbe made, allowing to approximately sample from it. We test our method both for\nsynthetic target distributions and particular Bayesian inverse problems and\nreport comparable or better performance than the baseline Metropolis-Hastings\nMCMC with same amount of resources. Finally, the fitted model can be modified\nto facilitate the solution of certain associated problems, which we demonstrate\nby fitting an importance distribution for a particular quantity of interest."
                },
                "authors": [
                    {
                        "name": "Vitalii Aksenov"
                    },
                    {
                        "name": "Martin Eigel"
                    }
                ],
                "author_detail": {
                    "name": "Martin Eigel"
                },
                "author": "Martin Eigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "46E27, 49Q22, 62F15, 68W25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12161v1",
                "updated": "2024-11-19T01:55:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    55,
                    26,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T01:55:26Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    55,
                    26,
                    1,
                    324,
                    0
                ],
                "title": "Adaptive Cache Management for Complex Storage Systems Using\n  CNN-LSTM-Based Spatiotemporal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cache Management for Complex Storage Systems Using\n  CNN-LSTM-Based Spatiotemporal Prediction"
                },
                "summary": "This paper proposes an intelligent cache management strategy based on\nCNN-LSTM to improve the performance and cache hit rate of storage systems.\nThrough comparative experiments with traditional algorithms (such as LRU and\nLFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the\nresults show that the CNN-LSTM model has significant advantages in cache demand\nprediction. The MSE and MAE values of this model are significantly reduced,\nproving its effectiveness under complex data access patterns. This study not\nonly verifies the potential of deep learning technology in storage system\noptimization, but also provides direction and reference for further optimizing\nand improving cache management strategies. This intelligent cache management\nstrategy performs well in complex storage environments. By combining the\nspatial feature extraction capabilities of convolutional neural networks and\nthe time series modeling capabilities of long short-term memory networks, the\nCNN-LSTM model can more accurately predict cache needs, thereby Dynamically\noptimize cache allocation to improve system response speed and resource\nutilization. This research provides theoretical support and practical reference\nfor cache optimization under large-scale data access modes, and is of great\nsignificance to improving the performance of future storage systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes an intelligent cache management strategy based on\nCNN-LSTM to improve the performance and cache hit rate of storage systems.\nThrough comparative experiments with traditional algorithms (such as LRU and\nLFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the\nresults show that the CNN-LSTM model has significant advantages in cache demand\nprediction. The MSE and MAE values of this model are significantly reduced,\nproving its effectiveness under complex data access patterns. This study not\nonly verifies the potential of deep learning technology in storage system\noptimization, but also provides direction and reference for further optimizing\nand improving cache management strategies. This intelligent cache management\nstrategy performs well in complex storage environments. By combining the\nspatial feature extraction capabilities of convolutional neural networks and\nthe time series modeling capabilities of long short-term memory networks, the\nCNN-LSTM model can more accurately predict cache needs, thereby Dynamically\noptimize cache allocation to improve system response speed and resource\nutilization. This research provides theoretical support and practical reference\nfor cache optimization under large-scale data access modes, and is of great\nsignificance to improving the performance of future storage systems."
                },
                "authors": [
                    {
                        "name": "Xiaoye Wang"
                    },
                    {
                        "name": "Xuan Li"
                    },
                    {
                        "name": "Linji Wang"
                    },
                    {
                        "name": "Tingyi Ruan"
                    },
                    {
                        "name": "Pochun Li"
                    }
                ],
                "author_detail": {
                    "name": "Pochun Li"
                },
                "author": "Pochun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11843v1",
                "updated": "2024-11-18T18:59:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:59:15Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "title": "Bi-Mamba: Towards Accurate 1-Bit State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Mamba: Towards Accurate 1-Bit State Space Models"
                },
                "summary": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs."
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11739v1",
                "updated": "2024-11-18T17:08:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:08:35Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "title": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou"
                },
                "summary": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels."
                },
                "authors": [
                    {
                        "name": "Xinchen Luo"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Tianyu Sun"
                    },
                    {
                        "name": "Jinkai Yu"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Hezheng Lin"
                    },
                    {
                        "name": "Yichen Zheng"
                    },
                    {
                        "name": "Shiyao Wang"
                    },
                    {
                        "name": "Qigen Hu"
                    },
                    {
                        "name": "Changqing Qiu"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Zhiheng Yan"
                    },
                    {
                        "name": "Jingming Zhang"
                    },
                    {
                        "name": "Simin Zhang"
                    },
                    {
                        "name": "Mingxing Wen"
                    },
                    {
                        "name": "Zhaojie Liu"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11300v1",
                "updated": "2024-11-18T05:50:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    50,
                    58,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T05:50:58Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    50,
                    58,
                    0,
                    323,
                    0
                ],
                "title": "Accelerating spherical K-means clustering for large-scale sparse\n  document data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating spherical K-means clustering for large-scale sparse\n  document data"
                },
                "summary": "This paper presents an accelerated spherical K-means clustering algorithm for\nlarge-scale and high-dimensional sparse document data sets. We design an\nalgorithm working in an architecture-friendly manner (AFM), which is a\nprocedure of suppressing performance-degradation factors such as the numbers of\ninstructions, branch mispredictions, and cache misses in CPUs of a modern\ncomputer system. For the AFM operation, we leverage unique universal\ncharacteristics (UCs) of a data-object and a cluster's mean set, which are\nskewed distributions on data relationships such as Zipf's law and a\nfeature-value concentration phenomenon. The UCs indicate that the most part of\nthe number of multiplications for similarity calculations is executed regarding\nterms with high document frequencies (df) and the most part of a similarity\nbetween an object- and a mean-feature vector is obtained by the multiplications\nregarding a few high mean-feature values. Our proposed algorithm applies an\ninverted-index data structure to a mean set, extracts the specific region with\nhigh-df terms and high mean-feature values in the mean-inverted index by newly\nintroduced two structural parameters, and exploits the index divided into three\nparts for efficient pruning. The algorithm determines the two structural\nparameters by minimizing the approximate number of multiplications related to\nthat of instructions, reduces the branch mispredictions by sharing the index\nstructure including the two parameters with all the objects, and suppressing\nthe cache misses by keeping in the caches the frequently used data in the\nforegoing specific region, resulting in working in the AFM. We experimentally\ndemonstrate that our algorithm efficiently achieves superior speed performance\nin large-scale documents compared with algorithms using the state-of-the-art\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an accelerated spherical K-means clustering algorithm for\nlarge-scale and high-dimensional sparse document data sets. We design an\nalgorithm working in an architecture-friendly manner (AFM), which is a\nprocedure of suppressing performance-degradation factors such as the numbers of\ninstructions, branch mispredictions, and cache misses in CPUs of a modern\ncomputer system. For the AFM operation, we leverage unique universal\ncharacteristics (UCs) of a data-object and a cluster's mean set, which are\nskewed distributions on data relationships such as Zipf's law and a\nfeature-value concentration phenomenon. The UCs indicate that the most part of\nthe number of multiplications for similarity calculations is executed regarding\nterms with high document frequencies (df) and the most part of a similarity\nbetween an object- and a mean-feature vector is obtained by the multiplications\nregarding a few high mean-feature values. Our proposed algorithm applies an\ninverted-index data structure to a mean set, extracts the specific region with\nhigh-df terms and high mean-feature values in the mean-inverted index by newly\nintroduced two structural parameters, and exploits the index divided into three\nparts for efficient pruning. The algorithm determines the two structural\nparameters by minimizing the approximate number of multiplications related to\nthat of instructions, reduces the branch mispredictions by sharing the index\nstructure including the two parameters with all the objects, and suppressing\nthe cache misses by keeping in the caches the frequently used data in the\nforegoing specific region, resulting in working in the AFM. We experimentally\ndemonstrate that our algorithm efficiently achieves superior speed performance\nin large-scale documents compared with algorithms using the state-of-the-art\ntechniques."
                },
                "authors": [
                    {
                        "name": "Kazuo Aoyama"
                    },
                    {
                        "name": "Kazumi Saito"
                    }
                ],
                "author_detail": {
                    "name": "Kazumi Saito"
                },
                "author": "Kazumi Saito",
                "arxiv_comment": "28 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13588v1",
                "updated": "2024-11-18T02:49:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    49,
                    23,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T02:49:23Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    49,
                    23,
                    0,
                    323,
                    0
                ],
                "title": "Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic\n  Study"
                },
                "summary": "The increased model capacity of Diffusion Transformers (DiTs) and the demand\nfor generating higher resolutions of images and videos have led to a\nsignificant rise in inference latency, impacting real-time performance\nadversely. While prior research has highlighted the presence of high similarity\nin activation values between adjacent diffusion steps (referred to as\nredundancy) and proposed various caching mechanisms to mitigate computational\noverhead, the exploration of redundancy in existing literature remains limited,\nwith findings often not generalizable across different DiT models. This study\naims to address this gap by conducting a comprehensive investigation into\nredundancy across a broad spectrum of mainstream DiT models. Our experimental\nanalysis reveals substantial variations in the distribution of redundancy\nacross diffusion steps among different DiT models. Interestingly, within a\nsingle model, the redundancy distribution remains stable regardless of\nvariations in input prompts, step counts, or scheduling strategies. Given the\nlack of a consistent pattern across diverse models, caching strategies designed\nfor a specific group of models may not easily transfer to others. To overcome\nthis challenge, we introduce a tool for analyzing the redundancy of individual\nmodels, enabling subsequent research to develop tailored caching strategies for\nspecific model architectures. The project is publicly available at\nhttps://github.com/xdit-project/DiTCacheAnalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increased model capacity of Diffusion Transformers (DiTs) and the demand\nfor generating higher resolutions of images and videos have led to a\nsignificant rise in inference latency, impacting real-time performance\nadversely. While prior research has highlighted the presence of high similarity\nin activation values between adjacent diffusion steps (referred to as\nredundancy) and proposed various caching mechanisms to mitigate computational\noverhead, the exploration of redundancy in existing literature remains limited,\nwith findings often not generalizable across different DiT models. This study\naims to address this gap by conducting a comprehensive investigation into\nredundancy across a broad spectrum of mainstream DiT models. Our experimental\nanalysis reveals substantial variations in the distribution of redundancy\nacross diffusion steps among different DiT models. Interestingly, within a\nsingle model, the redundancy distribution remains stable regardless of\nvariations in input prompts, step counts, or scheduling strategies. Given the\nlack of a consistent pattern across diverse models, caching strategies designed\nfor a specific group of models may not easily transfer to others. To overcome\nthis challenge, we introduce a tool for analyzing the redundancy of individual\nmodels, enabling subsequent research to develop tailored caching strategies for\nspecific model architectures. The project is publicly available at\nhttps://github.com/xdit-project/DiTCacheAnalysis."
                },
                "authors": [
                    {
                        "name": "Xibo Sun"
                    },
                    {
                        "name": "Jiarui Fang"
                    },
                    {
                        "name": "Aoyu Li"
                    },
                    {
                        "name": "Jinzhe Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jinzhe Pan"
                },
                "author": "Jinzhe Pan",
                "arxiv_comment": "9 pages including reference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06392v2",
                "updated": "2024-11-18T02:10:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    10,
                    28,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-10T08:31:18Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    8,
                    31,
                    18,
                    6,
                    315,
                    0
                ],
                "title": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR"
                },
                "summary": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Pengxi Liu"
                    },
                    {
                        "name": "Zhixin Zhang"
                    },
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Xiaojian Luo"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11091v1",
                "updated": "2024-11-17T14:47:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-17T14:47:15Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "title": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines"
                },
                "summary": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains."
                },
                "authors": [
                    {
                        "name": "Edward Bortnikov"
                    },
                    {
                        "name": "Michael Azran"
                    },
                    {
                        "name": "Asa Bornstein"
                    },
                    {
                        "name": "Shmuel Dashevsky"
                    },
                    {
                        "name": "Dennis Huang"
                    },
                    {
                        "name": "Omer Kepten"
                    },
                    {
                        "name": "Michael Pan"
                    },
                    {
                        "name": "Gali Sheffi"
                    },
                    {
                        "name": "Moshe Twitto"
                    },
                    {
                        "name": "Tamar Weiss Orzech"
                    },
                    {
                        "name": "Idit Keidar"
                    },
                    {
                        "name": "Guy Gueta"
                    },
                    {
                        "name": "Roey Maor"
                    },
                    {
                        "name": "Niv Dayan"
                    }
                ],
                "author_detail": {
                    "name": "Niv Dayan"
                },
                "author": "Niv Dayan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v3",
                "updated": "2024-11-17T12:56:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    12,
                    56,
                    16,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10883v1",
                "updated": "2024-11-16T20:40:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T20:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "title": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs"
                },
                "summary": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack."
                },
                "authors": [
                    {
                        "name": "Cheng Gu"
                    },
                    {
                        "name": "Yicheng Zhang"
                    },
                    {
                        "name": "Nael Abu-Ghazaleh"
                    }
                ],
                "author_detail": {
                    "name": "Nael Abu-Ghazaleh"
                },
                "author": "Nael Abu-Ghazaleh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13112v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13112v3",
                "updated": "2024-11-16T20:39:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    39,
                    46,
                    5,
                    321,
                    0
                ],
                "published": "2024-03-19T19:27:23Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    19,
                    27,
                    23,
                    1,
                    79,
                    0
                ],
                "title": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks"
                },
                "summary": "Transformer-based NLP models are powerful but have high computational costs\nthat limit deployment. Finetuned encoder-decoder models are popular in\nspecialized domains and can outperform larger more generalized decoder-only\nmodels, such as GPT-4. We introduce a new configuration for encoder-decoder\nmodels that improves efficiency on structured output and decomposable tasks\nwhere multiple outputs are required for a single shared input. Our method,\nprompt-in-decoder (PiD), encodes the input once and decodes the output in\nparallel, boosting both training and inference efficiency by avoiding duplicate\ninput encoding and increasing the operational intensity (ratio of numbers of\narithmetic operation to memory access) of decoding process by sharing the input\nkey-value cache. We achieve computation reduction that roughly scales with the\nnumber of subtasks, gaining up to 4.6x speed-up over state-of-the-art models\nfor dialogue state tracking, summarization, and question-answering tasks, with\ncomparable or better performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based NLP models are powerful but have high computational costs\nthat limit deployment. Finetuned encoder-decoder models are popular in\nspecialized domains and can outperform larger more generalized decoder-only\nmodels, such as GPT-4. We introduce a new configuration for encoder-decoder\nmodels that improves efficiency on structured output and decomposable tasks\nwhere multiple outputs are required for a single shared input. Our method,\nprompt-in-decoder (PiD), encodes the input once and decodes the output in\nparallel, boosting both training and inference efficiency by avoiding duplicate\ninput encoding and increasing the operational intensity (ratio of numbers of\narithmetic operation to memory access) of decoding process by sharing the input\nkey-value cache. We achieve computation reduction that roughly scales with the\nnumber of subtasks, gaining up to 4.6x speed-up over state-of-the-art models\nfor dialogue state tracking, summarization, and question-answering tasks, with\ncomparable or better performance."
                },
                "authors": [
                    {
                        "name": "Bo-Ru Lu"
                    },
                    {
                        "name": "Nikita Haduong"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Noah A. Smith"
                    },
                    {
                        "name": "Mari Ostendorf"
                    }
                ],
                "author_detail": {
                    "name": "Mari Ostendorf"
                },
                "author": "Mari Ostendorf",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13112v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13112v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10803v1",
                "updated": "2024-11-16T13:45:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    45,
                    33,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T13:45:33Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    45,
                    33,
                    5,
                    321,
                    0
                ],
                "title": "Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large\n  Language Model"
                },
                "summary": "The vision tokens in multimodal large language models usually exhibit\nsignificant spatial and temporal redundancy and take up most of the input\ntokens, which harms their inference efficiency. To solve this problem, some\nrecent works were introduced to drop the unimportant tokens during inference\nwhere the importance of each token is decided only by the information in either\nthe vision encoding stage or the prefilling stage. In this paper, we propose\nMulti-stage Token Dropping (MustDrop) to measure the importance of each token\nfrom the whole lifecycle, including the vision encoding stage, prefilling\nstage, and decoding stage. Concretely, in the visual encoding stage, MustDrop\nmerges spatially adjacent tokens with high similarity, and establishes a key\ntoken set to retain the most vision-critical tokens, preventing them from being\ndiscarded in later stages. In the prefilling stage, MustDrop further compresses\nvision tokens by the guidance of text semantics, with a dual-attention\nfiltering strategy. In the decoding stage, an output-aware cache policy is\nproposed to further reduce the size of the KV cache. By leveraging tailored\nstrategies in the multi-stage process, MustDrop can more precisely recognize\nthe important and redundant tokens, thus achieving an optimal balance between\nperformance and efficiency. For instance, MustDrop reduces about 88.5\\% FLOPs\non LLaVA with a compression ratio of 92.2\\% while maintaining comparable\naccuracy. Our codes are available at\n\\url{https://github.com/liuting20/MustDrop}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vision tokens in multimodal large language models usually exhibit\nsignificant spatial and temporal redundancy and take up most of the input\ntokens, which harms their inference efficiency. To solve this problem, some\nrecent works were introduced to drop the unimportant tokens during inference\nwhere the importance of each token is decided only by the information in either\nthe vision encoding stage or the prefilling stage. In this paper, we propose\nMulti-stage Token Dropping (MustDrop) to measure the importance of each token\nfrom the whole lifecycle, including the vision encoding stage, prefilling\nstage, and decoding stage. Concretely, in the visual encoding stage, MustDrop\nmerges spatially adjacent tokens with high similarity, and establishes a key\ntoken set to retain the most vision-critical tokens, preventing them from being\ndiscarded in later stages. In the prefilling stage, MustDrop further compresses\nvision tokens by the guidance of text semantics, with a dual-attention\nfiltering strategy. In the decoding stage, an output-aware cache policy is\nproposed to further reduce the size of the KV cache. By leveraging tailored\nstrategies in the multi-stage process, MustDrop can more precisely recognize\nthe important and redundant tokens, thus achieving an optimal balance between\nperformance and efficiency. For instance, MustDrop reduces about 88.5\\% FLOPs\non LLaVA with a compression ratio of 92.2\\% while maintaining comparable\naccuracy. Our codes are available at\n\\url{https://github.com/liuting20/MustDrop}."
                },
                "authors": [
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Liangtao Shi"
                    },
                    {
                        "name": "Richang Hong"
                    },
                    {
                        "name": "Yue Hu"
                    },
                    {
                        "name": "Quanjun Yin"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "8 pages, 4figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01733v2",
                "updated": "2024-11-16T07:43:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    7,
                    43,
                    28,
                    5,
                    321,
                    0
                ],
                "published": "2024-06-03T18:49:57Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    18,
                    49,
                    57,
                    0,
                    155,
                    0
                ],
                "title": "Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching"
                },
                "summary": "Diffusion Transformers have recently demonstrated unprecedented generative\ncapabilities for various tasks. The encouraging results, however, come with the\ncost of slow inference, since each denoising step requires inference on a\ntransformer model with a large scale of parameters. In this study, we make an\ninteresting and somehow surprising observation: the computation of a large\nproportion of layers in the diffusion transformer, through introducing a\ncaching mechanism, can be readily removed even without updating the model\nparameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68%\nof the computation in the cache steps (46.84% for all steps), with less than\n0.01 drop in FID. To achieve this, we introduce a novel scheme, named\nLearning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for\ndiffusion transformers. Specifically, by leveraging the identical structure of\nlayers in transformers and the sequential nature of diffusion, we explore\nredundant computations between timesteps by treating each layer as the\nfundamental unit for caching. To address the challenge of the exponential\nsearch space in deep models for identifying layers to cache and remove, we\npropose a novel differentiable optimization objective. An input-invariant yet\ntimestep-variant router is then optimized, which can finally produce a static\ncomputation graph. Experimental results show that L2C largely outperforms\nsamplers such as DDIM and DPM-Solver, alongside prior cache-based methods at\nthe same inference speed. Code is available at\nhttps://github.com/horseee/learning-to-cache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have recently demonstrated unprecedented generative\ncapabilities for various tasks. The encouraging results, however, come with the\ncost of slow inference, since each denoising step requires inference on a\ntransformer model with a large scale of parameters. In this study, we make an\ninteresting and somehow surprising observation: the computation of a large\nproportion of layers in the diffusion transformer, through introducing a\ncaching mechanism, can be readily removed even without updating the model\nparameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68%\nof the computation in the cache steps (46.84% for all steps), with less than\n0.01 drop in FID. To achieve this, we introduce a novel scheme, named\nLearning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for\ndiffusion transformers. Specifically, by leveraging the identical structure of\nlayers in transformers and the sequential nature of diffusion, we explore\nredundant computations between timesteps by treating each layer as the\nfundamental unit for caching. To address the challenge of the exponential\nsearch space in deep models for identifying layers to cache and remove, we\npropose a novel differentiable optimization objective. An input-invariant yet\ntimestep-variant router is then optimized, which can finally produce a static\ncomputation graph. Experimental results show that L2C largely outperforms\nsamplers such as DDIM and DPM-Solver, alongside prior cache-based methods at\nthe same inference speed. Code is available at\nhttps://github.com/horseee/learning-to-cache"
                },
                "authors": [
                    {
                        "name": "Xinyin Ma"
                    },
                    {
                        "name": "Gongfan Fang"
                    },
                    {
                        "name": "Michael Bi Mi"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v1",
                "updated": "2024-11-16T01:39:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v4",
                "updated": "2024-11-15T22:37:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    22,
                    37,
                    48,
                    4,
                    320,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v2",
                "updated": "2024-11-15T22:30:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    22,
                    30,
                    38,
                    4,
                    320,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Forecasting GPU Performance for Deep Learning Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting GPU Performance for Deep Learning Training and Inference"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "arxiv_comment": "Accepted at the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13853v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10510v1",
                "updated": "2024-11-15T16:24:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T16:24:02Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "title": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models."
                },
                "authors": [
                    {
                        "name": "Joseph Liu"
                    },
                    {
                        "name": "Joshua Geddes"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Haomiao Jiang"
                    },
                    {
                        "name": "Mahesh Kumar Nandwana"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Kumar Nandwana"
                },
                "author": "Mahesh Kumar Nandwana",
                "arxiv_comment": "Code can be found at https://github.com/Roblox/SmoothCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v2",
                "updated": "2024-11-15T07:25:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    25,
                    54,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09859v1",
                "updated": "2024-11-15T00:37:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T00:37:31Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "title": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures"
                },
                "summary": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra (DLA), particularly in comparison to that of\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. A\nmotivating example is the factorization $X=LTL^T$ of a skew-symmetric matrix\n$X$, which is used in practical applications as a means of determining the\ndeterminant of $X$ as the square of the (cheaply-computed) Pfaffian of the\nskew-symmetric tridiagonal matrix $T$, for example in fields such as quantum\nelectronic structure and machine learning. Such applications also often require\npivoting in order to improve numerical stability. In this work we explore a\ncombination of known literature algorithms and new algorithms recently derived\nusing formal methods. High-performance parallel CPU implementations are\ncreated, leveraging the concept of fusion at multiple levels in order to reduce\nmemory traffic overhead, as well as the BLIS framework which provides\nhigh-performance GEMM kernels, hierarchical parallelism, and cache blocking. We\nfind that operation fusion and improved use of available bandwidth via\nparallelization of bandwidth-bound (level-2 BLAS) operations are essential for\nobtaining high performance, while a concise C++ implementation provides a clear\nand close connection to the formal derivation process without sacrificing\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra (DLA), particularly in comparison to that of\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. A\nmotivating example is the factorization $X=LTL^T$ of a skew-symmetric matrix\n$X$, which is used in practical applications as a means of determining the\ndeterminant of $X$ as the square of the (cheaply-computed) Pfaffian of the\nskew-symmetric tridiagonal matrix $T$, for example in fields such as quantum\nelectronic structure and machine learning. Such applications also often require\npivoting in order to improve numerical stability. In this work we explore a\ncombination of known literature algorithms and new algorithms recently derived\nusing formal methods. High-performance parallel CPU implementations are\ncreated, leveraging the concept of fusion at multiple levels in order to reduce\nmemory traffic overhead, as well as the BLIS framework which provides\nhigh-performance GEMM kernels, hierarchical parallelism, and cache blocking. We\nfind that operation fusion and improved use of available bandwidth via\nparallelization of bandwidth-bound (level-2 BLAS) operations are essential for\nobtaining high performance, while a concise C++ implementation provides a clear\nand close connection to the formal derivation process without sacrificing\nperformance."
                },
                "authors": [
                    {
                        "name": "Ishna Satyarth"
                    },
                    {
                        "name": "Chao Yin"
                    },
                    {
                        "name": "RuQing G. Xu"
                    },
                    {
                        "name": "Devin A. Matthews"
                    }
                ],
                "author_detail": {
                    "name": "Devin A. Matthews"
                },
                "author": "Devin A. Matthews",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09812v1",
                "updated": "2024-11-14T21:01:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    21,
                    1,
                    29,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T21:01:29Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    21,
                    1,
                    29,
                    3,
                    319,
                    0
                ],
                "title": "Edge Caching Optimization with PPO and Transfer Learning for Dynamic\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Caching Optimization with PPO and Transfer Learning for Dynamic\n  Environments"
                },
                "summary": "This paper addresses the challenge of edge caching in dynamic environments,\nwhere rising traffic loads strain backhaul links and core networks. We propose\na Proximal Policy Optimization (PPO)-based caching strategy that fully\nincorporates key file attributes such as size, lifetime, importance, and\npopularity, while also considering random file request arrivals, reflecting\nmore realistic edge caching scenarios. In dynamic environments, changes such as\nshifts in content popularity and variations in request rates frequently occur,\nmaking previously learned policies less effective as they were optimized for\nearlier conditions. Without adaptation, caching efficiency and response times\ncan degrade. While learning a new policy from scratch in a new environment is\nan option, it is highly inefficient and computationally expensive. Thus,\nadapting an existing policy to these changes is critical. To address this, we\ndevelop a mechanism that detects changes in content popularity and request\nrates, ensuring timely adjustments to the caching strategy. We also propose a\ntransfer learning-based PPO algorithm that accelerates convergence in new\nenvironments by leveraging prior knowledge. Simulation results demonstrate the\nsignificant effectiveness of our approach, outperforming a recent Deep\nReinforcement Learning (DRL)-based method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenge of edge caching in dynamic environments,\nwhere rising traffic loads strain backhaul links and core networks. We propose\na Proximal Policy Optimization (PPO)-based caching strategy that fully\nincorporates key file attributes such as size, lifetime, importance, and\npopularity, while also considering random file request arrivals, reflecting\nmore realistic edge caching scenarios. In dynamic environments, changes such as\nshifts in content popularity and variations in request rates frequently occur,\nmaking previously learned policies less effective as they were optimized for\nearlier conditions. Without adaptation, caching efficiency and response times\ncan degrade. While learning a new policy from scratch in a new environment is\nan option, it is highly inefficient and computationally expensive. Thus,\nadapting an existing policy to these changes is critical. To address this, we\ndevelop a mechanism that detects changes in content popularity and request\nrates, ensuring timely adjustments to the caching strategy. We also propose a\ntransfer learning-based PPO algorithm that accelerates convergence in new\nenvironments by leveraging prior knowledge. Simulation results demonstrate the\nsignificant effectiveness of our approach, outperforming a recent Deep\nReinforcement Learning (DRL)-based method."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v2",
                "updated": "2024-11-14T17:46:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    46,
                    4,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the $KV$ cache by nearly 50\\%. Comprehensive\nempirical evidence demonstrates that ResFormer mitigates attention\nconcentration problem in deeper layers and enhances representation across most\nlayers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in\ntraining error as well as downstream tasks. Further visualization results\nsuggest that Resformer alleviates attention sinks through avoiding value-state\ndrains. SVFormer trains significantly faster than the vanilla Transformer and\nperforms better than other methods like GQA and CLA, with performance\ninfluenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the $KV$ cache by nearly 50\\%. Comprehensive\nempirical evidence demonstrates that ResFormer mitigates attention\nconcentration problem in deeper layers and enhances representation across most\nlayers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in\ntraining error as well as downstream tasks. Further visualization results\nsuggest that Resformer alleviates attention sinks through avoiding value-state\ndrains. SVFormer trains significantly faster than the vanilla Transformer and\nperforms better than other methods like GQA and CLA, with performance\ninfluenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09546v1",
                "updated": "2024-11-14T16:01:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    1,
                    5,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T16:01:05Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    1,
                    5,
                    3,
                    319,
                    0
                ],
                "title": "Architectural Exploration of Application-Specific Resonant SRAM\n  Compute-in-Memory (rCiM)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architectural Exploration of Application-Specific Resonant SRAM\n  Compute-in-Memory (rCiM)"
                },
                "summary": "While general-purpose computing follows Von Neumann's architecture, the data\nmovement between memory and processor elements dictates the processor's\nperformance. The evolving compute-in-memory (CiM) paradigm tackles this issue\nby facilitating simultaneous processing and storage within static random-access\nmemory (SRAM) elements. Numerous design decisions taken at different levels of\nhierarchy affect the figure of merits (FoMs) of SRAM, such as power,\nperformance, area, and yield. The absence of a rapid assessment mechanism for\nthe impact of changes at different hierarchy levels on global FoMs poses a\nchallenge to accurately evaluating innovative SRAM designs. This paper presents\nan automation tool designed to optimize the energy and latency of SRAM designs\nincorporating diverse implementation strategies for executing logic operations\nwithin the SRAM. The tool structure allows easy comparison across different\narray topologies and various design strategies to result in energy-efficient\nimplementations. Our study involves a comprehensive comparison of over 6900+\ndistinct design implementation strategies for EPFL combinational benchmark\ncircuits on the energy-recycling resonant compute-in-memory (rCiM) architecture\ndesigned using TSMC 28 nm technology. When provided with a combinational\ncircuit, the tool aims to generate an energy-efficient implementation strategy\ntailored to the specified input memory and latency constraints. The tool\nreduces 80.9% of energy consumption on average across all benchmarks while\nusing the six-topology implementation compared to baseline implementation of\nsingle-macro topology by considering the parallel processing capability of rCiM\ncache size ranging from 4KB to 192KB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While general-purpose computing follows Von Neumann's architecture, the data\nmovement between memory and processor elements dictates the processor's\nperformance. The evolving compute-in-memory (CiM) paradigm tackles this issue\nby facilitating simultaneous processing and storage within static random-access\nmemory (SRAM) elements. Numerous design decisions taken at different levels of\nhierarchy affect the figure of merits (FoMs) of SRAM, such as power,\nperformance, area, and yield. The absence of a rapid assessment mechanism for\nthe impact of changes at different hierarchy levels on global FoMs poses a\nchallenge to accurately evaluating innovative SRAM designs. This paper presents\nan automation tool designed to optimize the energy and latency of SRAM designs\nincorporating diverse implementation strategies for executing logic operations\nwithin the SRAM. The tool structure allows easy comparison across different\narray topologies and various design strategies to result in energy-efficient\nimplementations. Our study involves a comprehensive comparison of over 6900+\ndistinct design implementation strategies for EPFL combinational benchmark\ncircuits on the energy-recycling resonant compute-in-memory (rCiM) architecture\ndesigned using TSMC 28 nm technology. When provided with a combinational\ncircuit, the tool aims to generate an energy-efficient implementation strategy\ntailored to the specified input memory and latency constraints. The tool\nreduces 80.9% of energy consumption on average across all benchmarks while\nusing the six-topology implementation compared to baseline implementation of\nsingle-macro topology by considering the parallel processing capability of rCiM\ncache size ranging from 4KB to 192KB."
                },
                "authors": [
                    {
                        "name": "Dhandeep Challagundla"
                    },
                    {
                        "name": "Ignatius Bezzam"
                    },
                    {
                        "name": "Riadul Islam"
                    }
                ],
                "author_detail": {
                    "name": "Riadul Islam"
                },
                "author": "Riadul Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09473v1",
                "updated": "2024-11-14T14:28:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T14:28:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Enhancing Scalability and Performance in Influence Maximization with\n  Optimized Parallel Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Scalability and Performance in Influence Maximization with\n  Optimized Parallel Processing"
                },
                "summary": "Influence Maximization (IM) is vital in viral marketing and biological\nnetwork analysis for identifying key influencers. Given its NP-hard nature,\napproximate solutions are employed. This paper addresses scalability challenges\nin scale-out shared memory system by focusing on the state-of-the-art Influence\nMaximization via Martingales (IMM) benchmark. To enhance the work efficiency of\nthe current IMM implementation, we propose EFFICIENTIMM with key strategies,\nincluding new parallelization scheme, NUMA-aware memory usage, dynamic load\nbalancing and fine-grained adaptive data structures. Benchmarking on a 128-core\nCPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance\nimprovements, achieving an average 5.9x speedup over Ripples across 8 diverse\nSNAP datasets, when compared to the best execution times of the original\nRipples framework. Additionally, on the Youtube graph, EFFICIENTIMM\ndemonstrates a better memory access pattern with 357.4x reduction in L1+L2\ncache misses as compared to Ripples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Influence Maximization (IM) is vital in viral marketing and biological\nnetwork analysis for identifying key influencers. Given its NP-hard nature,\napproximate solutions are employed. This paper addresses scalability challenges\nin scale-out shared memory system by focusing on the state-of-the-art Influence\nMaximization via Martingales (IMM) benchmark. To enhance the work efficiency of\nthe current IMM implementation, we propose EFFICIENTIMM with key strategies,\nincluding new parallelization scheme, NUMA-aware memory usage, dynamic load\nbalancing and fine-grained adaptive data structures. Benchmarking on a 128-core\nCPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance\nimprovements, achieving an average 5.9x speedup over Ripples across 8 diverse\nSNAP datasets, when compared to the best execution times of the original\nRipples framework. Additionally, on the Youtube graph, EFFICIENTIMM\ndemonstrates a better memory access pattern with 357.4x reduction in L1+L2\ncache misses as compared to Ripples."
                },
                "authors": [
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Huan Xu"
                    },
                    {
                        "name": "Joongun Park"
                    },
                    {
                        "name": "Jesmin Jahan Tithi"
                    },
                    {
                        "name": "Fabio Checconi"
                    },
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v1",
                "updated": "2024-11-14T13:22:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09317v1",
                "updated": "2024-11-14T09:50:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T09:50:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "Pie: Pooling CPU Memory for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pie: Pooling CPU Memory for LLM Inference"
                },
                "summary": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput."
                },
                "authors": [
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09275v1",
                "updated": "2024-11-14T08:25:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T08:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Pkd-tree: Parallel $k$d-tree with Batch Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pkd-tree: Parallel $k$d-tree with Batch Updates"
                },
                "summary": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code."
                },
                "authors": [
                    {
                        "name": "Ziyang Men"
                    },
                    {
                        "name": "Zheqi Shen"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Yihan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Sun"
                },
                "author": "Yihan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v3",
                "updated": "2024-11-14T01:56:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    1,
                    56,
                    11,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV"
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "18pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08672v1",
                "updated": "2024-11-13T15:07:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    15,
                    7,
                    15,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T15:07:15Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    15,
                    7,
                    15,
                    2,
                    318,
                    0
                ],
                "title": "Joint Model Caching and Resource Allocation in Generative AI-Enabled\n  Wireless Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Model Caching and Resource Allocation in Generative AI-Enabled\n  Wireless Edge Networks"
                },
                "summary": "With the rapid advancement of artificial intelligence (AI), generative AI\n(GenAI) has emerged as a transformative tool, enabling customized and\npersonalized AI-generated content (AIGC) services. However, GenAI models with\nbillions of parameters require substantial memory capacity and computational\npower for deployment and execution, presenting significant challenges to\nresource-limited edge networks. In this paper, we address the joint model\ncaching and resource allocation problem in GenAI-enabled wireless edge\nnetworks. Our objective is to balance the trade-off between delivering\nhigh-quality AIGC and minimizing the delay in AIGC service provisioning. To\ntackle this problem, we employ a deep deterministic policy gradient\n(DDPG)-based reinforcement learning approach, capable of efficiently\ndetermining optimal model caching and resource allocation decisions for AIGC\nservices in response to user mobility and time-varying channel conditions.\nNumerical results demonstrate that DDPG achieves a higher model hit ratio and\nprovides superior-quality, lower-latency AIGC services compared to other\nbenchmark solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of artificial intelligence (AI), generative AI\n(GenAI) has emerged as a transformative tool, enabling customized and\npersonalized AI-generated content (AIGC) services. However, GenAI models with\nbillions of parameters require substantial memory capacity and computational\npower for deployment and execution, presenting significant challenges to\nresource-limited edge networks. In this paper, we address the joint model\ncaching and resource allocation problem in GenAI-enabled wireless edge\nnetworks. Our objective is to balance the trade-off between delivering\nhigh-quality AIGC and minimizing the delay in AIGC service provisioning. To\ntackle this problem, we employ a deep deterministic policy gradient\n(DDPG)-based reinforcement learning approach, capable of efficiently\ndetermining optimal model caching and resource allocation decisions for AIGC\nservices in response to user mobility and time-varying channel conditions.\nNumerical results demonstrate that DDPG achieves a higher model hit ratio and\nprovides superior-quality, lower-latency AIGC services compared to other\nbenchmark solutions."
                },
                "authors": [
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Lianfen Huang"
                    },
                    {
                        "name": "Zhibin Gao"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "arxiv_comment": "conference paper with 6 pages and 5 figures. arXiv admin note: text\n  overlap with arXiv:2411.01458",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08312v1",
                "updated": "2024-11-13T03:28:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    3,
                    28,
                    44,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T03:28:44Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    3,
                    28,
                    44,
                    2,
                    318,
                    0
                ],
                "title": "A Novel Extensible Simulation Framework for CXL-Enabled Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Extensible Simulation Framework for CXL-Enabled Systems"
                },
                "summary": "Compute Express Link (CXL) serves as a rising industry standard, delivering\nhigh-speed cache-coherent links to a variety of devices, including host CPUs,\ncomputational accelerators, and memory devices. It is designed to promote\nsystem scalability, enable peer-to-peer exchanges, and accelerate data\ntransmissions. To achieve these objectives, the most recent CXL protocol has\nbrought forth several innovative features, such as port-focused routing,\ndevice-handled coherence, and PCIe 6.0 compatibility. However, due to the\nlimited availability of hardware prototypes and simulators compatible with CXL,\nearlier CXL research has largely depended on emulating CXL devices using remote\nNUMA nodes. Unfortunately, these NUMA-based emulators have difficulties in\naccurately representing the new features due to fundamental differences in\nhardware and protocols. Moreover, the absence of support for non-tree topology\nand PCIe links makes it complex to merely adapt existing simulators for CXL\nsimulation. To overcome these problems, we introduce ESF, a simulation\nframework specifically designed for CXL systems. ESF has been developed to\naccurately reflect the unique features of the latest CXL protocol from the\nground up. It uses a specialized interconnect layer to facilitate connections\nwithin a wide range of system topologies and also includes key components to\ncarry out specific functions required by these features. By utilizing ESF, we\nthoroughly investigate various aspects of CXL systems, including system\ntopology, device-handled coherence, and the effects of PCIe characteristics,\nleading to important findings that can guide the creation of high-performance\nCXL systems. The ESF source codes are fully open-source and can be accessed at\nhttps://anonymous.4open.science/r/ESF-1CE3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) serves as a rising industry standard, delivering\nhigh-speed cache-coherent links to a variety of devices, including host CPUs,\ncomputational accelerators, and memory devices. It is designed to promote\nsystem scalability, enable peer-to-peer exchanges, and accelerate data\ntransmissions. To achieve these objectives, the most recent CXL protocol has\nbrought forth several innovative features, such as port-focused routing,\ndevice-handled coherence, and PCIe 6.0 compatibility. However, due to the\nlimited availability of hardware prototypes and simulators compatible with CXL,\nearlier CXL research has largely depended on emulating CXL devices using remote\nNUMA nodes. Unfortunately, these NUMA-based emulators have difficulties in\naccurately representing the new features due to fundamental differences in\nhardware and protocols. Moreover, the absence of support for non-tree topology\nand PCIe links makes it complex to merely adapt existing simulators for CXL\nsimulation. To overcome these problems, we introduce ESF, a simulation\nframework specifically designed for CXL systems. ESF has been developed to\naccurately reflect the unique features of the latest CXL protocol from the\nground up. It uses a specialized interconnect layer to facilitate connections\nwithin a wide range of system topologies and also includes key components to\ncarry out specific functions required by these features. By utilizing ESF, we\nthoroughly investigate various aspects of CXL systems, including system\ntopology, device-handled coherence, and the effects of PCIe characteristics,\nleading to important findings that can guide the creation of high-performance\nCXL systems. The ESF source codes are fully open-source and can be accessed at\nhttps://anonymous.4open.science/r/ESF-1CE3."
                },
                "authors": [
                    {
                        "name": "Yuda An"
                    },
                    {
                        "name": "Shushu Yi"
                    },
                    {
                        "name": "Bo Mao"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Mingzhe Zhang"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Guangyu Sun"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "arxiv_affiliation": "Peking University",
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08203v1",
                "updated": "2024-11-12T21:50:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    21,
                    50,
                    3,
                    1,
                    317,
                    0
                ],
                "published": "2024-11-12T21:50:03Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    21,
                    50,
                    3,
                    1,
                    317,
                    0
                ],
                "title": "FaaS and Furious: abstractions and differential caching for efficient\n  data pre-processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaaS and Furious: abstractions and differential caching for efficient\n  data pre-processing"
                },
                "summary": "Data pre-processing pipelines are the bread and butter of any successful AI\nproject. We introduce a novel programming model for pipelines in a data\nlakehouse, allowing users to interact declaratively with assets in object\nstorage. Motivated by real-world industry usage patterns, we exploit these new\nabstractions with a columnar and differential cache to maximize iteration speed\nfor data scientists, who spent most of their time in pre-processing - adding or\nremoving features, restricting or relaxing time windows, wrangling current or\nolder datasets. We show how the new cache works transparently across\nprogramming languages, schemas and time windows, and provide preliminary\nevidence on its efficiency on standard data workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data pre-processing pipelines are the bread and butter of any successful AI\nproject. We introduce a novel programming model for pipelines in a data\nlakehouse, allowing users to interact declaratively with assets in object\nstorage. Motivated by real-world industry usage patterns, we exploit these new\nabstractions with a columnar and differential cache to maximize iteration speed\nfor data scientists, who spent most of their time in pre-processing - adding or\nremoving features, restricting or relaxing time windows, wrangling current or\nolder datasets. We show how the new cache works transparently across\nprogramming languages, schemas and time windows, and provide preliminary\nevidence on its efficiency on standard data workloads."
                },
                "authors": [
                    {
                        "name": "Jacopo Tagliabue"
                    },
                    {
                        "name": "Ryan Curtin"
                    },
                    {
                        "name": "Ciro Greco"
                    }
                ],
                "author_detail": {
                    "name": "Ciro Greco"
                },
                "author": "Ciro Greco",
                "arxiv_comment": "Pre-print of the paper accepted at DEMAI@IEEE Big Data 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06219v3",
                "updated": "2024-11-12T08:18:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    18,
                    45,
                    1,
                    317,
                    0
                ],
                "published": "2024-05-10T03:06:24Z",
                "published_parsed": [
                    2024,
                    5,
                    10,
                    3,
                    6,
                    24,
                    4,
                    131,
                    0
                ],
                "title": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) can now handle longer sequences of tokens,\nenabling complex tasks like book understanding and generating lengthy novels.\nHowever, the key-value (KV) cache required for LLMs consumes substantial memory\nas context length increasing, becoming the bottleneck for deployment. In this\npaper, we present a strategy called SKVQ, which stands for sliding-window KV\ncache quantization, to address the issue of extremely low bitwidth KV cache\nquantization. To achieve this, SKVQ rearranges the channels of the KV cache in\norder to improve the similarity of channels in quantization groups, and applies\nclipped dynamic quantization at the group level. Additionally, SKVQ ensures\nthat the most recent window tokens in the KV cache are preserved with high\nprecision. This helps maintain the accuracy of a small but important portion of\nthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.\nOur evaluation on LLMs demonstrates that SKVQ surpasses previous quantization\napproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit\nvalues with minimal loss of accuracy. With SKVQ, it is possible to process\ncontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7\ntimes faster decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can now handle longer sequences of tokens,\nenabling complex tasks like book understanding and generating lengthy novels.\nHowever, the key-value (KV) cache required for LLMs consumes substantial memory\nas context length increasing, becoming the bottleneck for deployment. In this\npaper, we present a strategy called SKVQ, which stands for sliding-window KV\ncache quantization, to address the issue of extremely low bitwidth KV cache\nquantization. To achieve this, SKVQ rearranges the channels of the KV cache in\norder to improve the similarity of channels in quantization groups, and applies\nclipped dynamic quantization at the group level. Additionally, SKVQ ensures\nthat the most recent window tokens in the KV cache are preserved with high\nprecision. This helps maintain the accuracy of a small but important portion of\nthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.\nOur evaluation on LLMs demonstrates that SKVQ surpasses previous quantization\napproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit\nvalues with minimal loss of accuracy. With SKVQ, it is possible to process\ncontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7\ntimes faster decoding."
                },
                "authors": [
                    {
                        "name": "Haojie Duanmu"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Jiangfei Duan"
                    },
                    {
                        "name": "Xingcheng Zhang"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07627v1",
                "updated": "2024-11-12T08:17:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "published": "2024-11-12T08:17:15Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "title": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion"
                },
                "summary": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively."
                },
                "authors": [
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06681v1",
                "updated": "2024-11-11T02:48:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    48,
                    0,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T02:48:00Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    48,
                    0,
                    0,
                    316,
                    0
                ],
                "title": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance."
                },
                "authors": [
                    {
                        "name": "Nan Xue"
                    },
                    {
                        "name": "Yaping Sun"
                    },
                    {
                        "name": "Zhiyong Chen"
                    },
                    {
                        "name": "Meixia Tao"
                    },
                    {
                        "name": "Xiaodong Xu"
                    },
                    {
                        "name": "Liang Qian"
                    },
                    {
                        "name": "Shuguang Cui"
                    },
                    {
                        "name": "Wenjun Zhang"
                    },
                    {
                        "name": "Ping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Zhang"
                },
                "author": "Ping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06680v1",
                "updated": "2024-11-11T02:47:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    47,
                    5,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T02:47:05Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    47,
                    5,
                    0,
                    316,
                    0
                ],
                "title": "Anchor Attention, Small Cache: Code Generation with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anchor Attention, Small Cache: Code Generation with Large Language\n  Models"
                },
                "summary": "The development of large language models (LLMs) has revolutionized automated\ncode generation. However, their high demand of computation resources has\nhindered a broader deployment and raised environmental concerns. A common\nstrategy for diminishing computational demands is to cache Key-Value (KV)\nstates from the attention mechanism which is adopted predominately by\nmainstream LLMs. It can mitigate the need of repeated attention computations,\nbut brings significant memory overhead. Current practices in NLP often use\nsparse attention which may, unfortunately, lead to substantial inaccuracies, or\nhallucinations, in code generation tasks. In this paper, we analyze the\nattention weights distribution within code generation models via an empirical\nstudy, uncovering a sparsity pattern, i.e., the aggregation of information at\nspecific anchor points. Based on this observation, we propose a novel approach,\nAnchorCoder, which features token-wise anchor attention designed to extract and\ncompress the contextual information, and layer-wise anchor attention enabling\ncross-layer communication to mitigate the issue of excessive superposition\ncaused by the compression. The extensive experiments across multiple benchmark\ndatasets confirm the effectiveness of AnchorCoder, which can consistently\nachieve a significant (at least 70%) reduction in KV cache requirements, while\npreserving the majority of model's performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has revolutionized automated\ncode generation. However, their high demand of computation resources has\nhindered a broader deployment and raised environmental concerns. A common\nstrategy for diminishing computational demands is to cache Key-Value (KV)\nstates from the attention mechanism which is adopted predominately by\nmainstream LLMs. It can mitigate the need of repeated attention computations,\nbut brings significant memory overhead. Current practices in NLP often use\nsparse attention which may, unfortunately, lead to substantial inaccuracies, or\nhallucinations, in code generation tasks. In this paper, we analyze the\nattention weights distribution within code generation models via an empirical\nstudy, uncovering a sparsity pattern, i.e., the aggregation of information at\nspecific anchor points. Based on this observation, we propose a novel approach,\nAnchorCoder, which features token-wise anchor attention designed to extract and\ncompress the contextual information, and layer-wise anchor attention enabling\ncross-layer communication to mitigate the issue of excessive superposition\ncaused by the compression. The extensive experiments across multiple benchmark\ndatasets confirm the effectiveness of AnchorCoder, which can consistently\nachieve a significant (at least 70%) reduction in KV cache requirements, while\npreserving the majority of model's performance."
                },
                "authors": [
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Harald C. Gall"
                    },
                    {
                        "name": "Taolue Chen"
                    }
                ],
                "author_detail": {
                    "name": "Taolue Chen"
                },
                "author": "Taolue Chen",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68N19",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06659v1",
                "updated": "2024-11-11T01:53:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    53,
                    14,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T01:53:14Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    53,
                    14,
                    0,
                    316,
                    0
                ],
                "title": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning"
                },
                "summary": "Incremental graph learning has gained significant attention for its ability\nto address the catastrophic forgetting problem in graph representation\nlearning. However, traditional methods often rely on a large number of labels\nfor node classification, which is impractical in real-world applications. This\nmakes few-shot incremental learning on graphs a pressing need. Current methods\ntypically require extensive training samples from meta-learning to build memory\nand perform intensive fine-tuning of GNN parameters, leading to high memory\nconsumption and potential loss of previously learned knowledge. To tackle these\nchallenges, we introduce Mecoin, an efficient method for building and\nmaintaining memory. Mecoin employs Structured Memory Units to cache prototypes\nof learned categories, as well as Memory Construction Modules to update these\nprototypes for new categories through interactions between the nodes and the\ncached prototypes. Additionally, we have designed a Memory Representation\nAdaptation Module to store probabilities associated with each class prototype,\nreducing the need for parameter fine-tuning and lowering the forgetting rate.\nWhen a sample matches its corresponding class prototype, the relevant\nprobabilities are retrieved from the MRaM. Knowledge is then distilled back\ninto the GNN through a Graph Knowledge Distillation Module, preserving the\nmodel's memory. We analyze the effectiveness of Mecoin in terms of\ngeneralization error and explore the impact of different distillation\nstrategies on model performance through experiments and VC-dimension analysis.\nCompared to other related works, Mecoin shows superior performance in accuracy\nand forgetting rate. Our code is publicly available on the\nhttps://github.com/Arvin0313/Mecoin-GFSCIL.git .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incremental graph learning has gained significant attention for its ability\nto address the catastrophic forgetting problem in graph representation\nlearning. However, traditional methods often rely on a large number of labels\nfor node classification, which is impractical in real-world applications. This\nmakes few-shot incremental learning on graphs a pressing need. Current methods\ntypically require extensive training samples from meta-learning to build memory\nand perform intensive fine-tuning of GNN parameters, leading to high memory\nconsumption and potential loss of previously learned knowledge. To tackle these\nchallenges, we introduce Mecoin, an efficient method for building and\nmaintaining memory. Mecoin employs Structured Memory Units to cache prototypes\nof learned categories, as well as Memory Construction Modules to update these\nprototypes for new categories through interactions between the nodes and the\ncached prototypes. Additionally, we have designed a Memory Representation\nAdaptation Module to store probabilities associated with each class prototype,\nreducing the need for parameter fine-tuning and lowering the forgetting rate.\nWhen a sample matches its corresponding class prototype, the relevant\nprobabilities are retrieved from the MRaM. Knowledge is then distilled back\ninto the GNN through a Graph Knowledge Distillation Module, preserving the\nmodel's memory. We analyze the effectiveness of Mecoin in terms of\ngeneralization error and explore the impact of different distillation\nstrategies on model performance through experiments and VC-dimension analysis.\nCompared to other related works, Mecoin shows superior performance in accuracy\nand forgetting rate. Our code is publicly available on the\nhttps://github.com/Arvin0313/Mecoin-GFSCIL.git ."
                },
                "authors": [
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Aijia Zhang"
                    },
                    {
                        "name": "Junqi Gao"
                    },
                    {
                        "name": "Biqing Qi"
                    }
                ],
                "author_detail": {
                    "name": "Biqing Qi"
                },
                "author": "Biqing Qi",
                "arxiv_comment": "16 pages, 6 figures, 38th Conference on Neural Information Processing\n  Systems, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v2",
                "updated": "2024-11-10T23:04:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    23,
                    4,
                    12,
                    6,
                    315,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v4",
                "updated": "2024-11-10T15:58:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    15,
                    58,
                    7,
                    6,
                    315,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongfu Li"
                },
                "author": "Hongfu Li",
                "arxiv_comment": "This paper was accepted by VLDB2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04873v2",
                "updated": "2024-11-10T10:08:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    10,
                    8,
                    37,
                    6,
                    315,
                    0
                ],
                "published": "2024-06-07T12:12:25Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    12,
                    12,
                    25,
                    4,
                    159,
                    0
                ],
                "title": "Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion\n  Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion\n  Prior"
                },
                "summary": "Video-to-video synthesis poses significant challenges in maintaining\ncharacter consistency, smooth temporal transitions, and preserving visual\nquality during fast motion. While recent fully cross-frame self-attention\nmechanisms have improved character consistency across multiple frames, they\ncome with high computational costs and often include redundant operations,\nespecially for videos with higher frame rates. To address these inefficiencies,\nwe propose an adaptive motion-guided cross-frame attention mechanism that\nselectively reduces redundant computations. This enables a greater number of\ncross-frame attentions over more frames within the same computational budget,\nthereby enhancing both video quality and temporal coherence. Our method\nleverages optical flow to focus on moving regions while sparsely attending to\nstationary areas, allowing for the joint editing of more frames without\nincreasing computational demands. Traditional frame interpolation techniques\nstruggle with motion blur and flickering in intermediate frames, which\ncompromises visual fidelity. To mitigate this, we introduce KV-caching for\njointly edited frames, reusing keys and values across intermediate frames to\npreserve visual quality and maintain temporal consistency throughout the video.\nWith our adaptive cross-frame self-attention approach, we achieve a threefold\nincrease in the number of keyframes processed compared to existing methods, all\nwithin the same computational budget as fully cross-frame attention baselines.\nThis results in significant improvements in prediction accuracy and temporal\nconsistency, outperforming state-of-the-art approaches. Code will be made\npublicly available at https://github.com/tanvir-utexas/AdaVE/tree/main",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-to-video synthesis poses significant challenges in maintaining\ncharacter consistency, smooth temporal transitions, and preserving visual\nquality during fast motion. While recent fully cross-frame self-attention\nmechanisms have improved character consistency across multiple frames, they\ncome with high computational costs and often include redundant operations,\nespecially for videos with higher frame rates. To address these inefficiencies,\nwe propose an adaptive motion-guided cross-frame attention mechanism that\nselectively reduces redundant computations. This enables a greater number of\ncross-frame attentions over more frames within the same computational budget,\nthereby enhancing both video quality and temporal coherence. Our method\nleverages optical flow to focus on moving regions while sparsely attending to\nstationary areas, allowing for the joint editing of more frames without\nincreasing computational demands. Traditional frame interpolation techniques\nstruggle with motion blur and flickering in intermediate frames, which\ncompromises visual fidelity. To mitigate this, we introduce KV-caching for\njointly edited frames, reusing keys and values across intermediate frames to\npreserve visual quality and maintain temporal consistency throughout the video.\nWith our adaptive cross-frame self-attention approach, we achieve a threefold\nincrease in the number of keyframes processed compared to existing methods, all\nwithin the same computational budget as fully cross-frame attention baselines.\nThis results in significant improvements in prediction accuracy and temporal\nconsistency, outperforming state-of-the-art approaches. Code will be made\npublicly available at https://github.com/tanvir-utexas/AdaVE/tree/main"
                },
                "authors": [
                    {
                        "name": "Tanvir Mahmud"
                    },
                    {
                        "name": "Mustafa Munir"
                    },
                    {
                        "name": "Radu Marculescu"
                    },
                    {
                        "name": "Diana Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Diana Marculescu"
                },
                "author": "Diana Marculescu",
                "arxiv_comment": "Accepted in WACV 2025. Project page:\n  https://tanvir-utexas.github.io/AdaVE_Demo/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06364v1",
                "updated": "2024-11-10T05:12:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "published": "2024-11-10T05:12:51Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "title": "EcoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EcoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving"
                },
                "summary": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EcoServe.\nEcoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EcoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EcoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EcoServe.\nEcoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EcoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EcoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v2",
                "updated": "2024-11-08T16:29:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    29,
                    33,
                    4,
                    313,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05555v1",
                "updated": "2024-11-08T13:24:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T13:24:01Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "title": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality"
                },
                "summary": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively."
                },
                "authors": [
                    {
                        "name": "Ilias Bournias"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    },
                    {
                        "name": "Georgios Zacharopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Zacharopoulos"
                },
                "author": "Georgios Zacharopoulos",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v1",
                "updated": "2024-11-08T02:21:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT (Radford et al., 2019), have\nsignificantly advanced artificial intelligence by enabling sophisticated\nnatural language understanding and generation. However, the high computational\nand financial costs associated with frequent API calls to these models present\na substantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique reduces operational costs and improves response times, enhancing\nthe efficiency of LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT (Radford et al., 2019), have\nsignificantly advanced artificial intelligence by enabling sophisticated\nnatural language understanding and generation. However, the high computational\nand financial costs associated with frequent API calls to these models present\na substantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique reduces operational costs and improves response times, enhancing\nthe efficiency of LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02542v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02542v2",
                "updated": "2024-11-07T18:58:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    58,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-06-04T17:58:03Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    58,
                    3,
                    1,
                    156,
                    0
                ],
                "title": "Loki: Low-rank Keys for Efficient Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Loki: Low-rank Keys for Efficient Sparse Attention"
                },
                "summary": "Inference on large language models (LLMs) can be expensive in terms of the\ncompute and memory costs involved, especially when long sequence lengths are\nused. In particular, the self-attention mechanism used in LLM inference\ncontributes significantly to these costs, which has sparked an interest in\napproximating the self-attention computation to reduce such costs. In this\nwork, we propose to approximate self-attention by focusing on the\ndimensionality of key vectors computed in the attention block. Our analysis\nreveals that key vectors lie in a significantly lower-dimensional space,\nconsistently across several datasets and models. Exploiting this observation,\nwe propose Loki, a novel sparse attention method that ranks and selects tokens\nin the KV-cache based on attention scores computed in low-dimensional space.\nOur evaluations show that Loki is able to speed up the attention computation\ndue to reduced data movement (load/store) and compute costs while maintaining\nthe efficacy of the models better than other popular approximation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference on large language models (LLMs) can be expensive in terms of the\ncompute and memory costs involved, especially when long sequence lengths are\nused. In particular, the self-attention mechanism used in LLM inference\ncontributes significantly to these costs, which has sparked an interest in\napproximating the self-attention computation to reduce such costs. In this\nwork, we propose to approximate self-attention by focusing on the\ndimensionality of key vectors computed in the attention block. Our analysis\nreveals that key vectors lie in a significantly lower-dimensional space,\nconsistently across several datasets and models. Exploiting this observation,\nwe propose Loki, a novel sparse attention method that ranks and selects tokens\nin the KV-cache based on attention scores computed in low-dimensional space.\nOur evaluations show that Loki is able to speed up the attention computation\ndue to reduced data movement (load/store) and compute costs while maintaining\nthe efficacy of the models better than other popular approximation methods."
                },
                "authors": [
                    {
                        "name": "Prajwal Singhania"
                    },
                    {
                        "name": "Siddharth Singh"
                    },
                    {
                        "name": "Shwai He"
                    },
                    {
                        "name": "Soheil Feizi"
                    },
                    {
                        "name": "Abhinav Bhatele"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Bhatele"
                },
                "author": "Abhinav Bhatele",
                "arxiv_comment": "Proceedings of the Thirty-Eighth Annual Conference on Neural\n  Information Processing Systems (Main Conference Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02542v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02542v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04965v1",
                "updated": "2024-11-07T18:41:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:41:50Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "title": "BitNet a4.8: 4-bit Activations for 1-bit LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitNet a4.8: 4-bit Activations for 1-bit LLMs"
                },
                "summary": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference."
                },
                "authors": [
                    {
                        "name": "Hongyu Wang"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02397v2",
                "updated": "2024-11-07T17:06:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    6,
                    32,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-04T18:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    44,
                    0,
                    309,
                    0
                ],
                "title": "Adaptive Caching for Faster Video Generation with Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Caching for Faster Video Generation with Diffusion Transformers"
                },
                "summary": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines."
                },
                "authors": [
                    {
                        "name": "Kumara Kahatapitiya"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Sen He"
                    },
                    {
                        "name": "Ding Liu"
                    },
                    {
                        "name": "Menglin Jia"
                    },
                    {
                        "name": "Chenyang Zhang"
                    },
                    {
                        "name": "Michael S. Ryoo"
                    },
                    {
                        "name": "Tian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tian Xie"
                },
                "author": "Tian Xie",
                "arxiv_comment": "Project-page is available at https://adacache-dit.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04762v1",
                "updated": "2024-11-07T14:59:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T14:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "title": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems"
                },
                "summary": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Jiaxu Wu"
                    },
                    {
                        "name": "Long He"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Shiwen Mao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Mao"
                },
                "author": "Shiwen Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16591v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16591v2",
                "updated": "2024-11-07T09:33:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    9,
                    33,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-05-26T14:50:40Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    14,
                    50,
                    40,
                    6,
                    147,
                    0
                ],
                "title": "CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification"
                },
                "summary": "Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter."
                },
                "authors": [
                    {
                        "name": "Qijie Wang"
                    },
                    {
                        "name": "Guandu Liu"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "arxiv_doi": "10.1145/3664647.3681566",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3664647.3681566",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.16591v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16591v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACM Multimedia 2024 Poster",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v2",
                "updated": "2024-11-07T06:40:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    6,
                    40,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02265v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02265v3",
                "updated": "2024-11-06T09:15:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    9,
                    15,
                    27,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-04T16:56:26Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    56,
                    26,
                    0,
                    309,
                    0
                ],
                "title": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent"
                },
                "summary": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large"
                },
                "authors": [
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Jonny Han"
                    },
                    {
                        "name": "Xiaobo Shu"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Xipeng Zhang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Ze Zhao"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Fusheng Xiang"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Bin Hu"
                    },
                    {
                        "name": "Xuebin Hou"
                    },
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Jianqiang Ma"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Weiwen Jia"
                    },
                    {
                        "name": "Hu Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Rui Yuan"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Zhenxiang Yan"
                    },
                    {
                        "name": "Tengfei Cao"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Yinben Xia"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Zekun He"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Fan Jiang"
                    },
                    {
                        "name": "Chongqing Zhao"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Hao Gong"
                    },
                    {
                        "name": "Rong Gan"
                    },
                    {
                        "name": "Winston Hu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Jie Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Jiang"
                },
                "author": "Jie Jiang",
                "arxiv_comment": "17 pages, 4 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02265v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02265v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03731v1",
                "updated": "2024-11-06T07:53:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T07:53:04Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "title": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness"
                },
                "summary": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations)."
                },
                "authors": [
                    {
                        "name": "Abdelmajid Essofi"
                    },
                    {
                        "name": "Ridwan Salahuddeen"
                    },
                    {
                        "name": "Munachiso Nwadike"
                    },
                    {
                        "name": "Elnura Zhalieva"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "Willie Neiswanger"
                    },
                    {
                        "name": "Qirong Ho"
                    }
                ],
                "author_detail": {
                    "name": "Qirong Ho"
                },
                "author": "Qirong Ho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v2",
                "updated": "2024-11-06T07:12:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    12,
                    55,
                    2,
                    311,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01433v2",
                "updated": "2024-11-06T01:49:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    1,
                    49,
                    45,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-03T04:25:46Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    4,
                    25,
                    46,
                    6,
                    308,
                    0
                ],
                "title": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference"
                },
                "summary": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems."
                },
                "authors": [
                    {
                        "name": "Peng Tang"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xiaofeng Hou"
                    },
                    {
                        "name": "Yifei Pu"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.05591v3",
                "updated": "2024-11-05T08:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    8,
                    34,
                    44,
                    1,
                    310,
                    0
                ],
                "published": "2023-08-10T13:57:37Z",
                "published_parsed": [
                    2023,
                    8,
                    10,
                    13,
                    57,
                    37,
                    3,
                    222,
                    0
                ],
                "title": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks"
                },
                "summary": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions."
                },
                "authors": [
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Giovanni Geraci"
                    },
                    {
                        "name": "Lingxiang Li"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "This work is expanded on our paper presented at IEEE Globecom 2023:\n  F. Wang, G. Geraci and T. Q. S. Quek, \"Optimizing Cache Content Placement in\n  Integrated Terrestrial and Non-terrestrial Networks,\" GLOBECOM 2023 - 2023\n  IEEE Global Communications Conference, Kuala Lumpur, Malaysia, 2023, pp.\n  6609-6614",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v1",
                "updated": "2024-11-05T07:56:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v1",
                "updated": "2024-11-05T05:41:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: Enhancing Cross-LLM Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: Enhancing Cross-LLM Communication"
                },
                "summary": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Esha Choukse"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Madan Musuvathi"
                    }
                ],
                "author_detail": {
                    "name": "Madan Musuvathi"
                },
                "author": "Madan Musuvathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02295v1",
                "updated": "2024-11-04T17:21:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:21:58Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "title": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating"
                },
                "summary": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world."
                },
                "authors": [
                    {
                        "name": "Di Ni"
                    },
                    {
                        "name": "Ved Gund"
                    },
                    {
                        "name": "Landon Ivy"
                    },
                    {
                        "name": "Amit Lal"
                    }
                ],
                "author_detail": {
                    "name": "Amit Lal"
                },
                "author": "Amit Lal",
                "arxiv_doi": "10.31438/trf.hh2022.16",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.31438/trf.hh2022.16",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.02295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted and published at Hilton Head Workshop 2022: A Solid-State\n  Sensors, Actuators and Microsystems Workshop",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10740v2",
                "updated": "2024-11-04T12:14:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    14,
                    7,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-15T14:09:00Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    9,
                    0,
                    0,
                    197,
                    0
                ],
                "title": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption"
                },
                "summary": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite."
                },
                "authors": [
                    {
                        "name": "Martin Unterguggenberger"
                    },
                    {
                        "name": "Lukas Lamster"
                    },
                    {
                        "name": "David Schrammel"
                    },
                    {
                        "name": "Martin Schwarzl"
                    },
                    {
                        "name": "Stefan Mangard"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Mangard"
                },
                "author": "Stefan Mangard",
                "arxiv_comment": "To appear in the Network and Distributed System Security (NDSS)\n  Symposium, February 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00601v2",
                "updated": "2024-11-04T09:40:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    9,
                    40,
                    27,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-01T14:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    3,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Diversity in Network-Friendly Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in Network-Friendly Recommendations"
                },
                "summary": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms."
                },
                "authors": [
                    {
                        "name": "Evangelia Tzimpimpaki"
                    },
                    {
                        "name": "Thrasyvoulos Spyropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Thrasyvoulos Spyropoulos"
                },
                "author": "Thrasyvoulos Spyropoulos",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01754v1",
                "updated": "2024-11-04T02:35:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T02:35:03Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "title": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun"
                },
                "summary": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "X. -H. Wang"
                    },
                    {
                        "name": "G. Shu"
                    },
                    {
                        "name": "H. Qian"
                    },
                    {
                        "name": "X. Li"
                    },
                    {
                        "name": "Z. Liu"
                    },
                    {
                        "name": "Z. Jiang"
                    },
                    {
                        "name": "H. Meng"
                    },
                    {
                        "name": "C. Xing"
                    },
                    {
                        "name": "Q. Zhou"
                    },
                    {
                        "name": "H. Deng"
                    }
                ],
                "author_detail": {
                    "name": "H. Deng"
                },
                "author": "H. Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v2",
                "updated": "2024-11-04T02:08:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    8,
                    55,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu"
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v4",
                "updated": "2024-11-03T09:42:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    9,
                    42,
                    35,
                    6,
                    308,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "arxiv_comment": "This is an extended version of a paper published in the proceedings\n  of the 2024 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2024); this version was presented at the 4th NeurIPS Workshop on\n  Efficient Natural Language and Speech Processing (ENLSP-IV)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01458v1",
                "updated": "2024-11-03T07:01:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "published": "2024-11-03T07:01:13Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "title": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services"
                },
                "summary": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations."
                },
                "authors": [
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Xiangwang Hou"
                    },
                    {
                        "name": "Lianfen Huang"
                    },
                    {
                        "name": "Seyyedali Hosseinalipour"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Khaled Ben Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled Ben Letaief"
                },
                "author": "Khaled Ben Letaief",
                "arxiv_comment": "14 pages, 8 figures, 39 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01269v1",
                "updated": "2024-11-02T14:40:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    14,
                    40,
                    36,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T14:40:36Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    14,
                    40,
                    36,
                    5,
                    307,
                    0
                ],
                "title": "Disaggregated Database Management Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Database Management Systems"
                },
                "summary": "Modern applications demand high performance and cost efficient database\nmanagement systems (DBMSs). Their workloads may be diverse, ranging from online\ntransaction processing to analytics and decision support. The cloud\ninfrastructure enables disaggregation of monolithic DBMSs into components that\nfacilitate software-hardware co-design. This is realized using pools of\nhardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using\nhigh-speed networks. This disaggregation trend is being adopted by cloud DBMSs\nbecause hardware re-provisioning can be achieved by simply invoking software\nAPIs. Disaggregated DBMSs separate processing from storage, enabling each to\nscale elastically and independently. They may disaggregate compute usage based\non functionality, e.g., compute needed for writes from compute needed for\nqueries and compute needed for compaction. They may also use disaggregated\nmemory, e.g., for intermediate results in a shuffle or for remote caching. The\nDBMS monitors the characteristics of a workload and dynamically assembles its\ncomponents that are most efficient and cost effective for the workload. This\npaper is a summary of a panel session that discussed the capability,\nchallenges, and opportunities of these emerging DBMSs and disaggregated\nhardware systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern applications demand high performance and cost efficient database\nmanagement systems (DBMSs). Their workloads may be diverse, ranging from online\ntransaction processing to analytics and decision support. The cloud\ninfrastructure enables disaggregation of monolithic DBMSs into components that\nfacilitate software-hardware co-design. This is realized using pools of\nhardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using\nhigh-speed networks. This disaggregation trend is being adopted by cloud DBMSs\nbecause hardware re-provisioning can be achieved by simply invoking software\nAPIs. Disaggregated DBMSs separate processing from storage, enabling each to\nscale elastically and independently. They may disaggregate compute usage based\non functionality, e.g., compute needed for writes from compute needed for\nqueries and compute needed for compaction. They may also use disaggregated\nmemory, e.g., for intermediate results in a shuffle or for remote caching. The\nDBMS monitors the characteristics of a workload and dynamically assembles its\ncomponents that are most efficient and cost effective for the workload. This\npaper is a summary of a panel session that discussed the capability,\nchallenges, and opportunities of these emerging DBMSs and disaggregated\nhardware systems."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Philip A. Bernstein"
                    },
                    {
                        "name": "Dhruba Borthakur"
                    },
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Jai Menon"
                    },
                    {
                        "name": "Sumit Puri"
                    }
                ],
                "author_detail": {
                    "name": "Sumit Puri"
                },
                "author": "Sumit Puri",
                "arxiv_comment": "This paper appeared in the {\\em Performance Evaluation and\n  Benchmarking} - 14th TPC Technology Conference, TPCTC 2022, Sydney, NSW,\n  Australia, September 5, 2022, Revised Selected Papers. Lecture Notes in\n  Computer Science 13860, Springer 2023, ISBN 978-3-031-29575-1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01246v1",
                "updated": "2024-11-02T13:52:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    13,
                    52,
                    49,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T13:52:49Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    13,
                    52,
                    49,
                    5,
                    307,
                    0
                ],
                "title": "CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores"
                },
                "summary": "Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a\ngeneral purpose key-value store (KVS) that manages key-value pairs computed by\napplications with different access patterns, key-value sizes, and varying costs\nfor each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS)\nalgorithm that can be implemented as efficiently as LRU. In particular, CAMP's\neviction policies are as effective as those of GDS but require only a small\nfraction of the updates to an internal data structure in order to make those\ndecisions. Similar to an implementation of LRU using queues, it adapts to\nchanging workload patterns based on the history of requests for different\nkey-value pairs. It is superior to LRU because it considers both the size and\ncost of key-value pairs to maximize the utility of the available memory across\ncompeting applications. We compare CAMP with both LRU and an alternative that\nrequires human intervention to partition memory into pools and assign grouping\nof key-value pairs to different pools. The results demonstrate CAMP is as fast\nas LRU while outperforming both LRU and the pooled alternative. We also present\nresults from an implementation of CAMP using Twitter's version of memcached.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a\ngeneral purpose key-value store (KVS) that manages key-value pairs computed by\napplications with different access patterns, key-value sizes, and varying costs\nfor each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS)\nalgorithm that can be implemented as efficiently as LRU. In particular, CAMP's\neviction policies are as effective as those of GDS but require only a small\nfraction of the updates to an internal data structure in order to make those\ndecisions. Similar to an implementation of LRU using queues, it adapts to\nchanging workload patterns based on the history of requests for different\nkey-value pairs. It is superior to LRU because it considers both the size and\ncost of key-value pairs to maximize the utility of the available memory across\ncompeting applications. We compare CAMP with both LRU and an alternative that\nrequires human intervention to partition memory into pools and assign grouping\nof key-value pairs to different pools. The results demonstrate CAMP is as fast\nas LRU while outperforming both LRU and the pooled alternative. We also present\nresults from an implementation of CAMP using Twitter's version of memcached."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Sandy Irani"
                    },
                    {
                        "name": "Jenny Lam"
                    },
                    {
                        "name": "Jason Yap"
                    }
                ],
                "author_detail": {
                    "name": "Jason Yap"
                },
                "author": "Jason Yap",
                "arxiv_doi": "10.1145/2663165.2663317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/2663165.2663317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.01246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A shorter version of CAMP appeared in the Proceedings of the\n  ACM/IFIP/USENIX Middleware Conference, Bordeaux, France, December 2014. See\n  https://github.com/scdblab/CAMP for an implementation",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01142v1",
                "updated": "2024-11-02T05:15:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    5,
                    15,
                    44,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T05:15:44Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    5,
                    15,
                    44,
                    5,
                    307,
                    0
                ],
                "title": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM\n  Inference"
                },
                "summary": "Online LLM inference powers many exciting applications such as intelligent\nchatbots and autonomous agents. Modern LLM inference engines widely rely on\nrequest batching to improve inference throughput, aiming to make it\ncost-efficient when running on expensive GPU accelerators. However, the limited\nGPU memory has largely limited the batch size achieved in practice, leaving\nsignificant GPU compute resources wasted.\n  We present NEO, an online LLM inference system that offloads part of\nattention compute and KV cache states from the GPU to the local host CPU,\neffectively increasing the GPU batch size and thus inference throughput. To\nthis end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling\nto balance GPU and CPU loads and fully utilize their compute and memory\nresources. We evaluate NEO on a wide range of workloads (i.e., code generation,\ntext summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B,\n70B). NEO achieves up to 7.5$\\times$, 26%, and 14% higher throughput compared\nto GPU-only approach on T4, A10G, and H100 GPUs, respectively, while\nmaintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3%\nthroughput gain on A10G GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online LLM inference powers many exciting applications such as intelligent\nchatbots and autonomous agents. Modern LLM inference engines widely rely on\nrequest batching to improve inference throughput, aiming to make it\ncost-efficient when running on expensive GPU accelerators. However, the limited\nGPU memory has largely limited the batch size achieved in practice, leaving\nsignificant GPU compute resources wasted.\n  We present NEO, an online LLM inference system that offloads part of\nattention compute and KV cache states from the GPU to the local host CPU,\neffectively increasing the GPU batch size and thus inference throughput. To\nthis end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling\nto balance GPU and CPU loads and fully utilize their compute and memory\nresources. We evaluate NEO on a wide range of workloads (i.e., code generation,\ntext summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B,\n70B). NEO achieves up to 7.5$\\times$, 26%, and 14% higher throughput compared\nto GPU-only approach on T4, A10G, and H100 GPUs, respectively, while\nmaintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3%\nthroughput gain on A10G GPU."
                },
                "authors": [
                    {
                        "name": "Xuanlin Jiang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15420v3",
                "updated": "2024-11-01T14:56:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    56,
                    52,
                    4,
                    306,
                    0
                ],
                "published": "2024-04-23T18:10:42Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    18,
                    10,
                    42,
                    1,
                    114,
                    0
                ],
                "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference"
                },
                "summary": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "Joo Monteiro"
                    },
                    {
                        "name": "tienne Marcotte"
                    },
                    {
                        "name": "Pierre-Andr Nol"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "David Vzquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Perouz Taslakian"
                    }
                ],
                "author_detail": {
                    "name": "Perouz Taslakian"
                },
                "author": "Perouz Taslakian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02657v2",
                "updated": "2024-11-01T08:52:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    8,
                    52,
                    18,
                    4,
                    306,
                    0
                ],
                "published": "2024-06-04T17:45:26Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    45,
                    26,
                    1,
                    156,
                    0
                ],
                "title": "Block Transformer: Global-to-Local Language Modeling for Fast Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Transformer: Global-to-Local Language Modeling for Fast Inference"
                },
                "summary": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer."
                },
                "authors": [
                    {
                        "name": "Namgyu Ho"
                    },
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Taehyeon Kim"
                    },
                    {
                        "name": "Hyunjik Jo"
                    },
                    {
                        "name": "Yireun Kim"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "James Thorne"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "37 pages, 24 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00131v1",
                "updated": "2024-10-31T18:31:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    31,
                    13,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T18:31:13Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    31,
                    13,
                    3,
                    305,
                    0
                ],
                "title": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence"
                },
                "summary": "We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared."
                },
                "authors": [
                    {
                        "name": "John Whitington"
                    }
                ],
                "author_detail": {
                    "name": "John Whitington"
                },
                "author": "John Whitington",
                "arxiv_doi": "10.1145/2788539.27885",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/2788539.27885",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.00131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 14 figures",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24174v1",
                "updated": "2024-10-31T17:41:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:41:14Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "title": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices"
                },
                "summary": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations."
                },
                "authors": [
                    {
                        "name": "Biman Barua"
                    },
                    {
                        "name": "M. Shamim Kaiser"
                    }
                ],
                "author_detail": {
                    "name": "M. Shamim Kaiser"
                },
                "author": "M. Shamim Kaiser",
                "arxiv_comment": "20 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23805v1",
                "updated": "2024-10-31T10:45:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T10:45:02Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "title": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware"
                },
                "summary": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions."
                },
                "authors": [
                    {
                        "name": "Sitian Chen"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Yucheng Shi"
                    },
                    {
                        "name": "Yusen Li"
                    },
                    {
                        "name": "Xin Yao"
                    }
                ],
                "author_detail": {
                    "name": "Xin Yao"
                },
                "author": "Xin Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23537v1",
                "updated": "2024-10-31T00:58:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T00:58:11Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "title": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling"
                },
                "summary": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively."
                },
                "authors": [
                    {
                        "name": "Youpeng Zhao"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "ICCAD 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18400v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18400v6",
                "updated": "2024-10-30T21:22:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    21,
                    22,
                    54,
                    2,
                    304,
                    0
                ],
                "published": "2024-05-28T17:40:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    40,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass"
                },
                "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding."
                },
                "authors": [
                    {
                        "name": "Ethan Shen"
                    },
                    {
                        "name": "Alan Fan"
                    },
                    {
                        "name": "Sarah M. Pratt"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "arxiv_comment": "23 pages, 16 figures, accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18400v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18400v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14576v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14576v3",
                "updated": "2024-10-30T16:06:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    6,
                    21,
                    2,
                    304,
                    0
                ],
                "published": "2024-02-08T17:17:46Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    17,
                    17,
                    46,
                    3,
                    39,
                    0
                ],
                "title": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching"
                },
                "summary": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    },
                    {
                        "name": "Zixu Wang"
                    },
                    {
                        "name": "Aakash Agarwal"
                    },
                    {
                        "name": "Adib S. Rezaei"
                    }
                ],
                "author_detail": {
                    "name": "Adib S. Rezaei"
                },
                "author": "Adib S. Rezaei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14576v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14576v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23079v1",
                "updated": "2024-10-30T14:53:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:53:37Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "title": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm."
                },
                "authors": [
                    {
                        "name": "Junqi Zhao"
                    },
                    {
                        "name": "Zhijin Fang"
                    },
                    {
                        "name": "Shu Li"
                    },
                    {
                        "name": "Shaohui Yang"
                    },
                    {
                        "name": "Shichao He"
                    }
                ],
                "author_detail": {
                    "name": "Shichao He"
                },
                "author": "Shichao He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v2",
                "updated": "2024-10-30T03:31:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    3,
                    31,
                    9,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08043v1",
                "updated": "2024-10-30T02:18:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    18,
                    59,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T02:18:59Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    18,
                    59,
                    2,
                    304,
                    0
                ],
                "title": "Graph-GIC: A Smart and Parallelized Geomagnetically Induced Current\n  Modelling Algorithm Based on Graph Theory for Space Weather Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-GIC: A Smart and Parallelized Geomagnetically Induced Current\n  Modelling Algorithm Based on Graph Theory for Space Weather Applications"
                },
                "summary": "Geomagnetically Induced Current (GIC) refers to the electromagnetic response\nof the Earth and its conductive modern infrastructures to space weather and\nwould pose a significant threat to high-voltage power grids designed for the\nalternative current operation. To assess the impact of space weather on the\npower grid, one needs to calculate the GIC on a national or continental scale.\nIn this study, we developed a smart and parallelized GIC modelling algorithm,\nGraph GIC. This algorithm deploys a graph representing a power grid in a\nsingle-line diagram, in which substations/transformers act as nodes and\ntransmission lines as edges. With these denotations, a power grid and its\nelectric parameters are mathematically represented with an adjacency matrix and\nan admittance matrix. We used sparse matrix and parallelisation techniques to\nexpedite the intensive computation in cases of large-scale power grids. The\nGraph GIC was validated with a benchmark grid, applied to the GIC calculation\nof the 500 kV power grid of Guangdong, China, and conducted preliminary\nanalysis on the grid's susceptibility to geomagnetic storms. The Graph GIC\nalgorithm has the advantage of an intuitive and highly scalable graph\nrepresentation of a power grid at any scale. It achieves high-accuracy\ncalculation and a speedup of about 18 times after parallelisation. This\nalgorithm could be applied to assess the impact of space weather on a power\ngrid up to continental scales and could be incorporated into global space\nweather modelling frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geomagnetically Induced Current (GIC) refers to the electromagnetic response\nof the Earth and its conductive modern infrastructures to space weather and\nwould pose a significant threat to high-voltage power grids designed for the\nalternative current operation. To assess the impact of space weather on the\npower grid, one needs to calculate the GIC on a national or continental scale.\nIn this study, we developed a smart and parallelized GIC modelling algorithm,\nGraph GIC. This algorithm deploys a graph representing a power grid in a\nsingle-line diagram, in which substations/transformers act as nodes and\ntransmission lines as edges. With these denotations, a power grid and its\nelectric parameters are mathematically represented with an adjacency matrix and\nan admittance matrix. We used sparse matrix and parallelisation techniques to\nexpedite the intensive computation in cases of large-scale power grids. The\nGraph GIC was validated with a benchmark grid, applied to the GIC calculation\nof the 500 kV power grid of Guangdong, China, and conducted preliminary\nanalysis on the grid's susceptibility to geomagnetic storms. The Graph GIC\nalgorithm has the advantage of an intuitive and highly scalable graph\nrepresentation of a power grid at any scale. It achieves high-accuracy\ncalculation and a speedup of about 18 times after parallelisation. This\nalgorithm could be applied to assess the impact of space weather on a power\ngrid up to continental scales and could be incorporated into global space\nweather modelling frameworks."
                },
                "authors": [
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Ding Yuan"
                    },
                    {
                        "name": "Xueshang Feng"
                    },
                    {
                        "name": "Stefaan Poedts"
                    },
                    {
                        "name": "Zhengyang Zou"
                    },
                    {
                        "name": "Song Feng"
                    },
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Tong Yin"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yin"
                },
                "author": "Tong Yin",
                "arxiv_comment": "19 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.space-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23317v1",
                "updated": "2024-10-29T20:04:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T20:04:34Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "title": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration"
                },
                "summary": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%."
                },
                "authors": [
                    {
                        "name": "Dezhan Tu"
                    },
                    {
                        "name": "Danylo Vashchilenko"
                    },
                    {
                        "name": "Yuzhe Lu"
                    },
                    {
                        "name": "Panpan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Panpan Xu"
                },
                "author": "Panpan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.01801v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.01801v4",
                "updated": "2024-10-29T18:26:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    18,
                    26,
                    9,
                    1,
                    303,
                    0
                ],
                "published": "2023-10-03T05:17:08Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    5,
                    17,
                    8,
                    1,
                    276,
                    0
                ],
                "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"
                },
                "summary": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Jianfeng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Gao"
                },
                "author": "Jianfeng Gao",
                "arxiv_comment": "ICLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.01801v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.01801v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.16679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16679v1",
                "updated": "2024-11-25T18:59:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    59,
                    30,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T18:59:30Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    59,
                    30,
                    0,
                    330,
                    0
                ],
                "title": "Do Large Language Models Perform Latent Multi-Hop Reasoning without\n  Exploiting Shortcuts?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Perform Latent Multi-Hop Reasoning without\n  Exploiting Shortcuts?"
                },
                "summary": "We evaluate how well Large Language Models (LLMs) latently recall and compose\nfacts to answer multi-hop queries like \"In the year Scarlett Johansson was\nborn, the Summer Olympics were hosted in the country of\". One major challenge\nin evaluating this ability is that LLMs may have developed shortcuts by\nencounters of the head entity \"Scarlett Johansson\" and the answer entity\n\"United States\" in the same training sequences or merely guess the answer based\non frequency-based priors. To prevent shortcuts, we exclude test queries where\nthe head and answer entities co-appear in pretraining corpora. Through careful\nselection of relations and facts and systematic removal of cases where models\nmight guess answers or exploit partial matches, we construct an evaluation\ndataset SOCRATES (ShOrtCut-fRee lATent rEaSoning). We observe that LLMs\ndemonstrate promising latent multi-hop reasoning abilities without exploiting\nshortcuts, but only for certain types of queries. For queries requiring latent\nrecall of countries as the intermediate answer, the best models achieve 80%\nlatent composability, but this drops to just 5% for the recall of years.\nComparisons with Chain-of-Thought composability highlight a significant gap\nbetween the ability of models to reason latently versus explicitly. Analysis\nreveals that latent representations of the intermediate answer are constructed\nmore often in queries with higher latent composability, and shows the emergence\nof latent multi-hop reasoning during pretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We evaluate how well Large Language Models (LLMs) latently recall and compose\nfacts to answer multi-hop queries like \"In the year Scarlett Johansson was\nborn, the Summer Olympics were hosted in the country of\". One major challenge\nin evaluating this ability is that LLMs may have developed shortcuts by\nencounters of the head entity \"Scarlett Johansson\" and the answer entity\n\"United States\" in the same training sequences or merely guess the answer based\non frequency-based priors. To prevent shortcuts, we exclude test queries where\nthe head and answer entities co-appear in pretraining corpora. Through careful\nselection of relations and facts and systematic removal of cases where models\nmight guess answers or exploit partial matches, we construct an evaluation\ndataset SOCRATES (ShOrtCut-fRee lATent rEaSoning). We observe that LLMs\ndemonstrate promising latent multi-hop reasoning abilities without exploiting\nshortcuts, but only for certain types of queries. For queries requiring latent\nrecall of countries as the intermediate answer, the best models achieve 80%\nlatent composability, but this drops to just 5% for the recall of years.\nComparisons with Chain-of-Thought composability highlight a significant gap\nbetween the ability of models to reason latently versus explicitly. Analysis\nreveals that latent representations of the intermediate answer are constructed\nmore often in queries with higher latent composability, and shows the emergence\nof latent multi-hop reasoning during pretraining."
                },
                "authors": [
                    {
                        "name": "Sohee Yang"
                    },
                    {
                        "name": "Nora Kassner"
                    },
                    {
                        "name": "Elena Gribovskaya"
                    },
                    {
                        "name": "Sebastian Riedel"
                    },
                    {
                        "name": "Mor Geva"
                    }
                ],
                "author_detail": {
                    "name": "Mor Geva"
                },
                "author": "Mor Geva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06507v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06507v3",
                "updated": "2024-11-25T18:58:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    58,
                    7,
                    0,
                    330,
                    0
                ],
                "published": "2024-04-09T17:55:41Z",
                "published_parsed": [
                    2024,
                    4,
                    9,
                    17,
                    55,
                    41,
                    1,
                    100,
                    0
                ],
                "title": "Reconstructing Hand-Held Objects in 3D from Images and Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing Hand-Held Objects in 3D from Images and Videos"
                },
                "summary": "Objects manipulated by the hand (i.e., manipulanda) are particularly\nchallenging to reconstruct from Internet videos. Not only does the hand occlude\nmuch of the object, but also the object is often only visible in a small number\nof image pixels. At the same time, two strong anchors emerge in this setting:\n(1) estimated 3D hands help disambiguate the location and scale of the object,\nand (2) the set of manipulanda is small relative to all possible objects. With\nthese insights in mind, we present a scalable paradigm for hand-held object\nreconstruction that builds on recent breakthroughs in large language/vision\nmodels and 3D object datasets. Given a monocular RGB video, we aim to\nreconstruct hand-held object geometry in 3D, over time. In order to obtain the\nbest performing single frame model, we first present MCC-Hand-Object (MCC-HO),\nwhich jointly reconstructs hand and object geometry given a single RGB image\nand inferred 3D hand as inputs. Subsequently, we prompt a text-to-3D generative\nmodel using GPT-4(V) to retrieve a 3D object model that matches the object in\nthe image(s); we call this alignment Retrieval-Augmented Reconstruction (RAR).\nRAR provides unified object geometry across all frames, and the result is\nrigidly aligned with both the input images and 3D MCC-HO observations in a\ntemporally consistent manner. Experiments demonstrate that our approach\nachieves state-of-the-art performance on lab and Internet image/video datasets.\nWe make our code and models available on the project website:\nhttps://janehwu.github.io/mcc-ho",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objects manipulated by the hand (i.e., manipulanda) are particularly\nchallenging to reconstruct from Internet videos. Not only does the hand occlude\nmuch of the object, but also the object is often only visible in a small number\nof image pixels. At the same time, two strong anchors emerge in this setting:\n(1) estimated 3D hands help disambiguate the location and scale of the object,\nand (2) the set of manipulanda is small relative to all possible objects. With\nthese insights in mind, we present a scalable paradigm for hand-held object\nreconstruction that builds on recent breakthroughs in large language/vision\nmodels and 3D object datasets. Given a monocular RGB video, we aim to\nreconstruct hand-held object geometry in 3D, over time. In order to obtain the\nbest performing single frame model, we first present MCC-Hand-Object (MCC-HO),\nwhich jointly reconstructs hand and object geometry given a single RGB image\nand inferred 3D hand as inputs. Subsequently, we prompt a text-to-3D generative\nmodel using GPT-4(V) to retrieve a 3D object model that matches the object in\nthe image(s); we call this alignment Retrieval-Augmented Reconstruction (RAR).\nRAR provides unified object geometry across all frames, and the result is\nrigidly aligned with both the input images and 3D MCC-HO observations in a\ntemporally consistent manner. Experiments demonstrate that our approach\nachieves state-of-the-art performance on lab and Internet image/video datasets.\nWe make our code and models available on the project website:\nhttps://janehwu.github.io/mcc-ho"
                },
                "authors": [
                    {
                        "name": "Jane Wu"
                    },
                    {
                        "name": "Georgios Pavlakos"
                    },
                    {
                        "name": "Georgia Gkioxari"
                    },
                    {
                        "name": "Jitendra Malik"
                    }
                ],
                "author_detail": {
                    "name": "Jitendra Malik"
                },
                "author": "Jitendra Malik",
                "arxiv_comment": "Project page: https://janehwu.github.io/mcc-ho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06507v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06507v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14081v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14081v3",
                "updated": "2024-11-25T18:57:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    57,
                    35,
                    0,
                    330,
                    0
                ],
                "published": "2024-02-21T19:10:08Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    19,
                    10,
                    8,
                    2,
                    52,
                    0
                ],
                "title": "Motion Code: Robust Time Series Classification and Forecasting via\n  Sparse Variational Multi-Stochastic Processes Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motion Code: Robust Time Series Classification and Forecasting via\n  Sparse Variational Multi-Stochastic Processes Learning"
                },
                "summary": "Despite extensive research, time series classification and forecasting on\nnoisy data remain highly challenging. The main difficulties lie in finding\nsuitable mathematical concepts to describe time series and effectively separate\nnoise from the true signals. Unlike traditional methods treating time series as\nstatic vectors or fixed sequences, we propose a novel framework that views each\ntime series, regardless of length, as a realization of a continuous-time\nstochastic process. This mathematical approach captures dependencies across\ntimestamps and detects hidden, time-varying signals within the noise. However,\nreal-world data often involves multiple distinct dynamics, making it\ninsufficient to model the entire process with a single stochastic model. To\naddress this, we assign each dynamic a unique signature vector and introduce\nthe concept of \"most informative timestamps\" to infer a sparse approximation of\nthe individual dynamics from these vectors. The resulting model, called Motion\nCode, includes parameters that fully capture diverse underlying dynamics in an\nintegrated manner, enabling simultaneous classification and forecasting of time\nseries. Extensive experiments on noisy datasets, including real-world\nParkinson's disease sensor tracking, demonstrate Motion Code's strong\nperformance against established benchmarks for time series classification and\nforecasting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite extensive research, time series classification and forecasting on\nnoisy data remain highly challenging. The main difficulties lie in finding\nsuitable mathematical concepts to describe time series and effectively separate\nnoise from the true signals. Unlike traditional methods treating time series as\nstatic vectors or fixed sequences, we propose a novel framework that views each\ntime series, regardless of length, as a realization of a continuous-time\nstochastic process. This mathematical approach captures dependencies across\ntimestamps and detects hidden, time-varying signals within the noise. However,\nreal-world data often involves multiple distinct dynamics, making it\ninsufficient to model the entire process with a single stochastic model. To\naddress this, we assign each dynamic a unique signature vector and introduce\nthe concept of \"most informative timestamps\" to infer a sparse approximation of\nthe individual dynamics from these vectors. The resulting model, called Motion\nCode, includes parameters that fully capture diverse underlying dynamics in an\nintegrated manner, enabling simultaneous classification and forecasting of time\nseries. Extensive experiments on noisy datasets, including real-world\nParkinson's disease sensor tracking, demonstrate Motion Code's strong\nperformance against established benchmarks for time series classification and\nforecasting."
                },
                "authors": [
                    {
                        "name": "Chandrajit Bajaj"
                    },
                    {
                        "name": "Minh Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Minh Nguyen"
                },
                "author": "Minh Nguyen",
                "arxiv_comment": "20 pages, 5 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14081v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14081v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16657v1",
                "updated": "2024-11-25T18:41:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    41,
                    56,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T18:41:56Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    41,
                    56,
                    0,
                    330,
                    0
                ],
                "title": "DreamRunner: Fine-Grained Storytelling Video Generation with\n  Retrieval-Augmented Motion Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DreamRunner: Fine-Grained Storytelling Video Generation with\n  Retrieval-Augmented Motion Adaptation"
                },
                "summary": "Storytelling video generation (SVG) has recently emerged as a task to create\nlong, multi-motion, multi-scene videos that consistently represent the story\ndescribed in the input text script. SVG holds great potential for diverse\ncontent creation in media and entertainment; however, it also presents\nsignificant challenges: (1) objects must exhibit a range of fine-grained,\ncomplex motions, (2) multiple objects need to appear consistently across\nscenes, and (3) subjects may require multiple motions with seamless transitions\nwithin a single scene. To address these challenges, we propose DreamRunner, a\nnovel story-to-video generation method: First, we structure the input script\nusing a large language model (LLM) to facilitate both coarse-grained scene\nplanning as well as fine-grained object-level layout and motion planning. Next,\nDreamRunner presents retrieval-augmented test-time adaptation to capture target\nmotion priors for objects in each scene, supporting diverse motion\ncustomization based on retrieved videos, thus facilitating the generation of\nnew videos with complex, scripted motions. Lastly, we propose a novel\nspatial-temporal region-based 3D attention and prior injection module SR3AI for\nfine-grained object-motion binding and frame-by-frame semantic control. We\ncompare DreamRunner with various SVG baselines, demonstrating state-of-the-art\nperformance in character consistency, text alignment, and smooth transitions.\nAdditionally, DreamRunner exhibits strong fine-grained condition-following\nability in compositional text-to-video generation, significantly outperforming\nbaselines on T2V-ComBench. Finally, we validate DreamRunner's robust ability to\ngenerate multi-object interactions with qualitative examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Storytelling video generation (SVG) has recently emerged as a task to create\nlong, multi-motion, multi-scene videos that consistently represent the story\ndescribed in the input text script. SVG holds great potential for diverse\ncontent creation in media and entertainment; however, it also presents\nsignificant challenges: (1) objects must exhibit a range of fine-grained,\ncomplex motions, (2) multiple objects need to appear consistently across\nscenes, and (3) subjects may require multiple motions with seamless transitions\nwithin a single scene. To address these challenges, we propose DreamRunner, a\nnovel story-to-video generation method: First, we structure the input script\nusing a large language model (LLM) to facilitate both coarse-grained scene\nplanning as well as fine-grained object-level layout and motion planning. Next,\nDreamRunner presents retrieval-augmented test-time adaptation to capture target\nmotion priors for objects in each scene, supporting diverse motion\ncustomization based on retrieved videos, thus facilitating the generation of\nnew videos with complex, scripted motions. Lastly, we propose a novel\nspatial-temporal region-based 3D attention and prior injection module SR3AI for\nfine-grained object-motion binding and frame-by-frame semantic control. We\ncompare DreamRunner with various SVG baselines, demonstrating state-of-the-art\nperformance in character consistency, text alignment, and smooth transitions.\nAdditionally, DreamRunner exhibits strong fine-grained condition-following\nability in compositional text-to-video generation, significantly outperforming\nbaselines on T2V-ComBench. Finally, we validate DreamRunner's robust ability to\ngenerate multi-object interactions with qualitative examples."
                },
                "authors": [
                    {
                        "name": "Zun Wang"
                    },
                    {
                        "name": "Jialu Li"
                    },
                    {
                        "name": "Han Lin"
                    },
                    {
                        "name": "Jaehong Yoon"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "Project website: https://dreamrunner-story2video.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16646v1",
                "updated": "2024-11-25T18:28:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    28,
                    26,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T18:28:26Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    28,
                    26,
                    0,
                    330,
                    0
                ],
                "title": "Self-Generated Critiques Boost Reward Modeling for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Generated Critiques Boost Reward Modeling for Language Models"
                },
                "summary": "Reward modeling is crucial for aligning large language models (LLMs) with\nhuman preferences, especially in reinforcement learning from human feedback\n(RLHF). However, current reward models mainly produce scalar scores and\nstruggle to incorporate critiques in a natural language format. We hypothesize\nthat predicting both critiques and the scalar reward would improve reward\nmodeling ability. Motivated by this, we propose Critic-RM, a framework that\nimproves reward models using self-generated critiques without extra\nsupervision. Critic-RM employs a two-stage process: generating and filtering\nhigh-quality critiques, followed by joint fine-tuning on reward prediction and\ncritique generation. Experiments across benchmarks show that Critic-RM improves\nreward modeling accuracy by 3.7%-7.3% compared to standard reward models and\nLLM judges, demonstrating strong performance and data efficiency. Additional\nstudies further validate the effectiveness of generated critiques in rectifying\nflawed reasoning steps with 2.5%-3.2% gains in improving reasoning accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward modeling is crucial for aligning large language models (LLMs) with\nhuman preferences, especially in reinforcement learning from human feedback\n(RLHF). However, current reward models mainly produce scalar scores and\nstruggle to incorporate critiques in a natural language format. We hypothesize\nthat predicting both critiques and the scalar reward would improve reward\nmodeling ability. Motivated by this, we propose Critic-RM, a framework that\nimproves reward models using self-generated critiques without extra\nsupervision. Critic-RM employs a two-stage process: generating and filtering\nhigh-quality critiques, followed by joint fine-tuning on reward prediction and\ncritique generation. Experiments across benchmarks show that Critic-RM improves\nreward modeling accuracy by 3.7%-7.3% compared to standard reward models and\nLLM judges, demonstrating strong performance and data efficiency. Additional\nstudies further validate the effectiveness of generated critiques in rectifying\nflawed reasoning steps with 2.5%-3.2% gains in improving reasoning accuracy."
                },
                "authors": [
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Zhengxing Chen"
                    },
                    {
                        "name": "Aston Zhang"
                    },
                    {
                        "name": "Liang Tan"
                    },
                    {
                        "name": "Chenguang Zhu"
                    },
                    {
                        "name": "Richard Yuanzhe Pang"
                    },
                    {
                        "name": "Yundi Qian"
                    },
                    {
                        "name": "Xuewei Wang"
                    },
                    {
                        "name": "Suchin Gururangan"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Melanie Kambadur"
                    },
                    {
                        "name": "Dhruv Mahajan"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06000v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06000v2",
                "updated": "2024-11-25T18:19:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    19,
                    52,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-08T22:43:12Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    22,
                    43,
                    12,
                    4,
                    313,
                    0
                ],
                "title": "Cosmology From CMB Lensing and Delensed EE Power Spectra Using 2019-2020\n  SPT-3G Polarization Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmology From CMB Lensing and Delensed EE Power Spectra Using 2019-2020\n  SPT-3G Polarization Data"
                },
                "summary": "From CMB polarization data alone we reconstruct the CMB lensing power\nspectrum, comparable in overall constraining power to previous\ntemperature-based reconstructions, and an unlensed E-mode power spectrum. The\nobservations, taken in 2019 and 2020 with the South Pole Telescope (SPT) and\nthe SPT-3G camera, cover 1500 deg$^2$ at 95, 150, and 220 GHz with arcminute\nresolution and roughly 4.9$\\mu$K-arcmin coadded noise in polarization. The\npower spectrum estimates, together with systematic parameter estimates and a\njoint covariance matrix, follow from a Bayesian analysis using the Marginal\nUnbiased Score Expansion (MUSE) method. The E-mode spectrum at $\\ell>2000$ and\nlensing spectrum at $L>350$ are the most precise to date. Assuming the\n$\\Lambda$CDM model, and using only these SPT data and priors on $\\tau$ and\nabsolute calibration from Planck, we find $H_0=66.81\\pm0.81$ km/s/Mpc,\ncomparable in precision to the Planck determination and in 5.4$\\sigma$ tension\nwith the most precise $H_0$ inference derived via the distance ladder. We also\nfind $S_8=0.850\\pm0.017$, providing further independent evidence of a slight\ntension with low-redshift structure probes. The $\\Lambda$CDM model provides a\ngood simultaneous fit to the combined Planck, ACT, and SPT data, and thus\npasses a powerful test. Combining these CMB datasets with BAO observations, we\nfind that the effective number of neutrino species, spatial curvature, and\nprimordial helium fraction are consistent with standard model values, and that\nthe 95% confidence upper limit on the neutrino mass sum is 0.075 eV. The SPT\ndata are consistent with the somewhat weak preference for excess lensing power\nseen in Planck and ACT data relative to predictions of the $\\Lambda$CDM model.\nWe also detect at greater than 3$\\sigma$ the influence of non-linear evolution\nin the CMB lensing power spectrum and discuss it in the context of the $S_8$\ntension.(abridged)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From CMB polarization data alone we reconstruct the CMB lensing power\nspectrum, comparable in overall constraining power to previous\ntemperature-based reconstructions, and an unlensed E-mode power spectrum. The\nobservations, taken in 2019 and 2020 with the South Pole Telescope (SPT) and\nthe SPT-3G camera, cover 1500 deg$^2$ at 95, 150, and 220 GHz with arcminute\nresolution and roughly 4.9$\\mu$K-arcmin coadded noise in polarization. The\npower spectrum estimates, together with systematic parameter estimates and a\njoint covariance matrix, follow from a Bayesian analysis using the Marginal\nUnbiased Score Expansion (MUSE) method. The E-mode spectrum at $\\ell>2000$ and\nlensing spectrum at $L>350$ are the most precise to date. Assuming the\n$\\Lambda$CDM model, and using only these SPT data and priors on $\\tau$ and\nabsolute calibration from Planck, we find $H_0=66.81\\pm0.81$ km/s/Mpc,\ncomparable in precision to the Planck determination and in 5.4$\\sigma$ tension\nwith the most precise $H_0$ inference derived via the distance ladder. We also\nfind $S_8=0.850\\pm0.017$, providing further independent evidence of a slight\ntension with low-redshift structure probes. The $\\Lambda$CDM model provides a\ngood simultaneous fit to the combined Planck, ACT, and SPT data, and thus\npasses a powerful test. Combining these CMB datasets with BAO observations, we\nfind that the effective number of neutrino species, spatial curvature, and\nprimordial helium fraction are consistent with standard model values, and that\nthe 95% confidence upper limit on the neutrino mass sum is 0.075 eV. The SPT\ndata are consistent with the somewhat weak preference for excess lensing power\nseen in Planck and ACT data relative to predictions of the $\\Lambda$CDM model.\nWe also detect at greater than 3$\\sigma$ the influence of non-linear evolution\nin the CMB lensing power spectrum and discuss it in the context of the $S_8$\ntension.(abridged)"
                },
                "authors": [
                    {
                        "name": "F. Ge"
                    },
                    {
                        "name": "M. Millea"
                    },
                    {
                        "name": "E. Camphuis"
                    },
                    {
                        "name": "C. Daley"
                    },
                    {
                        "name": "N. Huang"
                    },
                    {
                        "name": "Y. Omori"
                    },
                    {
                        "name": "W. Quan"
                    },
                    {
                        "name": "E. Anderes"
                    },
                    {
                        "name": "A. J. Anderson"
                    },
                    {
                        "name": "B. Ansarinejad"
                    },
                    {
                        "name": "M. Archipley"
                    },
                    {
                        "name": "L. Balkenhol"
                    },
                    {
                        "name": "K. Benabed"
                    },
                    {
                        "name": "A. N. Bender"
                    },
                    {
                        "name": "B. A. Benson"
                    },
                    {
                        "name": "F. Bianchini"
                    },
                    {
                        "name": "L. E. Bleem"
                    },
                    {
                        "name": "F. R. Bouchet"
                    },
                    {
                        "name": "L. Bryant"
                    },
                    {
                        "name": "J. E. Carlstrom"
                    },
                    {
                        "name": "C. L. Chang"
                    },
                    {
                        "name": "P. Chaubal"
                    },
                    {
                        "name": "G. Chen"
                    },
                    {
                        "name": "P. M. Chichura"
                    },
                    {
                        "name": "A. Chokshi"
                    },
                    {
                        "name": "T. -L. Chou"
                    },
                    {
                        "name": "A. Coerver"
                    },
                    {
                        "name": "T. M. Crawford"
                    },
                    {
                        "name": "T. de Haan"
                    },
                    {
                        "name": "K. R. Dibert"
                    },
                    {
                        "name": "M. A. Dobbs"
                    },
                    {
                        "name": "M. Doohan"
                    },
                    {
                        "name": "A. Doussot"
                    },
                    {
                        "name": "D. Dutcher"
                    },
                    {
                        "name": "W. Everett"
                    },
                    {
                        "name": "C. Feng"
                    },
                    {
                        "name": "K. R. Ferguson"
                    },
                    {
                        "name": "K. Fichman"
                    },
                    {
                        "name": "A. Foster"
                    },
                    {
                        "name": "S. Galli"
                    },
                    {
                        "name": "A. E. Gambrel"
                    },
                    {
                        "name": "R. W. Gardner"
                    },
                    {
                        "name": "N. Goeckner-Wald"
                    },
                    {
                        "name": "R. Gualtieri"
                    },
                    {
                        "name": "F. Guidi"
                    },
                    {
                        "name": "S. Guns"
                    },
                    {
                        "name": "N. W. Halverson"
                    },
                    {
                        "name": "E. Hivon"
                    },
                    {
                        "name": "G. P. Holder"
                    },
                    {
                        "name": "W. L. Holzapfel"
                    },
                    {
                        "name": "J. C. Hood"
                    },
                    {
                        "name": "D. Howe"
                    },
                    {
                        "name": "A. Hryciuk"
                    },
                    {
                        "name": "F. Kruzor"
                    },
                    {
                        "name": "A. R. Khalife"
                    },
                    {
                        "name": "L. Knox"
                    },
                    {
                        "name": "M. Korman"
                    },
                    {
                        "name": "K. Kornoelje"
                    },
                    {
                        "name": "C. -L. Kuo"
                    },
                    {
                        "name": "A. T. Lee"
                    },
                    {
                        "name": "K. Levy"
                    },
                    {
                        "name": "A. E. Lowitz"
                    },
                    {
                        "name": "C. Lu"
                    },
                    {
                        "name": "A. Maniyar"
                    },
                    {
                        "name": "E. S. Martsen"
                    },
                    {
                        "name": "F. Menanteau"
                    },
                    {
                        "name": "J. Montgomery"
                    },
                    {
                        "name": "Y. Nakato"
                    },
                    {
                        "name": "T. Natoli"
                    },
                    {
                        "name": "G. I. Noble"
                    },
                    {
                        "name": "Z. Pan"
                    },
                    {
                        "name": "P. Paschos"
                    },
                    {
                        "name": "K. A. Phadke"
                    },
                    {
                        "name": "A. W. Pollak"
                    },
                    {
                        "name": "K. Prabhu"
                    },
                    {
                        "name": "M. Rahimi"
                    },
                    {
                        "name": "A. Rahlin"
                    },
                    {
                        "name": "C. L. Reichardt"
                    },
                    {
                        "name": "D. Riebel"
                    },
                    {
                        "name": "M. Rouble"
                    },
                    {
                        "name": "J. E. Ruhl"
                    },
                    {
                        "name": "E. Schiappucci"
                    },
                    {
                        "name": "J. A. Sobrin"
                    },
                    {
                        "name": "A. A. Stark"
                    },
                    {
                        "name": "J. Stephen"
                    },
                    {
                        "name": "C. Tandoi"
                    },
                    {
                        "name": "B. Thorne"
                    },
                    {
                        "name": "C. Trendafilova"
                    },
                    {
                        "name": "C. Umilta"
                    },
                    {
                        "name": "J. D. Vieira"
                    },
                    {
                        "name": "A. Vitrier"
                    },
                    {
                        "name": "Y. Wan"
                    },
                    {
                        "name": "N. Whitehorn"
                    },
                    {
                        "name": "W. L. K. Wu"
                    },
                    {
                        "name": "M. R. Young"
                    },
                    {
                        "name": "J. A. Zebrowski"
                    }
                ],
                "author_detail": {
                    "name": "J. A. Zebrowski"
                },
                "author": "J. A. Zebrowski",
                "arxiv_comment": "28 pages, 21 figures + appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06000v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06000v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16638v1",
                "updated": "2024-11-25T18:15:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    15,
                    15,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T18:15:15Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    15,
                    15,
                    0,
                    330,
                    0
                ],
                "title": "Do Automatic Factuality Metrics Measure Factuality? A Critical\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Automatic Factuality Metrics Measure Factuality? A Critical\n  Evaluation"
                },
                "summary": "Modern LLMs can now produce highly readable abstractive summaries, to the\npoint where traditional automated metrics for evaluating summary quality, such\nas ROUGE, have become saturated. However, LLMs still sometimes introduce\nunwanted content into summaries, i.e., information inconsistent with or\nunsupported by their source. Measuring the occurrence of these often subtle\n``hallucinations'' automatically has proved to be challenging. This in turn has\nmotivated development of a variety of metrics intended to measure the factual\nconsistency of generated summaries against their source. But are these\napproaches measuring what they purport to do? In this work, we stress-test\nautomatic factuality metrics. Specifically, we investigate whether and to what\ndegree superficial attributes of summary texts suffice to predict\n``factuality'', finding that a (supervised) model using only such shallow\nfeatures is reasonably competitive with SOTA factuality scoring methods. We\nthen evaluate how factuality metrics respond to factual corrections in\ninconsistent summaries and find that only a few show meaningful improvements.\nIn contrast, some metrics are more sensitive to benign, non-factual edits.\nMotivated by these insights, we show that one can ``game'' (most) automatic\nfactuality metrics, i.e., reliably inflate ``factuality'' scores by appending\ninnocuous sentences to generated summaries.Taken together, our results raise\nquestions about the degree to which we should rely on existing automated\nfactuality metrics and what exactly we want ``factuality metrics'' to measure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLMs can now produce highly readable abstractive summaries, to the\npoint where traditional automated metrics for evaluating summary quality, such\nas ROUGE, have become saturated. However, LLMs still sometimes introduce\nunwanted content into summaries, i.e., information inconsistent with or\nunsupported by their source. Measuring the occurrence of these often subtle\n``hallucinations'' automatically has proved to be challenging. This in turn has\nmotivated development of a variety of metrics intended to measure the factual\nconsistency of generated summaries against their source. But are these\napproaches measuring what they purport to do? In this work, we stress-test\nautomatic factuality metrics. Specifically, we investigate whether and to what\ndegree superficial attributes of summary texts suffice to predict\n``factuality'', finding that a (supervised) model using only such shallow\nfeatures is reasonably competitive with SOTA factuality scoring methods. We\nthen evaluate how factuality metrics respond to factual corrections in\ninconsistent summaries and find that only a few show meaningful improvements.\nIn contrast, some metrics are more sensitive to benign, non-factual edits.\nMotivated by these insights, we show that one can ``game'' (most) automatic\nfactuality metrics, i.e., reliably inflate ``factuality'' scores by appending\ninnocuous sentences to generated summaries.Taken together, our results raise\nquestions about the degree to which we should rely on existing automated\nfactuality metrics and what exactly we want ``factuality metrics'' to measure."
                },
                "authors": [
                    {
                        "name": "Sanjana Ramprasad"
                    },
                    {
                        "name": "Byron C. Wallace"
                    }
                ],
                "author_detail": {
                    "name": "Byron C. Wallace"
                },
                "author": "Byron C. Wallace",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16627v1",
                "updated": "2024-11-25T18:03:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    3,
                    50,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T18:03:50Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    3,
                    50,
                    0,
                    330,
                    0
                ],
                "title": "Inference-Time Policy Steering through Human Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Policy Steering through Human Interactions"
                },
                "summary": "Generative policies trained with human demonstrations can autonomously\naccomplish multimodal, long-horizon tasks. However, during inference, humans\nare often removed from the policy execution loop, limiting the ability to guide\na pre-trained policy towards a specific sub-goal or trajectory shape among\nmultiple predictions. Naive human intervention may inadvertently exacerbate\ndistribution shift, leading to constraint violations or execution failures. To\nbetter align policy output with human intent without inducing\nout-of-distribution errors, we propose an Inference-Time Policy Steering (ITPS)\nframework that leverages human interactions to bias the generative sampling\nprocess, rather than fine-tuning the policy on interaction data. We evaluate\nITPS across three simulated and real-world benchmarks, testing three forms of\nhuman interaction and associated alignment distance metrics. Among six sampling\nstrategies, our proposed stochastic sampling with diffusion policy achieves the\nbest trade-off between alignment and distribution shift. Videos are available\nat https://yanweiw.github.io/itps/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative policies trained with human demonstrations can autonomously\naccomplish multimodal, long-horizon tasks. However, during inference, humans\nare often removed from the policy execution loop, limiting the ability to guide\na pre-trained policy towards a specific sub-goal or trajectory shape among\nmultiple predictions. Naive human intervention may inadvertently exacerbate\ndistribution shift, leading to constraint violations or execution failures. To\nbetter align policy output with human intent without inducing\nout-of-distribution errors, we propose an Inference-Time Policy Steering (ITPS)\nframework that leverages human interactions to bias the generative sampling\nprocess, rather than fine-tuning the policy on interaction data. We evaluate\nITPS across three simulated and real-world benchmarks, testing three forms of\nhuman interaction and associated alignment distance metrics. Among six sampling\nstrategies, our proposed stochastic sampling with diffusion policy achieves the\nbest trade-off between alignment and distribution shift. Videos are available\nat https://yanweiw.github.io/itps/."
                },
                "authors": [
                    {
                        "name": "Yanwei Wang"
                    },
                    {
                        "name": "Lirui Wang"
                    },
                    {
                        "name": "Yilun Du"
                    },
                    {
                        "name": "Balakumar Sundaralingam"
                    },
                    {
                        "name": "Xuning Yang"
                    },
                    {
                        "name": "Yu-Wei Chao"
                    },
                    {
                        "name": "Claudia Perez-D'Arpino"
                    },
                    {
                        "name": "Dieter Fox"
                    },
                    {
                        "name": "Julie Shah"
                    }
                ],
                "author_detail": {
                    "name": "Julie Shah"
                },
                "author": "Julie Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19631v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19631v2",
                "updated": "2024-11-25T17:51:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    17,
                    51,
                    33,
                    0,
                    330,
                    0
                ],
                "published": "2024-10-25T15:34:03Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    15,
                    34,
                    3,
                    4,
                    299,
                    0
                ],
                "title": "Efficient Biological Data Acquisition through Inference Set Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Biological Data Acquisition through Inference Set Design"
                },
                "summary": "In drug discovery, highly automated high-throughput laboratories are used to\nscreen a large number of compounds in search of effective drugs. These\nexperiments are expensive, so one might hope to reduce their cost by\nexperimenting on a subset of the compounds, and predicting the outcomes of the\nremaining experiments. In this work, we model this scenario as a sequential\nsubset selection problem: we aim to select the smallest set of candidates in\norder to achieve some desired level of accuracy for the system as a whole. Our\nkey observation is that, if there is heterogeneity in the difficulty of the\nprediction problem across the input space, selectively obtaining the labels for\nthe hardest examples in the acquisition pool will leave only the relatively\neasy examples to remain in the inference set, leading to better overall system\nperformance. We call this mechanism inference set design, and propose the use\nof a confidence-based active learning solution to prune out these challenging\nexamples. Our algorithm includes an explicit stopping criterion that stops\nrunning the experiments when it is sufficiently confident that the system has\nreached the target performance. Our empirical studies on image and molecular\ndatasets, as well as a real-world large-scale biological assay, show that\nactive learning for inference set design leads to significant reduction in\nexperimental cost while retaining high system performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In drug discovery, highly automated high-throughput laboratories are used to\nscreen a large number of compounds in search of effective drugs. These\nexperiments are expensive, so one might hope to reduce their cost by\nexperimenting on a subset of the compounds, and predicting the outcomes of the\nremaining experiments. In this work, we model this scenario as a sequential\nsubset selection problem: we aim to select the smallest set of candidates in\norder to achieve some desired level of accuracy for the system as a whole. Our\nkey observation is that, if there is heterogeneity in the difficulty of the\nprediction problem across the input space, selectively obtaining the labels for\nthe hardest examples in the acquisition pool will leave only the relatively\neasy examples to remain in the inference set, leading to better overall system\nperformance. We call this mechanism inference set design, and propose the use\nof a confidence-based active learning solution to prune out these challenging\nexamples. Our algorithm includes an explicit stopping criterion that stops\nrunning the experiments when it is sufficiently confident that the system has\nreached the target performance. Our empirical studies on image and molecular\ndatasets, as well as a real-world large-scale biological assay, show that\nactive learning for inference set design leads to significant reduction in\nexperimental cost while retaining high system performance."
                },
                "authors": [
                    {
                        "name": "Ihor Neporozhnii"
                    },
                    {
                        "name": "Julien Roy"
                    },
                    {
                        "name": "Emmanuel Bengio"
                    },
                    {
                        "name": "Jason Hartford"
                    }
                ],
                "author_detail": {
                    "name": "Jason Hartford"
                },
                "author": "Jason Hartford",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19631v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19631v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08509v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08509v2",
                "updated": "2024-11-25T17:35:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    17,
                    35,
                    7,
                    0,
                    330,
                    0
                ],
                "published": "2024-04-12T14:46:15Z",
                "published_parsed": [
                    2024,
                    4,
                    12,
                    14,
                    46,
                    15,
                    4,
                    103,
                    0
                ],
                "title": "Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\n  Prediction"
                },
                "summary": "Large language models (LLMs) have been driving a new wave of interactive AI\napplications across numerous domains. However, efficiently serving LLM\ninference requests is challenging due to their unpredictable execution times\noriginating from the autoregressive nature of generative models. Existing LLM\nserving systems exploit first-come-first-serve (FCFS) scheduling, suffering\nfrom head-of-line blocking issues. To address the non-deterministic nature of\nLLMs and enable efficient interactive LLM serving, we present a speculative\nshortest-job-first (SSJF) scheduler that uses a light proxy model to predict\nLLM output sequence lengths. Our open-source SSJF implementation does not\nrequire changes to memory management or batching strategies. Evaluations on\nreal-world datasets and production workload traces show that SSJF reduces\naverage job completion times by 30.5-39.6% and increases throughput by 2.2-3.6x\ncompared to FCFS schedulers, across no batching, dynamic batching, and\ncontinuous batching settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been driving a new wave of interactive AI\napplications across numerous domains. However, efficiently serving LLM\ninference requests is challenging due to their unpredictable execution times\noriginating from the autoregressive nature of generative models. Existing LLM\nserving systems exploit first-come-first-serve (FCFS) scheduling, suffering\nfrom head-of-line blocking issues. To address the non-deterministic nature of\nLLMs and enable efficient interactive LLM serving, we present a speculative\nshortest-job-first (SSJF) scheduler that uses a light proxy model to predict\nLLM output sequence lengths. Our open-source SSJF implementation does not\nrequire changes to memory management or batching strategies. Evaluations on\nreal-world datasets and production workload traces show that SSJF reduces\naverage job completion times by 30.5-39.6% and increases throughput by 2.2-3.6x\ncompared to FCFS schedulers, across no batching, dynamic batching, and\ncontinuous batching settings."
                },
                "authors": [
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Weichao Mao"
                    },
                    {
                        "name": "Archit Patke"
                    },
                    {
                        "name": "Shengkun Cui"
                    },
                    {
                        "name": "Saurabh Jha"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Hubertus Franke"
                    },
                    {
                        "name": "Zbigniew T. Kalbarczyk"
                    },
                    {
                        "name": "Tamer Baar"
                    },
                    {
                        "name": "Ravishankar K. Iyer"
                    }
                ],
                "author_detail": {
                    "name": "Ravishankar K. Iyer"
                },
                "author": "Ravishankar K. Iyer",
                "arxiv_comment": "Accepted at AIOps'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08509v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08509v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16602v1",
                "updated": "2024-11-25T17:31:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    17,
                    31,
                    57,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T17:31:57Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    17,
                    31,
                    57,
                    0,
                    330,
                    0
                ],
                "title": "Chat2SVG: Vector Graphics Generation with Large Language Models and\n  Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chat2SVG: Vector Graphics Generation with Large Language Models and\n  Image Diffusion Models"
                },
                "summary": "Scalable Vector Graphics (SVG) has become the de facto standard for vector\ngraphics in digital design, offering resolution independence and precise\ncontrol over individual elements. Despite their advantages, creating\nhigh-quality SVG content remains challenging, as it demands technical expertise\nwith professional editing software and a considerable time investment to craft\ncomplex shapes. Recent text-to-SVG generation methods aim to make vector\ngraphics creation more accessible, but they still encounter limitations in\nshape regularity, generalization ability, and expressiveness. To address these\nchallenges, we introduce Chat2SVG, a hybrid framework that combines the\nstrengths of Large Language Models (LLMs) and image diffusion models for\ntext-to-SVG generation. Our approach first uses an LLM to generate semantically\nmeaningful SVG templates from basic geometric primitives. Guided by image\ndiffusion models, a dual-stage optimization pipeline refines paths in latent\nspace and adjusts point coordinates to enhance geometric complexity. Extensive\nexperiments show that Chat2SVG outperforms existing methods in visual fidelity,\npath regularity, and semantic alignment. Additionally, our system enables\nintuitive editing through natural language instructions, making professional\nvector graphics creation accessible to all users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Vector Graphics (SVG) has become the de facto standard for vector\ngraphics in digital design, offering resolution independence and precise\ncontrol over individual elements. Despite their advantages, creating\nhigh-quality SVG content remains challenging, as it demands technical expertise\nwith professional editing software and a considerable time investment to craft\ncomplex shapes. Recent text-to-SVG generation methods aim to make vector\ngraphics creation more accessible, but they still encounter limitations in\nshape regularity, generalization ability, and expressiveness. To address these\nchallenges, we introduce Chat2SVG, a hybrid framework that combines the\nstrengths of Large Language Models (LLMs) and image diffusion models for\ntext-to-SVG generation. Our approach first uses an LLM to generate semantically\nmeaningful SVG templates from basic geometric primitives. Guided by image\ndiffusion models, a dual-stage optimization pipeline refines paths in latent\nspace and adjusts point coordinates to enhance geometric complexity. Extensive\nexperiments show that Chat2SVG outperforms existing methods in visual fidelity,\npath regularity, and semantic alignment. Additionally, our system enables\nintuitive editing through natural language instructions, making professional\nvector graphics creation accessible to all users."
                },
                "authors": [
                    {
                        "name": "Ronghuan Wu"
                    },
                    {
                        "name": "Wanchao Su"
                    },
                    {
                        "name": "Jing Liao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Liao"
                },
                "author": "Jing Liao",
                "arxiv_comment": "Project Page: https://chat2svg.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16594v1",
                "updated": "2024-11-25T17:28:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    17,
                    28,
                    44,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T17:28:44Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    17,
                    28,
                    44,
                    0,
                    330,
                    0
                ],
                "title": "From Generation to Judgment: Opportunities and Challenges of\n  LLM-as-a-judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Generation to Judgment: Opportunities and Challenges of\n  LLM-as-a-judge"
                },
                "summary": "Assessment and evaluation have long been critical challenges in artificial\nintelligence (AI) and natural language processing (NLP). However, traditional\nmethods, whether matching-based or embedding-based, often fall short of judging\nsubtle attributes and delivering satisfactory results. Recent advancements in\nLarge Language Models (LLMs) inspire the \"LLM-as-a-judge\" paradigm, where LLMs\nare leveraged to perform scoring, ranking, or selection across various tasks\nand applications. This paper provides a comprehensive survey of LLM-based\njudgment and assessment, offering an in-depth overview to advance this emerging\nfield. We begin by giving detailed definitions from both input and output\nperspectives. Then we introduce a comprehensive taxonomy to explore\nLLM-as-a-judge from three dimensions: what to judge, how to judge and where to\njudge. Finally, we compile benchmarks for evaluating LLM-as-a-judge and\nhighlight key challenges and promising directions, aiming to provide valuable\ninsights and inspire future research in this promising research area. Paper\nlist and more resources about LLM-as-a-judge can be found at\n\\url{https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge} and\n\\url{https://llm-as-a-judge.github.io}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessment and evaluation have long been critical challenges in artificial\nintelligence (AI) and natural language processing (NLP). However, traditional\nmethods, whether matching-based or embedding-based, often fall short of judging\nsubtle attributes and delivering satisfactory results. Recent advancements in\nLarge Language Models (LLMs) inspire the \"LLM-as-a-judge\" paradigm, where LLMs\nare leveraged to perform scoring, ranking, or selection across various tasks\nand applications. This paper provides a comprehensive survey of LLM-based\njudgment and assessment, offering an in-depth overview to advance this emerging\nfield. We begin by giving detailed definitions from both input and output\nperspectives. Then we introduce a comprehensive taxonomy to explore\nLLM-as-a-judge from three dimensions: what to judge, how to judge and where to\njudge. Finally, we compile benchmarks for evaluating LLM-as-a-judge and\nhighlight key challenges and promising directions, aiming to provide valuable\ninsights and inspire future research in this promising research area. Paper\nlist and more resources about LLM-as-a-judge can be found at\n\\url{https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge} and\n\\url{https://llm-as-a-judge.github.io}."
                },
                "authors": [
                    {
                        "name": "Dawei Li"
                    },
                    {
                        "name": "Bohan Jiang"
                    },
                    {
                        "name": "Liangjie Huang"
                    },
                    {
                        "name": "Alimohammad Beigi"
                    },
                    {
                        "name": "Chengshuai Zhao"
                    },
                    {
                        "name": "Zhen Tan"
                    },
                    {
                        "name": "Amrita Bhattacharjee"
                    },
                    {
                        "name": "Yuxuan Jiang"
                    },
                    {
                        "name": "Canyu Chen"
                    },
                    {
                        "name": "Tianhao Wu"
                    },
                    {
                        "name": "Kai Shu"
                    },
                    {
                        "name": "Lu Cheng"
                    },
                    {
                        "name": "Huan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huan Liu"
                },
                "author": "Huan Liu",
                "arxiv_comment": "32 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16595v1",
                "updated": "2024-11-25T17:28:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    17,
                    28,
                    44,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T17:28:44Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    17,
                    28,
                    44,
                    0,
                    330,
                    0
                ],
                "title": "Location-Based Service (LBS) Data Quality Metrics and Effects on\n  Mobility Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Location-Based Service (LBS) Data Quality Metrics and Effects on\n  Mobility Inference"
                },
                "summary": "Today, GPS-equipped mobile devices are ubiquitous, and they generate\nLocation-Based Service (LBS) data, which has become a critical resource for\nunderstanding human mobility. However, inherent limitations in LBS datasets,\nprimarily characterized by discontinuity and sparsity, may introduce\nsignificant biases in representing individual movement patterns. This study\ndevelops data quality metrics for LBS data, examines their disparities among\ndifferent populations, and quantifies their effects on inferred individual\nmovement, stays in particular, in the Boston Metropolitan Area. We find that\ndata from higher-income, more educated, and predominantly white census block\ngroups (CBGs) show higher sampling rates but paradoxically lower data quality.\nThis contradiction may stem from greater privacy awareness in these\ncommunities. Additionally, we propose a new framework to resample LBS data and\nquantitatively evaluate the inferential biases associated with data of varying\nquality. This versatile framework can analyze the impacts originating from\ndifferent data processing workflows with LBS data. Using linear regression\nmodels with clustered standard error, we assess the impact of data quality\nmetrics on inferring the number of stay points. The results show that better\ndata quality, characterized by the number of observations and temporal\noccupancy, can significantly reduce the bias when calculating the stay points\nof an individual. The introduction of additional data quality metrics into the\nregression model can further explain the bias. Overall, this study provides\ninsights into how data quality can influence our understanding of human\nmobility patterns, highlighting the importance of carefully handling LBS data\nin research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today, GPS-equipped mobile devices are ubiquitous, and they generate\nLocation-Based Service (LBS) data, which has become a critical resource for\nunderstanding human mobility. However, inherent limitations in LBS datasets,\nprimarily characterized by discontinuity and sparsity, may introduce\nsignificant biases in representing individual movement patterns. This study\ndevelops data quality metrics for LBS data, examines their disparities among\ndifferent populations, and quantifies their effects on inferred individual\nmovement, stays in particular, in the Boston Metropolitan Area. We find that\ndata from higher-income, more educated, and predominantly white census block\ngroups (CBGs) show higher sampling rates but paradoxically lower data quality.\nThis contradiction may stem from greater privacy awareness in these\ncommunities. Additionally, we propose a new framework to resample LBS data and\nquantitatively evaluate the inferential biases associated with data of varying\nquality. This versatile framework can analyze the impacts originating from\ndifferent data processing workflows with LBS data. Using linear regression\nmodels with clustered standard error, we assess the impact of data quality\nmetrics on inferring the number of stay points. The results show that better\ndata quality, characterized by the number of observations and temporal\noccupancy, can significantly reduce the bias when calculating the stay points\nof an individual. The introduction of additional data quality metrics into the\nregression model can further explain the bias. Overall, this study provides\ninsights into how data quality can influence our understanding of human\nmobility patterns, highlighting the importance of carefully handling LBS data\nin research."
                },
                "authors": [
                    {
                        "name": "Xinhua Wu"
                    },
                    {
                        "name": "Yanchao Wang"
                    },
                    {
                        "name": "Ekin Ugurel"
                    },
                    {
                        "name": "Cynthia Chen"
                    },
                    {
                        "name": "Shuai Huang"
                    },
                    {
                        "name": "Qi R. Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qi R. Wang"
                },
                "author": "Qi R. Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16587v1",
                "updated": "2024-11-25T17:22:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    17,
                    22,
                    10,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T17:22:10Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    17,
                    22,
                    10,
                    0,
                    330,
                    0
                ],
                "title": "Large Language Model-based Decision-making for COLREGs and the Control\n  of Autonomous Surface Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-based Decision-making for COLREGs and the Control\n  of Autonomous Surface Vehicles"
                },
                "summary": "In the field of autonomous surface vehicles (ASVs), devising decision-making\nand obstacle avoidance solutions that address maritime COLREGs (Collision\nRegulations), primarily defined for human operators, has long been a pressing\nchallenge. Recent advancements in explainable Artificial Intelligence (AI) and\nmachine learning have shown promise in enabling human-like decision-making.\nNotably, significant developments have occurred in the application of Large\nLanguage Models (LLMs) to the decision-making of complex systems, such as\nself-driving cars. The textual and somewhat ambiguous nature of COLREGs (from\nan algorithmic perspective), however, poses challenges that align well with the\ncapabilities of LLMs, suggesting that LLMs may become increasingly suitable for\nthis application soon. This paper presents and demonstrates the first\napplication of LLM-based decision-making and control for ASVs. The proposed\nmethod establishes a high-level decision-maker that uses online collision risk\nindices and key measurements to make decisions for safe manoeuvres. A tailored\ndesign and runtime structure is developed to support training and real-time\naction generation on a realistic ASV model. Local planning and control\nalgorithms are integrated to execute the commands for waypoint following and\ncollision avoidance at a lower level. To the authors' knowledge, this study\nrepresents the first attempt to apply explainable AI to the dynamic control\nproblem of maritime systems recognising the COLREGs rules, opening new avenues\nfor research in this challenging area. Results obtained across multiple test\nscenarios demonstrate the system's ability to maintain online COLREGs\ncompliance, accurate waypoint tracking, and feasible control, while providing\nhuman-interpretable reasoning for each decision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of autonomous surface vehicles (ASVs), devising decision-making\nand obstacle avoidance solutions that address maritime COLREGs (Collision\nRegulations), primarily defined for human operators, has long been a pressing\nchallenge. Recent advancements in explainable Artificial Intelligence (AI) and\nmachine learning have shown promise in enabling human-like decision-making.\nNotably, significant developments have occurred in the application of Large\nLanguage Models (LLMs) to the decision-making of complex systems, such as\nself-driving cars. The textual and somewhat ambiguous nature of COLREGs (from\nan algorithmic perspective), however, poses challenges that align well with the\ncapabilities of LLMs, suggesting that LLMs may become increasingly suitable for\nthis application soon. This paper presents and demonstrates the first\napplication of LLM-based decision-making and control for ASVs. The proposed\nmethod establishes a high-level decision-maker that uses online collision risk\nindices and key measurements to make decisions for safe manoeuvres. A tailored\ndesign and runtime structure is developed to support training and real-time\naction generation on a realistic ASV model. Local planning and control\nalgorithms are integrated to execute the commands for waypoint following and\ncollision avoidance at a lower level. To the authors' knowledge, this study\nrepresents the first attempt to apply explainable AI to the dynamic control\nproblem of maritime systems recognising the COLREGs rules, opening new avenues\nfor research in this challenging area. Results obtained across multiple test\nscenarios demonstrate the system's ability to maintain online COLREGs\ncompliance, accurate waypoint tracking, and feasible control, while providing\nhuman-interpretable reasoning for each decision."
                },
                "authors": [
                    {
                        "name": "Klinsmann Agyei"
                    },
                    {
                        "name": "Pouria Sarhadi"
                    },
                    {
                        "name": "Wasif Naeem"
                    }
                ],
                "author_detail": {
                    "name": "Wasif Naeem"
                },
                "author": "Wasif Naeem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16579v1",
                "updated": "2024-11-25T17:11:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    17,
                    11,
                    54,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T17:11:54Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    17,
                    11,
                    54,
                    0,
                    330,
                    0
                ],
                "title": "Enhancing LLM Reasoning via Critique Models with Test-Time and\n  Training-Time Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Reasoning via Critique Models with Test-Time and\n  Training-Time Supervision"
                },
                "summary": "Training large language models (LLMs) to spend more time thinking and\nreflection before responding is crucial for effectively solving complex\nreasoning tasks in fields such as science, coding, and mathematics. However,\nthe effectiveness of mechanisms like self-reflection and self-correction\ndepends on the model's capacity to accurately assess its own performance, which\ncan be limited by factors such as initial accuracy, question difficulty, and\nthe lack of external feedback. In this paper, we delve into a two-player\nparadigm that separates the roles of reasoning and critique models, where the\ncritique model provides step-level feedback to supervise the reasoning (actor)\nmodel during both test-time and train-time. We first propose AutoMathCritique,\nan automated and scalable framework for collecting critique data, resulting in\na dataset of $76,321$ responses paired with step-level feedback. Fine-tuning\nlanguage models with this dataset enables them to generate natural language\nfeedback for mathematical reasoning. We demonstrate that the critique models\nconsistently improve the actor's performance on difficult queries at test-time,\nespecially when scaling up inference-time computation. Motivated by these\nfindings, we introduce the critique-based supervision to the actor's\nself-training process, and propose a critique-in-the-loop self-improvement\nmethod. Experiments show that the method improves the actor's exploration\nefficiency and solution diversity, especially on challenging queries, leading\nto a stronger reasoning model. Lastly, we take the preliminary step to explore\ntraining self-talk reasoning models via critique supervision and showcase its\npotential. Our code and datasets are at\n\\href{https://mathcritique.github.io/}{https://mathcritique.github.io/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) to spend more time thinking and\nreflection before responding is crucial for effectively solving complex\nreasoning tasks in fields such as science, coding, and mathematics. However,\nthe effectiveness of mechanisms like self-reflection and self-correction\ndepends on the model's capacity to accurately assess its own performance, which\ncan be limited by factors such as initial accuracy, question difficulty, and\nthe lack of external feedback. In this paper, we delve into a two-player\nparadigm that separates the roles of reasoning and critique models, where the\ncritique model provides step-level feedback to supervise the reasoning (actor)\nmodel during both test-time and train-time. We first propose AutoMathCritique,\nan automated and scalable framework for collecting critique data, resulting in\na dataset of $76,321$ responses paired with step-level feedback. Fine-tuning\nlanguage models with this dataset enables them to generate natural language\nfeedback for mathematical reasoning. We demonstrate that the critique models\nconsistently improve the actor's performance on difficult queries at test-time,\nespecially when scaling up inference-time computation. Motivated by these\nfindings, we introduce the critique-based supervision to the actor's\nself-training process, and propose a critique-in-the-loop self-improvement\nmethod. Experiments show that the method improves the actor's exploration\nefficiency and solution diversity, especially on challenging queries, leading\nto a stronger reasoning model. Lastly, we take the preliminary step to explore\ntraining self-talk reasoning models via critique supervision and showcase its\npotential. Our code and datasets are at\n\\href{https://mathcritique.github.io/}{https://mathcritique.github.io/}."
                },
                "authors": [
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Dingwen Yang"
                    },
                    {
                        "name": "Jixuan Huang"
                    },
                    {
                        "name": "Jiafu Tang"
                    },
                    {
                        "name": "Guanyu Li"
                    },
                    {
                        "name": "Yiwen Ding"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Boyang Hong"
                    },
                    {
                        "name": "Shihan Do"
                    },
                    {
                        "name": "Wenyu Zhan"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Rui Zheng"
                    },
                    {
                        "name": "Tao Ji"
                    },
                    {
                        "name": "Xiaowei Shi"
                    },
                    {
                        "name": "Yitao Zhai"
                    },
                    {
                        "name": "Rongxiang Weng"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Zuxuan Wu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Gang Jiang"
                },
                "author": "Yu-Gang Jiang",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07610v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07610v2",
                "updated": "2024-11-25T17:01:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    17,
                    1,
                    53,
                    0,
                    330,
                    0
                ],
                "published": "2024-10-10T04:54:37Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    4,
                    54,
                    37,
                    3,
                    284,
                    0
                ],
                "title": "CSA: Data-efficient Mapping of Unimodal Features to Multimodal Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSA: Data-efficient Mapping of Unimodal Features to Multimodal Features"
                },
                "summary": "Multimodal encoders like CLIP excel in tasks such as zero-shot image\nclassification and cross-modal retrieval. However, they require excessive\ntraining data. We propose canonical similarity analysis (CSA), which uses two\nunimodal encoders to replicate multimodal encoders using limited data. CSA maps\nunimodal features into a multimodal space, using a new similarity score to\nretain only the multimodal information. CSA only involves the inference of\nunimodal encoders and a cubic-complexity matrix decomposition, eliminating the\nneed for extensive GPU-based model training. Experiments show that CSA\noutperforms CLIP while requiring $300,000\\times$ fewer multimodal data pairs\nand $6\\times$ fewer unimodal data for ImageNet classification and\nmisinformative news captions detection. CSA surpasses the state-of-the-art\nmethod to map unimodal features to multimodal features. We also demonstrate the\nability of CSA with modalities beyond image and text, paving the way for future\nmodality pairs with limited paired multimodal data but abundant unpaired\nunimodal data, such as lidar and text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal encoders like CLIP excel in tasks such as zero-shot image\nclassification and cross-modal retrieval. However, they require excessive\ntraining data. We propose canonical similarity analysis (CSA), which uses two\nunimodal encoders to replicate multimodal encoders using limited data. CSA maps\nunimodal features into a multimodal space, using a new similarity score to\nretain only the multimodal information. CSA only involves the inference of\nunimodal encoders and a cubic-complexity matrix decomposition, eliminating the\nneed for extensive GPU-based model training. Experiments show that CSA\noutperforms CLIP while requiring $300,000\\times$ fewer multimodal data pairs\nand $6\\times$ fewer unimodal data for ImageNet classification and\nmisinformative news captions detection. CSA surpasses the state-of-the-art\nmethod to map unimodal features to multimodal features. We also demonstrate the\nability of CSA with modalities beyond image and text, paving the way for future\nmodality pairs with limited paired multimodal data but abundant unpaired\nunimodal data, such as lidar and text."
                },
                "authors": [
                    {
                        "name": "Po-han Li"
                    },
                    {
                        "name": "Sandeep P. Chinchali"
                    },
                    {
                        "name": "Ufuk Topcu"
                    }
                ],
                "author_detail": {
                    "name": "Ufuk Topcu"
                },
                "author": "Ufuk Topcu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07610v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07610v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10371v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10371v2",
                "updated": "2024-11-25T16:55:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    16,
                    55,
                    9,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-15T17:19:42Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    17,
                    19,
                    42,
                    4,
                    320,
                    0
                ],
                "title": "A Survey of Event Causality Identification: Principles, Taxonomy,\n  Challenges, and Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Event Causality Identification: Principles, Taxonomy,\n  Challenges, and Assessment"
                },
                "summary": "Event Causality Identification (ECI) has become a crucial task in Natural\nLanguage Processing (NLP), aimed at automatically extracting causalities from\ntextual data. In this survey, we systematically address the foundational\nprinciples, technical frameworks, and challenges of ECI, offering a\ncomprehensive taxonomy to categorize and clarify current research\nmethodologies, as well as a quantitative assessment of existing models. We\nfirst establish a conceptual framework for ECI, outlining key definitions,\nproblem formulations, and evaluation standards. Our taxonomy classifies ECI\nmethods according to the two primary tasks of sentence-level (SECI) and\ndocument-level (DECI) event causality identification. For SECI, we examine\nfeature pattern-based matching, deep semantic encoding, causal knowledge\npre-training and prompt-based fine-tuning, and external knowledge enhancement\nmethods. For DECI, we highlight approaches focused on event graph reasoning and\nprompt-based techniques to address the complexity of cross-sentence causal\ninference. Additionally, we analyze the strengths, limitations, and open\nchallenges of each approach. We further conduct an extensive quantitative\nevaluation of various ECI methods on two benchmark datasets. Finally, we\nexplore future research directions, highlighting promising pathways to overcome\ncurrent limitations and broaden ECI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event Causality Identification (ECI) has become a crucial task in Natural\nLanguage Processing (NLP), aimed at automatically extracting causalities from\ntextual data. In this survey, we systematically address the foundational\nprinciples, technical frameworks, and challenges of ECI, offering a\ncomprehensive taxonomy to categorize and clarify current research\nmethodologies, as well as a quantitative assessment of existing models. We\nfirst establish a conceptual framework for ECI, outlining key definitions,\nproblem formulations, and evaluation standards. Our taxonomy classifies ECI\nmethods according to the two primary tasks of sentence-level (SECI) and\ndocument-level (DECI) event causality identification. For SECI, we examine\nfeature pattern-based matching, deep semantic encoding, causal knowledge\npre-training and prompt-based fine-tuning, and external knowledge enhancement\nmethods. For DECI, we highlight approaches focused on event graph reasoning and\nprompt-based techniques to address the complexity of cross-sentence causal\ninference. Additionally, we analyze the strengths, limitations, and open\nchallenges of each approach. We further conduct an extensive quantitative\nevaluation of various ECI methods on two benchmark datasets. Finally, we\nexplore future research directions, highlighting promising pathways to overcome\ncurrent limitations and broaden ECI applications."
                },
                "authors": [
                    {
                        "name": "Qing Cheng"
                    },
                    {
                        "name": "Zefan Zeng"
                    },
                    {
                        "name": "Xingchen Hu"
                    },
                    {
                        "name": "Yuehang Si"
                    },
                    {
                        "name": "Zhong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhong Liu"
                },
                "author": "Zhong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10371v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10371v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16569v1",
                "updated": "2024-11-25T16:53:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    16,
                    53,
                    22,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T16:53:22Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    16,
                    53,
                    22,
                    0,
                    330,
                    0
                ],
                "title": "Predictive Power of LLMs in Financial Markets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive Power of LLMs in Financial Markets"
                },
                "summary": "Predicting the movement of the stock market and other assets has been\nvaluable over the past few decades. Knowing how the value of a certain sector\nmarket may move in the future provides much information for investors, as they\nuse that information to develop strategies to maximize profit or minimize risk.\nHowever, market data are quite noisy, and it is challenging to choose the right\ndata or the right model to create such predictions. With the rise of large\nlanguage models, there are ways to analyze certain data much more efficiently\nthan before.\n  Our goal is to determine whether the GPT model provides more useful\ninformation compared to other traditional transformer models, such as the BERT\nmodel. We shall use data from the Federal Reserve Beige Book, which provides\nsummaries of economic conditions in different districts in the US. Using such\ndata, we then employ the LLM's to make predictions on the correlations. Using\nthese correlations, we then compare the results with well-known strategies and\ndetermine whether knowing the economic conditions improves investment\ndecisions. We conclude that the Beige Book does contain information regarding\ncorrelations amongst different assets, yet the GPT model has too much\nlook-ahead bias and that traditional models still triumph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting the movement of the stock market and other assets has been\nvaluable over the past few decades. Knowing how the value of a certain sector\nmarket may move in the future provides much information for investors, as they\nuse that information to develop strategies to maximize profit or minimize risk.\nHowever, market data are quite noisy, and it is challenging to choose the right\ndata or the right model to create such predictions. With the rise of large\nlanguage models, there are ways to analyze certain data much more efficiently\nthan before.\n  Our goal is to determine whether the GPT model provides more useful\ninformation compared to other traditional transformer models, such as the BERT\nmodel. We shall use data from the Federal Reserve Beige Book, which provides\nsummaries of economic conditions in different districts in the US. Using such\ndata, we then employ the LLM's to make predictions on the correlations. Using\nthese correlations, we then compare the results with well-known strategies and\ndetermine whether knowing the economic conditions improves investment\ndecisions. We conclude that the Beige Book does contain information regarding\ncorrelations amongst different assets, yet the GPT model has too much\nlook-ahead bias and that traditional models still triumph."
                },
                "authors": [
                    {
                        "name": "Jerick Shi"
                    },
                    {
                        "name": "Burton Hollifield"
                    }
                ],
                "author_detail": {
                    "name": "Burton Hollifield"
                },
                "author": "Burton Hollifield",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.PM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16561v1",
                "updated": "2024-11-25T16:47:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    16,
                    47,
                    10,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T16:47:10Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    16,
                    47,
                    10,
                    0,
                    330,
                    0
                ],
                "title": "EnStack: An Ensemble Stacking Framework of Large Language Models for\n  Enhanced Vulnerability Detection in Source Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EnStack: An Ensemble Stacking Framework of Large Language Models for\n  Enhanced Vulnerability Detection in Source Code"
                },
                "summary": "Automated detection of software vulnerabilities is critical for enhancing\nsecurity, yet existing methods often struggle with the complexity and diversity\nof modern codebases. In this paper, we introduce EnStack, a novel ensemble\nstacking framework that enhances vulnerability detection using natural language\nprocessing (NLP) techniques. Our approach synergizes multiple pre-trained large\nlanguage models (LLMs) specialized in code understanding CodeBERT for semantic\nanalysis, GraphCodeBERT for structural representation, and UniXcoder for\ncross-modal capabilities. By fine-tuning these models on the Draper VDISC\ndataset and integrating their outputs through meta-classifiers such as Logistic\nRegression, Support Vector Machines (SVM), Random Forest, and XGBoost, EnStack\neffectively captures intricate code patterns and vulnerabilities that\nindividual models may overlook. The meta-classifiers consolidate the strengths\nof each LLM, resulting in a comprehensive model that excels in detecting subtle\nand complex vulnerabilities across diverse programming contexts. Experimental\nresults demonstrate that EnStack significantly outperforms existing methods,\nachieving notable improvements in accuracy, precision, recall, and F1-score.\nThis work highlights the potential of ensemble LLM approaches in code analysis\ntasks and offers valuable insights into applying NLP techniques for advancing\nautomated vulnerability detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated detection of software vulnerabilities is critical for enhancing\nsecurity, yet existing methods often struggle with the complexity and diversity\nof modern codebases. In this paper, we introduce EnStack, a novel ensemble\nstacking framework that enhances vulnerability detection using natural language\nprocessing (NLP) techniques. Our approach synergizes multiple pre-trained large\nlanguage models (LLMs) specialized in code understanding CodeBERT for semantic\nanalysis, GraphCodeBERT for structural representation, and UniXcoder for\ncross-modal capabilities. By fine-tuning these models on the Draper VDISC\ndataset and integrating their outputs through meta-classifiers such as Logistic\nRegression, Support Vector Machines (SVM), Random Forest, and XGBoost, EnStack\neffectively captures intricate code patterns and vulnerabilities that\nindividual models may overlook. The meta-classifiers consolidate the strengths\nof each LLM, resulting in a comprehensive model that excels in detecting subtle\nand complex vulnerabilities across diverse programming contexts. Experimental\nresults demonstrate that EnStack significantly outperforms existing methods,\nachieving notable improvements in accuracy, precision, recall, and F1-score.\nThis work highlights the potential of ensemble LLM approaches in code analysis\ntasks and offers valuable insights into applying NLP techniques for advancing\nautomated vulnerability detection."
                },
                "authors": [
                    {
                        "name": "Shahriyar Zaman Ridoy"
                    },
                    {
                        "name": "Md. Shazzad Hossain Shaon"
                    },
                    {
                        "name": "Alfredo Cuzzocrea"
                    },
                    {
                        "name": "Mst Shapna Akter"
                    }
                ],
                "author_detail": {
                    "name": "Mst Shapna Akter"
                },
                "author": "Mst Shapna Akter",
                "arxiv_comment": "Accepted in 2024 IEEE International Conference on Big Data (IEEE\n  BigData 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01964v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01964v2",
                "updated": "2024-11-25T16:41:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    16,
                    41,
                    1,
                    0,
                    330,
                    0
                ],
                "published": "2024-02-03T00:12:36Z",
                "published_parsed": [
                    2024,
                    2,
                    3,
                    0,
                    12,
                    36,
                    5,
                    34,
                    0
                ],
                "title": "Scalable and Efficient Temporal Graph Representation Learning via\n  Forward Recent Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable and Efficient Temporal Graph Representation Learning via\n  Forward Recent Sampling"
                },
                "summary": "Temporal graph representation learning (TGRL) is essential for modeling\ndynamic systems in real-world networks. However, traditional TGRL methods,\ndespite their effectiveness, often face significant computational challenges\nand inference delays due to the inefficient sampling of temporal neighbors.\nConventional sampling methods typically involve backtracking through the\ninteraction history of each node. In this paper, we propose a novel TGRL\nframework, No-Looking-Back (NLB), which overcomes these challenges by\nintroducing a forward recent sampling strategy. This strategy eliminates the\nneed to backtrack through historical interactions by utilizing a\nGPU-executable, size-constrained hash table for each node. The hash table\nrecords a down-sampled set of recent interactions, enabling rapid query\nresponses with minimal inference latency. The maintenance of this hash table is\nhighly efficient, operating with $O(1)$ complexity. Fully compatible with GPU\nprocessing, NLB maximizes programmability, parallelism, and power efficiency.\nEmpirical evaluations demonstrate that NLB not only matches or surpasses\nstate-of-the-art methods in accuracy for tasks like link prediction and node\nclassification across six real-world datasets but also achieves 1.32-4.40x\nfaster training, 1.2-7.94x greater energy efficiency, and 1.63-12.95x lower\ninference latency compared to competitive baselines. The link to the code:\nhttps://github.com/Graph-COM/NLB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal graph representation learning (TGRL) is essential for modeling\ndynamic systems in real-world networks. However, traditional TGRL methods,\ndespite their effectiveness, often face significant computational challenges\nand inference delays due to the inefficient sampling of temporal neighbors.\nConventional sampling methods typically involve backtracking through the\ninteraction history of each node. In this paper, we propose a novel TGRL\nframework, No-Looking-Back (NLB), which overcomes these challenges by\nintroducing a forward recent sampling strategy. This strategy eliminates the\nneed to backtrack through historical interactions by utilizing a\nGPU-executable, size-constrained hash table for each node. The hash table\nrecords a down-sampled set of recent interactions, enabling rapid query\nresponses with minimal inference latency. The maintenance of this hash table is\nhighly efficient, operating with $O(1)$ complexity. Fully compatible with GPU\nprocessing, NLB maximizes programmability, parallelism, and power efficiency.\nEmpirical evaluations demonstrate that NLB not only matches or surpasses\nstate-of-the-art methods in accuracy for tasks like link prediction and node\nclassification across six real-world datasets but also achieves 1.32-4.40x\nfaster training, 1.2-7.94x greater energy efficiency, and 1.63-12.95x lower\ninference latency compared to competitive baselines. The link to the code:\nhttps://github.com/Graph-COM/NLB."
                },
                "authors": [
                    {
                        "name": "Yuhong Luo"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "arxiv_comment": "Learning on Graphs Conference (LoG 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01964v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01964v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16554v1",
                "updated": "2024-11-25T16:38:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    16,
                    38,
                    17,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T16:38:17Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    16,
                    38,
                    17,
                    0,
                    330,
                    0
                ],
                "title": "Generating Out-Of-Distribution Scenarios Using Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Out-Of-Distribution Scenarios Using Language Models"
                },
                "summary": "The deployment of autonomous vehicles controlled by machine learning\ntechniques requires extensive testing in diverse real-world environments,\nrobust handling of edge cases and out-of-distribution scenarios, and\ncomprehensive safety validation to ensure that these systems can navigate\nsafely and effectively under unpredictable conditions. Addressing\nOut-Of-Distribution (OOD) driving scenarios is essential for enhancing safety,\nas OOD scenarios help validate the reliability of the models within the\nvehicle's autonomy stack. However, generating OOD scenarios is challenging due\nto their long-tailed distribution and rarity in urban driving dataset.\nRecently, Large Language Models (LLMs) have shown promise in autonomous\ndriving, particularly for their zero-shot generalization and common-sense\nreasoning capabilities. In this paper, we leverage these LLM strengths to\nintroduce a framework for generating diverse OOD driving scenarios. Our\napproach uses LLMs to construct a branching tree, where each branch represents\na unique OOD scenario. These scenarios are then simulated in the CARLA\nsimulator using an automated framework that aligns scene augmentation with the\ncorresponding textual descriptions. We evaluate our framework through extensive\nsimulations, and assess its performance via a diversity metric that measures\nthe richness of the scenarios. Additionally, we introduce a new \"OOD-ness\"\nmetric, which quantifies how much the generated scenarios deviate from typical\nurban driving conditions. Furthermore, we explore the capacity of modern\nVision-Language Models (VLMs) to interpret and safely navigate through the\nsimulated OOD scenarios. Our findings offer valuable insights into the\nreliability of language models in addressing OOD scenarios within the context\nof urban driving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of autonomous vehicles controlled by machine learning\ntechniques requires extensive testing in diverse real-world environments,\nrobust handling of edge cases and out-of-distribution scenarios, and\ncomprehensive safety validation to ensure that these systems can navigate\nsafely and effectively under unpredictable conditions. Addressing\nOut-Of-Distribution (OOD) driving scenarios is essential for enhancing safety,\nas OOD scenarios help validate the reliability of the models within the\nvehicle's autonomy stack. However, generating OOD scenarios is challenging due\nto their long-tailed distribution and rarity in urban driving dataset.\nRecently, Large Language Models (LLMs) have shown promise in autonomous\ndriving, particularly for their zero-shot generalization and common-sense\nreasoning capabilities. In this paper, we leverage these LLM strengths to\nintroduce a framework for generating diverse OOD driving scenarios. Our\napproach uses LLMs to construct a branching tree, where each branch represents\na unique OOD scenario. These scenarios are then simulated in the CARLA\nsimulator using an automated framework that aligns scene augmentation with the\ncorresponding textual descriptions. We evaluate our framework through extensive\nsimulations, and assess its performance via a diversity metric that measures\nthe richness of the scenarios. Additionally, we introduce a new \"OOD-ness\"\nmetric, which quantifies how much the generated scenarios deviate from typical\nurban driving conditions. Furthermore, we explore the capacity of modern\nVision-Language Models (VLMs) to interpret and safely navigate through the\nsimulated OOD scenarios. Our findings offer valuable insights into the\nreliability of language models in addressing OOD scenarios within the context\nof urban driving."
                },
                "authors": [
                    {
                        "name": "Erfan Aasi"
                    },
                    {
                        "name": "Phat Nguyen"
                    },
                    {
                        "name": "Shiva Sreeram"
                    },
                    {
                        "name": "Guy Rosman"
                    },
                    {
                        "name": "Sertac Karaman"
                    },
                    {
                        "name": "Daniela Rus"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Rus"
                },
                "author": "Daniela Rus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14541v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14541v2",
                "updated": "2024-11-25T16:35:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    16,
                    35,
                    55,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-21T19:15:45Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    19,
                    15,
                    45,
                    3,
                    326,
                    0
                ],
                "title": "Protosolar D-to-H abundance and one part-per-billion PH$_{3}$ in the\n  coldest brown dwarf",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protosolar D-to-H abundance and one part-per-billion PH$_{3}$ in the\n  coldest brown dwarf"
                },
                "summary": "The coldest Y spectral type brown dwarfs are similar in mass and temperature\nto cool and warm ($\\sim$200 -- 400 K) giant exoplanets. We can therefore use\ntheir atmospheres as proxies for planetary atmospheres, testing our\nunderstanding of physics and chemistry for these complex, cool worlds. At these\ncold temperatures, their atmospheres are cold enough for water clouds to form,\nand chemical timescales increase, increasing the likelihood of disequilibrium\nchemistry compared to warmer classes of planets. JWST observations are\nrevolutionizing the characterization of these worlds with high signal-to-noise,\nmoderate resolution near- and mid-infrared spectra. The spectra have been used\nto measure the abundances of prominent species like water, methane, and\nammonia; species that trace chemical reactions like carbon monoxide; and even\nisotopologues of carbon monoxide and ammonia. Here, we present atmospheric\nretrieval results using both published fixed-slit (GTO program 1230) and new\naveraged time series observations (GO program 2327) of the coldest known Y\ndwarf, WISE 0855-0714 (using NIRSpec G395M spectra), which has an effective\ntemperature of $\\sim$ 264 K. We present a detection of deuterium in an\natmosphere outside of the solar system via a relative measurement of deuterated\nmethane (CH$_{3}$D) and standard methane. From this, we infer the D/H ratio of\na substellar object outside the solar system for the first time. We also\npresent a well-constrained part-per-billion abundance of phosphine (PH$_{3}$).\nWe discuss our interpretation of these results and the implications for brown\ndwarf and giant exoplanet formation and evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coldest Y spectral type brown dwarfs are similar in mass and temperature\nto cool and warm ($\\sim$200 -- 400 K) giant exoplanets. We can therefore use\ntheir atmospheres as proxies for planetary atmospheres, testing our\nunderstanding of physics and chemistry for these complex, cool worlds. At these\ncold temperatures, their atmospheres are cold enough for water clouds to form,\nand chemical timescales increase, increasing the likelihood of disequilibrium\nchemistry compared to warmer classes of planets. JWST observations are\nrevolutionizing the characterization of these worlds with high signal-to-noise,\nmoderate resolution near- and mid-infrared spectra. The spectra have been used\nto measure the abundances of prominent species like water, methane, and\nammonia; species that trace chemical reactions like carbon monoxide; and even\nisotopologues of carbon monoxide and ammonia. Here, we present atmospheric\nretrieval results using both published fixed-slit (GTO program 1230) and new\naveraged time series observations (GO program 2327) of the coldest known Y\ndwarf, WISE 0855-0714 (using NIRSpec G395M spectra), which has an effective\ntemperature of $\\sim$ 264 K. We present a detection of deuterium in an\natmosphere outside of the solar system via a relative measurement of deuterated\nmethane (CH$_{3}$D) and standard methane. From this, we infer the D/H ratio of\na substellar object outside the solar system for the first time. We also\npresent a well-constrained part-per-billion abundance of phosphine (PH$_{3}$).\nWe discuss our interpretation of these results and the implications for brown\ndwarf and giant exoplanet formation and evolution."
                },
                "authors": [
                    {
                        "name": "Melanie J. Rowland"
                    },
                    {
                        "name": "Caroline V. Morley"
                    },
                    {
                        "name": "Brittany E. Miles"
                    },
                    {
                        "name": "Genaro Surez"
                    },
                    {
                        "name": "Jacqueline K. Faherty"
                    },
                    {
                        "name": "Andrew J. Skemer"
                    },
                    {
                        "name": "Samuel A. Beiler"
                    },
                    {
                        "name": "Michael R. Line"
                    },
                    {
                        "name": "Gordon L. Bjoraker"
                    },
                    {
                        "name": "Jonathan J. Fortney"
                    },
                    {
                        "name": "Johanna M. Vos"
                    },
                    {
                        "name": "Sherelyn Alejandro Merchan"
                    },
                    {
                        "name": "Mark Marley"
                    },
                    {
                        "name": "Ben Burningham"
                    },
                    {
                        "name": "Richard Freedman"
                    },
                    {
                        "name": "Ehsan Gharib-Nezhad"
                    },
                    {
                        "name": "Natasha Batalha"
                    },
                    {
                        "name": "Roxana Lupu"
                    },
                    {
                        "name": "Channon Visscher"
                    },
                    {
                        "name": "Adam C. Schneider"
                    },
                    {
                        "name": "T. R. Geballe"
                    },
                    {
                        "name": "Aarynn Carter"
                    },
                    {
                        "name": "Katelyn Allers"
                    },
                    {
                        "name": "James Mang"
                    },
                    {
                        "name": "Dniel Apai"
                    },
                    {
                        "name": "Mary Anne Limbach"
                    },
                    {
                        "name": "Mikayla J. Wilson"
                    }
                ],
                "author_detail": {
                    "name": "Mikayla J. Wilson"
                },
                "author": "Mikayla J. Wilson",
                "arxiv_comment": "17 pages, 7 figures, accepted to ApJ Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14541v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14541v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16527v1",
                "updated": "2024-11-25T16:14:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    16,
                    14,
                    45,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T16:14:45Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    16,
                    14,
                    45,
                    0,
                    330,
                    0
                ],
                "title": "Profiling Bias in LLMs: Stereotype Dimensions in Contextual Word\n  Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Profiling Bias in LLMs: Stereotype Dimensions in Contextual Word\n  Embeddings"
                },
                "summary": "Large language models (LLMs) are the foundation of the current successes of\nartificial intelligence (AI), however, they are unavoidably biased. To\neffectively communicate the risks and encourage mitigation efforts these models\nneed adequate and intuitive descriptions of their discriminatory properties,\nappropriate for all audiences of AI. We suggest bias profiles with respect to\nstereotype dimensions based on dictionaries from social psychology research.\nAlong these dimensions we investigate gender bias in contextual embeddings,\nacross contexts and layers, and generate stereotype profiles for twelve\ndifferent LLMs, demonstrating their intuition and use case for exposing and\nvisualizing bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are the foundation of the current successes of\nartificial intelligence (AI), however, they are unavoidably biased. To\neffectively communicate the risks and encourage mitigation efforts these models\nneed adequate and intuitive descriptions of their discriminatory properties,\nappropriate for all audiences of AI. We suggest bias profiles with respect to\nstereotype dimensions based on dictionaries from social psychology research.\nAlong these dimensions we investigate gender bias in contextual embeddings,\nacross contexts and layers, and generate stereotype profiles for twelve\ndifferent LLMs, demonstrating their intuition and use case for exposing and\nvisualizing bias."
                },
                "authors": [
                    {
                        "name": "Carolin M. Schuster"
                    },
                    {
                        "name": "Maria-Alexandra Dinisor"
                    },
                    {
                        "name": "Shashwat Ghatiwala"
                    },
                    {
                        "name": "Georg Groh"
                    }
                ],
                "author_detail": {
                    "name": "Georg Groh"
                },
                "author": "Georg Groh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16525v1",
                "updated": "2024-11-25T16:12:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    16,
                    12,
                    17,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T16:12:17Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    16,
                    12,
                    17,
                    0,
                    330,
                    0
                ],
                "title": "Fundamental Limits of Prompt Tuning Transformers: Universality, Capacity\n  and Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limits of Prompt Tuning Transformers: Universality, Capacity\n  and Efficiency"
                },
                "summary": "We investigate the statistical and computational limits of prompt tuning for\ntransformer-based foundation models. Our key contributions are prompt tuning on\n\\textit{single-head} transformers with only a \\textit{single} self-attention\nlayer: (i) is universal, and (ii) supports efficient (even almost-linear time)\nalgorithms under the Strong Exponential Time Hypothesis (SETH). Statistically,\nwe prove that prompt tuning on such simplest possible transformers are\nuniversal approximators for sequence-to-sequence Lipschitz functions. In\naddition, we provide an exponential-in-$dL$ and -in-$(1/\\epsilon)$ lower bound\non the required soft-prompt tokens for prompt tuning to memorize any dataset\nwith 1-layer, 1-head transformers. Computationally, we identify a phase\ntransition in the efficiency of prompt tuning, determined by the norm of the\n\\textit{soft-prompt-induced} keys and queries, and provide an upper bound\ncriterion. Beyond this criterion, no sub-quadratic (efficient) algorithm for\nprompt tuning exists under SETH. Within this criterion, we showcase our theory\nby proving the existence of almost-linear time prompt tuning inference\nalgorithms. These fundamental limits provide important necessary conditions for\ndesigning expressive and efficient prompt tuning methods for practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the statistical and computational limits of prompt tuning for\ntransformer-based foundation models. Our key contributions are prompt tuning on\n\\textit{single-head} transformers with only a \\textit{single} self-attention\nlayer: (i) is universal, and (ii) supports efficient (even almost-linear time)\nalgorithms under the Strong Exponential Time Hypothesis (SETH). Statistically,\nwe prove that prompt tuning on such simplest possible transformers are\nuniversal approximators for sequence-to-sequence Lipschitz functions. In\naddition, we provide an exponential-in-$dL$ and -in-$(1/\\epsilon)$ lower bound\non the required soft-prompt tokens for prompt tuning to memorize any dataset\nwith 1-layer, 1-head transformers. Computationally, we identify a phase\ntransition in the efficiency of prompt tuning, determined by the norm of the\n\\textit{soft-prompt-induced} keys and queries, and provide an upper bound\ncriterion. Beyond this criterion, no sub-quadratic (efficient) algorithm for\nprompt tuning exists under SETH. Within this criterion, we showcase our theory\nby proving the existence of almost-linear time prompt tuning inference\nalgorithms. These fundamental limits provide important necessary conditions for\ndesigning expressive and efficient prompt tuning methods for practitioners."
                },
                "authors": [
                    {
                        "name": "Jerry Yao-Chieh Hu"
                    },
                    {
                        "name": "Wei-Po Wang"
                    },
                    {
                        "name": "Ammar Gilani"
                    },
                    {
                        "name": "Chenyang Li"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Han Liu"
                    }
                ],
                "author_detail": {
                    "name": "Han Liu"
                },
                "author": "Han Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16523v1",
                "updated": "2024-11-25T16:10:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    16,
                    10,
                    5,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T16:10:05Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    16,
                    10,
                    5,
                    0,
                    330,
                    0
                ],
                "title": "LaB-RAG: Label Boosted Retrieval Augmented Generation for Radiology\n  Report Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaB-RAG: Label Boosted Retrieval Augmented Generation for Radiology\n  Report Generation"
                },
                "summary": "In the current paradigm of image captioning, deep learning models are trained\nto generate text from image embeddings of latent features. We challenge the\nassumption that these latent features ought to be high-dimensional vectors\nwhich require model fine tuning to handle. Here we propose Label Boosted\nRetrieval Augmented Generation (LaB-RAG), a text-based approach to image\ncaptioning that leverages image descriptors in the form of categorical labels\nto boost standard retrieval augmented generation (RAG) with pretrained large\nlanguage models (LLMs). We study our method in the context of radiology report\ngeneration (RRG), where the task is to generate a clinician's report detailing\ntheir observations from a set of radiological images, such as X-rays. We argue\nthat simple linear classifiers over extracted image embeddings can effectively\ntransform X-rays into text-space as radiology-specific labels. In combination\nwith standard RAG, we show that these derived text labels can be used with\ngeneral-domain LLMs to generate radiology reports. Without ever training our\ngenerative language model or image feature encoder models, and without ever\ndirectly \"showing\" the LLM an X-ray, we demonstrate that LaB-RAG achieves\nbetter results across natural language and radiology language metrics compared\nwith other retrieval-based RRG methods, while attaining competitive results\ncompared to other fine-tuned vision-language RRG models. We further present\nresults of our experiments with various components of LaB-RAG to better\nunderstand our method. Finally, we critique the use of a popular RRG metric,\narguing it is possible to artificially inflate its results without true\ndata-leakage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the current paradigm of image captioning, deep learning models are trained\nto generate text from image embeddings of latent features. We challenge the\nassumption that these latent features ought to be high-dimensional vectors\nwhich require model fine tuning to handle. Here we propose Label Boosted\nRetrieval Augmented Generation (LaB-RAG), a text-based approach to image\ncaptioning that leverages image descriptors in the form of categorical labels\nto boost standard retrieval augmented generation (RAG) with pretrained large\nlanguage models (LLMs). We study our method in the context of radiology report\ngeneration (RRG), where the task is to generate a clinician's report detailing\ntheir observations from a set of radiological images, such as X-rays. We argue\nthat simple linear classifiers over extracted image embeddings can effectively\ntransform X-rays into text-space as radiology-specific labels. In combination\nwith standard RAG, we show that these derived text labels can be used with\ngeneral-domain LLMs to generate radiology reports. Without ever training our\ngenerative language model or image feature encoder models, and without ever\ndirectly \"showing\" the LLM an X-ray, we demonstrate that LaB-RAG achieves\nbetter results across natural language and radiology language metrics compared\nwith other retrieval-based RRG methods, while attaining competitive results\ncompared to other fine-tuned vision-language RRG models. We further present\nresults of our experiments with various components of LaB-RAG to better\nunderstand our method. Finally, we critique the use of a popular RRG metric,\narguing it is possible to artificially inflate its results without true\ndata-leakage."
                },
                "authors": [
                    {
                        "name": "Steven Song"
                    },
                    {
                        "name": "Anirudh Subramanyam"
                    },
                    {
                        "name": "Irene Madejski"
                    },
                    {
                        "name": "Robert L. Grossman"
                    }
                ],
                "author_detail": {
                    "name": "Robert L. Grossman"
                },
                "author": "Robert L. Grossman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16502v1",
                "updated": "2024-11-25T15:37:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    37,
                    27,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T15:37:27Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    37,
                    27,
                    0,
                    330,
                    0
                ],
                "title": "Interpreting Language Reward Models via Contrastive Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting Language Reward Models via Contrastive Explanations"
                },
                "summary": "Reward models (RMs) are a crucial component in the alignment of large\nlanguage models' (LLMs) outputs with human values. RMs approximate human\npreferences over possible LLM responses to the same prompt by predicting and\ncomparing reward scores. However, as they are typically modified versions of\nLLMs with scalar output heads, RMs are large black boxes whose predictions are\nnot explainable. More transparent RMs would enable improved trust in the\nalignment of LLMs. In this work, we propose to use contrastive explanations to\nexplain any binary response comparison made by an RM. Specifically, we generate\na diverse set of new comparisons similar to the original one to characterise\nthe RM's local behaviour. The perturbed responses forming the new comparisons\nare generated to explicitly modify manually specified high-level evaluation\nattributes, on which analyses of RM behaviour are grounded. In quantitative\nexperiments, we validate the effectiveness of our method for finding\nhigh-quality contrastive explanations. We then showcase the qualitative\nusefulness of our method for investigating global sensitivity of RMs to each\nevaluation attribute, and demonstrate how representative examples can be\nautomatically extracted to explain and compare behaviours of different RMs. We\nsee our method as a flexible framework for RM explanation, providing a basis\nfor more interpretable and trustworthy LLM alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models (RMs) are a crucial component in the alignment of large\nlanguage models' (LLMs) outputs with human values. RMs approximate human\npreferences over possible LLM responses to the same prompt by predicting and\ncomparing reward scores. However, as they are typically modified versions of\nLLMs with scalar output heads, RMs are large black boxes whose predictions are\nnot explainable. More transparent RMs would enable improved trust in the\nalignment of LLMs. In this work, we propose to use contrastive explanations to\nexplain any binary response comparison made by an RM. Specifically, we generate\na diverse set of new comparisons similar to the original one to characterise\nthe RM's local behaviour. The perturbed responses forming the new comparisons\nare generated to explicitly modify manually specified high-level evaluation\nattributes, on which analyses of RM behaviour are grounded. In quantitative\nexperiments, we validate the effectiveness of our method for finding\nhigh-quality contrastive explanations. We then showcase the qualitative\nusefulness of our method for investigating global sensitivity of RMs to each\nevaluation attribute, and demonstrate how representative examples can be\nautomatically extracted to explain and compare behaviours of different RMs. We\nsee our method as a flexible framework for RM explanation, providing a basis\nfor more interpretable and trustworthy LLM alignment."
                },
                "authors": [
                    {
                        "name": "Junqi Jiang"
                    },
                    {
                        "name": "Tom Bewley"
                    },
                    {
                        "name": "Saumitra Mishra"
                    },
                    {
                        "name": "Freddy Lecue"
                    },
                    {
                        "name": "Manuela Veloso"
                    }
                ],
                "author_detail": {
                    "name": "Manuela Veloso"
                },
                "author": "Manuela Veloso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16495v1",
                "updated": "2024-11-25T15:35:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    35,
                    51,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T15:35:51Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    35,
                    51,
                    0,
                    330,
                    0
                ],
                "title": "AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous\n  Knowledge Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous\n  Knowledge Reasoning"
                },
                "summary": "Recent advancements in large language models (LLMs) have led to significant\nimprovements in various natural language processing tasks, but it is still\nchallenging for LLMs to perform knowledge-intensive complex question answering\ndue to LLMs' inefficacy in reasoning planning and the hallucination problem. A\ntypical solution is to employ retrieval-augmented generation (RAG) coupled with\nchain-of-thought (CoT) reasoning, which decomposes complex questions into\nchain-like sub-questions and applies iterative RAG at each sub-question.\nHowever, prior works exhibit sub-optimal reasoning planning and overlook\ndynamic knowledge retrieval from heterogeneous sources. In this paper, we\npropose AtomR, a novel heterogeneous knowledge reasoning framework that\nconducts multi-source reasoning at the atomic level. Drawing inspiration from\nthe graph modeling of knowledge, AtomR leverages large language models (LLMs)\nto decompose complex questions into combinations of three atomic knowledge\noperators, significantly enhancing the reasoning process at both the planning\nand execution stages. We also introduce BlendQA, a novel evaluation benchmark\ntailored to assess complex heterogeneous knowledge reasoning. Experiments show\nthat AtomR significantly outperforms state-of-the-art baselines across three\nsingle-source and two multi-source reasoning benchmarks, with notable\nperformance gains of 9.4% on 2WikiMultihop and 9.5% on BlendQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have led to significant\nimprovements in various natural language processing tasks, but it is still\nchallenging for LLMs to perform knowledge-intensive complex question answering\ndue to LLMs' inefficacy in reasoning planning and the hallucination problem. A\ntypical solution is to employ retrieval-augmented generation (RAG) coupled with\nchain-of-thought (CoT) reasoning, which decomposes complex questions into\nchain-like sub-questions and applies iterative RAG at each sub-question.\nHowever, prior works exhibit sub-optimal reasoning planning and overlook\ndynamic knowledge retrieval from heterogeneous sources. In this paper, we\npropose AtomR, a novel heterogeneous knowledge reasoning framework that\nconducts multi-source reasoning at the atomic level. Drawing inspiration from\nthe graph modeling of knowledge, AtomR leverages large language models (LLMs)\nto decompose complex questions into combinations of three atomic knowledge\noperators, significantly enhancing the reasoning process at both the planning\nand execution stages. We also introduce BlendQA, a novel evaluation benchmark\ntailored to assess complex heterogeneous knowledge reasoning. Experiments show\nthat AtomR significantly outperforms state-of-the-art baselines across three\nsingle-source and two multi-source reasoning benchmarks, with notable\nperformance gains of 9.4% on 2WikiMultihop and 9.5% on BlendQA."
                },
                "authors": [
                    {
                        "name": "Amy Xin"
                    },
                    {
                        "name": "Jinxin Liu"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Zhicheng Li"
                    },
                    {
                        "name": "Shulin Cao"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09297v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09297v2",
                "updated": "2024-11-25T15:34:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    34,
                    1,
                    0,
                    330,
                    0
                ],
                "published": "2024-04-14T16:38:02Z",
                "published_parsed": [
                    2024,
                    4,
                    14,
                    16,
                    38,
                    2,
                    6,
                    105,
                    0
                ],
                "title": "Belief Bias Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Belief Bias Identification"
                },
                "summary": "This paper proposes a unified theoretical model to identify and test a\ncomprehensive set of probabilistic updating biases within a single framework.\nThe model achieves separate identification by focusing on the updating of\nbelief distributions, rather than classic point-belief measurements. Testing\nthe model in a laboratory experiment reveals significant heterogeneity at the\nindividual level: All tested biases are present, and each participant exhibits\nat least one identifiable bias. Notably, motivated-belief biases (optimism and\npessimism) and sequence-related biases (gambler's fallacy and hot hand fallacy)\nare identified as key drivers of biased inference. Moreover, at the population\nlevel, base rate neglect emerges as a persistent influence. This study\ncontributes to the belief-updating literature by providing a methodological\ntoolkit for researchers examining links between different conflicting biases,\nor exploring connections between updating biases and other behavioral\nphenomena.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a unified theoretical model to identify and test a\ncomprehensive set of probabilistic updating biases within a single framework.\nThe model achieves separate identification by focusing on the updating of\nbelief distributions, rather than classic point-belief measurements. Testing\nthe model in a laboratory experiment reveals significant heterogeneity at the\nindividual level: All tested biases are present, and each participant exhibits\nat least one identifiable bias. Notably, motivated-belief biases (optimism and\npessimism) and sequence-related biases (gambler's fallacy and hot hand fallacy)\nare identified as key drivers of biased inference. Moreover, at the population\nlevel, base rate neglect emerges as a persistent influence. This study\ncontributes to the belief-updating literature by providing a methodological\ntoolkit for researchers examining links between different conflicting biases,\nor exploring connections between updating biases and other behavioral\nphenomena."
                },
                "authors": [
                    {
                        "name": "Pedro Gonzalez-Fernandez"
                    }
                ],
                "author_detail": {
                    "name": "Pedro Gonzalez-Fernandez"
                },
                "author": "Pedro Gonzalez-Fernandez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09297v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09297v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16476v1",
                "updated": "2024-11-25T15:19:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    19,
                    54,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T15:19:54Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    19,
                    54,
                    0,
                    330,
                    0
                ],
                "title": "Luminosity predictions for the first three ionisation stages of W, Pt\n  and Au to probe potential sources of emission in kilonova",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Luminosity predictions for the first three ionisation stages of W, Pt\n  and Au to probe potential sources of emission in kilonova"
                },
                "summary": "A large number of R-matrix calculations of electron impact excitation for\nheavy elements (Z > 70) have been performed in recent years for applications in\nfusion and astrophysics research. With the expanding interest in heavy ions due\nto kilonova (KN) events such as AT2017gfo and GRB230307A, this new data can be\nutilised for the diagnosis and study of observed KN spectra. In this work\nrecently computed electron-impact excitation effective collision strengths are\nused, for the first three ionisation stages of tungsten (W, Z = 74), platinum\n(Pt, Z = 78) and gold (Au, Z = 79), to construct basic collisional radiative\nmodels tailored for the late stage nebular phases of KN. Line luminosities are\ncalculated at a range of electron temperatures and densities and the strengths\nof these lines for a representative ion mass are compared. For the case of W\nIII, these optically thin intensities are additionally used to constrain the\nmass of this ion in both AT2017gfo and GRB230307A. Comparing with theoretical\npredictions of nucleosynthesis yields from neutron star merger simulations,\nbroad agreement with the inferred ion masses of W is found. Furthermore, we\nhighlight the value of W measurements by showing that the abundance of other\ngroups of elements and outflow properties are constrained by exploiting\ntheoretically motivated correlations between the abundance of W and that of\nlanthanides or third r-process peak elements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A large number of R-matrix calculations of electron impact excitation for\nheavy elements (Z > 70) have been performed in recent years for applications in\nfusion and astrophysics research. With the expanding interest in heavy ions due\nto kilonova (KN) events such as AT2017gfo and GRB230307A, this new data can be\nutilised for the diagnosis and study of observed KN spectra. In this work\nrecently computed electron-impact excitation effective collision strengths are\nused, for the first three ionisation stages of tungsten (W, Z = 74), platinum\n(Pt, Z = 78) and gold (Au, Z = 79), to construct basic collisional radiative\nmodels tailored for the late stage nebular phases of KN. Line luminosities are\ncalculated at a range of electron temperatures and densities and the strengths\nof these lines for a representative ion mass are compared. For the case of W\nIII, these optically thin intensities are additionally used to constrain the\nmass of this ion in both AT2017gfo and GRB230307A. Comparing with theoretical\npredictions of nucleosynthesis yields from neutron star merger simulations,\nbroad agreement with the inferred ion masses of W is found. Furthermore, we\nhighlight the value of W measurements by showing that the abundance of other\ngroups of elements and outflow properties are constrained by exploiting\ntheoretically motivated correlations between the abundance of W and that of\nlanthanides or third r-process peak elements."
                },
                "authors": [
                    {
                        "name": "M. McCann"
                    },
                    {
                        "name": "L. P. Mulholland"
                    },
                    {
                        "name": "Z. Xiong"
                    },
                    {
                        "name": "C. A. Ramsbottom"
                    },
                    {
                        "name": "C. P. Ballance"
                    },
                    {
                        "name": "O. Just"
                    },
                    {
                        "name": "A. Bauswein"
                    },
                    {
                        "name": "G. Martnez-Pinedo"
                    },
                    {
                        "name": "F. McNeill"
                    },
                    {
                        "name": "S. A. Sim"
                    }
                ],
                "author_detail": {
                    "name": "S. A. Sim"
                },
                "author": "S. A. Sim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17348v2",
                "updated": "2024-11-25T15:17:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    17,
                    3,
                    0,
                    330,
                    0
                ],
                "published": "2024-09-25T20:49:41Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    20,
                    49,
                    41,
                    2,
                    269,
                    0
                ],
                "title": "Language Grounded Multi-agent Reinforcement Learning with\n  Human-interpretable Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Grounded Multi-agent Reinforcement Learning with\n  Human-interpretable Communication"
                },
                "summary": "Multi-Agent Reinforcement Learning (MARL) methods have shown promise in\nenabling agents to learn a shared communication protocol from scratch and\naccomplish challenging team tasks. However, the learned language is usually not\ninterpretable to humans or other agents not co-trained together, limiting its\napplicability in ad-hoc teamwork scenarios. In this work, we propose a novel\ncomputational pipeline that aligns the communication space between MARL agents\nwith an embedding space of human natural language by grounding agent\ncommunications on synthetic data generated by embodied Large Language Models\n(LLMs) in interactive teamwork scenarios. Our results demonstrate that\nintroducing language grounding not only maintains task performance but also\naccelerates the emergence of communication. Furthermore, the learned\ncommunication protocols exhibit zero-shot generalization capabilities in ad-hoc\nteamwork scenarios with unseen teammates and novel task states. This work\npresents a significant step toward enabling effective communication and\ncollaboration between artificial agents and humans in real-world teamwork\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Reinforcement Learning (MARL) methods have shown promise in\nenabling agents to learn a shared communication protocol from scratch and\naccomplish challenging team tasks. However, the learned language is usually not\ninterpretable to humans or other agents not co-trained together, limiting its\napplicability in ad-hoc teamwork scenarios. In this work, we propose a novel\ncomputational pipeline that aligns the communication space between MARL agents\nwith an embedding space of human natural language by grounding agent\ncommunications on synthetic data generated by embodied Large Language Models\n(LLMs) in interactive teamwork scenarios. Our results demonstrate that\nintroducing language grounding not only maintains task performance but also\naccelerates the emergence of communication. Furthermore, the learned\ncommunication protocols exhibit zero-shot generalization capabilities in ad-hoc\nteamwork scenarios with unseen teammates and novel task states. This work\npresents a significant step toward enabling effective communication and\ncollaboration between artificial agents and humans in real-world teamwork\nsettings."
                },
                "authors": [
                    {
                        "name": "Huao Li"
                    },
                    {
                        "name": "Hossein Nourkhiz Mahjoub"
                    },
                    {
                        "name": "Behdad Chalaki"
                    },
                    {
                        "name": "Vaishnav Tadiparthi"
                    },
                    {
                        "name": "Kwonjoon Lee"
                    },
                    {
                        "name": "Ehsan Moradi-Pari"
                    },
                    {
                        "name": "Charles Michael Lewis"
                    },
                    {
                        "name": "Katia P Sycara"
                    }
                ],
                "author_detail": {
                    "name": "Katia P Sycara"
                },
                "author": "Katia P Sycara",
                "arxiv_comment": "Accepted to Neurips 2024, 19 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16467v1",
                "updated": "2024-11-25T15:13:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    13,
                    56,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T15:13:56Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    13,
                    56,
                    0,
                    330,
                    0
                ],
                "title": "Observations of umbral flashes in the resonant sunspot chromosphere",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observations of umbral flashes in the resonant sunspot chromosphere"
                },
                "summary": "In sunspot umbrae, the core of some chromospheric lines exhibits periodic\nbrightness enhancements known as umbral flashes. The consensus is that they are\nproduced by the upward propagation of shock waves. This view has recently been\nchallenged by the detection of downflowing umbral flashes and the confirmation\nof the existence of a resonant cavity above sunspots. We aim to determine\nwaves' propagating or standing nature in the low umbral chromosphere and\nconfirm or refute the existence of downflowing umbral flashes. Spectroscopic\ntemporal series of Ca II 8542 \\AA, Ca II H, and Halpha in a sunspot were\nacquired with the Swedish Solar Telescope. The Halpha velocity was inferred\nusing bisectors. Simultaneous inversions of the Ca II 8542 \\AA\\ line and the Ca\nII H core were performed using the NICOLE code. The nature of the oscillations\nand insights into the resonant oscillatory pattern were determined by analyzing\nthe phase shift between the velocity signals and examining the temporal\nevolution. Propagating waves in the low chromosphere are more common in regions\nwith frequent umbral flashes, where the transition region is shifted upward,\nmaking resonant cavity signatures less noticeable. In contrast, areas with\nfewer umbral flashes show velocity fluctuations that align with standing\noscillations. Evidence suggests dynamic changes in the location of velocity\nresonant nodes due to variations in transition region height. Downflowing\nprofiles appear at the onset of some umbral flashes, but upflowing motion\ndominates during most of the flash. These downflowing flashes are more common\nin standing umbral flashes. We confirm the existence of a chromospheric\nresonant cavity above sunspot umbrae produced by wave reflections at the\ntransition region. The oscillatory pattern depends on the transition region\nheight, which exhibits spatial and temporal variations due to the impact of the\nwaves.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In sunspot umbrae, the core of some chromospheric lines exhibits periodic\nbrightness enhancements known as umbral flashes. The consensus is that they are\nproduced by the upward propagation of shock waves. This view has recently been\nchallenged by the detection of downflowing umbral flashes and the confirmation\nof the existence of a resonant cavity above sunspots. We aim to determine\nwaves' propagating or standing nature in the low umbral chromosphere and\nconfirm or refute the existence of downflowing umbral flashes. Spectroscopic\ntemporal series of Ca II 8542 \\AA, Ca II H, and Halpha in a sunspot were\nacquired with the Swedish Solar Telescope. The Halpha velocity was inferred\nusing bisectors. Simultaneous inversions of the Ca II 8542 \\AA\\ line and the Ca\nII H core were performed using the NICOLE code. The nature of the oscillations\nand insights into the resonant oscillatory pattern were determined by analyzing\nthe phase shift between the velocity signals and examining the temporal\nevolution. Propagating waves in the low chromosphere are more common in regions\nwith frequent umbral flashes, where the transition region is shifted upward,\nmaking resonant cavity signatures less noticeable. In contrast, areas with\nfewer umbral flashes show velocity fluctuations that align with standing\noscillations. Evidence suggests dynamic changes in the location of velocity\nresonant nodes due to variations in transition region height. Downflowing\nprofiles appear at the onset of some umbral flashes, but upflowing motion\ndominates during most of the flash. These downflowing flashes are more common\nin standing umbral flashes. We confirm the existence of a chromospheric\nresonant cavity above sunspot umbrae produced by wave reflections at the\ntransition region. The oscillatory pattern depends on the transition region\nheight, which exhibits spatial and temporal variations due to the impact of the\nwaves."
                },
                "authors": [
                    {
                        "name": "T. Felipe"
                    },
                    {
                        "name": "S. J. Gonzlez Manrique"
                    },
                    {
                        "name": "D. Martnez-Gmez"
                    },
                    {
                        "name": "M. M. Gmez-Mguez"
                    },
                    {
                        "name": "E. Khomenko"
                    },
                    {
                        "name": "C. Quintero Noda"
                    },
                    {
                        "name": "H. Socas-Navarro"
                    }
                ],
                "author_detail": {
                    "name": "H. Socas-Navarro"
                },
                "author": "H. Socas-Navarro",
                "arxiv_comment": "Accepted for publication in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16454v1",
                "updated": "2024-11-25T15:01:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    1,
                    25,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T15:01:25Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    1,
                    25,
                    0,
                    330,
                    0
                ],
                "title": "Learning by Analogy: Enhancing Few-Shot Prompting for Math Word Problem\n  Solving with Computational Graph-Based Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning by Analogy: Enhancing Few-Shot Prompting for Math Word Problem\n  Solving with Computational Graph-Based Retrieval"
                },
                "summary": "Large language models (LLMs) are known to struggle with complicated reasoning\ntasks such as math word problems (MWPs). In this paper, we present how analogy\nfrom similarly structured questions can improve LLMs' problem-solving\ncapabilities for MWPs. Specifically, we rely on the retrieval of problems with\nsimilar computational graphs to the given question to serve as exemplars in the\nprompt, providing the correct reasoning path for the generation model to refer\nto. Empirical results across six math word problem datasets demonstrate the\neffectiveness of our proposed method, which achieves a significant improvement\nof up to 6.7 percent on average in absolute value, compared to baseline\nmethods. These results highlight our method's potential in addressing the\nreasoning challenges in current LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are known to struggle with complicated reasoning\ntasks such as math word problems (MWPs). In this paper, we present how analogy\nfrom similarly structured questions can improve LLMs' problem-solving\ncapabilities for MWPs. Specifically, we rely on the retrieval of problems with\nsimilar computational graphs to the given question to serve as exemplars in the\nprompt, providing the correct reasoning path for the generation model to refer\nto. Empirical results across six math word problem datasets demonstrate the\neffectiveness of our proposed method, which achieves a significant improvement\nof up to 6.7 percent on average in absolute value, compared to baseline\nmethods. These results highlight our method's potential in addressing the\nreasoning challenges in current LLMs."
                },
                "authors": [
                    {
                        "name": "Xiaocong Yang"
                    },
                    {
                        "name": "Jiacheng Lin"
                    },
                    {
                        "name": "Ziqi Wang"
                    },
                    {
                        "name": "Chengxiang Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Chengxiang Zhai"
                },
                "author": "Chengxiang Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16442v1",
                "updated": "2024-11-25T14:44:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    14,
                    44,
                    26,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T14:44:26Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    14,
                    44,
                    26,
                    0,
                    330,
                    0
                ],
                "title": "TIFeD: a Tiny Integer-based Federated learning algorithm with Direct\n  feedback alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TIFeD: a Tiny Integer-based Federated learning algorithm with Direct\n  feedback alignment"
                },
                "summary": "Training machine and deep learning models directly on extremely\nresource-constrained devices is the next challenge in the field of tiny machine\nlearning. The related literature in this field is very limited, since most of\nthe solutions focus only on on-device inference or model adaptation through\nonline learning, leaving the training to be carried out on external Cloud\nservices. An interesting technological perspective is to exploit Federated\nLearning (FL), which allows multiple devices to collaboratively train a shared\nmodel in a distributed way. However, the main drawback of state-of-the-art FL\nalgorithms is that they are not suitable for running on tiny devices. For the\nfirst time in the literature, in this paper we introduce TIFeD, a Tiny\nInteger-based Federated learning algorithm with Direct Feedback Alignment (DFA)\nentirely implemented by using an integer-only arithmetic and being specifically\ndesigned to operate on devices with limited resources in terms of memory,\ncomputation and energy. Besides the traditional full-network operating\nmodality, in which each device of the FL setting trains the entire neural\nnetwork on its own local data, we propose an innovative single-layer TIFeD\nimplementation, which enables each device to train only a portion of the neural\nnetwork model and opens the door to a new way of distributing the learning\nprocedure across multiple devices. The experimental results show the\nfeasibility and effectiveness of the proposed solution. The proposed TIFeD\nalgorithm, with its full-network and single-layer implementations, is made\navailable to the scientific community as a public repository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training machine and deep learning models directly on extremely\nresource-constrained devices is the next challenge in the field of tiny machine\nlearning. The related literature in this field is very limited, since most of\nthe solutions focus only on on-device inference or model adaptation through\nonline learning, leaving the training to be carried out on external Cloud\nservices. An interesting technological perspective is to exploit Federated\nLearning (FL), which allows multiple devices to collaboratively train a shared\nmodel in a distributed way. However, the main drawback of state-of-the-art FL\nalgorithms is that they are not suitable for running on tiny devices. For the\nfirst time in the literature, in this paper we introduce TIFeD, a Tiny\nInteger-based Federated learning algorithm with Direct Feedback Alignment (DFA)\nentirely implemented by using an integer-only arithmetic and being specifically\ndesigned to operate on devices with limited resources in terms of memory,\ncomputation and energy. Besides the traditional full-network operating\nmodality, in which each device of the FL setting trains the entire neural\nnetwork on its own local data, we propose an innovative single-layer TIFeD\nimplementation, which enables each device to train only a portion of the neural\nnetwork model and opens the door to a new way of distributing the learning\nprocedure across multiple devices. The experimental results show the\nfeasibility and effectiveness of the proposed solution. The proposed TIFeD\nalgorithm, with its full-network and single-layer implementations, is made\navailable to the scientific community as a public repository."
                },
                "authors": [
                    {
                        "name": "Luca Colombo"
                    },
                    {
                        "name": "Alessandro Falcetta"
                    },
                    {
                        "name": "Manuel Roveri"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Roveri"
                },
                "author": "Manuel Roveri",
                "arxiv_doi": "10.1145/3639856.3639867",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3639856.3639867",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.16442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the Third International Conference on AI-ML\n  Systems, 2023, pp. 1-8",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16425v1",
                "updated": "2024-11-25T14:27:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    14,
                    27,
                    55,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T14:27:55Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    14,
                    27,
                    55,
                    0,
                    330,
                    0
                ],
                "title": "TopV-Nav: Unlocking the Top-View Spatial Reasoning Potential of MLLM for\n  Zero-shot Object Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TopV-Nav: Unlocking the Top-View Spatial Reasoning Potential of MLLM for\n  Zero-shot Object Navigation"
                },
                "summary": "The Zero-Shot Object Navigation (ZSON) task requires embodied agents to find\na previously unseen object by navigating in unfamiliar environments. Such a\ngoal-oriented exploration heavily relies on the ability to perceive,\nunderstand, and reason based on the spatial information of the environment.\nHowever, current LLM-based approaches convert visual observations to language\ndescriptions and reason in the linguistic space, leading to the loss of spatial\ninformation. In this paper, we introduce TopV-Nav, a MLLM-based method that\ndirectly reasons on the top-view map with complete spatial information. To\nfully unlock the MLLM's spatial reasoning potential in top-view perspective, we\npropose the Adaptive Visual Prompt Generation (AVPG) method to adaptively\nconstruct semantically-rich top-view map. It enables the agent to directly\nutilize spatial information contained in the top-view map to conduct thorough\nreasoning. Besides, we design a Dynamic Map Scaling (DMS) mechanism to\ndynamically zoom top-view map at preferred scales, enhancing local fine-grained\nreasoning. Additionally, we devise a Target-Guided Navigation (TGN) mechanism\nto predict and to utilize target locations, facilitating global and human-like\nexploration. Experiments on MP3D and HM3D benchmarks demonstrate the\nsuperiority of our TopV-Nav, e.g., $+3.9\\%$ SR and $+2.0\\%$ SPL absolute\nimprovements on HM3D.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Zero-Shot Object Navigation (ZSON) task requires embodied agents to find\na previously unseen object by navigating in unfamiliar environments. Such a\ngoal-oriented exploration heavily relies on the ability to perceive,\nunderstand, and reason based on the spatial information of the environment.\nHowever, current LLM-based approaches convert visual observations to language\ndescriptions and reason in the linguistic space, leading to the loss of spatial\ninformation. In this paper, we introduce TopV-Nav, a MLLM-based method that\ndirectly reasons on the top-view map with complete spatial information. To\nfully unlock the MLLM's spatial reasoning potential in top-view perspective, we\npropose the Adaptive Visual Prompt Generation (AVPG) method to adaptively\nconstruct semantically-rich top-view map. It enables the agent to directly\nutilize spatial information contained in the top-view map to conduct thorough\nreasoning. Besides, we design a Dynamic Map Scaling (DMS) mechanism to\ndynamically zoom top-view map at preferred scales, enhancing local fine-grained\nreasoning. Additionally, we devise a Target-Guided Navigation (TGN) mechanism\nto predict and to utilize target locations, facilitating global and human-like\nexploration. Experiments on MP3D and HM3D benchmarks demonstrate the\nsuperiority of our TopV-Nav, e.g., $+3.9\\%$ SR and $+2.0\\%$ SPL absolute\nimprovements on HM3D."
                },
                "authors": [
                    {
                        "name": "Linqing Zhong"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Zihan Ding"
                    },
                    {
                        "name": "Yue Liao"
                    },
                    {
                        "name": "Si Liu"
                    }
                ],
                "author_detail": {
                    "name": "Si Liu"
                },
                "author": "Si Liu",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16417v1",
                "updated": "2024-11-25T14:20:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    14,
                    20,
                    53,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T14:20:53Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    14,
                    20,
                    53,
                    0,
                    330,
                    0
                ],
                "title": "Comparison of Generative Learning Methods for Turbulence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparison of Generative Learning Methods for Turbulence Modeling"
                },
                "summary": "Numerical simulations of turbulent flows present significant challenges in\nfluid dynamics due to their complexity and high computational cost. High\nresolution techniques such as Direct Numerical Simulation (DNS) and Large Eddy\nSimulation (LES) are generally not computationally affordable, particularly for\ntechnologically relevant problems. Recent advances in machine learning,\nspecifically in generative probabilistic models, offer promising alternatives\nfor turbulence modeling. This paper investigates the application of three\ngenerative models - Variational Autoencoders (VAE), Deep Convolutional\nGenerative Adversarial Networks (DCGAN), and Denoising Diffusion Probabilistic\nModels (DDPM) - in simulating a 2D K\\'arm\\'an vortex street around a fixed\ncylinder. Training data was obtained by means of LES. We evaluate each model's\nability to capture the statistical properties and spatial structures of the\nturbulent flow. Our results demonstrate that DDPM and DCGAN effectively\nreplicate the flow distribution, highlighting their potential as efficient and\naccurate tools for turbulence modeling. We find a strong argument for DCGAN, as\nalthough they are more difficult to train (due to problems such as mode\ncollapse), they gave the fastest inference and training time, require less data\nto train compared to VAE and DDPM, and provide the results most closely aligned\nwith the input stream. In contrast, VAE train quickly (and can generate samples\nquickly) but do not produce adequate results, and DDPM, whilst effective, is\nsignificantly slower at both inference and training time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical simulations of turbulent flows present significant challenges in\nfluid dynamics due to their complexity and high computational cost. High\nresolution techniques such as Direct Numerical Simulation (DNS) and Large Eddy\nSimulation (LES) are generally not computationally affordable, particularly for\ntechnologically relevant problems. Recent advances in machine learning,\nspecifically in generative probabilistic models, offer promising alternatives\nfor turbulence modeling. This paper investigates the application of three\ngenerative models - Variational Autoencoders (VAE), Deep Convolutional\nGenerative Adversarial Networks (DCGAN), and Denoising Diffusion Probabilistic\nModels (DDPM) - in simulating a 2D K\\'arm\\'an vortex street around a fixed\ncylinder. Training data was obtained by means of LES. We evaluate each model's\nability to capture the statistical properties and spatial structures of the\nturbulent flow. Our results demonstrate that DDPM and DCGAN effectively\nreplicate the flow distribution, highlighting their potential as efficient and\naccurate tools for turbulence modeling. We find a strong argument for DCGAN, as\nalthough they are more difficult to train (due to problems such as mode\ncollapse), they gave the fastest inference and training time, require less data\nto train compared to VAE and DDPM, and provide the results most closely aligned\nwith the input stream. In contrast, VAE train quickly (and can generate samples\nquickly) but do not produce adequate results, and DDPM, whilst effective, is\nsignificantly slower at both inference and training time."
                },
                "authors": [
                    {
                        "name": "Claudia Drygala"
                    },
                    {
                        "name": "Edmund Ross"
                    },
                    {
                        "name": "Francesca di Mare"
                    },
                    {
                        "name": "Hanno Gottschalk"
                    }
                ],
                "author_detail": {
                    "name": "Hanno Gottschalk"
                },
                "author": "Hanno Gottschalk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16416v1",
                "updated": "2024-11-25T14:19:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    14,
                    19,
                    35,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T14:19:35Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    14,
                    19,
                    35,
                    0,
                    330,
                    0
                ],
                "title": "A Multi-agent Framework for Materials Laws Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-agent Framework for Materials Laws Discovery"
                },
                "summary": "Uncovering the underlying laws governing correlations between different\nmaterials properties, and the structure-composition-property relationship, is\nessential for advancing materials theory and enabling efficient materials\ndesign. With recent advances in artificial intelligence (AI), particularly in\nlarge language models (LLMs), symbolic regression has emerged as a powerful\nmethod for deriving explicit formulas for materials laws. LLMs, with their\npre-trained, cross-disciplinary knowledge, present a promising direction in \"AI\nfor Materials\". In this work, we introduce a multi-agent framework based on\nLLMs specifically designed for symbolic regression in materials science. We\ndemonstrate the effectiveness of the framework using the glass-forming ability\n(GFA) of metallic glasses as a case study, employing three characteristic\ntemperatures as independent variables. Our framework derived an interpretable\nformula to describe GFA, achieving a correlation coefficient of up to 0.948\nwith low formula complexity. This approach outperforms standard packages such\nas GPlearn and demonstrates a ~30% improvement over random generation methods,\nowing to integrated memory and reflection mechanisms. The proposed framework\ncan be extended to discover laws in various materials applications, supporting\nnew materials design and enhancing the interpretation of experimental and\nsimulation data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering the underlying laws governing correlations between different\nmaterials properties, and the structure-composition-property relationship, is\nessential for advancing materials theory and enabling efficient materials\ndesign. With recent advances in artificial intelligence (AI), particularly in\nlarge language models (LLMs), symbolic regression has emerged as a powerful\nmethod for deriving explicit formulas for materials laws. LLMs, with their\npre-trained, cross-disciplinary knowledge, present a promising direction in \"AI\nfor Materials\". In this work, we introduce a multi-agent framework based on\nLLMs specifically designed for symbolic regression in materials science. We\ndemonstrate the effectiveness of the framework using the glass-forming ability\n(GFA) of metallic glasses as a case study, employing three characteristic\ntemperatures as independent variables. Our framework derived an interpretable\nformula to describe GFA, achieving a correlation coefficient of up to 0.948\nwith low formula complexity. This approach outperforms standard packages such\nas GPlearn and demonstrates a ~30% improvement over random generation methods,\nowing to integrated memory and reflection mechanisms. The proposed framework\ncan be extended to discover laws in various materials applications, supporting\nnew materials design and enhancing the interpretation of experimental and\nsimulation data."
                },
                "authors": [
                    {
                        "name": "Bo Hu"
                    },
                    {
                        "name": "Siyu Liu"
                    },
                    {
                        "name": "Beilin Ye"
                    },
                    {
                        "name": "Yun Hao"
                    },
                    {
                        "name": "Tongqi Wen"
                    }
                ],
                "author_detail": {
                    "name": "Tongqi Wen"
                },
                "author": "Tongqi Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14938v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14938v2",
                "updated": "2024-11-25T14:19:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    14,
                    19,
                    30,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-22T13:50:02Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    13,
                    50,
                    2,
                    4,
                    327,
                    0
                ],
                "title": "Bayesian inference of strangeon matter using the measurements of PSR\n  J0437-4715 and GW190814",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference of strangeon matter using the measurements of PSR\n  J0437-4715 and GW190814"
                },
                "summary": "The observations of compact star inspirals from LIGO/Virgo combined with mass\nand radius measurements from NICER provide a valuable tool to study the highly\nuncertain equation of state (EOS) of dense matter at the densities\ncharacteristic of compact stars. In this work, we constrain the solid states of\nstrange-cluster matter, called strangeon matter, as the putative basic units of\nthe ground state of bulk strong matter using a Bayesian statistical method,\nincorporating the mass and radius measurements of PSR J0030+0451, PSR\nJ0740+6620, and the recent data for the $1.4\\ M_{\\odot}$ pulsar PSR J0437-4715.\nWe also include constraints from gravitational wave events GW170817 and\nGW190814. Under the prior assumption of a finite number of quarks in a\nstrangeon, $N_{\\rm q}$, our analysis reveals that current mass-radius\nmeasurements favor a larger $N_{\\rm q}$. Specifically, the results support the\nscenario where a strangeon forms a stable bound state with $N_{\\rm q}=18$,\nsymmetric in color, flavor, and spin spaces, compared to the minimum $N_{\\rm\nq}$ prior. The comparative analyses of the posterior EOS parameter spaces\nderived from three-parameter model and two-parameter model demonstrate a\nconsistent prediction under identical observational constraints. In particular,\nour results indicate that the most probable values of the maximum mass are\nfound to be $3.58^{+0.16}_{-0.12}\\ M_{\\odot}$ ($3.65^{+0.18}_{-0.16}\\\nM_{\\odot}$) at $90\\%$ confidence level for three-parameter (two-parameter) EOS\nconsidering the constraints of GW190814. The corresponding radii for $1.4\\\nM_{\\odot}$ and $2.1\\ M_{\\odot}$ stars are $12.04^{+0.27}_{-0.31}~\\rm km$\n($12.16^{+0.26}_{-0.31}~\\rm km$) and $13.43^{+0.31}_{-0.32}~\\rm km$\n($13.60^{+0.29}_{-0.34}~\\rm km$), respectively. This result may impact\ninterestingly on the research of multiquark states, which could improve our\nunderstanding of the nonperturbative strong force.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The observations of compact star inspirals from LIGO/Virgo combined with mass\nand radius measurements from NICER provide a valuable tool to study the highly\nuncertain equation of state (EOS) of dense matter at the densities\ncharacteristic of compact stars. In this work, we constrain the solid states of\nstrange-cluster matter, called strangeon matter, as the putative basic units of\nthe ground state of bulk strong matter using a Bayesian statistical method,\nincorporating the mass and radius measurements of PSR J0030+0451, PSR\nJ0740+6620, and the recent data for the $1.4\\ M_{\\odot}$ pulsar PSR J0437-4715.\nWe also include constraints from gravitational wave events GW170817 and\nGW190814. Under the prior assumption of a finite number of quarks in a\nstrangeon, $N_{\\rm q}$, our analysis reveals that current mass-radius\nmeasurements favor a larger $N_{\\rm q}$. Specifically, the results support the\nscenario where a strangeon forms a stable bound state with $N_{\\rm q}=18$,\nsymmetric in color, flavor, and spin spaces, compared to the minimum $N_{\\rm\nq}$ prior. The comparative analyses of the posterior EOS parameter spaces\nderived from three-parameter model and two-parameter model demonstrate a\nconsistent prediction under identical observational constraints. In particular,\nour results indicate that the most probable values of the maximum mass are\nfound to be $3.58^{+0.16}_{-0.12}\\ M_{\\odot}$ ($3.65^{+0.18}_{-0.16}\\\nM_{\\odot}$) at $90\\%$ confidence level for three-parameter (two-parameter) EOS\nconsidering the constraints of GW190814. The corresponding radii for $1.4\\\nM_{\\odot}$ and $2.1\\ M_{\\odot}$ stars are $12.04^{+0.27}_{-0.31}~\\rm km$\n($12.16^{+0.26}_{-0.31}~\\rm km$) and $13.43^{+0.31}_{-0.32}~\\rm km$\n($13.60^{+0.29}_{-0.34}~\\rm km$), respectively. This result may impact\ninterestingly on the research of multiquark states, which could improve our\nunderstanding of the nonperturbative strong force."
                },
                "authors": [
                    {
                        "name": "Wen-Li Yuan"
                    },
                    {
                        "name": "Chun Huang"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Enping Zhou"
                    },
                    {
                        "name": "Renxin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Renxin Xu"
                },
                "author": "Renxin Xu",
                "arxiv_comment": "10 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14938v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14938v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16396v1",
                "updated": "2024-11-25T14:01:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    14,
                    1,
                    55,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T14:01:55Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    14,
                    1,
                    55,
                    0,
                    330,
                    0
                ],
                "title": "Statistical inference for quantum singular models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical inference for quantum singular models"
                },
                "summary": "Deep learning has seen substantial achievements, with numerical and\ntheoretical evidence suggesting that singularities of statistical models are\nconsidered a contributing factor to its performance. From this remarkable\nsuccess of classical statistical models, it is naturally expected that quantum\nsingular models will play a vital role in many quantum statistical tasks.\nHowever, while the theory of quantum statistical models in regular cases has\nbeen established, theoretical understanding of quantum singular models is still\nlimited. To investigate the statistical properties of quantum singular models,\nwe focus on two prominent tasks in quantum statistical inference: quantum state\nestimation and model selection. In particular, we base our study on classical\nsingular learning theory and seek to extend it within the framework of Bayesian\nquantum state estimation. To this end, we define quantum generalization and\ntraining loss functions and give their asymptotic expansions through algebraic\ngeometrical methods. The key idea of the proof is the introduction of a quantum\nanalog of the likelihood function using classical shadows. Consequently, we\nconstruct an asymptotically unbiased estimator of the quantum generalization\nloss, the quantum widely applicable information criterion (QWAIC), as a\ncomputable model selection metric from given measurement outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning has seen substantial achievements, with numerical and\ntheoretical evidence suggesting that singularities of statistical models are\nconsidered a contributing factor to its performance. From this remarkable\nsuccess of classical statistical models, it is naturally expected that quantum\nsingular models will play a vital role in many quantum statistical tasks.\nHowever, while the theory of quantum statistical models in regular cases has\nbeen established, theoretical understanding of quantum singular models is still\nlimited. To investigate the statistical properties of quantum singular models,\nwe focus on two prominent tasks in quantum statistical inference: quantum state\nestimation and model selection. In particular, we base our study on classical\nsingular learning theory and seek to extend it within the framework of Bayesian\nquantum state estimation. To this end, we define quantum generalization and\ntraining loss functions and give their asymptotic expansions through algebraic\ngeometrical methods. The key idea of the proof is the introduction of a quantum\nanalog of the likelihood function using classical shadows. Consequently, we\nconstruct an asymptotically unbiased estimator of the quantum generalization\nloss, the quantum widely applicable information criterion (QWAIC), as a\ncomputable model selection metric from given measurement outcomes."
                },
                "authors": [
                    {
                        "name": "Hiroshi Yano"
                    },
                    {
                        "name": "Yota Maeda"
                    },
                    {
                        "name": "Naoki Yamamoto"
                    }
                ],
                "author_detail": {
                    "name": "Naoki Yamamoto"
                },
                "author": "Naoki Yamamoto",
                "arxiv_comment": "57 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16387v1",
                "updated": "2024-11-25T13:49:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    49,
                    45,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T13:49:45Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    49,
                    45,
                    0,
                    330,
                    0
                ],
                "title": "FineWeb-zhtw: Scalable Curation of Traditional Chinese Text Data from\n  the Web",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineWeb-zhtw: Scalable Curation of Traditional Chinese Text Data from\n  the Web"
                },
                "summary": "The quality and size of a pretraining dataset significantly influence the\nperformance of large language models (LLMs). While there have been numerous\nefforts in the curation of such a dataset for English users, there is a\nrelative lack of similar initiatives for Traditional Chinese. Building upon\nthis foundation of FineWeb, we introduce FineWeb-zhtw, a dataset tailored\nspecifically for Traditional Chinese users. We came up with multiple stages of\nmeticulously designed filters to cater to the linguistic difference between\nEnglish and Traditional Chinese, to ensure comprehensiveness and quality. We\ndetermined effectiveness from querying dataset samples with three main\nobjectives. Our code and datasets are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quality and size of a pretraining dataset significantly influence the\nperformance of large language models (LLMs). While there have been numerous\nefforts in the curation of such a dataset for English users, there is a\nrelative lack of similar initiatives for Traditional Chinese. Building upon\nthis foundation of FineWeb, we introduce FineWeb-zhtw, a dataset tailored\nspecifically for Traditional Chinese users. We came up with multiple stages of\nmeticulously designed filters to cater to the linguistic difference between\nEnglish and Traditional Chinese, to ensure comprehensiveness and quality. We\ndetermined effectiveness from querying dataset samples with three main\nobjectives. Our code and datasets are publicly available."
                },
                "authors": [
                    {
                        "name": "Cheng-Wei Lin"
                    },
                    {
                        "name": "Wan-Hsuan Hsieh"
                    },
                    {
                        "name": "Kai-Xin Guan"
                    },
                    {
                        "name": "Chan-Jan Hsu"
                    },
                    {
                        "name": "Chia-Chen Kuo"
                    },
                    {
                        "name": "Chuan-Lin Lai"
                    },
                    {
                        "name": "Chung-Wei Chung"
                    },
                    {
                        "name": "Ming-Jen Wang"
                    },
                    {
                        "name": "Da-Shan Shiu"
                    }
                ],
                "author_detail": {
                    "name": "Da-Shan Shiu"
                },
                "author": "Da-Shan Shiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16370v1",
                "updated": "2024-11-25T13:26:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    26,
                    9,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T13:26:09Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    26,
                    9,
                    0,
                    330,
                    0
                ],
                "title": "A Review of Bayesian Uncertainty Quantification in Deep Probabilistic\n  Image Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Review of Bayesian Uncertainty Quantification in Deep Probabilistic\n  Image Segmentation"
                },
                "summary": "Advancements in image segmentation play an integral role within the greater\nscope of Deep Learning-based computer vision. Furthermore, their widespread\napplicability in critical real-world tasks has given rise to challenges related\nto the reliability of such algorithms. Hence, uncertainty quantification has\nbeen extensively studied within this context, enabling expression of model\nignorance (epistemic uncertainty) or data ambiguity (aleatoric uncertainty) to\nprevent uninformed decision making. Due to the rapid adoption of Convolutional\nNeural Network (CNN)-based segmentation models in high-stake applications, a\nsubstantial body of research has been published on this very topic, causing its\nswift expansion into a distinct field. This work provides a comprehensive\noverview of probabilistic segmentation by discussing fundamental concepts in\nuncertainty that govern advancements in the field as well as the application to\nvarious tasks. We identify that quantifying aleatoric and epistemic uncertainty\napproximates Bayesian inference w.r.t. to either latent variables or model\nparameters, respectively. Moreover, literature on both uncertainties trace back\nto four key applications; (1) to quantify statistical inconsistencies in the\nannotation process due ambiguous images, (2) correlating prediction error with\nuncertainty, (3) expanding the model hypothesis space for better\ngeneralization, and (4) active learning. Then, a discussion follows that\nincludes an overview of utilized datasets for each of the applications and\ncomparison of the available methods. We also highlight challenges related to\narchitectures, uncertainty-based active learning, standardization and\nbenchmarking, and recommendations for future work such as methods based on\nsingle forward passes and models that appropriately leverage volumetric data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in image segmentation play an integral role within the greater\nscope of Deep Learning-based computer vision. Furthermore, their widespread\napplicability in critical real-world tasks has given rise to challenges related\nto the reliability of such algorithms. Hence, uncertainty quantification has\nbeen extensively studied within this context, enabling expression of model\nignorance (epistemic uncertainty) or data ambiguity (aleatoric uncertainty) to\nprevent uninformed decision making. Due to the rapid adoption of Convolutional\nNeural Network (CNN)-based segmentation models in high-stake applications, a\nsubstantial body of research has been published on this very topic, causing its\nswift expansion into a distinct field. This work provides a comprehensive\noverview of probabilistic segmentation by discussing fundamental concepts in\nuncertainty that govern advancements in the field as well as the application to\nvarious tasks. We identify that quantifying aleatoric and epistemic uncertainty\napproximates Bayesian inference w.r.t. to either latent variables or model\nparameters, respectively. Moreover, literature on both uncertainties trace back\nto four key applications; (1) to quantify statistical inconsistencies in the\nannotation process due ambiguous images, (2) correlating prediction error with\nuncertainty, (3) expanding the model hypothesis space for better\ngeneralization, and (4) active learning. Then, a discussion follows that\nincludes an overview of utilized datasets for each of the applications and\ncomparison of the available methods. We also highlight challenges related to\narchitectures, uncertainty-based active learning, standardization and\nbenchmarking, and recommendations for future work such as methods based on\nsingle forward passes and models that appropriately leverage volumetric data."
                },
                "authors": [
                    {
                        "name": "M. M. A. Valiuddin"
                    },
                    {
                        "name": "R. J. G. van Sloun"
                    },
                    {
                        "name": "C. G. A. Viviers"
                    },
                    {
                        "name": "P. H. N. de With"
                    },
                    {
                        "name": "F. van der Sommen"
                    }
                ],
                "author_detail": {
                    "name": "F. van der Sommen"
                },
                "author": "F. van der Sommen",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16623v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16623v2",
                "updated": "2024-11-25T13:25:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    25,
                    16,
                    0,
                    330,
                    0
                ],
                "published": "2024-05-26T16:39:19Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    16,
                    39,
                    19,
                    6,
                    147,
                    0
                ],
                "title": "Graph neural networks with configuration cross-attention for tensor\n  compilers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks with configuration cross-attention for tensor\n  compilers"
                },
                "summary": "With the recent popularity of neural networks comes the need for efficient\nserving of inference workloads. A neural network inference workload can be\nrepresented as a computational graph with nodes as operators transforming\nmultidimensional tensors. The tensors can be transposed and/or tiled in a\ncombinatorially large number of ways, some configurations leading to\naccelerated inference. We propose TGraph, a neural graph architecture that\nallows screening for fast configurations of the target computational graph,\nthus representing an artificial intelligence (AI) tensor compiler in contrast\nto the traditional heuristics-based compilers. The proposed solution improves\nmean Kendall's $\\tau$ across layout collections of TpuGraphs from 29.8% of the\nreliable baseline to 67.4% of TGraph. We estimate the potential CO$_2$ emission\nreduction associated with our work to be equivalent to over 50% of the total\nhousehold emissions in the areas hosting AI-oriented data centers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the recent popularity of neural networks comes the need for efficient\nserving of inference workloads. A neural network inference workload can be\nrepresented as a computational graph with nodes as operators transforming\nmultidimensional tensors. The tensors can be transposed and/or tiled in a\ncombinatorially large number of ways, some configurations leading to\naccelerated inference. We propose TGraph, a neural graph architecture that\nallows screening for fast configurations of the target computational graph,\nthus representing an artificial intelligence (AI) tensor compiler in contrast\nto the traditional heuristics-based compilers. The proposed solution improves\nmean Kendall's $\\tau$ across layout collections of TpuGraphs from 29.8% of the\nreliable baseline to 67.4% of TGraph. We estimate the potential CO$_2$ emission\nreduction associated with our work to be equivalent to over 50% of the total\nhousehold emissions in the areas hosting AI-oriented data centers."
                },
                "authors": [
                    {
                        "name": "Dmitrii Khizbullin"
                    },
                    {
                        "name": "Eduardo Rocha de Andrade"
                    },
                    {
                        "name": "Thanh Hau Nguyen"
                    },
                    {
                        "name": "Matheus Pedroza Ferreira"
                    },
                    {
                        "name": "David R. Pugh"
                    }
                ],
                "author_detail": {
                    "name": "David R. Pugh"
                },
                "author": "David R. Pugh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16623v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16623v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16366v1",
                "updated": "2024-11-25T13:20:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    20,
                    25,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T13:20:25Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    20,
                    25,
                    0,
                    330,
                    0
                ],
                "title": "Connections between sequential Bayesian inference and evolutionary\n  dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Connections between sequential Bayesian inference and evolutionary\n  dynamics"
                },
                "summary": "It has long been posited that there is a connection between the dynamical\nequations describing evolutionary processes in biology and sequential Bayesian\nlearning methods. This manuscript describes new research in which this precise\nconnection is rigorously established in the continuous time setting. Here we\nfocus on a partial differential equation known as the Kushner-Stratonovich\nequation describing the evolution of the posterior density in time. Of\nparticular importance is a piecewise smooth approximation of the observation\npath from which the discrete time filtering equations, which are shown to\nconverge to a Stratonovich interpretation of the Kushner-Stratonovich equation.\nThis smooth formulation will then be used to draw precise connections between\nnonlinear stochastic filtering and replicator-mutator dynamics. Additionally,\ngradient flow formulations will be investigated as well as a form of\nreplicator-mutator dynamics which is shown to be beneficial for the\nmisspecified model filtering problem. It is hoped this work will spur further\nresearch into exchanges between sequential learning and evolutionary biology\nand to inspire new algorithms in filtering and sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It has long been posited that there is a connection between the dynamical\nequations describing evolutionary processes in biology and sequential Bayesian\nlearning methods. This manuscript describes new research in which this precise\nconnection is rigorously established in the continuous time setting. Here we\nfocus on a partial differential equation known as the Kushner-Stratonovich\nequation describing the evolution of the posterior density in time. Of\nparticular importance is a piecewise smooth approximation of the observation\npath from which the discrete time filtering equations, which are shown to\nconverge to a Stratonovich interpretation of the Kushner-Stratonovich equation.\nThis smooth formulation will then be used to draw precise connections between\nnonlinear stochastic filtering and replicator-mutator dynamics. Additionally,\ngradient flow formulations will be investigated as well as a form of\nreplicator-mutator dynamics which is shown to be beneficial for the\nmisspecified model filtering problem. It is hoped this work will spur further\nresearch into exchanges between sequential learning and evolutionary biology\nand to inspire new algorithms in filtering and sampling."
                },
                "authors": [
                    {
                        "name": "Sahani Pathiraja"
                    },
                    {
                        "name": "Philipp Wacker"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Wacker"
                },
                "author": "Philipp Wacker",
                "arxiv_comment": "51 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "35R60, 92-10, 60G35, 62F15,",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12977v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12977v2",
                "updated": "2024-11-25T13:17:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    17,
                    1,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-20T02:10:44Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    10,
                    44,
                    2,
                    325,
                    0
                ],
                "title": "MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong\n  Collaborative Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong\n  Collaborative Learning"
                },
                "summary": "Contemporary embodied agents, such as Voyager in Minecraft, have demonstrated\npromising capabilities in open-ended individual learning. However, when powered\nwith open large language models (LLMs), these agents often struggle with\nrudimentary tasks, even when fine-tuned on domain-specific knowledge. Inspired\nby human cultural learning, we present \\collabvoyager, a novel framework that\nenhances Voyager with lifelong collaborative learning through explicit\nperspective-taking. \\collabvoyager introduces three key innovations: (1) theory\nof mind representations linking percepts, beliefs, desires, and actions; (2)\nnatural language communication between agents; and (3) semantic memory of task\nand environment knowledge and episodic memory of collaboration episodes. These\nadvancements enable agents to reason about their and others' mental states,\nempirically addressing two prevalent failure modes: false beliefs and faulty\ntask executions. In mixed-expertise Minecraft experiments, \\collabvoyager\nagents outperform Voyager counterparts, significantly improving task completion\nrate by $66.6\\% (+39.4\\%)$ for collecting one block of dirt and $70.8\\%\n(+20.8\\%)$ for collecting one wood block. They exhibit emergent behaviors like\nknowledge transfer from expert to novice agents and collaborative code\ncorrection. \\collabvoyager agents also demonstrate the ability to adapt to\nout-of-distribution tasks by using their previous experiences and beliefs\nobtained through collaboration. In this open-ended social learning paradigm,\n\\collabvoyager paves the way for the democratic development of embodied AI,\nwhere agents learn in deployment from both peer and environmental feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary embodied agents, such as Voyager in Minecraft, have demonstrated\npromising capabilities in open-ended individual learning. However, when powered\nwith open large language models (LLMs), these agents often struggle with\nrudimentary tasks, even when fine-tuned on domain-specific knowledge. Inspired\nby human cultural learning, we present \\collabvoyager, a novel framework that\nenhances Voyager with lifelong collaborative learning through explicit\nperspective-taking. \\collabvoyager introduces three key innovations: (1) theory\nof mind representations linking percepts, beliefs, desires, and actions; (2)\nnatural language communication between agents; and (3) semantic memory of task\nand environment knowledge and episodic memory of collaboration episodes. These\nadvancements enable agents to reason about their and others' mental states,\nempirically addressing two prevalent failure modes: false beliefs and faulty\ntask executions. In mixed-expertise Minecraft experiments, \\collabvoyager\nagents outperform Voyager counterparts, significantly improving task completion\nrate by $66.6\\% (+39.4\\%)$ for collecting one block of dirt and $70.8\\%\n(+20.8\\%)$ for collecting one wood block. They exhibit emergent behaviors like\nknowledge transfer from expert to novice agents and collaborative code\ncorrection. \\collabvoyager agents also demonstrate the ability to adapt to\nout-of-distribution tasks by using their previous experiences and beliefs\nobtained through collaboration. In this open-ended social learning paradigm,\n\\collabvoyager paves the way for the democratic development of embodied AI,\nwhere agents learn in deployment from both peer and environmental feedback."
                },
                "authors": [
                    {
                        "name": "Mircea Lic"
                    },
                    {
                        "name": "Ojas Shirekar"
                    },
                    {
                        "name": "Baptiste Colle"
                    },
                    {
                        "name": "Chirag Raman"
                    }
                ],
                "author_detail": {
                    "name": "Chirag Raman"
                },
                "author": "Chirag Raman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12977v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12977v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16362v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16362v1",
                "updated": "2024-11-25T13:15:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    15,
                    31,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T13:15:31Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    15,
                    31,
                    0,
                    330,
                    0
                ],
                "title": "Optimal switching strategies in multi-drug therapies for chronic\n  diseases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal switching strategies in multi-drug therapies for chronic\n  diseases"
                },
                "summary": "Antimicrobial resistance is a threat to public health with millions of deaths\nlinked to drug resistant infections every year. To mitigate resistance, common\nstrategies that are used are combination therapies and therapy switching.\nHowever, the stochastic nature of pathogenic mutation makes the optimization of\nthese strategies challenging. Here, we propose a two-scale stochastic model\nthat considers the effective evolution of therapies in a multidimensional\nefficacy space, where each dimension represents the efficacy of a specific drug\nin the therapy. The diffusion of therapies within this space is subject to\nstochastic resets, representing therapy switches. The boundaries of the space,\ninferred from coarser pathogen-host dynamics, can be either reflecting or\nabsorbing. Reflecting boundaries impede full recovery of the host, while\nabsorbing boundaries represent the development of antimicrobial resistance,\nleading to therapy failure. We derive analytical expressions for the average\nabsorption times, accounting for both continuous and discrete genomic changes\nusing the frameworks of Langevin and Master equations, respectively. These\nexpressions allow us to evaluate the relevance of times between drug-switches\nand the number of simultaneous drugs in relation to typical timescales for drug\nresistance development. We also explore realistic scenarios where therapy\nconstraints are imposed to the number of administered therapies and/or their\ncosts, finding non-trivial optimal drug-switching protocols that maximize the\ntime before antimicrobial resistance develops while reducing therapy costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Antimicrobial resistance is a threat to public health with millions of deaths\nlinked to drug resistant infections every year. To mitigate resistance, common\nstrategies that are used are combination therapies and therapy switching.\nHowever, the stochastic nature of pathogenic mutation makes the optimization of\nthese strategies challenging. Here, we propose a two-scale stochastic model\nthat considers the effective evolution of therapies in a multidimensional\nefficacy space, where each dimension represents the efficacy of a specific drug\nin the therapy. The diffusion of therapies within this space is subject to\nstochastic resets, representing therapy switches. The boundaries of the space,\ninferred from coarser pathogen-host dynamics, can be either reflecting or\nabsorbing. Reflecting boundaries impede full recovery of the host, while\nabsorbing boundaries represent the development of antimicrobial resistance,\nleading to therapy failure. We derive analytical expressions for the average\nabsorption times, accounting for both continuous and discrete genomic changes\nusing the frameworks of Langevin and Master equations, respectively. These\nexpressions allow us to evaluate the relevance of times between drug-switches\nand the number of simultaneous drugs in relation to typical timescales for drug\nresistance development. We also explore realistic scenarios where therapy\nconstraints are imposed to the number of administered therapies and/or their\ncosts, finding non-trivial optimal drug-switching protocols that maximize the\ntime before antimicrobial resistance develops while reducing therapy costs."
                },
                "authors": [
                    {
                        "name": "Juan Magalang"
                    },
                    {
                        "name": "Javier Aguilar"
                    },
                    {
                        "name": "Jose Perico Esguerra"
                    },
                    {
                        "name": "dgar Roldn"
                    },
                    {
                        "name": "Daniel Sanchez-Taltavull"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Sanchez-Taltavull"
                },
                "author": "Daniel Sanchez-Taltavull",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16362v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16362v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16358v1",
                "updated": "2024-11-25T13:11:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    11,
                    32,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T13:11:32Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    11,
                    32,
                    0,
                    330,
                    0
                ],
                "title": "Centaur 29P/Schwassmann-Wachmann 1 and its near-nucleus environment from\n  a stellar occultation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Centaur 29P/Schwassmann-Wachmann 1 and its near-nucleus environment from\n  a stellar occultation"
                },
                "summary": "Comets offer valuable insights into the early Solar System's conditions and\nprocesses. Stellar occultations enables detailed study of cometary nuclei\ntypically hidden by their coma. Observing the star's light passing through the\ncoma helps infer dust's optical depth near the nucleus and determine dust\nopacity detection limits. 29P/Schwassmann-Wachmann 1, a Centaur with a diameter\nof approximately 60 km, lies in a region transitioning from Centaurs to\nJupiter-Family comets. Our study presents the first-ever observed occultation\nby 29P, allowing in the future a more refined orbit and thus better predictions\nfor other occultations. The light curve reveals a solid-body detection lasting\n$3.65\\pm0.05$ seconds, corresponding to a chord length of approximately 54 km.\nThis provides a lower limit for the object's radius, measured at $27.0\\pm0.7$\nkm. We identified features on both sides of the main-body occultation around\n1,700 km from the nucleus in the sky plane for which upper limits on apparent\nopacity and equivalent width were determined. Gradual dimming within 23 km of\nthe nucleus during ingress only is interpreted as a localised dust cloud/jet\nabove the surface, with an optical depth of approximately $\\tau \\sim 0.18$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comets offer valuable insights into the early Solar System's conditions and\nprocesses. Stellar occultations enables detailed study of cometary nuclei\ntypically hidden by their coma. Observing the star's light passing through the\ncoma helps infer dust's optical depth near the nucleus and determine dust\nopacity detection limits. 29P/Schwassmann-Wachmann 1, a Centaur with a diameter\nof approximately 60 km, lies in a region transitioning from Centaurs to\nJupiter-Family comets. Our study presents the first-ever observed occultation\nby 29P, allowing in the future a more refined orbit and thus better predictions\nfor other occultations. The light curve reveals a solid-body detection lasting\n$3.65\\pm0.05$ seconds, corresponding to a chord length of approximately 54 km.\nThis provides a lower limit for the object's radius, measured at $27.0\\pm0.7$\nkm. We identified features on both sides of the main-body occultation around\n1,700 km from the nucleus in the sky plane for which upper limits on apparent\nopacity and equivalent width were determined. Gradual dimming within 23 km of\nthe nucleus during ingress only is interpreted as a localised dust cloud/jet\nabove the surface, with an optical depth of approximately $\\tau \\sim 0.18$."
                },
                "authors": [
                    {
                        "name": "C. L. Pereira"
                    },
                    {
                        "name": "F. Braga-Ribas"
                    },
                    {
                        "name": "B. Sicardy"
                    },
                    {
                        "name": "B. E. Morgado"
                    },
                    {
                        "name": "J. L. Ortiz"
                    },
                    {
                        "name": "M. Assafin"
                    },
                    {
                        "name": "R. Miles"
                    },
                    {
                        "name": "J. Desmars"
                    },
                    {
                        "name": "J. I. B. Camargo"
                    },
                    {
                        "name": "G. Benedetti-Rossi"
                    },
                    {
                        "name": "M. Kretlow"
                    },
                    {
                        "name": "R. Vieira-Martins"
                    }
                ],
                "author_detail": {
                    "name": "R. Vieira-Martins"
                },
                "author": "R. Vieira-Martins",
                "arxiv_comment": "16 pages. 7 figures. 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16353v1",
                "updated": "2024-11-25T13:04:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    4,
                    28,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T13:04:28Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    4,
                    28,
                    0,
                    330,
                    0
                ],
                "title": "The Two-Hop Curse: LLMs trained on A->B, B->C fail to learn A-->C",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Two-Hop Curse: LLMs trained on A->B, B->C fail to learn A-->C"
                },
                "summary": "While LLMs excel at multi-hop questions (e.g. \"Who is the spouse of the\nperformer of Imagine?\") when using chain-of-thought reasoning (CoT), they\nstruggle when forced to reason internally (without CoT). Previous work on the\nsize and nature of this gap produced mixed evidence with inconclusive results.\nIn this paper, we introduce a controlled setting for investigating two-hop\nreasoning in LLMs, where the above-chance performance constitutes undeniable\nevidence for latent reasoning. We fine-tune LLMs (including Llama 3 8B Instruct\nand GPT-4o) on fictional facts and confirm that they generalize to answering\ntwo-hop questions about them using CoT. We find that models can perform latent\nreasoning when facts appear together during training or in the prompt. However,\nto our surprise, models completely fail at two-hop reasoning without CoT when\nlearned facts only appear in different documents, achieving chance-level\naccuracy and chance-level test loss. We call this complete failure to compose\nseparately learned facts the Two-Hop Curse. Moreover, we evaluate 9 frontier\nLLMs on real-world facts, finding that models completely fail at two-hop no-CoT\nreasoning for over half of question categories while maintaining partial\nsuccess with CoT across most categories. These results suggest that LLMs lack a\ngeneral capability for latent multi-hop reasoning independent of the question\ntype.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While LLMs excel at multi-hop questions (e.g. \"Who is the spouse of the\nperformer of Imagine?\") when using chain-of-thought reasoning (CoT), they\nstruggle when forced to reason internally (without CoT). Previous work on the\nsize and nature of this gap produced mixed evidence with inconclusive results.\nIn this paper, we introduce a controlled setting for investigating two-hop\nreasoning in LLMs, where the above-chance performance constitutes undeniable\nevidence for latent reasoning. We fine-tune LLMs (including Llama 3 8B Instruct\nand GPT-4o) on fictional facts and confirm that they generalize to answering\ntwo-hop questions about them using CoT. We find that models can perform latent\nreasoning when facts appear together during training or in the prompt. However,\nto our surprise, models completely fail at two-hop reasoning without CoT when\nlearned facts only appear in different documents, achieving chance-level\naccuracy and chance-level test loss. We call this complete failure to compose\nseparately learned facts the Two-Hop Curse. Moreover, we evaluate 9 frontier\nLLMs on real-world facts, finding that models completely fail at two-hop no-CoT\nreasoning for over half of question categories while maintaining partial\nsuccess with CoT across most categories. These results suggest that LLMs lack a\ngeneral capability for latent multi-hop reasoning independent of the question\ntype."
                },
                "authors": [
                    {
                        "name": "Mikita Balesni"
                    },
                    {
                        "name": "Tomek Korbak"
                    },
                    {
                        "name": "Owain Evans"
                    }
                ],
                "author_detail": {
                    "name": "Owain Evans"
                },
                "author": "Owain Evans",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01775v2",
                "updated": "2024-11-25T12:45:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    45,
                    19,
                    0,
                    330,
                    0
                ],
                "published": "2024-06-03T20:37:27Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    20,
                    37,
                    27,
                    0,
                    155,
                    0
                ],
                "title": "OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models"
                },
                "summary": "The advent of large language models (LLMs) has revolutionized natural\nlanguage processing, enabling unprecedented capabilities in understanding and\ngenerating human-like text. However, the computational cost and convergence\ntimes associated with fine-tuning these models remain significant challenges.\nLow-Rank Adaptation (LoRA) has emerged as a promising method to mitigate these\nissues by introducing efficient fine-tuning techniques with a reduced number of\ntrainable parameters. In this paper, we present OLoRA, an enhancement to the\nLoRA method that leverages orthonormal matrix initialization through QR\ndecomposition. OLoRA significantly accelerates the convergence of LLM training\nwhile preserving the efficiency benefits of LoRA, such as the number of\ntrainable parameters and GPU memory footprint. Our empirical evaluations\ndemonstrate that OLoRA not only converges faster but also exhibits improved\nperformance compared to standard LoRA across a variety of language modeling\ntasks. This advancement opens new avenues for more efficient and accessible\nfine-tuning of LLMs, potentially enabling broader adoption and innovation in\nnatural language applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of large language models (LLMs) has revolutionized natural\nlanguage processing, enabling unprecedented capabilities in understanding and\ngenerating human-like text. However, the computational cost and convergence\ntimes associated with fine-tuning these models remain significant challenges.\nLow-Rank Adaptation (LoRA) has emerged as a promising method to mitigate these\nissues by introducing efficient fine-tuning techniques with a reduced number of\ntrainable parameters. In this paper, we present OLoRA, an enhancement to the\nLoRA method that leverages orthonormal matrix initialization through QR\ndecomposition. OLoRA significantly accelerates the convergence of LLM training\nwhile preserving the efficiency benefits of LoRA, such as the number of\ntrainable parameters and GPU memory footprint. Our empirical evaluations\ndemonstrate that OLoRA not only converges faster but also exhibits improved\nperformance compared to standard LoRA across a variety of language modeling\ntasks. This advancement opens new avenues for more efficient and accessible\nfine-tuning of LLMs, potentially enabling broader adoption and innovation in\nnatural language applications."
                },
                "authors": [
                    {
                        "name": "Kerim Bykakyz"
                    }
                ],
                "author_detail": {
                    "name": "Kerim Bykakyz"
                },
                "author": "Kerim Bykakyz",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16345v1",
                "updated": "2024-11-25T12:44:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    44,
                    2,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T12:44:02Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    44,
                    2,
                    0,
                    330,
                    0
                ],
                "title": "Preference Optimization for Reasoning with Pseudo Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference Optimization for Reasoning with Pseudo Feedback"
                },
                "summary": "Preference optimization techniques, such as Direct Preference Optimization\n(DPO), are frequently employed to enhance the reasoning capabilities of large\nlanguage models (LLMs) in domains like mathematical reasoning and coding,\ntypically following supervised fine-tuning. These methods rely on high-quality\nlabels for reasoning tasks to generate preference pairs; however, the\navailability of reasoning datasets with human-verified labels is limited. In\nthis study, we introduce a novel approach to generate pseudo feedback for\nreasoning tasks by framing the labeling of solutions to reason problems as an\nevaluation against associated test cases. We explore two forms of pseudo\nfeedback based on test cases: one generated by frontier LLMs and the other by\nextending self-consistency to multi-test-case. We conduct experiments on both\nmathematical reasoning and coding tasks using pseudo feedback for preference\noptimization, and observe improvements across both tasks. Specifically, using\nMathstral-7B as our base model, we improve MATH results from 58.3 to 68.6,\nsurpassing both NuminaMath-72B and GPT-4-Turbo-1106-preview. In GSM8K and\nCollege Math, our scores increase from 85.6 to 90.3 and from 34.3 to 42.3,\nrespectively. Building on Deepseek-coder-7B-v1.5, we achieve a score of 24.6 on\nLiveCodeBench (from 21.1), surpassing Claude-3-Haiku.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference optimization techniques, such as Direct Preference Optimization\n(DPO), are frequently employed to enhance the reasoning capabilities of large\nlanguage models (LLMs) in domains like mathematical reasoning and coding,\ntypically following supervised fine-tuning. These methods rely on high-quality\nlabels for reasoning tasks to generate preference pairs; however, the\navailability of reasoning datasets with human-verified labels is limited. In\nthis study, we introduce a novel approach to generate pseudo feedback for\nreasoning tasks by framing the labeling of solutions to reason problems as an\nevaluation against associated test cases. We explore two forms of pseudo\nfeedback based on test cases: one generated by frontier LLMs and the other by\nextending self-consistency to multi-test-case. We conduct experiments on both\nmathematical reasoning and coding tasks using pseudo feedback for preference\noptimization, and observe improvements across both tasks. Specifically, using\nMathstral-7B as our base model, we improve MATH results from 58.3 to 68.6,\nsurpassing both NuminaMath-72B and GPT-4-Turbo-1106-preview. In GSM8K and\nCollege Math, our scores increase from 85.6 to 90.3 and from 34.3 to 42.3,\nrespectively. Building on Deepseek-coder-7B-v1.5, we achieve a score of 24.6 on\nLiveCodeBench (from 21.1), surpassing Claude-3-Haiku."
                },
                "authors": [
                    {
                        "name": "Fangkai Jiao"
                    },
                    {
                        "name": "Geyang Guo"
                    },
                    {
                        "name": "Xingxing Zhang"
                    },
                    {
                        "name": "Nancy F. Chen"
                    },
                    {
                        "name": "Shafiq Joty"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "28 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16342v1",
                "updated": "2024-11-25T12:38:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    38,
                    59,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T12:38:59Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    38,
                    59,
                    0,
                    330,
                    0
                ],
                "title": "A Data-Driven Approach to Dataflow-Aware Online Scheduling for Graph\n  Neural Network Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Data-Driven Approach to Dataflow-Aware Online Scheduling for Graph\n  Neural Network Inference"
                },
                "summary": "Graph Neural Networks (GNNs) have shown significant promise in various\ndomains, such as recommendation systems, bioinformatics, and network analysis.\nHowever, the irregularity of graph data poses unique challenges for efficient\ncomputation, leading to the development of specialized GNN accelerator\narchitectures that surpass traditional CPU and GPU performance. Despite this,\nthe structural diversity of input graphs results in varying performance across\ndifferent GNN accelerators, depending on their dataflows. This variability in\nperformance due to differing dataflows and graph properties remains largely\nunexplored, limiting the adaptability of GNN accelerators. To address this, we\npropose a data-driven framework for dataflow-aware latency prediction in GNN\ninference. Our approach involves training regressors to predict the latency of\nexecuting specific graphs on particular dataflows, using simulations on\nsynthetic graphs. Experimental results indicate that our regressors can predict\nthe optimal dataflow for a given graph with up to 91.28% accuracy and a Mean\nAbsolute Percentage Error (MAPE) of 3.78%. Additionally, we introduce an online\nscheduling algorithm that uses these regressors to enhance scheduling\ndecisions. Our experiments demonstrate that this algorithm achieves up to\n$3.17\\times$ speedup in mean completion time and $6.26\\times$ speedup in mean\nexecution time compared to the best feasible baseline across all datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have shown significant promise in various\ndomains, such as recommendation systems, bioinformatics, and network analysis.\nHowever, the irregularity of graph data poses unique challenges for efficient\ncomputation, leading to the development of specialized GNN accelerator\narchitectures that surpass traditional CPU and GPU performance. Despite this,\nthe structural diversity of input graphs results in varying performance across\ndifferent GNN accelerators, depending on their dataflows. This variability in\nperformance due to differing dataflows and graph properties remains largely\nunexplored, limiting the adaptability of GNN accelerators. To address this, we\npropose a data-driven framework for dataflow-aware latency prediction in GNN\ninference. Our approach involves training regressors to predict the latency of\nexecuting specific graphs on particular dataflows, using simulations on\nsynthetic graphs. Experimental results indicate that our regressors can predict\nthe optimal dataflow for a given graph with up to 91.28% accuracy and a Mean\nAbsolute Percentage Error (MAPE) of 3.78%. Additionally, we introduce an online\nscheduling algorithm that uses these regressors to enhance scheduling\ndecisions. Our experiments demonstrate that this algorithm achieves up to\n$3.17\\times$ speedup in mean completion time and $6.26\\times$ speedup in mean\nexecution time compared to the best feasible baseline across all datasets."
                },
                "authors": [
                    {
                        "name": "Pol Puigdemont"
                    },
                    {
                        "name": "Enrico Russo"
                    },
                    {
                        "name": "Axel Wassington"
                    },
                    {
                        "name": "Abhijit Das"
                    },
                    {
                        "name": "Sergi Abadal"
                    },
                    {
                        "name": "Maurizio Palesi"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Palesi"
                },
                "author": "Maurizio Palesi",
                "arxiv_doi": "10.1145/3658617.3697660",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658617.3697660",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.16342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for ASP-DAC 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16341v1",
                "updated": "2024-11-25T12:37:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    37,
                    7,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T12:37:07Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    37,
                    7,
                    0,
                    330,
                    0
                ],
                "title": "From CISC to RISC: language-model guided assembly transpilation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From CISC to RISC: language-model guided assembly transpilation"
                },
                "summary": "The transition from x86 to ARM architecture is becoming increasingly common\nacross various domains, primarily driven by ARM's energy efficiency and\nimproved performance across traditional sectors. However, this ISA shift poses\nsignificant challenges, mainly due to the extensive legacy ecosystem of x86\nsoftware and lack of portability across proprietary ecosystems and software\nstacks. This paper introduces CRT, a lightweight LLM-based transpiler that\nautomatically converts x86 assembly to ARM assembly. Our approach bridges the\nfundamental architectural gap between x86's CISC-based and ARM's RISC-based\ncomputing paradigms while preserving program semantics and optimizing\nperformance. We evaluate CRT on diverse real-world applications, achieving\n79.25% translation accuracy from x86 to ARMv5 on our comprehensive test suite,\nand an 88.68% accuracy from x86 to RISC-V. In practical deployments on Apple M2\nhardware (ARMv8), our transpiled code achieves 1.73$\\times$ speedup compared to\nApple's Rosetta 2 virtualization engine, while delivering 2.41$\\times$ memory\nefficiency and 1.47$\\times$ better energy consumption. Through testing and\nanalysis, we show that CRT successfully navigates the CISC/RISC divide and\ngenerates correctly executable RISC code despite machine ``language'' barriers.\nWe release our code, models, training datasets, and benchmarks at:\n\\url{https://ahmedheakl.github.io/asm2asm/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transition from x86 to ARM architecture is becoming increasingly common\nacross various domains, primarily driven by ARM's energy efficiency and\nimproved performance across traditional sectors. However, this ISA shift poses\nsignificant challenges, mainly due to the extensive legacy ecosystem of x86\nsoftware and lack of portability across proprietary ecosystems and software\nstacks. This paper introduces CRT, a lightweight LLM-based transpiler that\nautomatically converts x86 assembly to ARM assembly. Our approach bridges the\nfundamental architectural gap between x86's CISC-based and ARM's RISC-based\ncomputing paradigms while preserving program semantics and optimizing\nperformance. We evaluate CRT on diverse real-world applications, achieving\n79.25% translation accuracy from x86 to ARMv5 on our comprehensive test suite,\nand an 88.68% accuracy from x86 to RISC-V. In practical deployments on Apple M2\nhardware (ARMv8), our transpiled code achieves 1.73$\\times$ speedup compared to\nApple's Rosetta 2 virtualization engine, while delivering 2.41$\\times$ memory\nefficiency and 1.47$\\times$ better energy consumption. Through testing and\nanalysis, we show that CRT successfully navigates the CISC/RISC divide and\ngenerates correctly executable RISC code despite machine ``language'' barriers.\nWe release our code, models, training datasets, and benchmarks at:\n\\url{https://ahmedheakl.github.io/asm2asm/}."
                },
                "authors": [
                    {
                        "name": "Ahmed Heakl"
                    },
                    {
                        "name": "Chaimaa Abi"
                    },
                    {
                        "name": "Rania Hossam"
                    },
                    {
                        "name": "Abdulrahman Mahmoud"
                    }
                ],
                "author_detail": {
                    "name": "Abdulrahman Mahmoud"
                },
                "author": "Abdulrahman Mahmoud",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16337v1",
                "updated": "2024-11-25T12:33:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    33,
                    14,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T12:33:14Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    33,
                    14,
                    0,
                    330,
                    0
                ],
                "title": "Can AI grade your essays? A comparative analysis of large language\n  models and teacher ratings in multidimensional essay scoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can AI grade your essays? A comparative analysis of large language\n  models and teacher ratings in multidimensional essay scoring"
                },
                "summary": "The manual assessment and grading of student writing is a time-consuming yet\ncritical task for teachers. Recent developments in generative AI, such as large\nlanguage models, offer potential solutions to facilitate essay-scoring tasks\nfor teachers. In our study, we evaluate the performance and reliability of both\nopen-source and closed-source LLMs in assessing German student essays,\ncomparing their evaluations to those of 37 teachers across 10 pre-defined\ncriteria (i.e., plot logic, expression). A corpus of 20 real-world essays from\nYear 7 and 8 students was analyzed using five LLMs: GPT-3.5, GPT-4, o1, LLaMA\n3-70B, and Mixtral 8x7B, aiming to provide in-depth insights into LLMs' scoring\ncapabilities. Closed-source GPT models outperform open-source models in both\ninternal consistency and alignment with human ratings, particularly excelling\nin language-related criteria. The novel o1 model outperforms all other LLMs,\nachieving Spearman's $r = .74$ with human assessments in the overall score, and\nan internal consistency of $ICC=.80$. These findings indicate that LLM-based\nassessment can be a useful tool to reduce teacher workload by supporting the\nevaluation of essays, especially with regard to language-related criteria.\nHowever, due to their tendency for higher scores, the models require further\nrefinement to better capture aspects of content quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The manual assessment and grading of student writing is a time-consuming yet\ncritical task for teachers. Recent developments in generative AI, such as large\nlanguage models, offer potential solutions to facilitate essay-scoring tasks\nfor teachers. In our study, we evaluate the performance and reliability of both\nopen-source and closed-source LLMs in assessing German student essays,\ncomparing their evaluations to those of 37 teachers across 10 pre-defined\ncriteria (i.e., plot logic, expression). A corpus of 20 real-world essays from\nYear 7 and 8 students was analyzed using five LLMs: GPT-3.5, GPT-4, o1, LLaMA\n3-70B, and Mixtral 8x7B, aiming to provide in-depth insights into LLMs' scoring\ncapabilities. Closed-source GPT models outperform open-source models in both\ninternal consistency and alignment with human ratings, particularly excelling\nin language-related criteria. The novel o1 model outperforms all other LLMs,\nachieving Spearman's $r = .74$ with human assessments in the overall score, and\nan internal consistency of $ICC=.80$. These findings indicate that LLM-based\nassessment can be a useful tool to reduce teacher workload by supporting the\nevaluation of essays, especially with regard to language-related criteria.\nHowever, due to their tendency for higher scores, the models require further\nrefinement to better capture aspects of content quality."
                },
                "authors": [
                    {
                        "name": "Kathrin Seler"
                    },
                    {
                        "name": "Maurice Frstenberg"
                    },
                    {
                        "name": "Babette Bhler"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Enkelejda Kasneci"
                },
                "author": "Enkelejda Kasneci",
                "arxiv_comment": "Accepted at LAK '25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16331v1",
                "updated": "2024-11-25T12:24:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    24,
                    52,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T12:24:52Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    24,
                    52,
                    0,
                    330,
                    0
                ],
                "title": "Sonic: Shifting Focus to Global Audio Perception in Portrait Animation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sonic: Shifting Focus to Global Audio Perception in Portrait Animation"
                },
                "summary": "The study of talking face generation mainly explores the intricacies of\nsynchronizing facial movements and crafting visually appealing,\ntemporally-coherent animations. However, due to the limited exploration of\nglobal audio perception, current approaches predominantly employ auxiliary\nvisual and spatial knowledge to stabilize the movements, which often results in\nthe deterioration of the naturalness and temporal inconsistencies.Considering\nthe essence of audio-driven animation, the audio signal serves as the ideal and\nunique priors to adjust facial expressions and lip movements, without resorting\nto interference of any visual signals. Based on this motivation, we propose a\nnovel paradigm, dubbed as Sonic, to {s}hift f{o}cus on the exploration of\nglobal audio per{c}ept{i}o{n}.To effectively leverage global audio knowledge,\nwe disentangle it into intra- and inter-clip audio perception and collaborate\nwith both aspects to enhance overall perception.For the intra-clip audio\nperception, 1). \\textbf{Context-enhanced audio learning}, in which long-range\nintra-clip temporal audio knowledge is extracted to provide facial expression\nand lip motion priors implicitly expressed as the tone and speed of speech. 2).\n\\textbf{Motion-decoupled controller}, in which the motion of the head and\nexpression movement are disentangled and independently controlled by\nintra-audio clips. Most importantly, for inter-clip audio perception, as a\nbridge to connect the intra-clips to achieve the global perception,\n\\textbf{Time-aware position shift fusion}, in which the global inter-clip audio\ninformation is considered and fused for long-audio inference via through\nconsecutively time-aware shifted windows. Extensive experiments demonstrate\nthat the novel audio-driven paradigm outperform existing SOTA methodologies in\nterms of video quality, temporally consistency, lip synchronization precision,\nand motion diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The study of talking face generation mainly explores the intricacies of\nsynchronizing facial movements and crafting visually appealing,\ntemporally-coherent animations. However, due to the limited exploration of\nglobal audio perception, current approaches predominantly employ auxiliary\nvisual and spatial knowledge to stabilize the movements, which often results in\nthe deterioration of the naturalness and temporal inconsistencies.Considering\nthe essence of audio-driven animation, the audio signal serves as the ideal and\nunique priors to adjust facial expressions and lip movements, without resorting\nto interference of any visual signals. Based on this motivation, we propose a\nnovel paradigm, dubbed as Sonic, to {s}hift f{o}cus on the exploration of\nglobal audio per{c}ept{i}o{n}.To effectively leverage global audio knowledge,\nwe disentangle it into intra- and inter-clip audio perception and collaborate\nwith both aspects to enhance overall perception.For the intra-clip audio\nperception, 1). \\textbf{Context-enhanced audio learning}, in which long-range\nintra-clip temporal audio knowledge is extracted to provide facial expression\nand lip motion priors implicitly expressed as the tone and speed of speech. 2).\n\\textbf{Motion-decoupled controller}, in which the motion of the head and\nexpression movement are disentangled and independently controlled by\nintra-audio clips. Most importantly, for inter-clip audio perception, as a\nbridge to connect the intra-clips to achieve the global perception,\n\\textbf{Time-aware position shift fusion}, in which the global inter-clip audio\ninformation is considered and fused for long-audio inference via through\nconsecutively time-aware shifted windows. Extensive experiments demonstrate\nthat the novel audio-driven paradigm outperform existing SOTA methodologies in\nterms of video quality, temporally consistency, lip synchronization precision,\nand motion diversity."
                },
                "authors": [
                    {
                        "name": "Xiaozhong Ji"
                    },
                    {
                        "name": "Xiaobin Hu"
                    },
                    {
                        "name": "Zhihong Xu"
                    },
                    {
                        "name": "Junwei Zhu"
                    },
                    {
                        "name": "Chuming Lin"
                    },
                    {
                        "name": "Qingdong He"
                    },
                    {
                        "name": "Jiangning Zhang"
                    },
                    {
                        "name": "Donghao Luo"
                    },
                    {
                        "name": "Yi Chen"
                    },
                    {
                        "name": "Qin Lin"
                    },
                    {
                        "name": "Qinglin Lu"
                    },
                    {
                        "name": "Chengjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chengjie Wang"
                },
                "author": "Chengjie Wang",
                "arxiv_comment": "refer to our main-page \\url{https://jixiaozhong.github.io/Sonic/}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11581v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11581v3",
                "updated": "2024-11-25T12:16:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    16,
                    0,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-18T13:57:35Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    57,
                    35,
                    0,
                    323,
                    0
                ],
                "title": "OASIS: Open Agent Social Interaction Simulations with One Million Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS: Open Agent Social Interaction Simulations with One Million Agents"
                },
                "summary": "There has been a growing interest in enhancing rule-based agent-based models\n(ABMs) for social media platforms (i.e., X, Reddit) with more realistic large\nlanguage model (LLM) agents, thereby allowing for a more nuanced study of\ncomplex systems. As a result, several LLM-based ABMs have been proposed in the\npast year. While they hold promise, each simulator is specifically designed to\nstudy a particular scenario, making it time-consuming and resource-intensive to\nexplore other phenomena using the same ABM. Additionally, these models simulate\nonly a limited number of agents, whereas real-world social media platforms\ninvolve millions of users. To this end, we propose OASIS, a generalizable and\nscalable social media simulator. OASIS is designed based on real-world social\nmedia platforms, incorporating dynamically updated environments (i.e., dynamic\nsocial networks and post information), diverse action spaces (i.e., following,\ncommenting), and recommendation systems (i.e., interest-based and\nhot-score-based). Additionally, OASIS supports large-scale user simulations,\ncapable of modeling up to one million users. With these features, OASIS can be\neasily extended to different social media platforms to study large-scale group\nphenomena and behaviors. We replicate various social phenomena, including\ninformation spreading, group polarization, and herd effects across X and Reddit\nplatforms. Moreover, we provide observations of social phenomena at different\nagent group scales. We observe that the larger agent group scale leads to more\nenhanced group dynamics and more diverse and helpful agents' opinions. These\nfindings demonstrate OASIS's potential as a powerful tool for studying complex\nsystems in digital environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been a growing interest in enhancing rule-based agent-based models\n(ABMs) for social media platforms (i.e., X, Reddit) with more realistic large\nlanguage model (LLM) agents, thereby allowing for a more nuanced study of\ncomplex systems. As a result, several LLM-based ABMs have been proposed in the\npast year. While they hold promise, each simulator is specifically designed to\nstudy a particular scenario, making it time-consuming and resource-intensive to\nexplore other phenomena using the same ABM. Additionally, these models simulate\nonly a limited number of agents, whereas real-world social media platforms\ninvolve millions of users. To this end, we propose OASIS, a generalizable and\nscalable social media simulator. OASIS is designed based on real-world social\nmedia platforms, incorporating dynamically updated environments (i.e., dynamic\nsocial networks and post information), diverse action spaces (i.e., following,\ncommenting), and recommendation systems (i.e., interest-based and\nhot-score-based). Additionally, OASIS supports large-scale user simulations,\ncapable of modeling up to one million users. With these features, OASIS can be\neasily extended to different social media platforms to study large-scale group\nphenomena and behaviors. We replicate various social phenomena, including\ninformation spreading, group polarization, and herd effects across X and Reddit\nplatforms. Moreover, we provide observations of social phenomena at different\nagent group scales. We observe that the larger agent group scale leads to more\nenhanced group dynamics and more diverse and helpful agents' opinions. These\nfindings demonstrate OASIS's potential as a powerful tool for studying complex\nsystems in digital environments."
                },
                "authors": [
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Zaibin Zhang"
                    },
                    {
                        "name": "Zirui Zheng"
                    },
                    {
                        "name": "Yuxian Jiang"
                    },
                    {
                        "name": "Ziyue Gan"
                    },
                    {
                        "name": "Zhiyu Wang"
                    },
                    {
                        "name": "Zijian Ling"
                    },
                    {
                        "name": "Jinsong Chen"
                    },
                    {
                        "name": "Martz Ma"
                    },
                    {
                        "name": "Bowen Dong"
                    },
                    {
                        "name": "Prateek Gupta"
                    },
                    {
                        "name": "Shuyue Hu"
                    },
                    {
                        "name": "Zhenfei Yin"
                    },
                    {
                        "name": "Guohao Li"
                    },
                    {
                        "name": "Xu Jia"
                    },
                    {
                        "name": "Lijun Wang"
                    },
                    {
                        "name": "Bernard Ghanem"
                    },
                    {
                        "name": "Huchuan Lu"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11581v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11581v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20807v2",
                "updated": "2024-11-25T12:14:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    14,
                    11,
                    0,
                    330,
                    0
                ],
                "published": "2024-10-28T07:54:29Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    54,
                    29,
                    0,
                    302,
                    0
                ],
                "title": "Long-Tailed Out-of-Distribution Detection via Normalized Outlier\n  Distribution Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Tailed Out-of-Distribution Detection via Normalized Outlier\n  Distribution Adaptation"
                },
                "summary": "One key challenge in Out-of-Distribution (OOD) detection is the absence of\nground-truth OOD samples during training. One principled approach to address\nthis issue is to use samples from external datasets as outliers (i.e., pseudo\nOOD samples) to train OOD detectors. However, we find empirically that the\noutlier samples often present a distribution shift compared to the true OOD\nsamples, especially in Long-Tailed Recognition (LTR) scenarios, where ID\nclasses are heavily imbalanced, \\ie, the true OOD samples exhibit very\ndifferent probability distribution to the head and tailed ID classes from the\noutliers. In this work, we propose a novel approach, namely normalized outlier\ndistribution adaptation (AdaptOD), to tackle this distribution shift problem.\nOne of its key components is dynamic outlier distribution adaptation that\neffectively adapts a vanilla outlier distribution based on the outlier samples\nto the true OOD distribution by utilizing the OOD knowledge in the predicted\nOOD samples during inference. Further, to obtain a more reliable set of\npredicted OOD samples on long-tailed ID data, a novel dual-normalized energy\nloss is introduced in AdaptOD, which leverages class- and sample-wise\nnormalized energy to enforce a more balanced prediction energy on imbalanced ID\nsamples. This helps avoid bias toward the head samples and learn a\nsubstantially better vanilla outlier distribution than existing energy losses\nduring training. It also eliminates the need of manually tuning the sensitive\nmargin hyperparameters in energy losses. Empirical results on three popular\nbenchmarks for OOD detection in LTR show the superior performance of AdaptOD\nover state-of-the-art methods. Code is available at\nhttps://github.com/mala-lab/AdaptOD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One key challenge in Out-of-Distribution (OOD) detection is the absence of\nground-truth OOD samples during training. One principled approach to address\nthis issue is to use samples from external datasets as outliers (i.e., pseudo\nOOD samples) to train OOD detectors. However, we find empirically that the\noutlier samples often present a distribution shift compared to the true OOD\nsamples, especially in Long-Tailed Recognition (LTR) scenarios, where ID\nclasses are heavily imbalanced, \\ie, the true OOD samples exhibit very\ndifferent probability distribution to the head and tailed ID classes from the\noutliers. In this work, we propose a novel approach, namely normalized outlier\ndistribution adaptation (AdaptOD), to tackle this distribution shift problem.\nOne of its key components is dynamic outlier distribution adaptation that\neffectively adapts a vanilla outlier distribution based on the outlier samples\nto the true OOD distribution by utilizing the OOD knowledge in the predicted\nOOD samples during inference. Further, to obtain a more reliable set of\npredicted OOD samples on long-tailed ID data, a novel dual-normalized energy\nloss is introduced in AdaptOD, which leverages class- and sample-wise\nnormalized energy to enforce a more balanced prediction energy on imbalanced ID\nsamples. This helps avoid bias toward the head samples and learn a\nsubstantially better vanilla outlier distribution than existing energy losses\nduring training. It also eliminates the need of manually tuning the sensitive\nmargin hyperparameters in energy losses. Empirical results on three popular\nbenchmarks for OOD detection in LTR show the superior performance of AdaptOD\nover state-of-the-art methods. Code is available at\nhttps://github.com/mala-lab/AdaptOD."
                },
                "authors": [
                    {
                        "name": "Wenjun Miao"
                    },
                    {
                        "name": "Guansong Pang"
                    },
                    {
                        "name": "Jin Zheng"
                    },
                    {
                        "name": "Xiao Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Bai"
                },
                "author": "Xiao Bai",
                "arxiv_comment": "NeurIPS2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16318v1",
                "updated": "2024-11-25T12:11:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    11,
                    5,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T12:11:05Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    11,
                    5,
                    0,
                    330,
                    0
                ],
                "title": "One Diffusion to Generate Them All",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Diffusion to Generate Them All"
                },
                "summary": "We introduce OneDiffusion, a versatile, large-scale diffusion model that\nseamlessly supports bidirectional image synthesis and understanding across\ndiverse tasks. It enables conditional generation from inputs such as text,\ndepth, pose, layout, and semantic maps, while also handling tasks like image\ndeblurring, upscaling, and reverse processes such as depth estimation and\nsegmentation. Additionally, OneDiffusion allows for multi-view generation,\ncamera pose estimation, and instant personalization using sequential image\ninputs. Our model takes a straightforward yet effective approach by treating\nall tasks as frame sequences with varying noise scales during training,\nallowing any frame to act as a conditioning image at inference time. Our\nunified training framework removes the need for specialized architectures,\nsupports scalable multi-task training, and adapts smoothly to any resolution,\nenhancing both generalization and scalability. Experimental results demonstrate\ncompetitive performance across tasks in both generation and prediction such as\ntext-to-image, multiview generation, ID preservation, depth estimation and\ncamera pose estimation despite relatively small training dataset. Our code and\ncheckpoint are freely available at https://github.com/lehduong/OneDiffusion",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce OneDiffusion, a versatile, large-scale diffusion model that\nseamlessly supports bidirectional image synthesis and understanding across\ndiverse tasks. It enables conditional generation from inputs such as text,\ndepth, pose, layout, and semantic maps, while also handling tasks like image\ndeblurring, upscaling, and reverse processes such as depth estimation and\nsegmentation. Additionally, OneDiffusion allows for multi-view generation,\ncamera pose estimation, and instant personalization using sequential image\ninputs. Our model takes a straightforward yet effective approach by treating\nall tasks as frame sequences with varying noise scales during training,\nallowing any frame to act as a conditioning image at inference time. Our\nunified training framework removes the need for specialized architectures,\nsupports scalable multi-task training, and adapts smoothly to any resolution,\nenhancing both generalization and scalability. Experimental results demonstrate\ncompetitive performance across tasks in both generation and prediction such as\ntext-to-image, multiview generation, ID preservation, depth estimation and\ncamera pose estimation despite relatively small training dataset. Our code and\ncheckpoint are freely available at https://github.com/lehduong/OneDiffusion"
                },
                "authors": [
                    {
                        "name": "Duong H. Le"
                    },
                    {
                        "name": "Tuan Pham"
                    },
                    {
                        "name": "Sangho Lee"
                    },
                    {
                        "name": "Christopher Clark"
                    },
                    {
                        "name": "Aniruddha Kembhavi"
                    },
                    {
                        "name": "Stephan Mandt"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Jiasen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiasen Lu"
                },
                "author": "Jiasen Lu",
                "arxiv_comment": "two first authors contribute equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16316v1",
                "updated": "2024-11-25T12:09:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    9,
                    43,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T12:09:43Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    9,
                    43,
                    0,
                    330,
                    0
                ],
                "title": "Monocular Lane Detection Based on Deep Learning: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monocular Lane Detection Based on Deep Learning: A Survey"
                },
                "summary": "Lane detection plays an important role in autonomous driving perception\nsystem. As deep learning algorithms gain popularity, monocular lane detection\nmethods based on deep learning have demonstrated superior performance and\nemerged as a key research direction in autonomous driving perception. The core\ndesign of these algorithmic frameworks can be summarized as follows: (1) Task\nparadigm, focusing on lane instance-level discrimination; (2) Lane modeling,\nrepresenting lanes as a set of learnable parameters in the neural network; (3)\nGlobal context supplementation, enhancing the detection of obscured lanes; (4)\nPerspective effect elimination, providing 3D lanes usable for downstream\napplications. From these perspectives, this paper presents a comprehensive\noverview of existing methods, encompassing both the increasingly mature 2D lane\ndetection approaches and the developing 3D lane detection works. For a\nrelatively fair comparison, in addition to comparing the performance of\nmainstream methods on different benchmarks, their inference speed is also\ninvestigated under a unified setting. Moreover, we present some extended works\non lane detection, including multi-task perception, video lane detection,\nonline high-definition (HD) map construction, and lane topology reasoning, to\noffer readers a comprehensive roadmap for the evolution of lane detection.\nFinally, we point out some potential future research directions in this field.\nWe exhaustively collect the papers and codes of existing works at\nhttps://github.com/Core9724/Awesome-Lane-Detection and will keep tracing the\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lane detection plays an important role in autonomous driving perception\nsystem. As deep learning algorithms gain popularity, monocular lane detection\nmethods based on deep learning have demonstrated superior performance and\nemerged as a key research direction in autonomous driving perception. The core\ndesign of these algorithmic frameworks can be summarized as follows: (1) Task\nparadigm, focusing on lane instance-level discrimination; (2) Lane modeling,\nrepresenting lanes as a set of learnable parameters in the neural network; (3)\nGlobal context supplementation, enhancing the detection of obscured lanes; (4)\nPerspective effect elimination, providing 3D lanes usable for downstream\napplications. From these perspectives, this paper presents a comprehensive\noverview of existing methods, encompassing both the increasingly mature 2D lane\ndetection approaches and the developing 3D lane detection works. For a\nrelatively fair comparison, in addition to comparing the performance of\nmainstream methods on different benchmarks, their inference speed is also\ninvestigated under a unified setting. Moreover, we present some extended works\non lane detection, including multi-task perception, video lane detection,\nonline high-definition (HD) map construction, and lane topology reasoning, to\noffer readers a comprehensive roadmap for the evolution of lane detection.\nFinally, we point out some potential future research directions in this field.\nWe exhaustively collect the papers and codes of existing works at\nhttps://github.com/Core9724/Awesome-Lane-Detection and will keep tracing the\nresearch."
                },
                "authors": [
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Haiyun Guo"
                    },
                    {
                        "name": "Kuan Zhu"
                    },
                    {
                        "name": "Bingke Zhu"
                    },
                    {
                        "name": "Xu Zhao"
                    },
                    {
                        "name": "Jianwu Fang"
                    },
                    {
                        "name": "Jinqiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinqiao Wang"
                },
                "author": "Jinqiao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16313v1",
                "updated": "2024-11-25T12:05:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    5,
                    49,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T12:05:49Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    5,
                    49,
                    0,
                    330,
                    0
                ],
                "title": "CATP-LLM: Empowering Large Language Models for Cost-Aware Tool Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CATP-LLM: Empowering Large Language Models for Cost-Aware Tool Planning"
                },
                "summary": "Utilizing large language models (LLMs) for tool planning has emerged as a\npromising avenue for developing general AI systems, where LLMs automatically\nschedule external tools (e.g. vision models) to tackle complex tasks based on\ntask descriptions. To push this paradigm toward practical applications, it is\ncrucial for LLMs to consider tool execution costs (e.g. execution time) for\ntool planning. Unfortunately, prior studies overlook the tool execution costs,\nleading to the generation of expensive plans of which the costs outweigh task\nperformance. To fill this gap, we propose the Cost-Aware Tool Planning with\nLLMs (CATP-LLM) framework, which for the first time provides a coherent design\nto empower LLMs for cost-aware tool planning. Specifically, CATP-LLM\nincorporates a tool planning language to enhance the LLM to generate\nnon-sequential plans of multiple branches for efficient concurrent tool\nexecution and cost reduction. Moreover, it further designs a cost-aware offline\nreinforcement learning algorithm to fine-tune the LLM to optimize the\nperformance-cost trade-off in tool planning. In lack of public cost-related\ndatasets, we further present OpenCATP, the first platform for cost-aware\nplanning evaluation. Experiments on OpenCATP show that CATP-LLM outperforms\nGPT-4 even when using Llama2-7B as its backbone, with the average improvement\nof 28.2%-30.2% higher plan performance and 24.7%-45.8% lower costs even on the\nchallenging planning tasks. The codes of CATP-LLM and OpenCATP will be publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing large language models (LLMs) for tool planning has emerged as a\npromising avenue for developing general AI systems, where LLMs automatically\nschedule external tools (e.g. vision models) to tackle complex tasks based on\ntask descriptions. To push this paradigm toward practical applications, it is\ncrucial for LLMs to consider tool execution costs (e.g. execution time) for\ntool planning. Unfortunately, prior studies overlook the tool execution costs,\nleading to the generation of expensive plans of which the costs outweigh task\nperformance. To fill this gap, we propose the Cost-Aware Tool Planning with\nLLMs (CATP-LLM) framework, which for the first time provides a coherent design\nto empower LLMs for cost-aware tool planning. Specifically, CATP-LLM\nincorporates a tool planning language to enhance the LLM to generate\nnon-sequential plans of multiple branches for efficient concurrent tool\nexecution and cost reduction. Moreover, it further designs a cost-aware offline\nreinforcement learning algorithm to fine-tune the LLM to optimize the\nperformance-cost trade-off in tool planning. In lack of public cost-related\ndatasets, we further present OpenCATP, the first platform for cost-aware\nplanning evaluation. Experiments on OpenCATP show that CATP-LLM outperforms\nGPT-4 even when using Llama2-7B as its backbone, with the average improvement\nof 28.2%-30.2% higher plan performance and 24.7%-45.8% lower costs even on the\nchallenging planning tasks. The codes of CATP-LLM and OpenCATP will be publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Duo Wu"
                    },
                    {
                        "name": "Jinghe Wang"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Yanning Zhang"
                    },
                    {
                        "name": "Le Sun"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "arxiv_comment": "In submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00208v2",
                "updated": "2024-11-25T12:04:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    4,
                    18,
                    0,
                    330,
                    0
                ],
                "published": "2024-10-31T21:07:58Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    21,
                    7,
                    58,
                    3,
                    305,
                    0
                ],
                "title": "Using Large Language Models for a standard assessment mapping for\n  sustainable communities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models for a standard assessment mapping for\n  sustainable communities"
                },
                "summary": "This paper presents a new approach to urban sustainability assessment through\nthe use of Large Language Models (LLMs) to streamline the use of the ISO 37101\nframework to automate and standardise the assessment of urban initiatives\nagainst the six \"sustainability purposes\" and twelve \"issues\" outlined in the\nstandard. The methodology includes the development of a custom prompt based on\nthe standard definitions and its application to two different datasets: 527\nprojects from the Paris Participatory Budget and 398 activities from the\nPROBONO Horizon 2020 project. The results show the effectiveness of LLMs in\nquickly and consistently categorising different urban initiatives according to\nsustainability criteria. The approach is particularly promising when it comes\nto breaking down silos in urban planning by providing a holistic view of the\nimpact of projects. The paper discusses the advantages of this method over\ntraditional human-led assessments, including significant time savings and\nimproved consistency. However, it also points out the importance of human\nexpertise in interpreting results and ethical considerations. This study\nhopefully can contribute to the growing body of work on AI applications in\nurban planning and provides a novel method for operationalising standardised\nsustainability frameworks in different urban contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a new approach to urban sustainability assessment through\nthe use of Large Language Models (LLMs) to streamline the use of the ISO 37101\nframework to automate and standardise the assessment of urban initiatives\nagainst the six \"sustainability purposes\" and twelve \"issues\" outlined in the\nstandard. The methodology includes the development of a custom prompt based on\nthe standard definitions and its application to two different datasets: 527\nprojects from the Paris Participatory Budget and 398 activities from the\nPROBONO Horizon 2020 project. The results show the effectiveness of LLMs in\nquickly and consistently categorising different urban initiatives according to\nsustainability criteria. The approach is particularly promising when it comes\nto breaking down silos in urban planning by providing a holistic view of the\nimpact of projects. The paper discusses the advantages of this method over\ntraditional human-led assessments, including significant time savings and\nimproved consistency. However, it also points out the importance of human\nexpertise in interpreting results and ethical considerations. This study\nhopefully can contribute to the growing body of work on AI applications in\nurban planning and provides a novel method for operationalising standardised\nsustainability frameworks in different urban contexts."
                },
                "authors": [
                    {
                        "name": "Luc Jonveaux"
                    }
                ],
                "author_detail": {
                    "name": "Luc Jonveaux"
                },
                "author": "Luc Jonveaux",
                "arxiv_comment": "8 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.4.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13474v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13474v2",
                "updated": "2024-11-25T12:03:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    3,
                    58,
                    0,
                    330,
                    0
                ],
                "published": "2024-08-24T05:18:50Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    5,
                    18,
                    50,
                    5,
                    237,
                    0
                ],
                "title": "Ridge, lasso, and elastic-net estimations of the modified Poisson and\n  least-squares regressions for binary outcome data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ridge, lasso, and elastic-net estimations of the modified Poisson and\n  least-squares regressions for binary outcome data"
                },
                "summary": "Logistic regression is a standard method in multivariate analysis for binary\noutcome data in epidemiological and clinical studies; however, the resultant\nodds-ratio estimates fail to provide directly interpretable effect measures.\nThe modified Poisson and least-squares regressions are alternative standard\nmethods that can provide risk-ratio and risk difference estimates without\ncomputational problems. However, the bias and invalid inference problems of\nthese regression analyses under small or sparse data conditions (i.e.,the\n\"separation\" problem) have been insufficiently investigated. We show that the\nseparation problem can adversely affect the inferences of the modified Poisson\nand least squares regressions, and to address these issues, we apply the ridge,\nlasso, and elastic-net estimating approaches to the two regression methods. As\nthe methods are not founded on the maximum likelihood principle, we propose\nregularized quasi-likelihood approaches based on the estimating equations for\nthese generalized linear models. The methods provide stable shrinkage estimates\nof risk ratios and risk differences even under separation conditions, and the\nlasso and elastic-net approaches enable simultaneous variable selection. We\nprovide a bootstrap method to calculate the confidence intervals on the basis\nof the regularized quasi-likelihood estimation. The proposed methods are\napplied to a hematopoietic stem cell transplantation cohort study and the\nNational Child Development Survey. We also provide an R package, regconfint, to\nimplement these methods with simple commands.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logistic regression is a standard method in multivariate analysis for binary\noutcome data in epidemiological and clinical studies; however, the resultant\nodds-ratio estimates fail to provide directly interpretable effect measures.\nThe modified Poisson and least-squares regressions are alternative standard\nmethods that can provide risk-ratio and risk difference estimates without\ncomputational problems. However, the bias and invalid inference problems of\nthese regression analyses under small or sparse data conditions (i.e.,the\n\"separation\" problem) have been insufficiently investigated. We show that the\nseparation problem can adversely affect the inferences of the modified Poisson\nand least squares regressions, and to address these issues, we apply the ridge,\nlasso, and elastic-net estimating approaches to the two regression methods. As\nthe methods are not founded on the maximum likelihood principle, we propose\nregularized quasi-likelihood approaches based on the estimating equations for\nthese generalized linear models. The methods provide stable shrinkage estimates\nof risk ratios and risk differences even under separation conditions, and the\nlasso and elastic-net approaches enable simultaneous variable selection. We\nprovide a bootstrap method to calculate the confidence intervals on the basis\nof the regularized quasi-likelihood estimation. The proposed methods are\napplied to a hematopoietic stem cell transplantation cohort study and the\nNational Child Development Survey. We also provide an R package, regconfint, to\nimplement these methods with simple commands."
                },
                "authors": [
                    {
                        "name": "Takahiro Kitano"
                    },
                    {
                        "name": "Hisashi Noma"
                    }
                ],
                "author_detail": {
                    "name": "Hisashi Noma"
                },
                "author": "Hisashi Noma",
                "arxiv_comment": "31 pages, 4 figures, 2 supplementary tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13474v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13474v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.13713v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.13713v3",
                "updated": "2024-11-25T12:03:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    3,
                    18,
                    0,
                    330,
                    0
                ],
                "published": "2023-08-25T23:58:38Z",
                "published_parsed": [
                    2023,
                    8,
                    25,
                    23,
                    58,
                    38,
                    4,
                    237,
                    0
                ],
                "title": "Causally Sound Priors for Binary Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causally Sound Priors for Binary Experiments"
                },
                "summary": "We introduce the BREASE framework for the Bayesian analysis of randomized\ncontrolled trials with a binary treatment and a binary outcome. Approaching the\nproblem from a causal inference perspective, we propose parameterizing the\nlikelihood in terms of the baselinerisk, efficacy, and adverse side effects of\nthe treatment, along with a flexible, yet intuitive and tractable jointly\nindependent beta prior distribution on these parameters, which we show to be a\ngeneralization of the Dirichlet prior for the joint distribution of potential\noutcomes. Our approach has a number of desirable characteristics when compared\nto current mainstream alternatives: (i) it naturally induces prior dependence\nbetween expected outcomes in the treatment and control groups; (ii) as the\nbaseline risk, efficacy and risk of adverse side effects are quantities\ncommonly present in the clinicians' vocabulary, the hyperparameters of the\nprior are directly interpretable, thus facilitating the elicitation of prior\nknowledge and sensitivity analysis; and (iii) we provide analytical formulae\nfor the marginal likelihood, Bayes factor, and other posterior quantities, as\nwell as an exact posterior sampling algorithm and an accurate and fast\ndata-augmented Gibbs sampler in cases where traditional MCMC fails. Empirical\nexamples demonstrate the utility of our methods for estimation, hypothesis\ntesting, and sensitivity analysis of treatment effects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the BREASE framework for the Bayesian analysis of randomized\ncontrolled trials with a binary treatment and a binary outcome. Approaching the\nproblem from a causal inference perspective, we propose parameterizing the\nlikelihood in terms of the baselinerisk, efficacy, and adverse side effects of\nthe treatment, along with a flexible, yet intuitive and tractable jointly\nindependent beta prior distribution on these parameters, which we show to be a\ngeneralization of the Dirichlet prior for the joint distribution of potential\noutcomes. Our approach has a number of desirable characteristics when compared\nto current mainstream alternatives: (i) it naturally induces prior dependence\nbetween expected outcomes in the treatment and control groups; (ii) as the\nbaseline risk, efficacy and risk of adverse side effects are quantities\ncommonly present in the clinicians' vocabulary, the hyperparameters of the\nprior are directly interpretable, thus facilitating the elicitation of prior\nknowledge and sensitivity analysis; and (iii) we provide analytical formulae\nfor the marginal likelihood, Bayes factor, and other posterior quantities, as\nwell as an exact posterior sampling algorithm and an accurate and fast\ndata-augmented Gibbs sampler in cases where traditional MCMC fails. Empirical\nexamples demonstrate the utility of our methods for estimation, hypothesis\ntesting, and sensitivity analysis of treatment effects."
                },
                "authors": [
                    {
                        "name": "Nicholas J. Irons"
                    },
                    {
                        "name": "Carlos Cinelli"
                    }
                ],
                "author_detail": {
                    "name": "Carlos Cinelli"
                },
                "author": "Carlos Cinelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.13713v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.13713v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16311v1",
                "updated": "2024-11-25T12:01:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    1,
                    6,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T12:01:06Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    1,
                    6,
                    0,
                    330,
                    0
                ],
                "title": "Bayesian models for missing and misclassified variables using integrated\n  nested Laplace approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian models for missing and misclassified variables using integrated\n  nested Laplace approximations"
                },
                "summary": "Misclassified variables used in regression models, either as a covariate or\nas the response, may lead to biased estimators and incorrect inference. Even\nthough Bayesian models to adjust for misclassification error exist, it has not\nbeen shown how these models can be implemented using integrated nested Laplace\napproximation (INLA), a popular framework for fitting Bayesian models due to\nits computational efficiency. Since INLA requires the latent field to be\nGaussian, and the Bayesian models adjusting for covariate misclassification\nerror necessarily introduce a latent categorical variable, it is not obvious\nhow to fit these models in INLA. Here, we show how INLA can be combined with\nimportance sampling to overcome this limitation. We also discuss how to account\nfor a misclassified response variable using INLA directly without any\nadditional sampling procedure. The proposed methods are illustrated through a\nnumber of simulations and applications to real-world data, and all examples are\npresented with detailed code in the supporting information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Misclassified variables used in regression models, either as a covariate or\nas the response, may lead to biased estimators and incorrect inference. Even\nthough Bayesian models to adjust for misclassification error exist, it has not\nbeen shown how these models can be implemented using integrated nested Laplace\napproximation (INLA), a popular framework for fitting Bayesian models due to\nits computational efficiency. Since INLA requires the latent field to be\nGaussian, and the Bayesian models adjusting for covariate misclassification\nerror necessarily introduce a latent categorical variable, it is not obvious\nhow to fit these models in INLA. Here, we show how INLA can be combined with\nimportance sampling to overcome this limitation. We also discuss how to account\nfor a misclassified response variable using INLA directly without any\nadditional sampling procedure. The proposed methods are illustrated through a\nnumber of simulations and applications to real-world data, and all examples are\npresented with detailed code in the supporting information."
                },
                "authors": [
                    {
                        "name": "Emma Skarstein"
                    },
                    {
                        "name": "Leonardo Soares Bastos"
                    },
                    {
                        "name": "Hvard Rue"
                    },
                    {
                        "name": "Stefanie Muff"
                    }
                ],
                "author_detail": {
                    "name": "Stefanie Muff"
                },
                "author": "Stefanie Muff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16310v1",
                "updated": "2024-11-25T11:57:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    11,
                    57,
                    48,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T11:57:48Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    11,
                    57,
                    48,
                    0,
                    330,
                    0
                ],
                "title": "Functionality understanding and segmentation in 3D scenes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functionality understanding and segmentation in 3D scenes"
                },
                "summary": "Understanding functionalities in 3D scenes involves interpreting natural\nlanguage descriptions to locate functional interactive objects, such as handles\nand buttons, in a 3D environment. Functionality understanding is highly\nchallenging, as it requires both world knowledge to interpret language and\nspatial perception to identify fine-grained objects. For example, given a task\nlike 'turn on the ceiling light', an embodied AI agent must infer that it needs\nto locate the light switch, even though the switch is not explicitly mentioned\nin the task description. To date, no dedicated methods have been developed for\nthis problem. In this paper, we introduce Fun3DU, the first approach designed\nfor functionality understanding in 3D scenes. Fun3DU uses a language model to\nparse the task description through Chain-of-Thought reasoning in order to\nidentify the object of interest. The identified object is segmented across\nmultiple views of the captured scene by using a vision and language model. The\nsegmentation results from each view are lifted in 3D and aggregated into the\npoint cloud using geometric information. Fun3DU is training-free, relying\nentirely on pre-trained models. We evaluate Fun3DU on SceneFun3D, the most\nrecent and only dataset to benchmark this task, which comprises over 3000 task\ndescriptions on 230 scenes. Our method significantly outperforms\nstate-of-the-art open-vocabulary 3D segmentation approaches. Code will be\nreleased publicly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding functionalities in 3D scenes involves interpreting natural\nlanguage descriptions to locate functional interactive objects, such as handles\nand buttons, in a 3D environment. Functionality understanding is highly\nchallenging, as it requires both world knowledge to interpret language and\nspatial perception to identify fine-grained objects. For example, given a task\nlike 'turn on the ceiling light', an embodied AI agent must infer that it needs\nto locate the light switch, even though the switch is not explicitly mentioned\nin the task description. To date, no dedicated methods have been developed for\nthis problem. In this paper, we introduce Fun3DU, the first approach designed\nfor functionality understanding in 3D scenes. Fun3DU uses a language model to\nparse the task description through Chain-of-Thought reasoning in order to\nidentify the object of interest. The identified object is segmented across\nmultiple views of the captured scene by using a vision and language model. The\nsegmentation results from each view are lifted in 3D and aggregated into the\npoint cloud using geometric information. Fun3DU is training-free, relying\nentirely on pre-trained models. We evaluate Fun3DU on SceneFun3D, the most\nrecent and only dataset to benchmark this task, which comprises over 3000 task\ndescriptions on 230 scenes. Our method significantly outperforms\nstate-of-the-art open-vocabulary 3D segmentation approaches. Code will be\nreleased publicly."
                },
                "authors": [
                    {
                        "name": "Jaime Corsetti"
                    },
                    {
                        "name": "Francesco Giuliari"
                    },
                    {
                        "name": "Alice Fasoli"
                    },
                    {
                        "name": "Davide Boscaini"
                    },
                    {
                        "name": "Fabio Poiesi"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Poiesi"
                },
                "author": "Fabio Poiesi",
                "arxiv_comment": "Technical report. 20 pages, 12 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12788v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12788v2",
                "updated": "2024-11-25T11:54:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    11,
                    54,
                    3,
                    0,
                    330,
                    0
                ],
                "published": "2024-10-16T17:59:32Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    59,
                    32,
                    2,
                    290,
                    0
                ],
                "title": "Meta-Chunking: Learning Efficient Text Segmentation via Logical\n  Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-Chunking: Learning Efficient Text Segmentation via Logical\n  Perception"
                },
                "summary": "Retrieval-Augmented Generation (RAG), while serving as a viable complement to\nlarge language models (LLMs), often overlooks the crucial aspect of text\nchunking within its pipeline, which impacts the quality of knowledge-intensive\ntasks. This paper introduces the concept of Meta-Chunking, which refers to a\ngranularity between sentences and paragraphs, consisting of a collection of\nsentences within a paragraph that have deep linguistic logical connections. To\nimplement Meta-Chunking, we designed Perplexity (PPL) Chunking, which balances\nperformance and speed, and precisely identifies the boundaries of text chunks\nby analyzing the characteristics of context perplexity distribution.\nAdditionally, considering the inherent complexity of different texts, we\npropose a strategy that combines PPL Chunking with dynamic merging to achieve a\nbalance between fine-grained and coarse-grained text chunking. Experiments\nconducted on eleven datasets demonstrate that Meta-Chunking can more\nefficiently improve the performance of single-hop and multi-hop question\nanswering based on RAG. For instance, on the 2WikiMultihopQA dataset, it\noutperforms similarity chunking by 1.32 while only consuming 45.8% of the time.\nFurthermore, through the analysis of models of various scales and types, we\nobserved that PPL Chunking exhibits notable flexibility and adaptability. Our\ncode is available at https://github.com/IAAR-Shanghai/Meta-Chunking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG), while serving as a viable complement to\nlarge language models (LLMs), often overlooks the crucial aspect of text\nchunking within its pipeline, which impacts the quality of knowledge-intensive\ntasks. This paper introduces the concept of Meta-Chunking, which refers to a\ngranularity between sentences and paragraphs, consisting of a collection of\nsentences within a paragraph that have deep linguistic logical connections. To\nimplement Meta-Chunking, we designed Perplexity (PPL) Chunking, which balances\nperformance and speed, and precisely identifies the boundaries of text chunks\nby analyzing the characteristics of context perplexity distribution.\nAdditionally, considering the inherent complexity of different texts, we\npropose a strategy that combines PPL Chunking with dynamic merging to achieve a\nbalance between fine-grained and coarse-grained text chunking. Experiments\nconducted on eleven datasets demonstrate that Meta-Chunking can more\nefficiently improve the performance of single-hop and multi-hop question\nanswering based on RAG. For instance, on the 2WikiMultihopQA dataset, it\noutperforms similarity chunking by 1.32 while only consuming 45.8% of the time.\nFurthermore, through the analysis of models of various scales and types, we\nobserved that PPL Chunking exhibits notable flexibility and adaptability. Our\ncode is available at https://github.com/IAAR-Shanghai/Meta-Chunking."
                },
                "authors": [
                    {
                        "name": "Jihao Zhao"
                    },
                    {
                        "name": "Zhiyuan Ji"
                    },
                    {
                        "name": "Yuchen Feng"
                    },
                    {
                        "name": "Pengnian Qi"
                    },
                    {
                        "name": "Simin Niu"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12788v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12788v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16308v1",
                "updated": "2024-11-25T11:53:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    11,
                    53,
                    55,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T11:53:55Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    11,
                    53,
                    55,
                    0,
                    330,
                    0
                ],
                "title": "An End-to-End Robust Point Cloud Semantic Segmentation Network with\n  Single-Step Conditional Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An End-to-End Robust Point Cloud Semantic Segmentation Network with\n  Single-Step Conditional Diffusion Models"
                },
                "summary": "Existing conditional Denoising Diffusion Probabilistic Models (DDPMs) with a\nNoise-Conditional Framework (NCF) remain challenging for 3D scene understanding\ntasks, as the complex geometric details in scenes increase the difficulty of\nfitting the gradients of the data distribution (the scores) from semantic\nlabels. This also results in longer training and inference time for DDPMs\ncompared to non-DDPMs. From a different perspective, we delve deeply into the\nmodel paradigm dominated by the Conditional Network. In this paper, we propose\nan end-to-end robust semantic \\textbf{Seg}mentation \\textbf{Net}work based on a\n\\textbf{C}onditional-Noise Framework (CNF) of D\\textbf{D}PMs, named\n\\textbf{CDSegNet}. Specifically, CDSegNet models the Noise Network (NN) as a\nlearnable noise-feature generator. This enables the Conditional Network (CN) to\nunderstand 3D scene semantics under multi-level feature perturbations,\nenhancing the generalization in unseen scenes. Meanwhile, benefiting from the\nnoise system of DDPMs, CDSegNet exhibits strong noise and sparsity robustness\nin experiments. Moreover, thanks to CNF, CDSegNet can generate the semantic\nlabels in a single-step inference like non-DDPMs, due to avoiding directly\nfitting the scores from semantic labels in the dominant network of CDSegNet. On\npublic indoor and outdoor benchmarks, CDSegNet significantly outperforms\nexisting methods, achieving state-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing conditional Denoising Diffusion Probabilistic Models (DDPMs) with a\nNoise-Conditional Framework (NCF) remain challenging for 3D scene understanding\ntasks, as the complex geometric details in scenes increase the difficulty of\nfitting the gradients of the data distribution (the scores) from semantic\nlabels. This also results in longer training and inference time for DDPMs\ncompared to non-DDPMs. From a different perspective, we delve deeply into the\nmodel paradigm dominated by the Conditional Network. In this paper, we propose\nan end-to-end robust semantic \\textbf{Seg}mentation \\textbf{Net}work based on a\n\\textbf{C}onditional-Noise Framework (CNF) of D\\textbf{D}PMs, named\n\\textbf{CDSegNet}. Specifically, CDSegNet models the Noise Network (NN) as a\nlearnable noise-feature generator. This enables the Conditional Network (CN) to\nunderstand 3D scene semantics under multi-level feature perturbations,\nenhancing the generalization in unseen scenes. Meanwhile, benefiting from the\nnoise system of DDPMs, CDSegNet exhibits strong noise and sparsity robustness\nin experiments. Moreover, thanks to CNF, CDSegNet can generate the semantic\nlabels in a single-step inference like non-DDPMs, due to avoiding directly\nfitting the scores from semantic labels in the dominant network of CDSegNet. On\npublic indoor and outdoor benchmarks, CDSegNet significantly outperforms\nexisting methods, achieving state-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Wentao Qu"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "YongShun Gong"
                    },
                    {
                        "name": "Xiaoshui Huang"
                    },
                    {
                        "name": "Liang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Liang Xiao"
                },
                "author": "Liang Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08620v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08620v2",
                "updated": "2024-11-25T11:44:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    11,
                    44,
                    51,
                    0,
                    330,
                    0
                ],
                "published": "2024-06-12T20:06:07Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    20,
                    6,
                    7,
                    2,
                    164,
                    0
                ],
                "title": "Proton-air interactions at ultra-high energies in muon-depleted air\n  showers with different depths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proton-air interactions at ultra-high energies in muon-depleted air\n  showers with different depths"
                },
                "summary": "The hardness of the energy spectrum of neutral pions produced in proton-air\ninteractions at ultra-high energies, above $10^{18}$ eV, is constrained by the\nsteepness of the shower-to-shower distribution of the number of muons in\nmuon-depleted extensive air showers. In this work, we find that this steepness,\nquantified by the parameter $\\Lambda_\\mu$, evolves with the depth of the shower\nmaximum, $X_{\\max}$, assuming a universal value for shallow showers and an\nenhanced dependence on the high-energy hadronic interaction model for deep\nshowers. We show that Xmax probes the so-called hadronic activity of the first\ninteraction, thus allowing direct access to the energy spectrum of neutral\npions in different regions of the kinematic phase space of the first\ninteraction. We verify that the unbiased measurement of $\\Lambda_\\mu$ is\npossible for realistic mass composition expectations. Finally, we infer that\nthe statistical precision in $\\Lambda_\\mu$ required to distinguish between\nhadronic interaction models can be achieved in current extensive air shower\ndetectors, given their resolution and exposure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The hardness of the energy spectrum of neutral pions produced in proton-air\ninteractions at ultra-high energies, above $10^{18}$ eV, is constrained by the\nsteepness of the shower-to-shower distribution of the number of muons in\nmuon-depleted extensive air showers. In this work, we find that this steepness,\nquantified by the parameter $\\Lambda_\\mu$, evolves with the depth of the shower\nmaximum, $X_{\\max}$, assuming a universal value for shallow showers and an\nenhanced dependence on the high-energy hadronic interaction model for deep\nshowers. We show that Xmax probes the so-called hadronic activity of the first\ninteraction, thus allowing direct access to the energy spectrum of neutral\npions in different regions of the kinematic phase space of the first\ninteraction. We verify that the unbiased measurement of $\\Lambda_\\mu$ is\npossible for realistic mass composition expectations. Finally, we infer that\nthe statistical precision in $\\Lambda_\\mu$ required to distinguish between\nhadronic interaction models can be achieved in current extensive air shower\ndetectors, given their resolution and exposure."
                },
                "authors": [
                    {
                        "name": "Lorenzo Cazon"
                    },
                    {
                        "name": "Ruben Conceio"
                    },
                    {
                        "name": "Miguel Alexandre Martins"
                    },
                    {
                        "name": "Felix Riehn"
                    }
                ],
                "author_detail": {
                    "name": "Felix Riehn"
                },
                "author": "Felix Riehn",
                "arxiv_doi": "10.1016/j.physletb.2024.139115",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.physletb.2024.139115",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.08620v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08620v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16300v1",
                "updated": "2024-11-25T11:35:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    11,
                    35,
                    8,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T11:35:08Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    11,
                    35,
                    8,
                    0,
                    330,
                    0
                ],
                "title": "BayLing 2: A Multilingual Large Language Model with Efficient Language\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BayLing 2: A Multilingual Large Language Model with Efficient Language\n  Alignment"
                },
                "summary": "Large language models (LLMs), with their powerful generative capabilities and\nvast knowledge, empower various tasks in everyday life. However, these\nabilities are primarily concentrated in high-resource languages, leaving\nlow-resource languages with weaker generative capabilities and relatively\nlimited knowledge. Enhancing the multilingual capabilities of LLMs is therefore\ncrucial for serving over 100 linguistic communities worldwide. An intuitive\napproach to enhance the multilingual capabilities would be to construct\ninstruction data for various languages, but constructing instruction data for\nover 100 languages is prohibitively costly. In this paper, we introduce BayLing\n2, which efficiently transfers generative capabilities and knowledge from\nhigh-resource languages to low-resource languages through language alignment.\nTo achieve this, we constructed a dataset of 3.2 million instructions,\ncomprising high-resource language instructions (Chinese and English) and\ncross-lingual instructions for 100+ languages and performed instruction tuning\nbased on the dataset to facilitate the capability transfer between languages.\nUsing Llama as the foundation model, we developed BayLing-2-7B, BayLing-2-13B,\nand BayLing-3-8B, and conducted a comprehensive evaluation of BayLing. For\nmultilingual translation across 100+ languages, BayLing shows superior\nperformance compared to open-source models of similar scale. For multilingual\nknowledge and understanding benchmarks, BayLing achieves significant\nimprovements across over 20 low-resource languages, demonstrating its\ncapability of effective knowledge transfer from high-resource to low-resource\nlanguages. Furthermore, results on English benchmarks indicate that BayLing\nmaintains high performance in highresource languages while enhancing the\nperformance in low-resource languages. Demo, homepage, code and models of\nBayLing are available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), with their powerful generative capabilities and\nvast knowledge, empower various tasks in everyday life. However, these\nabilities are primarily concentrated in high-resource languages, leaving\nlow-resource languages with weaker generative capabilities and relatively\nlimited knowledge. Enhancing the multilingual capabilities of LLMs is therefore\ncrucial for serving over 100 linguistic communities worldwide. An intuitive\napproach to enhance the multilingual capabilities would be to construct\ninstruction data for various languages, but constructing instruction data for\nover 100 languages is prohibitively costly. In this paper, we introduce BayLing\n2, which efficiently transfers generative capabilities and knowledge from\nhigh-resource languages to low-resource languages through language alignment.\nTo achieve this, we constructed a dataset of 3.2 million instructions,\ncomprising high-resource language instructions (Chinese and English) and\ncross-lingual instructions for 100+ languages and performed instruction tuning\nbased on the dataset to facilitate the capability transfer between languages.\nUsing Llama as the foundation model, we developed BayLing-2-7B, BayLing-2-13B,\nand BayLing-3-8B, and conducted a comprehensive evaluation of BayLing. For\nmultilingual translation across 100+ languages, BayLing shows superior\nperformance compared to open-source models of similar scale. For multilingual\nknowledge and understanding benchmarks, BayLing achieves significant\nimprovements across over 20 low-resource languages, demonstrating its\ncapability of effective knowledge transfer from high-resource to low-resource\nlanguages. Furthermore, results on English benchmarks indicate that BayLing\nmaintains high performance in highresource languages while enhancing the\nperformance in low-resource languages. Demo, homepage, code and models of\nBayLing are available."
                },
                "authors": [
                    {
                        "name": "Shaolei Zhang"
                    },
                    {
                        "name": "Kehao Zhang"
                    },
                    {
                        "name": "Qingkai Fang"
                    },
                    {
                        "name": "Shoutao Guo"
                    },
                    {
                        "name": "Yan Zhou"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Yang Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yang Feng"
                },
                "author": "Yang Feng",
                "arxiv_comment": "BayLing 2's online demo: http://nlp.ict.ac.cn/bayling/demo. BayLing\n  2's code and models: https://github.com/ictnlp/BayLing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06613v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06613v2",
                "updated": "2024-11-25T11:31:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    11,
                    31,
                    37,
                    0,
                    330,
                    0
                ],
                "published": "2024-04-09T20:52:55Z",
                "published_parsed": [
                    2024,
                    4,
                    9,
                    20,
                    52,
                    55,
                    1,
                    100,
                    0
                ],
                "title": "Figuring Out Gas & Galaxies In Enzo (FOGGIE) VIII: Complex and\n  Stochastic Metallicity Gradients at z > 2",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Figuring Out Gas & Galaxies In Enzo (FOGGIE) VIII: Complex and\n  Stochastic Metallicity Gradients at z > 2"
                },
                "summary": "Gas-phase metallicity gradients are a crucial element in understanding the\nchemical evolution of galaxies. We use the FOGGIE simulations to study the\nmetallicity gradients ($\\nabla Z$) of six Milky Way-like galaxies throughout\ntheir evolution. FOGGIE galaxies generally exhibit steep negative gradients for\nmost of their history, with only a few short-lived instances reaching positive\nslopes that appear to arise mainly from interactions with other galaxies.\nFOGGIE concurs with other simulation results but disagrees with the robust\nobservational finding that flat and positive gradients are common at $z>1$. By\ntracking the metallicity gradient at a rapid cadence of simulation outputs\n($\\sim 5$--10 Myr), we find that theoretical gradients are highly stochastic:\nthe FOGGIE galaxies spend $\\sim 30-50$\\% of their time far away from a smoothed\ntrajectory inferred from analytic models or other, less high-cadence\nsimulations. This rapid variation makes instantaneous gradients from\nobservations more difficult to interpret in terms of physical processes.\nBecause of these geometric and stochastic complications, we explore\nnon-parametric methods of quantifying the evolving metallicity distribution at\n$z > 1$. We investigate how efficiently non-parametric measures of the 2-D\nmetallicity distribution respond to metal production and mixing. Our results\nsuggest that new methods of quantifying and interpreting gas-phase metallicity\nwill be needed to relate trends in upcoming high-$z$ {\\it JWST} observations\nwith the underlying physics of gas accretion, expulsion, and recycling in early\ngalaxies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gas-phase metallicity gradients are a crucial element in understanding the\nchemical evolution of galaxies. We use the FOGGIE simulations to study the\nmetallicity gradients ($\\nabla Z$) of six Milky Way-like galaxies throughout\ntheir evolution. FOGGIE galaxies generally exhibit steep negative gradients for\nmost of their history, with only a few short-lived instances reaching positive\nslopes that appear to arise mainly from interactions with other galaxies.\nFOGGIE concurs with other simulation results but disagrees with the robust\nobservational finding that flat and positive gradients are common at $z>1$. By\ntracking the metallicity gradient at a rapid cadence of simulation outputs\n($\\sim 5$--10 Myr), we find that theoretical gradients are highly stochastic:\nthe FOGGIE galaxies spend $\\sim 30-50$\\% of their time far away from a smoothed\ntrajectory inferred from analytic models or other, less high-cadence\nsimulations. This rapid variation makes instantaneous gradients from\nobservations more difficult to interpret in terms of physical processes.\nBecause of these geometric and stochastic complications, we explore\nnon-parametric methods of quantifying the evolving metallicity distribution at\n$z > 1$. We investigate how efficiently non-parametric measures of the 2-D\nmetallicity distribution respond to metal production and mixing. Our results\nsuggest that new methods of quantifying and interpreting gas-phase metallicity\nwill be needed to relate trends in upcoming high-$z$ {\\it JWST} observations\nwith the underlying physics of gas accretion, expulsion, and recycling in early\ngalaxies."
                },
                "authors": [
                    {
                        "name": "Ayan Acharyya"
                    },
                    {
                        "name": "Molly S. Peeples"
                    },
                    {
                        "name": "Jason Tumlinson"
                    },
                    {
                        "name": "Brian W. O'Shea"
                    },
                    {
                        "name": "Cassandra Lochhaas"
                    },
                    {
                        "name": "Anna C. Wright"
                    },
                    {
                        "name": "Raymond C. Simons"
                    },
                    {
                        "name": "Ramona Augustin"
                    },
                    {
                        "name": "Britton D. Smith"
                    },
                    {
                        "name": "Eugene Hyeonmin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Hyeonmin Lee"
                },
                "author": "Eugene Hyeonmin Lee",
                "arxiv_comment": "Submitted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06613v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06613v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12564v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12564v2",
                "updated": "2024-11-25T11:24:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    11,
                    24,
                    23,
                    0,
                    330,
                    0
                ],
                "published": "2024-10-16T13:38:31Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    38,
                    31,
                    2,
                    290,
                    0
                ],
                "title": "FTII-Bench: A Comprehensive Multimodal Benchmark for Flow Text with\n  Image Insertion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FTII-Bench: A Comprehensive Multimodal Benchmark for Flow Text with\n  Image Insertion"
                },
                "summary": "Benefiting from the revolutionary advances in large language models (LLMs)\nand foundational vision models, large vision-language models (LVLMs) have also\nmade significant progress. However, current benchmarks focus on tasks that\nevaluating only a single aspect of LVLM capabilities (e.g., recognition,\ndetection, understanding). These tasks fail to fully demonstrate LVLMs'\npotential in complex application scenarios. To comprehensively assess the\nperformance of existing LVLMs, we propose a more challenging task called the\nFlow Text with Image Insertion task (FTII). This task requires LVLMs to\nsimultaneously possess outstanding abilities in image comprehension,\ninstruction understanding, and long-text interpretation. Specifically, given\nseveral text paragraphs and a set of candidate images, as the text paragraphs\naccumulate, the LVLMs are required to select the most suitable image from the\ncandidates to insert after the corresponding paragraph. Constructing a\nbenchmark for such a task is highly challenging, particularly in determining\nthe sequence of flowing text and images. To address this challenge, we turn to\nprofessional news reports, which naturally contain a gold standard for\nimage-text sequences. Based on this, we introduce the Flow Text with Image\nInsertion Benchmark (FTII-Bench), which includes 318 high-quality Chinese\nimage-text news articles and 307 high-quality English image-text news articles,\ncovering 10 different news domains. Using these 625 high-quality articles, we\nconstruct problems of two different types with multiple levels of difficulty.\nFurthermore, we establish two different evaluation pipelines based on the CLIP\nmodel and existing LVLMs. We evaluate 9 open-source and 2 closed-source LVLMs\nas well as 2 CLIP-based models. Results indicate that even the most advanced\nmodels (e.g., GPT-4o) face significant challenges when tackling the FTII task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benefiting from the revolutionary advances in large language models (LLMs)\nand foundational vision models, large vision-language models (LVLMs) have also\nmade significant progress. However, current benchmarks focus on tasks that\nevaluating only a single aspect of LVLM capabilities (e.g., recognition,\ndetection, understanding). These tasks fail to fully demonstrate LVLMs'\npotential in complex application scenarios. To comprehensively assess the\nperformance of existing LVLMs, we propose a more challenging task called the\nFlow Text with Image Insertion task (FTII). This task requires LVLMs to\nsimultaneously possess outstanding abilities in image comprehension,\ninstruction understanding, and long-text interpretation. Specifically, given\nseveral text paragraphs and a set of candidate images, as the text paragraphs\naccumulate, the LVLMs are required to select the most suitable image from the\ncandidates to insert after the corresponding paragraph. Constructing a\nbenchmark for such a task is highly challenging, particularly in determining\nthe sequence of flowing text and images. To address this challenge, we turn to\nprofessional news reports, which naturally contain a gold standard for\nimage-text sequences. Based on this, we introduce the Flow Text with Image\nInsertion Benchmark (FTII-Bench), which includes 318 high-quality Chinese\nimage-text news articles and 307 high-quality English image-text news articles,\ncovering 10 different news domains. Using these 625 high-quality articles, we\nconstruct problems of two different types with multiple levels of difficulty.\nFurthermore, we establish two different evaluation pipelines based on the CLIP\nmodel and existing LVLMs. We evaluate 9 open-source and 2 closed-source LVLMs\nas well as 2 CLIP-based models. Results indicate that even the most advanced\nmodels (e.g., GPT-4o) face significant challenges when tackling the FTII task."
                },
                "authors": [
                    {
                        "name": "Jiacheng Ruan"
                    },
                    {
                        "name": "Yebin Yang"
                    },
                    {
                        "name": "Zehao Lin"
                    },
                    {
                        "name": "Yuchen Feng"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zeyun Tang"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "arxiv_comment": "Work in progress. 9 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12564v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12564v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14986v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14986v2",
                "updated": "2024-11-25T11:10:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    11,
                    10,
                    34,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-22T14:47:00Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    47,
                    0,
                    4,
                    327,
                    0
                ],
                "title": "Generative AI may backfire for counterspeech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI may backfire for counterspeech"
                },
                "summary": "Online hate speech poses a serious threat to individual well-being and\nsocietal cohesion. A promising solution to curb online hate speech is\ncounterspeech. Counterspeech is aimed at encouraging users to reconsider\nhateful posts by direct replies. However, current methods lack scalability due\nto the need for human intervention or fail to adapt to the specific context of\nthe post. A potential remedy is the use of generative AI, specifically large\nlanguage models (LLMs), to write tailored counterspeech messages. In this\npaper, we analyze whether contextualized counterspeech generated by\nstate-of-the-art LLMs is effective in curbing online hate speech. To do so, we\nconducted a large-scale, pre-registered field experiment (N=2,664) on the\nsocial media platform Twitter/X. Our experiment followed a 2x2 between-subjects\ndesign and, additionally, a control condition with no counterspeech. On the one\nhand, users posting hateful content on Twitter/X were randomly assigned to\nreceive either (a) contextualized counterspeech or (b) non-contextualized\ncounterspeech. Here, the former is generated through LLMs, while the latter\nrelies on predefined, generic messages. On the other hand, we tested two\ncounterspeech strategies: (a) promoting empathy and (b) warning about the\nconsequences of online misbehavior. We then measured whether users deleted\ntheir initial hateful posts and whether their behavior changed after the\ncounterspeech intervention (e.g., whether users adopted a less toxic language).\nWe find that non-contextualized counterspeech employing a\nwarning-of-consequence strategy significantly reduces online hate speech.\nHowever, contextualized counterspeech generated by LLMs proves ineffective and\nmay even backfire.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online hate speech poses a serious threat to individual well-being and\nsocietal cohesion. A promising solution to curb online hate speech is\ncounterspeech. Counterspeech is aimed at encouraging users to reconsider\nhateful posts by direct replies. However, current methods lack scalability due\nto the need for human intervention or fail to adapt to the specific context of\nthe post. A potential remedy is the use of generative AI, specifically large\nlanguage models (LLMs), to write tailored counterspeech messages. In this\npaper, we analyze whether contextualized counterspeech generated by\nstate-of-the-art LLMs is effective in curbing online hate speech. To do so, we\nconducted a large-scale, pre-registered field experiment (N=2,664) on the\nsocial media platform Twitter/X. Our experiment followed a 2x2 between-subjects\ndesign and, additionally, a control condition with no counterspeech. On the one\nhand, users posting hateful content on Twitter/X were randomly assigned to\nreceive either (a) contextualized counterspeech or (b) non-contextualized\ncounterspeech. Here, the former is generated through LLMs, while the latter\nrelies on predefined, generic messages. On the other hand, we tested two\ncounterspeech strategies: (a) promoting empathy and (b) warning about the\nconsequences of online misbehavior. We then measured whether users deleted\ntheir initial hateful posts and whether their behavior changed after the\ncounterspeech intervention (e.g., whether users adopted a less toxic language).\nWe find that non-contextualized counterspeech employing a\nwarning-of-consequence strategy significantly reduces online hate speech.\nHowever, contextualized counterspeech generated by LLMs proves ineffective and\nmay even backfire."
                },
                "authors": [
                    {
                        "name": "Dominik Br"
                    },
                    {
                        "name": "Abdurahman Maarouf"
                    },
                    {
                        "name": "Stefan Feuerriegel"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Feuerriegel"
                },
                "author": "Stefan Feuerriegel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14986v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14986v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2211.11483v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2211.11483v5",
                "updated": "2024-11-25T10:55:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    10,
                    55,
                    29,
                    0,
                    330,
                    0
                ],
                "published": "2022-11-21T14:18:25Z",
                "published_parsed": [
                    2022,
                    11,
                    21,
                    14,
                    18,
                    25,
                    0,
                    325,
                    0
                ],
                "title": "Deanthropomorphising NLP: Can a Language Model Be Conscious?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deanthropomorphising NLP: Can a Language Model Be Conscious?"
                },
                "summary": "This work is intended as a voice in the discussion over previous claims that\na pretrained large language model (LLM) based on the Transformer model\narchitecture can be sentient. Such claims have been made concerning the LaMDA\nmodel and also concerning the current wave of LLM-powered chatbots, such as\nChatGPT. This claim, if confirmed, would have serious ramifications in the\nNatural Language Processing (NLP) community due to wide-spread use of similar\nmodels. However, here we take the position that such a large language model\ncannot be sentient, or conscious, and that LaMDA in particular exhibits no\nadvances over other similar models that would qualify it. We justify this by\nanalysing the Transformer architecture through Integrated Information Theory of\nconsciousness. We see the claims of sentience as part of a wider tendency to\nuse anthropomorphic language in NLP reporting. Regardless of the veracity of\nthe claims, we consider this an opportune moment to take stock of progress in\nlanguage modelling and consider the ethical implications of the task. In order\nto make this work helpful for readers outside the NLP community, we also\npresent the necessary background in language modelling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work is intended as a voice in the discussion over previous claims that\na pretrained large language model (LLM) based on the Transformer model\narchitecture can be sentient. Such claims have been made concerning the LaMDA\nmodel and also concerning the current wave of LLM-powered chatbots, such as\nChatGPT. This claim, if confirmed, would have serious ramifications in the\nNatural Language Processing (NLP) community due to wide-spread use of similar\nmodels. However, here we take the position that such a large language model\ncannot be sentient, or conscious, and that LaMDA in particular exhibits no\nadvances over other similar models that would qualify it. We justify this by\nanalysing the Transformer architecture through Integrated Information Theory of\nconsciousness. We see the claims of sentience as part of a wider tendency to\nuse anthropomorphic language in NLP reporting. Regardless of the veracity of\nthe claims, we consider this an opportune moment to take stock of progress in\nlanguage modelling and consider the ethical implications of the task. In order\nto make this work helpful for readers outside the NLP community, we also\npresent the necessary background in language modelling."
                },
                "authors": [
                    {
                        "name": "Matthew Shardlow"
                    },
                    {
                        "name": "Piotr Przybya"
                    }
                ],
                "author_detail": {
                    "name": "Piotr Przybya"
                },
                "author": "Piotr Przybya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2211.11483v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2211.11483v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.09852v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.09852v3",
                "updated": "2024-11-25T10:47:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    10,
                    47,
                    11,
                    0,
                    330,
                    0
                ],
                "published": "2023-12-15T14:58:34Z",
                "published_parsed": [
                    2023,
                    12,
                    15,
                    14,
                    58,
                    34,
                    4,
                    349,
                    0
                ],
                "title": "Learning Distributions on Manifolds with Free-Form Flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Distributions on Manifolds with Free-Form Flows"
                },
                "summary": "We propose Manifold Free-Form Flows (M-FFF), a simple new generative model\nfor data on manifolds. The existing approaches to learning a distribution on\narbitrary manifolds are expensive at inference time, since sampling requires\nsolving a differential equation. Our method overcomes this limitation by\nsampling in a single function evaluation. The key innovation is to optimize a\nneural network via maximum likelihood on the manifold, possible by adapting the\nfree-form flow framework to Riemannian manifolds. M-FFF is straightforwardly\nadapted to any manifold with a known projection. It consistently matches or\noutperforms previous single-step methods specialized to specific manifolds. It\nis typically two orders of magnitude faster than multi-step methods based on\ndiffusion or flow matching, achieving better likelihoods in several\nexperiments. We provide our code at https://github.com/vislearn/FFF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Manifold Free-Form Flows (M-FFF), a simple new generative model\nfor data on manifolds. The existing approaches to learning a distribution on\narbitrary manifolds are expensive at inference time, since sampling requires\nsolving a differential equation. Our method overcomes this limitation by\nsampling in a single function evaluation. The key innovation is to optimize a\nneural network via maximum likelihood on the manifold, possible by adapting the\nfree-form flow framework to Riemannian manifolds. M-FFF is straightforwardly\nadapted to any manifold with a known projection. It consistently matches or\noutperforms previous single-step methods specialized to specific manifolds. It\nis typically two orders of magnitude faster than multi-step methods based on\ndiffusion or flow matching, achieving better likelihoods in several\nexperiments. We provide our code at https://github.com/vislearn/FFF."
                },
                "authors": [
                    {
                        "name": "Peter Sorrenson"
                    },
                    {
                        "name": "Felix Draxler"
                    },
                    {
                        "name": "Armand Rousselot"
                    },
                    {
                        "name": "Sander Hummerich"
                    },
                    {
                        "name": "Ullrich Kthe"
                    }
                ],
                "author_detail": {
                    "name": "Ullrich Kthe"
                },
                "author": "Ullrich Kthe",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.09852v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.09852v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07838v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07838v2",
                "updated": "2024-11-25T10:30:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    10,
                    30,
                    36,
                    0,
                    330,
                    0
                ],
                "published": "2024-10-10T11:56:09Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    56,
                    9,
                    3,
                    284,
                    0
                ],
                "title": "Minority-Focused Text-to-Image Generation via Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minority-Focused Text-to-Image Generation via Prompt Optimization"
                },
                "summary": "We investigate the generation of minority samples using pretrained\ntext-to-image (T2I) latent diffusion models. Minority instances, in the context\nof T2I generation, can be defined as ones living on low-density regions of\ntext-conditional data distributions. They are valuable for various applications\nof modern T2I generators, such as data augmentation and creative AI.\nUnfortunately, existing pretrained T2I diffusion models primarily focus on\nhigh-density regions, largely due to the influence of guided samplers (like\nCFG) that are essential for producing high-quality generations. To address\nthis, we present a novel framework to counter the high-density-focus of T2I\ndiffusion models. Specifically, we first develop an online prompt optimization\nframework that can encourage the emergence of desired properties during\ninference while preserving semantic contents of user-provided prompts. We\nsubsequently tailor this generic prompt optimizer into a specialized solver\nthat promotes the generation of minority features by incorporating a\ncarefully-crafted likelihood objective. Our comprehensive experiments,\nconducted across various types of T2I models, demonstrate that our approach\nsignificantly enhances the capability to produce high-quality minority\ninstances compared to existing samplers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the generation of minority samples using pretrained\ntext-to-image (T2I) latent diffusion models. Minority instances, in the context\nof T2I generation, can be defined as ones living on low-density regions of\ntext-conditional data distributions. They are valuable for various applications\nof modern T2I generators, such as data augmentation and creative AI.\nUnfortunately, existing pretrained T2I diffusion models primarily focus on\nhigh-density regions, largely due to the influence of guided samplers (like\nCFG) that are essential for producing high-quality generations. To address\nthis, we present a novel framework to counter the high-density-focus of T2I\ndiffusion models. Specifically, we first develop an online prompt optimization\nframework that can encourage the emergence of desired properties during\ninference while preserving semantic contents of user-provided prompts. We\nsubsequently tailor this generic prompt optimizer into a specialized solver\nthat promotes the generation of minority features by incorporating a\ncarefully-crafted likelihood objective. Our comprehensive experiments,\nconducted across various types of T2I models, demonstrate that our approach\nsignificantly enhances the capability to produce high-quality minority\ninstances compared to existing samplers."
                },
                "authors": [
                    {
                        "name": "Soobin Um"
                    },
                    {
                        "name": "Jong Chul Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jong Chul Ye"
                },
                "author": "Jong Chul Ye",
                "arxiv_comment": "20 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07838v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07838v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16260v1",
                "updated": "2024-11-25T10:23:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    10,
                    23,
                    11,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T10:23:11Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    10,
                    23,
                    11,
                    0,
                    330,
                    0
                ],
                "title": "Unraveling Arithmetic in Large Language Models: The Role of Algebraic\n  Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unraveling Arithmetic in Large Language Models: The Role of Algebraic\n  Structures"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable mathematical\ncapabilities, largely driven by chain-of-thought (CoT) prompting, which\ndecomposes complex reasoning into step-by-step solutions. This approach has\nenabled significant advancements, as evidenced by performance on benchmarks\nlike GSM8K and MATH. However, the mechanisms underlying LLMs' ability to\nperform arithmetic in a single step of CoT remain poorly understood. Existing\nstudies debate whether LLMs encode numerical values or rely on symbolic\nreasoning, while others explore attention and multi-layered processing in\narithmetic tasks. In this work, we propose that LLMs learn arithmetic by\ncapturing algebraic structures, such as \\emph{Commutativity} and\n\\emph{Identity} properties. Since these structures are observable through\ninput-output relationships, they can generalize to unseen data. We empirically\ndemonstrate that LLMs can learn algebraic structures using a custom dataset of\narithmetic problems. Our findings indicate that leveraging algebraic structures\ncan enhance the LLMs' arithmetic capabilities, offering insights into improving\ntheir arithmetic performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable mathematical\ncapabilities, largely driven by chain-of-thought (CoT) prompting, which\ndecomposes complex reasoning into step-by-step solutions. This approach has\nenabled significant advancements, as evidenced by performance on benchmarks\nlike GSM8K and MATH. However, the mechanisms underlying LLMs' ability to\nperform arithmetic in a single step of CoT remain poorly understood. Existing\nstudies debate whether LLMs encode numerical values or rely on symbolic\nreasoning, while others explore attention and multi-layered processing in\narithmetic tasks. In this work, we propose that LLMs learn arithmetic by\ncapturing algebraic structures, such as \\emph{Commutativity} and\n\\emph{Identity} properties. Since these structures are observable through\ninput-output relationships, they can generalize to unseen data. We empirically\ndemonstrate that LLMs can learn algebraic structures using a custom dataset of\narithmetic problems. Our findings indicate that leveraging algebraic structures\ncan enhance the LLMs' arithmetic capabilities, offering insights into improving\ntheir arithmetic performance."
                },
                "authors": [
                    {
                        "name": "Fu-Chieh Chang"
                    },
                    {
                        "name": "Pei-Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Pei-Yuan Wu"
                },
                "author": "Pei-Yuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16252v1",
                "updated": "2024-11-25T10:12:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    10,
                    12,
                    27,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T10:12:27Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    10,
                    12,
                    27,
                    0,
                    330,
                    0
                ],
                "title": "NormXLogit: The Head-on-Top Never Lies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NormXLogit: The Head-on-Top Never Lies"
                },
                "summary": "The Transformer architecture has emerged as the dominant choice for building\nlarge language models (LLMs). However, with new LLMs emerging on a frequent\nbasis, it is important to consider the potential value of architecture-agnostic\napproaches that can provide interpretability across a variety of architectures.\nDespite recent successes in the interpretability of LLMs, many existing\napproaches rely on complex methods that are often tied to a specific model\ndesign and come with a significant computational cost. To address these\nlimitations, we propose a novel technique, called NormXLogit, for assessing the\nsignificance of individual input tokens. This method operates based on the\ninput and output representations associated with each token. First, we\ndemonstrate that during the pre-training of LLMs, the norms of word embeddings\ncapture the importance of input tokens. Second, we reveal a significant\nrelationship between a token's importance and the extent to which its\nrepresentation can resemble the model's final prediction. Through extensive\nanalysis, we show that our approach consistently outperforms existing\ngradient-based methods in terms of faithfulness. Additionally, our method\nachieves better performance in layer-wise explanations compared to the most\nprominent architecture-specific methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Transformer architecture has emerged as the dominant choice for building\nlarge language models (LLMs). However, with new LLMs emerging on a frequent\nbasis, it is important to consider the potential value of architecture-agnostic\napproaches that can provide interpretability across a variety of architectures.\nDespite recent successes in the interpretability of LLMs, many existing\napproaches rely on complex methods that are often tied to a specific model\ndesign and come with a significant computational cost. To address these\nlimitations, we propose a novel technique, called NormXLogit, for assessing the\nsignificance of individual input tokens. This method operates based on the\ninput and output representations associated with each token. First, we\ndemonstrate that during the pre-training of LLMs, the norms of word embeddings\ncapture the importance of input tokens. Second, we reveal a significant\nrelationship between a token's importance and the extent to which its\nrepresentation can resemble the model's final prediction. Through extensive\nanalysis, we show that our approach consistently outperforms existing\ngradient-based methods in terms of faithfulness. Additionally, our method\nachieves better performance in layer-wise explanations compared to the most\nprominent architecture-specific methods."
                },
                "authors": [
                    {
                        "name": "Sina Abbasi"
                    },
                    {
                        "name": "Mohammad Reza Modarres"
                    },
                    {
                        "name": "Mohammad Taher Pilehvar"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Taher Pilehvar"
                },
                "author": "Mohammad Taher Pilehvar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16239v1",
                "updated": "2024-11-25T09:54:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    54,
                    42,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T09:54:42Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    54,
                    42,
                    0,
                    330,
                    0
                ],
                "title": "CS-Eval: A Comprehensive Large Language Model Benchmark for\n  CyberSecurity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CS-Eval: A Comprehensive Large Language Model Benchmark for\n  CyberSecurity"
                },
                "summary": "Over the past year, there has been a notable rise in the use of large\nlanguage models (LLMs) for academic research and industrial practices within\nthe cybersecurity field. However, it remains a lack of comprehensive and\npublicly accessible benchmarks to evaluate the performance of LLMs on\ncybersecurity tasks. To address this gap, we introduce CS-Eval, a publicly\naccessible, comprehensive and bilingual LLM benchmark specifically designed for\ncybersecurity. CS-Eval synthesizes the research hotspots from academia and\npractical applications from industry, curating a diverse set of high-quality\nquestions across 42 categories within cybersecurity, systematically organized\ninto three cognitive levels: knowledge, ability, and application. Through an\nextensive evaluation of a wide range of LLMs using CS-Eval, we have uncovered\nvaluable insights. For instance, while GPT-4 generally excels overall, other\nmodels may outperform it in certain specific subcategories. Additionally, by\nconducting evaluations over several months, we observed significant\nimprovements in many LLMs' abilities to solve cybersecurity tasks. The\nbenchmarks are now publicly available at https://github.com/CS-EVAL/CS-Eval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past year, there has been a notable rise in the use of large\nlanguage models (LLMs) for academic research and industrial practices within\nthe cybersecurity field. However, it remains a lack of comprehensive and\npublicly accessible benchmarks to evaluate the performance of LLMs on\ncybersecurity tasks. To address this gap, we introduce CS-Eval, a publicly\naccessible, comprehensive and bilingual LLM benchmark specifically designed for\ncybersecurity. CS-Eval synthesizes the research hotspots from academia and\npractical applications from industry, curating a diverse set of high-quality\nquestions across 42 categories within cybersecurity, systematically organized\ninto three cognitive levels: knowledge, ability, and application. Through an\nextensive evaluation of a wide range of LLMs using CS-Eval, we have uncovered\nvaluable insights. For instance, while GPT-4 generally excels overall, other\nmodels may outperform it in certain specific subcategories. Additionally, by\nconducting evaluations over several months, we observed significant\nimprovements in many LLMs' abilities to solve cybersecurity tasks. The\nbenchmarks are now publicly available at https://github.com/CS-EVAL/CS-Eval."
                },
                "authors": [
                    {
                        "name": "Zhengmin Yu"
                    },
                    {
                        "name": "Jiutian Zeng"
                    },
                    {
                        "name": "Siyi Chen"
                    },
                    {
                        "name": "Wenhan Xu"
                    },
                    {
                        "name": "Dandan Xu"
                    },
                    {
                        "name": "Xiangyu Liu"
                    },
                    {
                        "name": "Zonghao Ying"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16238v1",
                "updated": "2024-11-25T09:53:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    53,
                    35,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T09:53:35Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    53,
                    35,
                    0,
                    330,
                    0
                ],
                "title": "UVLLM: An Automated Universal RTL Verification Framework using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UVLLM: An Automated Universal RTL Verification Framework using LLMs"
                },
                "summary": "Verifying hardware designs in embedded systems is crucial but often\nlabor-intensive and time-consuming. While existing solutions have improved\nautomation, they frequently rely on unrealistic assumptions. To address these\nchallenges, we introduce a novel framework, UVLLM, which combines Large\nLanguage Models (LLMs) with the Universal Verification Methodology (UVM) to\nrelax these assumptions. UVLLM significantly enhances the automation of testing\nand repairing error-prone Register Transfer Level (RTL) codes, a critical\naspect of verification development. Unlike existing methods, UVLLM ensures that\nall errors are triggered during verification, achieving a syntax error fix rate\nof 86.99% and a functional error fix rate of 71.92% on our proposed benchmark.\nThese results demonstrate a substantial improvement in verification efficiency.\nAdditionally, our study highlights the current limitations of LLM applications,\nparticularly their reliance on extensive training data. We emphasize the\ntransformative potential of LLMs in hardware design verification and suggest\npromising directions for future research in AI-driven hardware design\nmethodologies. The Repo. of dataset and code:\nhttps://anonymous.4open.science/r/UVLLM/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verifying hardware designs in embedded systems is crucial but often\nlabor-intensive and time-consuming. While existing solutions have improved\nautomation, they frequently rely on unrealistic assumptions. To address these\nchallenges, we introduce a novel framework, UVLLM, which combines Large\nLanguage Models (LLMs) with the Universal Verification Methodology (UVM) to\nrelax these assumptions. UVLLM significantly enhances the automation of testing\nand repairing error-prone Register Transfer Level (RTL) codes, a critical\naspect of verification development. Unlike existing methods, UVLLM ensures that\nall errors are triggered during verification, achieving a syntax error fix rate\nof 86.99% and a functional error fix rate of 71.92% on our proposed benchmark.\nThese results demonstrate a substantial improvement in verification efficiency.\nAdditionally, our study highlights the current limitations of LLM applications,\nparticularly their reliance on extensive training data. We emphasize the\ntransformative potential of LLMs in hardware design verification and suggest\npromising directions for future research in AI-driven hardware design\nmethodologies. The Repo. of dataset and code:\nhttps://anonymous.4open.science/r/UVLLM/."
                },
                "authors": [
                    {
                        "name": "Yuchen Hu"
                    },
                    {
                        "name": "Junhao Ye"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Jialin Sun"
                    },
                    {
                        "name": "Shiyue Zhang"
                    },
                    {
                        "name": "Xinyao Jiao"
                    },
                    {
                        "name": "Dingrong Pan"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Ning Wang"
                    },
                    {
                        "name": "Weiwei Shan"
                    },
                    {
                        "name": "Xinwei Fang"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Nan Guan"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16232v1",
                "updated": "2024-11-25T09:46:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    46,
                    41,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T09:46:41Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    46,
                    41,
                    0,
                    330,
                    0
                ],
                "title": "Poster: Could Large Language Models Perform Network Management?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Poster: Could Large Language Models Perform Network Management?"
                },
                "summary": "Modern wireless communication systems have become increasingly complex due to\nthe proliferation of wireless devices, increasing performance standards, and\ngrowing security threats. Managing these networks is becoming more challenging,\nrequiring the use of advanced network management methods and tools. AI-driven\nnetwork management systems such as Self-Optimizing Networks (SONs) are gaining\nattention. On the other hand, Large Language Models (LLMs) have been\ndemonstrating exceptional zero-shot learning and generalization capabilities\nacross several domains. In this paper, we leverage the potential of LLMs with\nSONs to enhance future network management systems. Specifically, we benchmark\nthe use of various LLMs such as GPT-4, Llama, and Falcon, in a zero-shot\nsetting based on their real-time network configuration recommendations. Our\nresults indicate promising prospects for integrating LLMs into future network\nmanagement systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern wireless communication systems have become increasingly complex due to\nthe proliferation of wireless devices, increasing performance standards, and\ngrowing security threats. Managing these networks is becoming more challenging,\nrequiring the use of advanced network management methods and tools. AI-driven\nnetwork management systems such as Self-Optimizing Networks (SONs) are gaining\nattention. On the other hand, Large Language Models (LLMs) have been\ndemonstrating exceptional zero-shot learning and generalization capabilities\nacross several domains. In this paper, we leverage the potential of LLMs with\nSONs to enhance future network management systems. Specifically, we benchmark\nthe use of various LLMs such as GPT-4, Llama, and Falcon, in a zero-shot\nsetting based on their real-time network configuration recommendations. Our\nresults indicate promising prospects for integrating LLMs into future network\nmanagement systems."
                },
                "authors": [
                    {
                        "name": "Zine el abidine Kherroubi"
                    },
                    {
                        "name": "Monika Prakash"
                    },
                    {
                        "name": "Jean-Pierre Giacalone"
                    },
                    {
                        "name": "Michael Baddeley"
                    }
                ],
                "author_detail": {
                    "name": "Michael Baddeley"
                },
                "author": "Michael Baddeley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17201v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17201v2",
                "updated": "2024-11-25T09:32:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    32,
                    53,
                    0,
                    330,
                    0
                ],
                "published": "2024-09-25T15:04:42Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    4,
                    42,
                    2,
                    269,
                    0
                ],
                "title": "Immersion and Invariance-based Coding for Privacy-Preserving Federated\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Immersion and Invariance-based Coding for Privacy-Preserving Federated\n  Learning"
                },
                "summary": "Federated learning (FL) has emerged as a method to preserve privacy in\ncollaborative distributed learning. In FL, clients train AI models directly on\ntheir devices rather than sharing data with a centralized server, which can\npose privacy risks. However, it has been shown that despite FL's partial\nprotection of local data privacy, information about clients' data can still be\ninferred from shared model updates during training. In recent years, several\nprivacy-preserving approaches have been developed to mitigate this privacy\nleakage in FL, though they often provide privacy at the cost of model\nperformance or system efficiency. Balancing these trade-offs presents a\nsignificant challenge in implementing FL schemes. In this manuscript, we\nintroduce a privacy-preserving FL framework that combines differential privacy\nand system immersion tools from control theory. The core idea is to treat the\noptimization algorithms used in standard FL schemes (e.g., gradient-based\nalgorithms) as a dynamical system that we seek to immerse into a\nhigher-dimensional system (referred to as the target optimization algorithm).\nThe target algorithm's dynamics are designed such that, first, the model\nparameters of the original algorithm are immersed in its parameters; second, it\noperates on distorted parameters; and third, it converges to an encoded version\nof the true model parameters from the original algorithm. These encoded\nparameters can then be decoded at the server to retrieve the original model\nparameters. We demonstrate that the proposed privacy-preserving scheme can be\ntailored to offer any desired level of differential privacy for both local and\nglobal model parameters, while maintaining the same accuracy and convergence\nrate as standard FL algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) has emerged as a method to preserve privacy in\ncollaborative distributed learning. In FL, clients train AI models directly on\ntheir devices rather than sharing data with a centralized server, which can\npose privacy risks. However, it has been shown that despite FL's partial\nprotection of local data privacy, information about clients' data can still be\ninferred from shared model updates during training. In recent years, several\nprivacy-preserving approaches have been developed to mitigate this privacy\nleakage in FL, though they often provide privacy at the cost of model\nperformance or system efficiency. Balancing these trade-offs presents a\nsignificant challenge in implementing FL schemes. In this manuscript, we\nintroduce a privacy-preserving FL framework that combines differential privacy\nand system immersion tools from control theory. The core idea is to treat the\noptimization algorithms used in standard FL schemes (e.g., gradient-based\nalgorithms) as a dynamical system that we seek to immerse into a\nhigher-dimensional system (referred to as the target optimization algorithm).\nThe target algorithm's dynamics are designed such that, first, the model\nparameters of the original algorithm are immersed in its parameters; second, it\noperates on distorted parameters; and third, it converges to an encoded version\nof the true model parameters from the original algorithm. These encoded\nparameters can then be decoded at the server to retrieve the original model\nparameters. We demonstrate that the proposed privacy-preserving scheme can be\ntailored to offer any desired level of differential privacy for both local and\nglobal model parameters, while maintaining the same accuracy and convergence\nrate as standard FL algorithms."
                },
                "authors": [
                    {
                        "name": "Haleh Hayati"
                    },
                    {
                        "name": "Carlos Murguia"
                    },
                    {
                        "name": "Nathan van de Wouw"
                    }
                ],
                "author_detail": {
                    "name": "Nathan van de Wouw"
                },
                "author": "Nathan van de Wouw",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17201v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17201v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.01632v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.01632v2",
                "updated": "2024-11-25T09:31:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    31,
                    21,
                    0,
                    330,
                    0
                ],
                "published": "2024-01-03T09:07:10Z",
                "published_parsed": [
                    2024,
                    1,
                    3,
                    9,
                    7,
                    10,
                    2,
                    3,
                    0
                ],
                "title": "Out-of-equlibrium inference of feeding rates through population data\n  from generic consumer-resource stochastic dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-equlibrium inference of feeding rates through population data\n  from generic consumer-resource stochastic dynamics"
                },
                "summary": "Statistical models are often structurally unidentifiable, because different\nset of parameters can lead to equal model outcomes. To be useful for prediction\nand parameter inference from data, stochastic population models need to be\nidentifiable, this meaning that model parameters can be uniquely inferred from\na large number of model observations. In particular, precise estimation of\nfeeding rates in consumer-resource dynamics is crucial, because\nconsumer-resource processes are central in determining biomass transport across\necosystems. Model parameters are usually estimated at stationarity, because in\nthat case model analyses are often easier. In this contribution we analyze the\nproblem of parameter redundancy in a multi-resource consumer-resource model,\nshowing that model indentifiability depends on whether the dynamics have\nreached stationarity or not. To be precise, we: (i) Calculate the steady-state\nand out-of-equilibrium probability distributions of predator's abundances\nanalytically using generating functions, which allow us to unveil parameter\nredundancy and carry out proper maximum likelihood estimation. (ii) Conduct in\nsilico experiments by tracking the abundance of consumers that are either\nsearching for or handling prey, data then used for maximum likelihood parameter\nestimation. (iii) Show that, when model observations are recorded out of\nequilibrium, feeding parameters are truly identifiable, whereas if sampling is\ndone at stationarity, only ratios of rates can be inferred from data (i.e.,\nparameters are redundant). We discuss the implications of our results when\ninferring parameters of general dynamical models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical models are often structurally unidentifiable, because different\nset of parameters can lead to equal model outcomes. To be useful for prediction\nand parameter inference from data, stochastic population models need to be\nidentifiable, this meaning that model parameters can be uniquely inferred from\na large number of model observations. In particular, precise estimation of\nfeeding rates in consumer-resource dynamics is crucial, because\nconsumer-resource processes are central in determining biomass transport across\necosystems. Model parameters are usually estimated at stationarity, because in\nthat case model analyses are often easier. In this contribution we analyze the\nproblem of parameter redundancy in a multi-resource consumer-resource model,\nshowing that model indentifiability depends on whether the dynamics have\nreached stationarity or not. To be precise, we: (i) Calculate the steady-state\nand out-of-equilibrium probability distributions of predator's abundances\nanalytically using generating functions, which allow us to unveil parameter\nredundancy and carry out proper maximum likelihood estimation. (ii) Conduct in\nsilico experiments by tracking the abundance of consumers that are either\nsearching for or handling prey, data then used for maximum likelihood parameter\nestimation. (iii) Show that, when model observations are recorded out of\nequilibrium, feeding parameters are truly identifiable, whereas if sampling is\ndone at stationarity, only ratios of rates can be inferred from data (i.e.,\nparameters are redundant). We discuss the implications of our results when\ninferring parameters of general dynamical models."
                },
                "authors": [
                    {
                        "name": "Jose A. Capitan"
                    },
                    {
                        "name": "David Alonso"
                    }
                ],
                "author_detail": {
                    "name": "David Alonso"
                },
                "author": "David Alonso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.01632v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.01632v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92-10 60Gxx 62Fxx (Primary), 05A15 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16220v1",
                "updated": "2024-11-25T09:29:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    29,
                    52,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T09:29:52Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    29,
                    52,
                    0,
                    330,
                    0
                ],
                "title": "On the achievability of efficiency bounds for covariate-adjusted\n  response-adaptive randomization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the achievability of efficiency bounds for covariate-adjusted\n  response-adaptive randomization"
                },
                "summary": "In the context of precision medicine, covariate-adjusted response-adaptive\nrandomization (CARA) has garnered much attention from both academia and\nindustry due to its benefits in providing ethical and tailored treatment\nassignments based on patients' profiles while still preserving favorable\nstatistical properties. Recent years have seen substantial progress in\nunderstanding the inference for various adaptive experimental designs. In\nparticular, research has focused on two important perspectives: how to obtain\nrobust inference in the presence of model misspecification, and what the\nsmallest variance, i.e., the efficiency bound, an estimator can achieve.\nNotably, Armstrong (2022) derived the asymptotic efficiency bound for any\nrandomization procedure that assigns treatments depending on covariates and\naccrued responses, thus including CARA, among others. However, to the best of\nour knowledge, no existing literature has addressed whether and how the\nasymptotic efficiency bound can be achieved under CARA. In this paper, by\nconnecting two strands of literature on adaptive randomization, namely robust\ninference and efficiency bound, we provide a definitive answer to this question\nfor an important practical scenario where only discrete covariates are observed\nand used to form stratification. We consider a specific type of CARA, i.e., a\nstratified version of doubly-adaptive biased coin design, and prove that the\nstratified difference-in-means estimator achieves Armstrong (2022)'s efficiency\nbound, with possible ethical constraints on treatment assignments. Our work\nprovides new insights and demonstrates the potential for more research\nregarding the design and analysis of CARA that maximizes efficiency while\nadhering to ethical considerations. Future studies could explore how to achieve\nthe asymptotic efficiency bound for general CARA with continuous covariates,\nwhich remains an open question.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the context of precision medicine, covariate-adjusted response-adaptive\nrandomization (CARA) has garnered much attention from both academia and\nindustry due to its benefits in providing ethical and tailored treatment\nassignments based on patients' profiles while still preserving favorable\nstatistical properties. Recent years have seen substantial progress in\nunderstanding the inference for various adaptive experimental designs. In\nparticular, research has focused on two important perspectives: how to obtain\nrobust inference in the presence of model misspecification, and what the\nsmallest variance, i.e., the efficiency bound, an estimator can achieve.\nNotably, Armstrong (2022) derived the asymptotic efficiency bound for any\nrandomization procedure that assigns treatments depending on covariates and\naccrued responses, thus including CARA, among others. However, to the best of\nour knowledge, no existing literature has addressed whether and how the\nasymptotic efficiency bound can be achieved under CARA. In this paper, by\nconnecting two strands of literature on adaptive randomization, namely robust\ninference and efficiency bound, we provide a definitive answer to this question\nfor an important practical scenario where only discrete covariates are observed\nand used to form stratification. We consider a specific type of CARA, i.e., a\nstratified version of doubly-adaptive biased coin design, and prove that the\nstratified difference-in-means estimator achieves Armstrong (2022)'s efficiency\nbound, with possible ethical constraints on treatment assignments. Our work\nprovides new insights and demonstrates the potential for more research\nregarding the design and analysis of CARA that maximizes efficiency while\nadhering to ethical considerations. Future studies could explore how to achieve\nthe asymptotic efficiency bound for general CARA with continuous covariates,\nwhich remains an open question."
                },
                "authors": [
                    {
                        "name": "Jiahui Xin"
                    },
                    {
                        "name": "Wei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Wei Ma"
                },
                "author": "Wei Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16216v1",
                "updated": "2024-11-25T09:25:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    25,
                    53,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T09:25:53Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    25,
                    53,
                    0,
                    330,
                    0
                ],
                "title": "SMGDiff: Soccer Motion Generation using diffusion probabilistic models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMGDiff: Soccer Motion Generation using diffusion probabilistic models"
                },
                "summary": "Soccer is a globally renowned sport with significant applications in video\ngames and VR/AR. However, generating realistic soccer motions remains\nchallenging due to the intricate interactions between the human player and the\nball. In this paper, we introduce SMGDiff, a novel two-stage framework for\ngenerating real-time and user-controllable soccer motions. Our key idea is to\nintegrate real-time character control with a powerful diffusion-based\ngenerative model, ensuring high-quality and diverse output motion. In the first\nstage, we instantly transform coarse user controls into diverse global\ntrajectories of the character. In the second stage, we employ a\ntransformer-based autoregressive diffusion model to generate soccer motions\nbased on trajectory conditioning. We further incorporate a contact guidance\nmodule during inference to optimize the contact details for realistic ball-foot\ninteractions. Moreover, we contribute a large-scale soccer motion dataset\nconsisting of over 1.08 million frames of diverse soccer motions. Extensive\nexperiments demonstrate that our SMGDiff significantly outperforms existing\nmethods in terms of motion quality and condition alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soccer is a globally renowned sport with significant applications in video\ngames and VR/AR. However, generating realistic soccer motions remains\nchallenging due to the intricate interactions between the human player and the\nball. In this paper, we introduce SMGDiff, a novel two-stage framework for\ngenerating real-time and user-controllable soccer motions. Our key idea is to\nintegrate real-time character control with a powerful diffusion-based\ngenerative model, ensuring high-quality and diverse output motion. In the first\nstage, we instantly transform coarse user controls into diverse global\ntrajectories of the character. In the second stage, we employ a\ntransformer-based autoregressive diffusion model to generate soccer motions\nbased on trajectory conditioning. We further incorporate a contact guidance\nmodule during inference to optimize the contact details for realistic ball-foot\ninteractions. Moreover, we contribute a large-scale soccer motion dataset\nconsisting of over 1.08 million frames of diverse soccer motions. Extensive\nexperiments demonstrate that our SMGDiff significantly outperforms existing\nmethods in terms of motion quality and condition alignment."
                },
                "authors": [
                    {
                        "name": "Hongdi Yang"
                    },
                    {
                        "name": "Chengyang Li"
                    },
                    {
                        "name": "Zhenxuan Wu"
                    },
                    {
                        "name": "Gaozheng Li"
                    },
                    {
                        "name": "Jingya Wang"
                    },
                    {
                        "name": "Jingyi Yu"
                    },
                    {
                        "name": "Zhuo Su"
                    },
                    {
                        "name": "Lan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Lan Xu"
                },
                "author": "Lan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16213v1",
                "updated": "2024-11-25T09:22:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    22,
                    13,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T09:22:13Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    22,
                    13,
                    0,
                    330,
                    0
                ],
                "title": "SAVEn-Vid: Synergistic Audio-Visual Integration for Enhanced\n  Understanding in Long Video Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAVEn-Vid: Synergistic Audio-Visual Integration for Enhanced\n  Understanding in Long Video Context"
                },
                "summary": "Endeavors have been made to explore Large Language Models for video analysis\n(Video-LLMs), particularly in understanding and interpreting long videos.\nHowever, existing Video-LLMs still face challenges in effectively integrating\nthe rich and diverse audio-visual information inherent in long videos, which is\ncrucial for comprehensive understanding. This raises the question: how can we\nleverage embedded audio-visual information to enhance long video understanding?\nTherefore, (i) we introduce SAVEn-Vid, the first-ever long audio-visual video\ndataset comprising over 58k audio-visual instructions. (ii) From the model\nperspective, we propose a time-aware Audio-Visual Large Language Model\n(AV-LLM), SAVEnVideo, fine-tuned on SAVEn-Vid. (iii) Besides, we present\nAVBench, a benchmark containing 2,500 QAs designed to evaluate models on\nenhanced audio-visual comprehension tasks within long video, challenging their\nability to handle intricate audio-visual interactions. Experiments on AVBench\nreveal the limitations of current AV-LLMs. Experiments also demonstrate that\nSAVEnVideo outperforms the best Video-LLM by 3.61% on the zero-shot long video\ntask (Video-MME) and surpasses the leading audio-visual LLM by 1.29% on the\nzero-shot audio-visual task (Music-AVQA). Consequently, at the 7B parameter\nscale, SAVEnVideo can achieve state-of-the-art performance. Our dataset and\ncode will be released at https://ljungang.github.io/SAVEn-Vid/ upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Endeavors have been made to explore Large Language Models for video analysis\n(Video-LLMs), particularly in understanding and interpreting long videos.\nHowever, existing Video-LLMs still face challenges in effectively integrating\nthe rich and diverse audio-visual information inherent in long videos, which is\ncrucial for comprehensive understanding. This raises the question: how can we\nleverage embedded audio-visual information to enhance long video understanding?\nTherefore, (i) we introduce SAVEn-Vid, the first-ever long audio-visual video\ndataset comprising over 58k audio-visual instructions. (ii) From the model\nperspective, we propose a time-aware Audio-Visual Large Language Model\n(AV-LLM), SAVEnVideo, fine-tuned on SAVEn-Vid. (iii) Besides, we present\nAVBench, a benchmark containing 2,500 QAs designed to evaluate models on\nenhanced audio-visual comprehension tasks within long video, challenging their\nability to handle intricate audio-visual interactions. Experiments on AVBench\nreveal the limitations of current AV-LLMs. Experiments also demonstrate that\nSAVEnVideo outperforms the best Video-LLM by 3.61% on the zero-shot long video\ntask (Video-MME) and surpasses the leading audio-visual LLM by 1.29% on the\nzero-shot audio-visual task (Music-AVQA). Consequently, at the 7B parameter\nscale, SAVEnVideo can achieve state-of-the-art performance. Our dataset and\ncode will be released at https://ljungang.github.io/SAVEn-Vid/ upon acceptance."
                },
                "authors": [
                    {
                        "name": "Jungang Li"
                    },
                    {
                        "name": "Sicheng Tao"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Xiaojie Gu"
                    },
                    {
                        "name": "Haodong Xu"
                    },
                    {
                        "name": "Xu Zheng"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16205v1",
                "updated": "2024-11-25T09:05:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    5,
                    36,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T09:05:36Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    5,
                    36,
                    0,
                    330,
                    0
                ],
                "title": "MH-MoE:Multi-Head Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MH-MoE:Multi-Head Mixture-of-Experts"
                },
                "summary": "Multi-Head Mixture-of-Experts (MH-MoE) demonstrates superior performance by\nusing the multi-head mechanism to collectively attend to information from\nvarious representation spaces within different experts. In this paper, we\npresent a novel implementation of MH-MoE that maintains both FLOPs and\nparameter parity with sparse Mixture of Experts models. Experimental results on\nlanguage models show that the new implementation yields quality improvements\nover both vanilla MoE and fine-grained MoE models. Additionally, our\nexperiments demonstrate that MH-MoE is compatible with 1-bit Large Language\nModels (LLMs) such as BitNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head Mixture-of-Experts (MH-MoE) demonstrates superior performance by\nusing the multi-head mechanism to collectively attend to information from\nvarious representation spaces within different experts. In this paper, we\npresent a novel implementation of MH-MoE that maintains both FLOPs and\nparameter parity with sparse Mixture of Experts models. Experimental results on\nlanguage models show that the new implementation yields quality improvements\nover both vanilla MoE and fine-grained MoE models. Additionally, our\nexperiments demonstrate that MH-MoE is compatible with 1-bit Large Language\nModels (LLMs) such as BitNet."
                },
                "authors": [
                    {
                        "name": "Shaohan Huang"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "7 pages, 0 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19198v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19198v2",
                "updated": "2024-11-25T08:57:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    57,
                    20,
                    0,
                    330,
                    0
                ],
                "published": "2024-07-27T07:34:49Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    7,
                    34,
                    49,
                    5,
                    209,
                    0
                ],
                "title": "Towards the Dynamics of a DNN Learning Symbolic Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards the Dynamics of a DNN Learning Symbolic Interactions"
                },
                "summary": "This study proves the two-phase dynamics of a deep neural network (DNN)\nlearning interactions. Despite the long disappointing view of the faithfulness\nof post-hoc explanation of a DNN, a series of theorems have been proven in\nrecent years to show that for a given input sample, a small set of interactions\nbetween input variables can be considered as primitive inference patterns that\nfaithfully represent a DNN's detailed inference logic on that sample.\nParticularly, Zhang et al. have observed that various DNNs all learn\ninteractions of different complexities in two distinct phases, and this\ntwo-phase dynamics well explains how a DNN changes from under-fitting to\nover-fitting. Therefore, in this study, we mathematically prove the two-phase\ndynamics of interactions, providing a theoretical mechanism for how the\ngeneralization power of a DNN changes during the training process. Experiments\nshow that our theory well predicts the real dynamics of interactions on\ndifferent DNNs trained for various tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study proves the two-phase dynamics of a deep neural network (DNN)\nlearning interactions. Despite the long disappointing view of the faithfulness\nof post-hoc explanation of a DNN, a series of theorems have been proven in\nrecent years to show that for a given input sample, a small set of interactions\nbetween input variables can be considered as primitive inference patterns that\nfaithfully represent a DNN's detailed inference logic on that sample.\nParticularly, Zhang et al. have observed that various DNNs all learn\ninteractions of different complexities in two distinct phases, and this\ntwo-phase dynamics well explains how a DNN changes from under-fitting to\nover-fitting. Therefore, in this study, we mathematically prove the two-phase\ndynamics of interactions, providing a theoretical mechanism for how the\ngeneralization power of a DNN changes during the training process. Experiments\nshow that our theory well predicts the real dynamics of interactions on\ndifferent DNNs trained for various tasks."
                },
                "authors": [
                    {
                        "name": "Qihan Ren"
                    },
                    {
                        "name": "Junpeng Zhang"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Yue Xin"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Quanshi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Quanshi Zhang"
                },
                "author": "Quanshi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19198v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19198v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16197v1",
                "updated": "2024-11-25T08:53:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    53,
                    23,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T08:53:23Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    53,
                    23,
                    0,
                    330,
                    0
                ],
                "title": "The bulk metallicity of giant planets around M stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The bulk metallicity of giant planets around M stars"
                },
                "summary": "The bulk-metallicity determination of giant exoplanets is essential to\nconstrain their formation and evolution pathways and to compare them to the\nsolar system. Previous studies inferred an inverse relation between the mass\nand bulk metallicity. However, the data almost exclusively contained planets\nthat orbit FGK stars. The recent discoveries of giant exoplanets around M-dwarf\nstars present an opportunity to probe whether they follow a mass-metallicity\ntrend different from that of their FGK counterparts. Using evolution models we\ncharacterised the interiors of giant exoplanets with reliable mass-radius\nmeasurements that orbit FGK and M-dwarf stars. We then inferred the\nmass-metallicity trends for both populations. We found that the bulk\nmetallicity of giant planets around M stars is overall lower compared to those\naround FGK stars. This yielded mass-metallicity relations for the two\npopulations with similar slopes but significantly different offsets. The lack\nof metal-rich giant planets around M dwarfs could explain the difference in the\ninferred offset and be a result of different formation conditions. However,\nthere were only 20 successful bulk-metallicity retrievals for the giant planets\naround M dwarfs, which resulted in rather large uncertainties. Therefore, it is\nof great importance to continue detecting these planets with both transit and\nradial velocities. Additionally, the characterisation of the atmospheres of\ngiant planets around M-stars can further help to constrain their interiors and\nto investigate the atmosp",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The bulk-metallicity determination of giant exoplanets is essential to\nconstrain their formation and evolution pathways and to compare them to the\nsolar system. Previous studies inferred an inverse relation between the mass\nand bulk metallicity. However, the data almost exclusively contained planets\nthat orbit FGK stars. The recent discoveries of giant exoplanets around M-dwarf\nstars present an opportunity to probe whether they follow a mass-metallicity\ntrend different from that of their FGK counterparts. Using evolution models we\ncharacterised the interiors of giant exoplanets with reliable mass-radius\nmeasurements that orbit FGK and M-dwarf stars. We then inferred the\nmass-metallicity trends for both populations. We found that the bulk\nmetallicity of giant planets around M stars is overall lower compared to those\naround FGK stars. This yielded mass-metallicity relations for the two\npopulations with similar slopes but significantly different offsets. The lack\nof metal-rich giant planets around M dwarfs could explain the difference in the\ninferred offset and be a result of different formation conditions. However,\nthere were only 20 successful bulk-metallicity retrievals for the giant planets\naround M dwarfs, which resulted in rather large uncertainties. Therefore, it is\nof great importance to continue detecting these planets with both transit and\nradial velocities. Additionally, the characterisation of the atmospheres of\ngiant planets around M-stars can further help to constrain their interiors and\nto investigate the atmosp"
                },
                "authors": [
                    {
                        "name": "Simon Mller"
                    },
                    {
                        "name": "Ravit Helled"
                    }
                ],
                "author_detail": {
                    "name": "Ravit Helled"
                },
                "author": "Ravit Helled",
                "arxiv_comment": "12 pages, 10 figures, 3 tables, accepted for publication at Astronomy\n  & Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16196v1",
                "updated": "2024-11-25T08:52:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    52,
                    46,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T08:52:46Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    52,
                    46,
                    0,
                    330,
                    0
                ],
                "title": "Learn from Foundation Model: Fruit Detection Model without Manual\n  Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learn from Foundation Model: Fruit Detection Model without Manual\n  Annotation"
                },
                "summary": "Recent breakthroughs in large foundation models have enabled the possibility\nof transferring knowledge pre-trained on vast datasets to domains with limited\ndata availability. Agriculture is one of the domains that lacks sufficient\ndata. This study proposes a framework to train effective, domain-specific,\nsmall models from foundation models without manual annotation. Our approach\nbegins with SDM (Segmentation-Description-Matching), a stage that leverages two\nfoundation models: SAM2 (Segment Anything in Images and Videos) for\nsegmentation and OpenCLIP (Open Contrastive Language-Image Pretraining) for\nzero-shot open-vocabulary classification. In the second stage, a novel\nknowledge distillation mechanism is utilized to distill compact,\nedge-deployable models from SDM, enhancing both inference speed and perception\naccuracy. The complete method, termed SDM-D\n(Segmentation-Description-Matching-Distilling), demonstrates strong performance\nacross various fruit detection tasks object detection, semantic segmentation,\nand instance segmentation) without manual annotation. It nearly matches the\nperformance of models trained with abundant labels. Notably, SDM-D outperforms\nopen-set detection methods such as Grounding SAM and YOLO-World on all tested\nfruit detection datasets. Additionally, we introduce MegaFruits, a\ncomprehensive fruit segmentation dataset encompassing over 25,000 images, and\nall code and datasets are made publicly available at\nhttps://github.com/AgRoboticsResearch/SDM-D.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in large foundation models have enabled the possibility\nof transferring knowledge pre-trained on vast datasets to domains with limited\ndata availability. Agriculture is one of the domains that lacks sufficient\ndata. This study proposes a framework to train effective, domain-specific,\nsmall models from foundation models without manual annotation. Our approach\nbegins with SDM (Segmentation-Description-Matching), a stage that leverages two\nfoundation models: SAM2 (Segment Anything in Images and Videos) for\nsegmentation and OpenCLIP (Open Contrastive Language-Image Pretraining) for\nzero-shot open-vocabulary classification. In the second stage, a novel\nknowledge distillation mechanism is utilized to distill compact,\nedge-deployable models from SDM, enhancing both inference speed and perception\naccuracy. The complete method, termed SDM-D\n(Segmentation-Description-Matching-Distilling), demonstrates strong performance\nacross various fruit detection tasks object detection, semantic segmentation,\nand instance segmentation) without manual annotation. It nearly matches the\nperformance of models trained with abundant labels. Notably, SDM-D outperforms\nopen-set detection methods such as Grounding SAM and YOLO-World on all tested\nfruit detection datasets. Additionally, we introduce MegaFruits, a\ncomprehensive fruit segmentation dataset encompassing over 25,000 images, and\nall code and datasets are made publicly available at\nhttps://github.com/AgRoboticsResearch/SDM-D.git."
                },
                "authors": [
                    {
                        "name": "Yanan Wang"
                    },
                    {
                        "name": "Zhenghao Fei"
                    },
                    {
                        "name": "Ruichen Li"
                    },
                    {
                        "name": "Yibin Ying"
                    }
                ],
                "author_detail": {
                    "name": "Yibin Ying"
                },
                "author": "Yibin Ying",
                "arxiv_comment": "17 pages, 12 figures, conference or other essential info",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16189v1",
                "updated": "2024-11-25T08:42:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    42,
                    33,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T08:42:33Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    42,
                    33,
                    0,
                    330,
                    0
                ],
                "title": "Enhancing Multi-Agent Consensus through Third-Party LLM Integration:\n  Analyzing Uncertainty and Mitigating Hallucinations in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Multi-Agent Consensus through Third-Party LLM Integration:\n  Analyzing Uncertainty and Mitigating Hallucinations in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) still face challenges when dealing with complex\nreasoning tasks, often resulting in hallucinations, which limit the practical\napplication of LLMs. To alleviate this issue, this paper proposes a new method\nthat integrates different LLMs to expand the knowledge boundary, reduce\ndependence on a single model, and promote in-depth debate among agents. The\nmain contributions include: 1) Introducing third-party LLMs to adjust the\nattention weights of agents through uncertainty estimation and confidence\nanalysis, optimizing consensus formation in multi-agent systems; 2) Experiments\non arithmetic datasets have validated the effectiveness of the method,\nsurpassing traditional multi-agent baselines. This research provides a new\nperspective for large models to alleviate hallucination phenomena when dealing\nwith complex tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) still face challenges when dealing with complex\nreasoning tasks, often resulting in hallucinations, which limit the practical\napplication of LLMs. To alleviate this issue, this paper proposes a new method\nthat integrates different LLMs to expand the knowledge boundary, reduce\ndependence on a single model, and promote in-depth debate among agents. The\nmain contributions include: 1) Introducing third-party LLMs to adjust the\nattention weights of agents through uncertainty estimation and confidence\nanalysis, optimizing consensus formation in multi-agent systems; 2) Experiments\non arithmetic datasets have validated the effectiveness of the method,\nsurpassing traditional multi-agent baselines. This research provides a new\nperspective for large models to alleviate hallucination phenomena when dealing\nwith complex tasks."
                },
                "authors": [
                    {
                        "name": "Zhihua Duan"
                    },
                    {
                        "name": "Jialin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jialin Wang"
                },
                "author": "Jialin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07745v2",
                "updated": "2024-11-25T08:38:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    38,
                    49,
                    0,
                    330,
                    0
                ],
                "published": "2024-10-10T09:23:26Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    9,
                    23,
                    26,
                    3,
                    284,
                    0
                ],
                "title": "StepTool: A Step-grained Reinforcement Learning Framework for Tool\n  Learning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StepTool: A Step-grained Reinforcement Learning Framework for Tool\n  Learning in LLMs"
                },
                "summary": "Despite having powerful reasoning and inference capabilities, Large Language\nModels (LLMs) still need external tools to acquire real-time information\nretrieval or domain-specific expertise to solve complex tasks, which is\nreferred to as tool learning. Existing tool learning methods primarily rely on\ntuning with expert trajectories, focusing on token-sequence learning from a\nlinguistic perspective. However, there are several challenges: 1) imitating\nstatic trajectories limits their ability to generalize to new tasks. 2) even\nexpert trajectories can be suboptimal, and better solution paths may exist. In\nthis work, we introduce StepTool, a novel step-grained reinforcement learning\nframework to improve tool learning in LLMs. It consists of two components:\nStep-grained Reward Shaping, which assigns rewards at each tool interaction\nbased on tool invocation success and its contribution to the task, and\nStep-grained Optimization, which uses policy gradient methods to optimize the\nmodel in a multi-step manner. Experimental results demonstrate that StepTool\nsignificantly outperforms existing methods in multi-step, tool-based tasks,\nproviding a robust solution for complex task environments. Codes are available\nat https://github.com/yuyq18/StepTool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite having powerful reasoning and inference capabilities, Large Language\nModels (LLMs) still need external tools to acquire real-time information\nretrieval or domain-specific expertise to solve complex tasks, which is\nreferred to as tool learning. Existing tool learning methods primarily rely on\ntuning with expert trajectories, focusing on token-sequence learning from a\nlinguistic perspective. However, there are several challenges: 1) imitating\nstatic trajectories limits their ability to generalize to new tasks. 2) even\nexpert trajectories can be suboptimal, and better solution paths may exist. In\nthis work, we introduce StepTool, a novel step-grained reinforcement learning\nframework to improve tool learning in LLMs. It consists of two components:\nStep-grained Reward Shaping, which assigns rewards at each tool interaction\nbased on tool invocation success and its contribution to the task, and\nStep-grained Optimization, which uses policy gradient methods to optimize the\nmodel in a multi-step manner. Experimental results demonstrate that StepTool\nsignificantly outperforms existing methods in multi-step, tool-based tasks,\nproviding a robust solution for complex task environments. Codes are available\nat https://github.com/yuyq18/StepTool."
                },
                "authors": [
                    {
                        "name": "Yuanqing Yu"
                    },
                    {
                        "name": "Zhefan Wang"
                    },
                    {
                        "name": "Weizhi Ma"
                    },
                    {
                        "name": "Zhicheng Guo"
                    },
                    {
                        "name": "Jingtao Zhan"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Chuhan Wu"
                    },
                    {
                        "name": "Zhiqiang Guo"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Ongoning Work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02801v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02801v3",
                "updated": "2024-11-25T08:32:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    32,
                    13,
                    0,
                    330,
                    0
                ],
                "published": "2024-05-05T03:15:52Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    3,
                    15,
                    52,
                    6,
                    126,
                    0
                ],
                "title": "Mozart's Touch: A Lightweight Multi-modal Music Generation Framework\n  Based on Pre-Trained Large Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mozart's Touch: A Lightweight Multi-modal Music Generation Framework\n  Based on Pre-Trained Large Models"
                },
                "summary": "In recent years, AI-Generated Content (AIGC) has witnessed rapid\nadvancements, facilitating the creation of music, images, and other artistic\nforms across a wide range of industries. However, current models for image- and\nvideo-to-music synthesis struggle to capture the nuanced emotions and\natmosphere conveyed by visual content. To fill this gap, we propose Mozart's\nTouch, a multi-modal music generation framework capable of generating music\naligned with cross-modal inputs such as images, videos, and text. The framework\nconsists of three key components: Multi-modal Captioning Module, Large Language\nModel (LLM) understanding \\& Bridging Module, and Music Generation Module.\nUnlike traditional end-to-end methods, Mozart's Touch uses LLMs to accurately\ninterpret visual elements without requiring the training or fine-tuning of\nmusic generation models, providing efficiency and transparency through clear,\ninterpretable prompts. We also introduce the \"LLM-Bridge\" method to resolve the\nheterogeneous representation challenges between descriptive texts from\ndifferent modalities. Through a series of objective and subjective evaluations,\nwe demonstrate that Mozart's Touch outperforms current state-of-the-art models.\nOur code and examples are available at\nhttps://github.com/TiffanyBlews/MozartsTouch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, AI-Generated Content (AIGC) has witnessed rapid\nadvancements, facilitating the creation of music, images, and other artistic\nforms across a wide range of industries. However, current models for image- and\nvideo-to-music synthesis struggle to capture the nuanced emotions and\natmosphere conveyed by visual content. To fill this gap, we propose Mozart's\nTouch, a multi-modal music generation framework capable of generating music\naligned with cross-modal inputs such as images, videos, and text. The framework\nconsists of three key components: Multi-modal Captioning Module, Large Language\nModel (LLM) understanding \\& Bridging Module, and Music Generation Module.\nUnlike traditional end-to-end methods, Mozart's Touch uses LLMs to accurately\ninterpret visual elements without requiring the training or fine-tuning of\nmusic generation models, providing efficiency and transparency through clear,\ninterpretable prompts. We also introduce the \"LLM-Bridge\" method to resolve the\nheterogeneous representation challenges between descriptive texts from\ndifferent modalities. Through a series of objective and subjective evaluations,\nwe demonstrate that Mozart's Touch outperforms current state-of-the-art models.\nOur code and examples are available at\nhttps://github.com/TiffanyBlews/MozartsTouch."
                },
                "authors": [
                    {
                        "name": "Jiajun Li"
                    },
                    {
                        "name": "Tianze Xu"
                    },
                    {
                        "name": "Xuesong Chen"
                    },
                    {
                        "name": "Xinrui Yao"
                    },
                    {
                        "name": "Shuchang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shuchang Liu"
                },
                "author": "Shuchang Liu",
                "arxiv_comment": "10 pages, 2 figures, submitted to AIGC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02801v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02801v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16185v1",
                "updated": "2024-11-25T08:31:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    31,
                    55,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T08:31:55Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    31,
                    55,
                    0,
                    330,
                    0
                ],
                "title": "Fancy123: One Image to High-Quality 3D Mesh Generation via Plug-and-Play\n  Deformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fancy123: One Image to High-Quality 3D Mesh Generation via Plug-and-Play\n  Deformation"
                },
                "summary": "Generating 3D meshes from a single image is an important but ill-posed task.\nExisting methods mainly adopt 2D multiview diffusion models to generate\nintermediate multiview images, and use the Large Reconstruction Model (LRM) to\ncreate the final meshes. However, the multiview images exhibit local\ninconsistencies, and the meshes often lack fidelity to the input image or look\nblurry. We propose Fancy123, featuring two enhancement modules and an\nunprojection operation to address the above three issues, respectively. The\nappearance enhancement module deforms the 2D multiview images to realign\nmisaligned pixels for better multiview consistency. The fidelity enhancement\nmodule deforms the 3D mesh to match the input image. The unprojection of the\ninput image and deformed multiview images onto LRM's generated mesh ensures\nhigh clarity, discarding LRM's predicted blurry-looking mesh colors. Extensive\nqualitative and quantitative experiments verify Fancy123's SoTA performance\nwith significant improvement. Also, the two enhancement modules are\nplug-and-play and work at inference time, allowing seamless integration into\nvarious existing single-image-to-3D methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating 3D meshes from a single image is an important but ill-posed task.\nExisting methods mainly adopt 2D multiview diffusion models to generate\nintermediate multiview images, and use the Large Reconstruction Model (LRM) to\ncreate the final meshes. However, the multiview images exhibit local\ninconsistencies, and the meshes often lack fidelity to the input image or look\nblurry. We propose Fancy123, featuring two enhancement modules and an\nunprojection operation to address the above three issues, respectively. The\nappearance enhancement module deforms the 2D multiview images to realign\nmisaligned pixels for better multiview consistency. The fidelity enhancement\nmodule deforms the 3D mesh to match the input image. The unprojection of the\ninput image and deformed multiview images onto LRM's generated mesh ensures\nhigh clarity, discarding LRM's predicted blurry-looking mesh colors. Extensive\nqualitative and quantitative experiments verify Fancy123's SoTA performance\nwith significant improvement. Also, the two enhancement modules are\nplug-and-play and work at inference time, allowing seamless integration into\nvarious existing single-image-to-3D methods."
                },
                "authors": [
                    {
                        "name": "Qiao Yu"
                    },
                    {
                        "name": "Xianzhi Li"
                    },
                    {
                        "name": "Yuan Tang"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Long Hu"
                    },
                    {
                        "name": "Yixue Hao"
                    },
                    {
                        "name": "Min Chen"
                    }
                ],
                "author_detail": {
                    "name": "Min Chen"
                },
                "author": "Min Chen",
                "arxiv_comment": "Project page: https://github.com/YuQiao0303/Fancy123",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14303v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14303v2",
                "updated": "2024-11-25T08:31:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    31,
                    0,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-21T16:56:33Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    56,
                    33,
                    3,
                    326,
                    0
                ],
                "title": "BugSpotter: Automated Generation of Code Debugging Exercises",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BugSpotter: Automated Generation of Code Debugging Exercises"
                },
                "summary": "Debugging is an essential skill when learning to program, yet its instruction\nand emphasis often vary widely across introductory courses. In the era of\ncode-generating large language models (LLMs), the ability for students to\nreason about code and identify errors is increasingly important. However,\nstudents frequently resort to trial-and-error methods to resolve bugs without\nfully understanding the underlying issues. Developing the ability to identify\nand hypothesize the cause of bugs is crucial but can be time-consuming to teach\neffectively through traditional means. This paper introduces BugSpotter, an\ninnovative tool that leverages an LLM to generate buggy code from a problem\ndescription and verify the synthesized bugs via a test suite. Students interact\nwith BugSpotter by designing failing test cases, where the buggy code's output\ndiffers from the expected result as defined by the problem specification. This\nnot only provides opportunities for students to enhance their debugging skills,\nbut also to practice reading and understanding problem specifications. We\ndeployed BugSpotter in a large classroom setting and compared the debugging\nexercises it generated to exercises hand-crafted by an instructor for the same\nproblems. We found that the LLM-generated exercises produced by BugSpotter\nvaried in difficulty and were well-matched to the problem specifications.\nImportantly, the LLM-generated exercises were comparable to those manually\ncreated by instructors with respect to student performance, suggesting that\nBugSpotter could be an effective and efficient aid for learning debugging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debugging is an essential skill when learning to program, yet its instruction\nand emphasis often vary widely across introductory courses. In the era of\ncode-generating large language models (LLMs), the ability for students to\nreason about code and identify errors is increasingly important. However,\nstudents frequently resort to trial-and-error methods to resolve bugs without\nfully understanding the underlying issues. Developing the ability to identify\nand hypothesize the cause of bugs is crucial but can be time-consuming to teach\neffectively through traditional means. This paper introduces BugSpotter, an\ninnovative tool that leverages an LLM to generate buggy code from a problem\ndescription and verify the synthesized bugs via a test suite. Students interact\nwith BugSpotter by designing failing test cases, where the buggy code's output\ndiffers from the expected result as defined by the problem specification. This\nnot only provides opportunities for students to enhance their debugging skills,\nbut also to practice reading and understanding problem specifications. We\ndeployed BugSpotter in a large classroom setting and compared the debugging\nexercises it generated to exercises hand-crafted by an instructor for the same\nproblems. We found that the LLM-generated exercises produced by BugSpotter\nvaried in difficulty and were well-matched to the problem specifications.\nImportantly, the LLM-generated exercises were comparable to those manually\ncreated by instructors with respect to student performance, suggesting that\nBugSpotter could be an effective and efficient aid for learning debugging."
                },
                "authors": [
                    {
                        "name": "Victor-Alexandru Pdurean"
                    },
                    {
                        "name": "Paul Denny"
                    },
                    {
                        "name": "Adish Singla"
                    }
                ],
                "author_detail": {
                    "name": "Adish Singla"
                },
                "author": "Adish Singla",
                "arxiv_comment": "Preprint of the SIGCSE'25 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14303v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14303v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07965v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07965v2",
                "updated": "2024-11-25T08:13:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    13,
                    0,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-12T17:41:16Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    17,
                    41,
                    16,
                    1,
                    317,
                    0
                ],
                "title": "From General to Specific: Utilizing General Hallucination to Benchmark\n  Specific Role-Playing Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From General to Specific: Utilizing General Hallucination to Benchmark\n  Specific Role-Playing Agents"
                },
                "summary": "The advanced role-playing capabilities of Large Language Models (LLMs) have\npaved the way for developing Role-Playing Agents (RPAs). However, existing\nbenchmarks in this domain, such as HPD and SocialBench face limitations like\npoor generalizability, implicit and inaccurate judgments, and the risk of model\nforgetting. To address the above issues, we propose an automatic, scalable, and\ngeneralizable paradigm. Specifically, we construct a benchmark, SHARP, by\nextracting relations from a general knowledge graph and leveraging the inherent\nhallucination properties of RPAs to simulate interactions across roles. We\nemploy ChatGPT for stance detection and define relationship hallucination along\nwith three related metrics based on stance transfer. Extensive experiments\nvalidate the effectiveness and stability of our paradigm. Our findings further\nexplore the factors influencing these metrics and discuss the trade-off between\nblind loyalty to relationships and adherence to facts in RPAs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advanced role-playing capabilities of Large Language Models (LLMs) have\npaved the way for developing Role-Playing Agents (RPAs). However, existing\nbenchmarks in this domain, such as HPD and SocialBench face limitations like\npoor generalizability, implicit and inaccurate judgments, and the risk of model\nforgetting. To address the above issues, we propose an automatic, scalable, and\ngeneralizable paradigm. Specifically, we construct a benchmark, SHARP, by\nextracting relations from a general knowledge graph and leveraging the inherent\nhallucination properties of RPAs to simulate interactions across roles. We\nemploy ChatGPT for stance detection and define relationship hallucination along\nwith three related metrics based on stance transfer. Extensive experiments\nvalidate the effectiveness and stability of our paradigm. Our findings further\nexplore the factors influencing these metrics and discuss the trade-off between\nblind loyalty to relationships and adherence to facts in RPAs."
                },
                "authors": [
                    {
                        "name": "Chuyi Kong"
                    },
                    {
                        "name": "Ziyang Luo"
                    },
                    {
                        "name": "Hongzhan Lin"
                    },
                    {
                        "name": "Zhiyuan Fan"
                    },
                    {
                        "name": "Yaxin Fan"
                    },
                    {
                        "name": "Yuxi Sun"
                    },
                    {
                        "name": "Jing Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jing Ma"
                },
                "author": "Jing Ma",
                "arxiv_comment": "Revise three typos in the abstract and methodology sections of the\n  introduction",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07965v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07965v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16173v1",
                "updated": "2024-11-25T08:04:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    4,
                    47,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T08:04:47Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    4,
                    47,
                    0,
                    330,
                    0
                ],
                "title": "SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval\n  and Routing in Long-Form Video Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval\n  and Routing in Long-Form Video Analysis"
                },
                "summary": "Despite advances in Large Multi-modal Models, applying them to long and\nuntrimmed video content remains challenging due to limitations in context\nlength and substantial memory overhead. These constraints often lead to\nsignificant information loss and reduced relevance in the model responses. With\nthe exponential growth of video data across web platforms, understanding\nlong-form video is crucial for advancing generalized intelligence. In this\npaper, we introduce SALOVA: Segment-Augmented LOng Video Assistant, a novel\nvideo-LLM framework designed to enhance the comprehension of lengthy video\ncontent through targeted retrieval process. We address two main challenges to\nachieve it: (i) We present the SceneWalk dataset, a high-quality collection of\n87.8K long videos, each densely captioned at the segment level to enable models\nto capture scene continuity and maintain rich descriptive context. (ii) We\ndevelop robust architectural designs integrating dynamic routing mechanism and\nspatio-temporal projector to efficiently retrieve and process relevant video\nsegments based on user queries. Our framework mitigates the limitations of\ncurrent video-LMMs by allowing for precise identification and retrieval of\nrelevant video segments in response to queries, thereby improving the\ncontextual relevance of the generated responses. Through extensive experiments,\nSALOVA demonstrates enhanced capability in processing complex long-form videos,\nshowing significant capability to maintain contextual integrity across extended\nsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advances in Large Multi-modal Models, applying them to long and\nuntrimmed video content remains challenging due to limitations in context\nlength and substantial memory overhead. These constraints often lead to\nsignificant information loss and reduced relevance in the model responses. With\nthe exponential growth of video data across web platforms, understanding\nlong-form video is crucial for advancing generalized intelligence. In this\npaper, we introduce SALOVA: Segment-Augmented LOng Video Assistant, a novel\nvideo-LLM framework designed to enhance the comprehension of lengthy video\ncontent through targeted retrieval process. We address two main challenges to\nachieve it: (i) We present the SceneWalk dataset, a high-quality collection of\n87.8K long videos, each densely captioned at the segment level to enable models\nto capture scene continuity and maintain rich descriptive context. (ii) We\ndevelop robust architectural designs integrating dynamic routing mechanism and\nspatio-temporal projector to efficiently retrieve and process relevant video\nsegments based on user queries. Our framework mitigates the limitations of\ncurrent video-LMMs by allowing for precise identification and retrieval of\nrelevant video segments in response to queries, thereby improving the\ncontextual relevance of the generated responses. Through extensive experiments,\nSALOVA demonstrates enhanced capability in processing complex long-form videos,\nshowing significant capability to maintain contextual integrity across extended\nsequences."
                },
                "authors": [
                    {
                        "name": "Junho Kim"
                    },
                    {
                        "name": "Hyunjun Kim"
                    },
                    {
                        "name": "Hosu Lee"
                    },
                    {
                        "name": "Yong Man Ro"
                    }
                ],
                "author_detail": {
                    "name": "Yong Man Ro"
                },
                "author": "Yong Man Ro",
                "arxiv_comment": "Project page: https://ivy-lvlm.github.io/SALOVA/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16158v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16158v1",
                "updated": "2024-11-25T07:34:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    7,
                    34,
                    53,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T07:34:53Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    7,
                    34,
                    53,
                    0,
                    330,
                    0
                ],
                "title": "MixPE: Quantization and Hardware Co-design for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MixPE: Quantization and Hardware Co-design for Efficient LLM Inference"
                },
                "summary": "Transformer-based large language models (LLMs) have achieved remarkable\nsuccess as model sizes continue to grow, yet their deployment remains\nchallenging due to significant computational and memory demands. Quantization\nhas emerged as a promising solution, and state-of-the-art quantization\nalgorithms for LLMs introduce the need for mixed-precision matrix\nmultiplication (mpGEMM), where lower-precision weights are multiplied with\nhigher-precision activations. Despite its benefits, current hardware\naccelerators such as GPUs and TPUs lack native support for efficient mpGEMM,\nleading to inefficient dequantization operations in the main sequential loop.\nTo address this limitation, we introduce MixPE, a specialized mixed-precision\nprocessing element designed for efficient low-bit quantization in LLM\ninference. MixPE leverages two key innovations to minimize dequantization\noverhead and unlock the full potential of low-bit quantization. First,\nrecognizing that scale and zero point are shared within each quantization\ngroup, we propose performing dequantization after per-group mpGEMM,\nsignificantly reducing dequantization overhead. Second, instead of relying on\nconventional multipliers, MixPE utilizes efficient shift\\&add operations for\nmultiplication, optimizing both computation and energy efficiency. Our\nexperimental results demonstrate that MixPE surpasses the state-of-the-art\nquantization accelerators by $2.6\\times$ speedup and $1.4\\times$ energy\nreduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have achieved remarkable\nsuccess as model sizes continue to grow, yet their deployment remains\nchallenging due to significant computational and memory demands. Quantization\nhas emerged as a promising solution, and state-of-the-art quantization\nalgorithms for LLMs introduce the need for mixed-precision matrix\nmultiplication (mpGEMM), where lower-precision weights are multiplied with\nhigher-precision activations. Despite its benefits, current hardware\naccelerators such as GPUs and TPUs lack native support for efficient mpGEMM,\nleading to inefficient dequantization operations in the main sequential loop.\nTo address this limitation, we introduce MixPE, a specialized mixed-precision\nprocessing element designed for efficient low-bit quantization in LLM\ninference. MixPE leverages two key innovations to minimize dequantization\noverhead and unlock the full potential of low-bit quantization. First,\nrecognizing that scale and zero point are shared within each quantization\ngroup, we propose performing dequantization after per-group mpGEMM,\nsignificantly reducing dequantization overhead. Second, instead of relying on\nconventional multipliers, MixPE utilizes efficient shift\\&add operations for\nmultiplication, optimizing both computation and energy efficiency. Our\nexperimental results demonstrate that MixPE surpasses the state-of-the-art\nquantization accelerators by $2.6\\times$ speedup and $1.4\\times$ energy\nreduction."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Lancheng Zou"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Bei Yu"
                },
                "author": "Bei Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16158v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16156v1",
                "updated": "2024-11-25T07:32:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    7,
                    32,
                    2,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T07:32:02Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    7,
                    32,
                    2,
                    0,
                    330,
                    0
                ],
                "title": "VideoOrion: Tokenizing Object Dynamics in Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoOrion: Tokenizing Object Dynamics in Videos"
                },
                "summary": "We present VideoOrion, a Video Large Language Model (Video-LLM) that\nexplicitly captures the key semantic information in videos--the\nspatial-temporal dynamics of objects throughout the videos. VideoOrion employs\nexpert vision models to extract object dynamics through a detect-segment-track\npipeline, encoding them into a set of object tokens by aggregating\nspatial-temporal object features. Our method addresses the persistent challenge\nin Video-LLMs of efficiently compressing high-dimensional video data into\nsemantic tokens that are comprehensible to LLMs. Compared to prior methods\nwhich resort to downsampling the original video or aggregating visual tokens\nusing resamplers, leading to information loss and entangled semantics,\nVideoOrion not only offers a more natural and efficient way to derive compact,\ndisentangled semantic representations but also enables explicit object modeling\nof video content with minimal computational cost. Moreover, the introduced\nobject tokens naturally allow VideoOrion to accomplish video-based referring\ntasks. Experimental results show that VideoOrion can learn to make good use of\nthe object tokens, and achieves competitive results on both general video\nquestion answering and video-based referring benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present VideoOrion, a Video Large Language Model (Video-LLM) that\nexplicitly captures the key semantic information in videos--the\nspatial-temporal dynamics of objects throughout the videos. VideoOrion employs\nexpert vision models to extract object dynamics through a detect-segment-track\npipeline, encoding them into a set of object tokens by aggregating\nspatial-temporal object features. Our method addresses the persistent challenge\nin Video-LLMs of efficiently compressing high-dimensional video data into\nsemantic tokens that are comprehensible to LLMs. Compared to prior methods\nwhich resort to downsampling the original video or aggregating visual tokens\nusing resamplers, leading to information loss and entangled semantics,\nVideoOrion not only offers a more natural and efficient way to derive compact,\ndisentangled semantic representations but also enables explicit object modeling\nof video content with minimal computational cost. Moreover, the introduced\nobject tokens naturally allow VideoOrion to accomplish video-based referring\ntasks. Experimental results show that VideoOrion can learn to make good use of\nthe object tokens, and achieves competitive results on both general video\nquestion answering and video-based referring benchmarks."
                },
                "authors": [
                    {
                        "name": "Yicheng Feng"
                    },
                    {
                        "name": "Yijiang Li"
                    },
                    {
                        "name": "Wanpeng Zhang"
                    },
                    {
                        "name": "Sipeng Zheng"
                    },
                    {
                        "name": "Zongqing Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zongqing Lu"
                },
                "author": "Zongqing Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05547v2",
                "updated": "2024-11-25T07:18:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    7,
                    18,
                    33,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-08T13:09:14Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    9,
                    14,
                    4,
                    313,
                    0
                ],
                "title": "Assessing the Answerability of Queries in Retrieval-Augmented Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Answerability of Queries in Retrieval-Augmented Code\n  Generation"
                },
                "summary": "Thanks to unprecedented language understanding and generation capabilities of\nlarge language model (LLM), Retrieval-augmented Code Generation (RaCG) has\nrecently been widely utilized among software developers. While this has\nincreased productivity, there are still frequent instances of incorrect codes\nbeing provided. In particular, there are cases where plausible yet incorrect\ncodes are generated for queries from users that cannot be answered with the\ngiven queries and API descriptions. This study proposes a task for evaluating\nanswerability, which assesses whether valid answers can be generated based on\nusers' queries and retrieved APIs in RaCG. Additionally, we build a benchmark\ndataset called Retrieval-augmented Code Generability Evaluation (RaCGEval) to\nevaluate the performance of models performing this task. Experimental results\nshow that this task remains at a very challenging level, with baseline models\nexhibiting a low performance of 46.7%. Furthermore, this study discusses\nmethods that could significantly improve performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thanks to unprecedented language understanding and generation capabilities of\nlarge language model (LLM), Retrieval-augmented Code Generation (RaCG) has\nrecently been widely utilized among software developers. While this has\nincreased productivity, there are still frequent instances of incorrect codes\nbeing provided. In particular, there are cases where plausible yet incorrect\ncodes are generated for queries from users that cannot be answered with the\ngiven queries and API descriptions. This study proposes a task for evaluating\nanswerability, which assesses whether valid answers can be generated based on\nusers' queries and retrieved APIs in RaCG. Additionally, we build a benchmark\ndataset called Retrieval-augmented Code Generability Evaluation (RaCGEval) to\nevaluate the performance of models performing this task. Experimental results\nshow that this task remains at a very challenging level, with baseline models\nexhibiting a low performance of 46.7%. Furthermore, this study discusses\nmethods that could significantly improve performance."
                },
                "authors": [
                    {
                        "name": "Geonmin Kim"
                    },
                    {
                        "name": "Jaeyeon Kim"
                    },
                    {
                        "name": "Hancheol Park"
                    },
                    {
                        "name": "Wooksu Shin"
                    },
                    {
                        "name": "Tae-Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Tae-Ho Kim"
                },
                "author": "Tae-Ho Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16710v2",
                "updated": "2024-11-25T07:12:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    7,
                    12,
                    35,
                    0,
                    330,
                    0
                ],
                "published": "2024-09-25T07:55:36Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    55,
                    36,
                    2,
                    269,
                    0
                ],
                "title": "Beyond Turing Test: Can GPT-4 Sway Experts' Decisions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Turing Test: Can GPT-4 Sway Experts' Decisions?"
                },
                "summary": "In the post-Turing era, evaluating large language models (LLMs) involves\nassessing generated text based on readers' reactions rather than merely its\nindistinguishability from human-produced content. This paper explores how\nLLM-generated text impacts readers' decisions, focusing on both amateur and\nexpert audiences. Our findings indicate that GPT-4 can generate persuasive\nanalyses affecting the decisions of both amateurs and professionals.\nFurthermore, we evaluate the generated text from the aspects of grammar,\nconvincingness, logical coherence, and usefulness. The results highlight a high\ncorrelation between real-world evaluation through audience reactions and the\ncurrent multi-dimensional evaluators commonly used for generative models.\nOverall, this paper shows the potential and risk of using generated text to\nsway human decisions and also points out a new direction for evaluating\ngenerated text, i.e., leveraging the reactions and decisions of readers. We\nrelease our dataset to assist future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the post-Turing era, evaluating large language models (LLMs) involves\nassessing generated text based on readers' reactions rather than merely its\nindistinguishability from human-produced content. This paper explores how\nLLM-generated text impacts readers' decisions, focusing on both amateur and\nexpert audiences. Our findings indicate that GPT-4 can generate persuasive\nanalyses affecting the decisions of both amateurs and professionals.\nFurthermore, we evaluate the generated text from the aspects of grammar,\nconvincingness, logical coherence, and usefulness. The results highlight a high\ncorrelation between real-world evaluation through audience reactions and the\ncurrent multi-dimensional evaluators commonly used for generative models.\nOverall, this paper shows the potential and risk of using generated text to\nsway human decisions and also points out a new direction for evaluating\ngenerated text, i.e., leveraging the reactions and decisions of readers. We\nrelease our dataset to assist future research."
                },
                "authors": [
                    {
                        "name": "Takehiro Takayanagi"
                    },
                    {
                        "name": "Hiroya Takamura"
                    },
                    {
                        "name": "Kiyoshi Izumi"
                    },
                    {
                        "name": "Chung-Chi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chung-Chi Chen"
                },
                "author": "Chung-Chi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.02684v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.02684v3",
                "updated": "2024-11-25T07:07:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    7,
                    7,
                    45,
                    0,
                    330,
                    0
                ],
                "published": "2023-11-05T15:48:29Z",
                "published_parsed": [
                    2023,
                    11,
                    5,
                    15,
                    48,
                    29,
                    6,
                    309,
                    0
                ],
                "title": "Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE"
                },
                "summary": "Recent studies have demonstrated Large Language Models (LLMs) can extend\ntheir zero-shot generalization capabilities to multimodal learning through\ninstruction tuning. As more modalities and downstream tasks are introduced,\nnegative conflicts and interference may have a worse impact on performance.\nWhile this phenomenon has been overlooked in previous work, we propose a novel\nand extensible framework, called Octavius, for comprehensive studies and\nexperimentation on multimodal learning with Multimodal Large Language Models\n(MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and\none of the representative PEFT techniques, i.e., LoRA, designing a novel\nLLM-based decoder, called LoRA-MoE, for multimodal learning. To the best of our\nknowledge, we are one of the pioneering efforts to introduce MoE into MLLMs to\naddress this problem. The experimental results (about 20% improvement) have\nshown the effectiveness and versatility of our design in various 2D and 3D\ndownstream tasks. Code and datasets are available at\nhttps://openlamm.github.io/tutorial/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have demonstrated Large Language Models (LLMs) can extend\ntheir zero-shot generalization capabilities to multimodal learning through\ninstruction tuning. As more modalities and downstream tasks are introduced,\nnegative conflicts and interference may have a worse impact on performance.\nWhile this phenomenon has been overlooked in previous work, we propose a novel\nand extensible framework, called Octavius, for comprehensive studies and\nexperimentation on multimodal learning with Multimodal Large Language Models\n(MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and\none of the representative PEFT techniques, i.e., LoRA, designing a novel\nLLM-based decoder, called LoRA-MoE, for multimodal learning. To the best of our\nknowledge, we are one of the pioneering efforts to introduce MoE into MLLMs to\naddress this problem. The experimental results (about 20% improvement) have\nshown the effectiveness and versatility of our design in various 2D and 3D\ndownstream tasks. Code and datasets are available at\nhttps://openlamm.github.io/tutorial/."
                },
                "authors": [
                    {
                        "name": "Zeren Chen"
                    },
                    {
                        "name": "Ziqin Wang"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Huayang Liu"
                    },
                    {
                        "name": "Zhenfei Yin"
                    },
                    {
                        "name": "Si Liu"
                    },
                    {
                        "name": "Lu Sheng"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "arxiv_comment": "22 pages, 12 figures. Accepted in ICLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.02684v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.02684v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16139v1",
                "updated": "2024-11-25T06:59:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    6,
                    59,
                    16,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T06:59:16Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    6,
                    59,
                    16,
                    0,
                    330,
                    0
                ],
                "title": "Beyond Task Vectors: Selective Task Arithmetic Based on Importance\n  Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Task Vectors: Selective Task Arithmetic Based on Importance\n  Metrics"
                },
                "summary": "Pretrained models have revolutionized deep learning by enabling significant\nperformance improvements across a wide range of tasks, leveraging large-scale,\npre-learned knowledge representations. However, deploying these models in\nreal-world multi-task learning (MTL) scenarios poses substantial challenges,\nprimarily due to high computational costs and inefficiencies in inference.\nTraditional approaches such as pruning, quantization, and knowledge\ndistillation have been explored to mitigate these issues, but they often fall\nshort in fully addressing the complexities of multi-task environments. This\npaper introduces \\textbf{\\underline{S}}elective \\textbf{\\underline{T}}ask\n\\textbf{\\underline{A}}rithmetic \\underline{\\textbf{(STA)}}, a training-free\nframework designed to enhance multi-task performance through task-specific\nparameter fusion. STA addresses three key challenges: (i) \\textbf{Parameter\nimportance diversity: } Recognizing that different tasks relie on distinct\nparameters, STA employs a loss-sensitive parameter importance metric derived\nfrom a first-order Taylor expansion to accurately measure the importance of\nparameters for each task. (ii) \\textbf{Over-reliance on hyperparameter tuning:\n}By enhancing the sparsity of task vectors through parameter importance\nmetrics, STA reduces the need for extensive hyperparameter tuning, thereby\nimproving the generalization and robustness of the model. (iii) \\textbf{Neglect\nof other abilities in task arithmetic: } Previous works have largely overlooked\nthe potential for more precise task forgetting. STA leverages its parameter\nimportance metric to achieve more controlled and effective task forgetting,\nminimizing the impact of noisy elements that can degrade model performance.\nExperimental results demonstrate that STA achieves superior multi-task\nperformance across benchmarks and excellent performance in task forgetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained models have revolutionized deep learning by enabling significant\nperformance improvements across a wide range of tasks, leveraging large-scale,\npre-learned knowledge representations. However, deploying these models in\nreal-world multi-task learning (MTL) scenarios poses substantial challenges,\nprimarily due to high computational costs and inefficiencies in inference.\nTraditional approaches such as pruning, quantization, and knowledge\ndistillation have been explored to mitigate these issues, but they often fall\nshort in fully addressing the complexities of multi-task environments. This\npaper introduces \\textbf{\\underline{S}}elective \\textbf{\\underline{T}}ask\n\\textbf{\\underline{A}}rithmetic \\underline{\\textbf{(STA)}}, a training-free\nframework designed to enhance multi-task performance through task-specific\nparameter fusion. STA addresses three key challenges: (i) \\textbf{Parameter\nimportance diversity: } Recognizing that different tasks relie on distinct\nparameters, STA employs a loss-sensitive parameter importance metric derived\nfrom a first-order Taylor expansion to accurately measure the importance of\nparameters for each task. (ii) \\textbf{Over-reliance on hyperparameter tuning:\n}By enhancing the sparsity of task vectors through parameter importance\nmetrics, STA reduces the need for extensive hyperparameter tuning, thereby\nimproving the generalization and robustness of the model. (iii) \\textbf{Neglect\nof other abilities in task arithmetic: } Previous works have largely overlooked\nthe potential for more precise task forgetting. STA leverages its parameter\nimportance metric to achieve more controlled and effective task forgetting,\nminimizing the impact of noisy elements that can degrade model performance.\nExperimental results demonstrate that STA achieves superior multi-task\nperformance across benchmarks and excellent performance in task forgetting."
                },
                "authors": [
                    {
                        "name": "Tian Bowen"
                    },
                    {
                        "name": "Lai Songning"
                    },
                    {
                        "name": "Wu Jiemin"
                    },
                    {
                        "name": "Shuai Zhihao"
                    },
                    {
                        "name": "Ge Shiming"
                    },
                    {
                        "name": "Yue Yutao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Yutao"
                },
                "author": "Yue Yutao",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13990v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13990v2",
                "updated": "2024-11-25T06:57:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    6,
                    57,
                    25,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-21T10:00:52Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    0,
                    52,
                    3,
                    326,
                    0
                ],
                "title": "Repository-level Code Translation Benchmark Targeting Rust",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repository-level Code Translation Benchmark Targeting Rust"
                },
                "summary": "Recent advances in large language models (LLMs) have shown significant\ncapabilities in code translation, often evaluated using benchmarks like\nCodeTransOcean. However, these evaluations typically focus on simple,\nfunction-level translations without considering dependencies, which does not\nreflect the complexities of real-world software development. Further, their\neffectiveness in translating to newer, lower-resource languages like Rust in\nrealistic scenarios is still under-explored. To address this gap, we introduce\nfirst repository-level code translation benchmark comprising 375 tasks\ntargeting Rust, complete with relevant dependencies. Using this benchmark, we\nstudy four state-of-the-art LLMs, analyzing their erroneous outputs to\nunderstand their performance in more complex translation scenarios. Our\nfindings reveal that LLMs exhibit substantially worse performance (41.5%-56.2%\nPass@1 drop of GPT-4) on repository-level translations compared to simpler\ntasks, highlighting limitations in existing evaluation methods. The model that\nperformed the best is Claude-3.5, demonstrating the strongest translation\ncapabilities in both basic functionality accuracy and several relevant\nadditional abilities. Additionally, we discover that LLMs struggle with\nidentifying language differences in complex tasks, and that increased\ndependencies correlate with greater translation difficulty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have shown significant\ncapabilities in code translation, often evaluated using benchmarks like\nCodeTransOcean. However, these evaluations typically focus on simple,\nfunction-level translations without considering dependencies, which does not\nreflect the complexities of real-world software development. Further, their\neffectiveness in translating to newer, lower-resource languages like Rust in\nrealistic scenarios is still under-explored. To address this gap, we introduce\nfirst repository-level code translation benchmark comprising 375 tasks\ntargeting Rust, complete with relevant dependencies. Using this benchmark, we\nstudy four state-of-the-art LLMs, analyzing their erroneous outputs to\nunderstand their performance in more complex translation scenarios. Our\nfindings reveal that LLMs exhibit substantially worse performance (41.5%-56.2%\nPass@1 drop of GPT-4) on repository-level translations compared to simpler\ntasks, highlighting limitations in existing evaluation methods. The model that\nperformed the best is Claude-3.5, demonstrating the strongest translation\ncapabilities in both basic functionality accuracy and several relevant\nadditional abilities. Additionally, we discover that LLMs struggle with\nidentifying language differences in complex tasks, and that increased\ndependencies correlate with greater translation difficulty."
                },
                "authors": [
                    {
                        "name": "Guangsheng Ou"
                    },
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Xing Peng"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13990v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13990v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.16679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16679v1",
                "updated": "2024-11-25T18:59:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    59,
                    30,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T18:59:30Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    59,
                    30,
                    0,
                    330,
                    0
                ],
                "title": "Do Large Language Models Perform Latent Multi-Hop Reasoning without\n  Exploiting Shortcuts?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Perform Latent Multi-Hop Reasoning without\n  Exploiting Shortcuts?"
                },
                "summary": "We evaluate how well Large Language Models (LLMs) latently recall and compose\nfacts to answer multi-hop queries like \"In the year Scarlett Johansson was\nborn, the Summer Olympics were hosted in the country of\". One major challenge\nin evaluating this ability is that LLMs may have developed shortcuts by\nencounters of the head entity \"Scarlett Johansson\" and the answer entity\n\"United States\" in the same training sequences or merely guess the answer based\non frequency-based priors. To prevent shortcuts, we exclude test queries where\nthe head and answer entities co-appear in pretraining corpora. Through careful\nselection of relations and facts and systematic removal of cases where models\nmight guess answers or exploit partial matches, we construct an evaluation\ndataset SOCRATES (ShOrtCut-fRee lATent rEaSoning). We observe that LLMs\ndemonstrate promising latent multi-hop reasoning abilities without exploiting\nshortcuts, but only for certain types of queries. For queries requiring latent\nrecall of countries as the intermediate answer, the best models achieve 80%\nlatent composability, but this drops to just 5% for the recall of years.\nComparisons with Chain-of-Thought composability highlight a significant gap\nbetween the ability of models to reason latently versus explicitly. Analysis\nreveals that latent representations of the intermediate answer are constructed\nmore often in queries with higher latent composability, and shows the emergence\nof latent multi-hop reasoning during pretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We evaluate how well Large Language Models (LLMs) latently recall and compose\nfacts to answer multi-hop queries like \"In the year Scarlett Johansson was\nborn, the Summer Olympics were hosted in the country of\". One major challenge\nin evaluating this ability is that LLMs may have developed shortcuts by\nencounters of the head entity \"Scarlett Johansson\" and the answer entity\n\"United States\" in the same training sequences or merely guess the answer based\non frequency-based priors. To prevent shortcuts, we exclude test queries where\nthe head and answer entities co-appear in pretraining corpora. Through careful\nselection of relations and facts and systematic removal of cases where models\nmight guess answers or exploit partial matches, we construct an evaluation\ndataset SOCRATES (ShOrtCut-fRee lATent rEaSoning). We observe that LLMs\ndemonstrate promising latent multi-hop reasoning abilities without exploiting\nshortcuts, but only for certain types of queries. For queries requiring latent\nrecall of countries as the intermediate answer, the best models achieve 80%\nlatent composability, but this drops to just 5% for the recall of years.\nComparisons with Chain-of-Thought composability highlight a significant gap\nbetween the ability of models to reason latently versus explicitly. Analysis\nreveals that latent representations of the intermediate answer are constructed\nmore often in queries with higher latent composability, and shows the emergence\nof latent multi-hop reasoning during pretraining."
                },
                "authors": [
                    {
                        "name": "Sohee Yang"
                    },
                    {
                        "name": "Nora Kassner"
                    },
                    {
                        "name": "Elena Gribovskaya"
                    },
                    {
                        "name": "Sebastian Riedel"
                    },
                    {
                        "name": "Mor Geva"
                    }
                ],
                "author_detail": {
                    "name": "Mor Geva"
                },
                "author": "Mor Geva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16657v1",
                "updated": "2024-11-25T18:41:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    41,
                    56,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T18:41:56Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    41,
                    56,
                    0,
                    330,
                    0
                ],
                "title": "DreamRunner: Fine-Grained Storytelling Video Generation with\n  Retrieval-Augmented Motion Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DreamRunner: Fine-Grained Storytelling Video Generation with\n  Retrieval-Augmented Motion Adaptation"
                },
                "summary": "Storytelling video generation (SVG) has recently emerged as a task to create\nlong, multi-motion, multi-scene videos that consistently represent the story\ndescribed in the input text script. SVG holds great potential for diverse\ncontent creation in media and entertainment; however, it also presents\nsignificant challenges: (1) objects must exhibit a range of fine-grained,\ncomplex motions, (2) multiple objects need to appear consistently across\nscenes, and (3) subjects may require multiple motions with seamless transitions\nwithin a single scene. To address these challenges, we propose DreamRunner, a\nnovel story-to-video generation method: First, we structure the input script\nusing a large language model (LLM) to facilitate both coarse-grained scene\nplanning as well as fine-grained object-level layout and motion planning. Next,\nDreamRunner presents retrieval-augmented test-time adaptation to capture target\nmotion priors for objects in each scene, supporting diverse motion\ncustomization based on retrieved videos, thus facilitating the generation of\nnew videos with complex, scripted motions. Lastly, we propose a novel\nspatial-temporal region-based 3D attention and prior injection module SR3AI for\nfine-grained object-motion binding and frame-by-frame semantic control. We\ncompare DreamRunner with various SVG baselines, demonstrating state-of-the-art\nperformance in character consistency, text alignment, and smooth transitions.\nAdditionally, DreamRunner exhibits strong fine-grained condition-following\nability in compositional text-to-video generation, significantly outperforming\nbaselines on T2V-ComBench. Finally, we validate DreamRunner's robust ability to\ngenerate multi-object interactions with qualitative examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Storytelling video generation (SVG) has recently emerged as a task to create\nlong, multi-motion, multi-scene videos that consistently represent the story\ndescribed in the input text script. SVG holds great potential for diverse\ncontent creation in media and entertainment; however, it also presents\nsignificant challenges: (1) objects must exhibit a range of fine-grained,\ncomplex motions, (2) multiple objects need to appear consistently across\nscenes, and (3) subjects may require multiple motions with seamless transitions\nwithin a single scene. To address these challenges, we propose DreamRunner, a\nnovel story-to-video generation method: First, we structure the input script\nusing a large language model (LLM) to facilitate both coarse-grained scene\nplanning as well as fine-grained object-level layout and motion planning. Next,\nDreamRunner presents retrieval-augmented test-time adaptation to capture target\nmotion priors for objects in each scene, supporting diverse motion\ncustomization based on retrieved videos, thus facilitating the generation of\nnew videos with complex, scripted motions. Lastly, we propose a novel\nspatial-temporal region-based 3D attention and prior injection module SR3AI for\nfine-grained object-motion binding and frame-by-frame semantic control. We\ncompare DreamRunner with various SVG baselines, demonstrating state-of-the-art\nperformance in character consistency, text alignment, and smooth transitions.\nAdditionally, DreamRunner exhibits strong fine-grained condition-following\nability in compositional text-to-video generation, significantly outperforming\nbaselines on T2V-ComBench. Finally, we validate DreamRunner's robust ability to\ngenerate multi-object interactions with qualitative examples."
                },
                "authors": [
                    {
                        "name": "Zun Wang"
                    },
                    {
                        "name": "Jialu Li"
                    },
                    {
                        "name": "Han Lin"
                    },
                    {
                        "name": "Jaehong Yoon"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "Project website: https://dreamrunner-story2video.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16646v1",
                "updated": "2024-11-25T18:28:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    28,
                    26,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T18:28:26Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    28,
                    26,
                    0,
                    330,
                    0
                ],
                "title": "Self-Generated Critiques Boost Reward Modeling for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Generated Critiques Boost Reward Modeling for Language Models"
                },
                "summary": "Reward modeling is crucial for aligning large language models (LLMs) with\nhuman preferences, especially in reinforcement learning from human feedback\n(RLHF). However, current reward models mainly produce scalar scores and\nstruggle to incorporate critiques in a natural language format. We hypothesize\nthat predicting both critiques and the scalar reward would improve reward\nmodeling ability. Motivated by this, we propose Critic-RM, a framework that\nimproves reward models using self-generated critiques without extra\nsupervision. Critic-RM employs a two-stage process: generating and filtering\nhigh-quality critiques, followed by joint fine-tuning on reward prediction and\ncritique generation. Experiments across benchmarks show that Critic-RM improves\nreward modeling accuracy by 3.7%-7.3% compared to standard reward models and\nLLM judges, demonstrating strong performance and data efficiency. Additional\nstudies further validate the effectiveness of generated critiques in rectifying\nflawed reasoning steps with 2.5%-3.2% gains in improving reasoning accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward modeling is crucial for aligning large language models (LLMs) with\nhuman preferences, especially in reinforcement learning from human feedback\n(RLHF). However, current reward models mainly produce scalar scores and\nstruggle to incorporate critiques in a natural language format. We hypothesize\nthat predicting both critiques and the scalar reward would improve reward\nmodeling ability. Motivated by this, we propose Critic-RM, a framework that\nimproves reward models using self-generated critiques without extra\nsupervision. Critic-RM employs a two-stage process: generating and filtering\nhigh-quality critiques, followed by joint fine-tuning on reward prediction and\ncritique generation. Experiments across benchmarks show that Critic-RM improves\nreward modeling accuracy by 3.7%-7.3% compared to standard reward models and\nLLM judges, demonstrating strong performance and data efficiency. Additional\nstudies further validate the effectiveness of generated critiques in rectifying\nflawed reasoning steps with 2.5%-3.2% gains in improving reasoning accuracy."
                },
                "authors": [
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Zhengxing Chen"
                    },
                    {
                        "name": "Aston Zhang"
                    },
                    {
                        "name": "Liang Tan"
                    },
                    {
                        "name": "Chenguang Zhu"
                    },
                    {
                        "name": "Richard Yuanzhe Pang"
                    },
                    {
                        "name": "Yundi Qian"
                    },
                    {
                        "name": "Xuewei Wang"
                    },
                    {
                        "name": "Suchin Gururangan"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Melanie Kambadur"
                    },
                    {
                        "name": "Dhruv Mahajan"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16638v1",
                "updated": "2024-11-25T18:15:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    15,
                    15,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T18:15:15Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    15,
                    15,
                    0,
                    330,
                    0
                ],
                "title": "Do Automatic Factuality Metrics Measure Factuality? A Critical\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Automatic Factuality Metrics Measure Factuality? A Critical\n  Evaluation"
                },
                "summary": "Modern LLMs can now produce highly readable abstractive summaries, to the\npoint where traditional automated metrics for evaluating summary quality, such\nas ROUGE, have become saturated. However, LLMs still sometimes introduce\nunwanted content into summaries, i.e., information inconsistent with or\nunsupported by their source. Measuring the occurrence of these often subtle\n``hallucinations'' automatically has proved to be challenging. This in turn has\nmotivated development of a variety of metrics intended to measure the factual\nconsistency of generated summaries against their source. But are these\napproaches measuring what they purport to do? In this work, we stress-test\nautomatic factuality metrics. Specifically, we investigate whether and to what\ndegree superficial attributes of summary texts suffice to predict\n``factuality'', finding that a (supervised) model using only such shallow\nfeatures is reasonably competitive with SOTA factuality scoring methods. We\nthen evaluate how factuality metrics respond to factual corrections in\ninconsistent summaries and find that only a few show meaningful improvements.\nIn contrast, some metrics are more sensitive to benign, non-factual edits.\nMotivated by these insights, we show that one can ``game'' (most) automatic\nfactuality metrics, i.e., reliably inflate ``factuality'' scores by appending\ninnocuous sentences to generated summaries.Taken together, our results raise\nquestions about the degree to which we should rely on existing automated\nfactuality metrics and what exactly we want ``factuality metrics'' to measure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLMs can now produce highly readable abstractive summaries, to the\npoint where traditional automated metrics for evaluating summary quality, such\nas ROUGE, have become saturated. However, LLMs still sometimes introduce\nunwanted content into summaries, i.e., information inconsistent with or\nunsupported by their source. Measuring the occurrence of these often subtle\n``hallucinations'' automatically has proved to be challenging. This in turn has\nmotivated development of a variety of metrics intended to measure the factual\nconsistency of generated summaries against their source. But are these\napproaches measuring what they purport to do? In this work, we stress-test\nautomatic factuality metrics. Specifically, we investigate whether and to what\ndegree superficial attributes of summary texts suffice to predict\n``factuality'', finding that a (supervised) model using only such shallow\nfeatures is reasonably competitive with SOTA factuality scoring methods. We\nthen evaluate how factuality metrics respond to factual corrections in\ninconsistent summaries and find that only a few show meaningful improvements.\nIn contrast, some metrics are more sensitive to benign, non-factual edits.\nMotivated by these insights, we show that one can ``game'' (most) automatic\nfactuality metrics, i.e., reliably inflate ``factuality'' scores by appending\ninnocuous sentences to generated summaries.Taken together, our results raise\nquestions about the degree to which we should rely on existing automated\nfactuality metrics and what exactly we want ``factuality metrics'' to measure."
                },
                "authors": [
                    {
                        "name": "Sanjana Ramprasad"
                    },
                    {
                        "name": "Byron C. Wallace"
                    }
                ],
                "author_detail": {
                    "name": "Byron C. Wallace"
                },
                "author": "Byron C. Wallace",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08509v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08509v2",
                "updated": "2024-11-25T17:35:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    17,
                    35,
                    7,
                    0,
                    330,
                    0
                ],
                "published": "2024-04-12T14:46:15Z",
                "published_parsed": [
                    2024,
                    4,
                    12,
                    14,
                    46,
                    15,
                    4,
                    103,
                    0
                ],
                "title": "Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\n  Prediction"
                },
                "summary": "Large language models (LLMs) have been driving a new wave of interactive AI\napplications across numerous domains. However, efficiently serving LLM\ninference requests is challenging due to their unpredictable execution times\noriginating from the autoregressive nature of generative models. Existing LLM\nserving systems exploit first-come-first-serve (FCFS) scheduling, suffering\nfrom head-of-line blocking issues. To address the non-deterministic nature of\nLLMs and enable efficient interactive LLM serving, we present a speculative\nshortest-job-first (SSJF) scheduler that uses a light proxy model to predict\nLLM output sequence lengths. Our open-source SSJF implementation does not\nrequire changes to memory management or batching strategies. Evaluations on\nreal-world datasets and production workload traces show that SSJF reduces\naverage job completion times by 30.5-39.6% and increases throughput by 2.2-3.6x\ncompared to FCFS schedulers, across no batching, dynamic batching, and\ncontinuous batching settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been driving a new wave of interactive AI\napplications across numerous domains. However, efficiently serving LLM\ninference requests is challenging due to their unpredictable execution times\noriginating from the autoregressive nature of generative models. Existing LLM\nserving systems exploit first-come-first-serve (FCFS) scheduling, suffering\nfrom head-of-line blocking issues. To address the non-deterministic nature of\nLLMs and enable efficient interactive LLM serving, we present a speculative\nshortest-job-first (SSJF) scheduler that uses a light proxy model to predict\nLLM output sequence lengths. Our open-source SSJF implementation does not\nrequire changes to memory management or batching strategies. Evaluations on\nreal-world datasets and production workload traces show that SSJF reduces\naverage job completion times by 30.5-39.6% and increases throughput by 2.2-3.6x\ncompared to FCFS schedulers, across no batching, dynamic batching, and\ncontinuous batching settings."
                },
                "authors": [
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Weichao Mao"
                    },
                    {
                        "name": "Archit Patke"
                    },
                    {
                        "name": "Shengkun Cui"
                    },
                    {
                        "name": "Saurabh Jha"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Hubertus Franke"
                    },
                    {
                        "name": "Zbigniew T. Kalbarczyk"
                    },
                    {
                        "name": "Tamer Baar"
                    },
                    {
                        "name": "Ravishankar K. Iyer"
                    }
                ],
                "author_detail": {
                    "name": "Ravishankar K. Iyer"
                },
                "author": "Ravishankar K. Iyer",
                "arxiv_comment": "Accepted at AIOps'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08509v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08509v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16602v1",
                "updated": "2024-11-25T17:31:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    17,
                    31,
                    57,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T17:31:57Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    17,
                    31,
                    57,
                    0,
                    330,
                    0
                ],
                "title": "Chat2SVG: Vector Graphics Generation with Large Language Models and\n  Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chat2SVG: Vector Graphics Generation with Large Language Models and\n  Image Diffusion Models"
                },
                "summary": "Scalable Vector Graphics (SVG) has become the de facto standard for vector\ngraphics in digital design, offering resolution independence and precise\ncontrol over individual elements. Despite their advantages, creating\nhigh-quality SVG content remains challenging, as it demands technical expertise\nwith professional editing software and a considerable time investment to craft\ncomplex shapes. Recent text-to-SVG generation methods aim to make vector\ngraphics creation more accessible, but they still encounter limitations in\nshape regularity, generalization ability, and expressiveness. To address these\nchallenges, we introduce Chat2SVG, a hybrid framework that combines the\nstrengths of Large Language Models (LLMs) and image diffusion models for\ntext-to-SVG generation. Our approach first uses an LLM to generate semantically\nmeaningful SVG templates from basic geometric primitives. Guided by image\ndiffusion models, a dual-stage optimization pipeline refines paths in latent\nspace and adjusts point coordinates to enhance geometric complexity. Extensive\nexperiments show that Chat2SVG outperforms existing methods in visual fidelity,\npath regularity, and semantic alignment. Additionally, our system enables\nintuitive editing through natural language instructions, making professional\nvector graphics creation accessible to all users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Vector Graphics (SVG) has become the de facto standard for vector\ngraphics in digital design, offering resolution independence and precise\ncontrol over individual elements. Despite their advantages, creating\nhigh-quality SVG content remains challenging, as it demands technical expertise\nwith professional editing software and a considerable time investment to craft\ncomplex shapes. Recent text-to-SVG generation methods aim to make vector\ngraphics creation more accessible, but they still encounter limitations in\nshape regularity, generalization ability, and expressiveness. To address these\nchallenges, we introduce Chat2SVG, a hybrid framework that combines the\nstrengths of Large Language Models (LLMs) and image diffusion models for\ntext-to-SVG generation. Our approach first uses an LLM to generate semantically\nmeaningful SVG templates from basic geometric primitives. Guided by image\ndiffusion models, a dual-stage optimization pipeline refines paths in latent\nspace and adjusts point coordinates to enhance geometric complexity. Extensive\nexperiments show that Chat2SVG outperforms existing methods in visual fidelity,\npath regularity, and semantic alignment. Additionally, our system enables\nintuitive editing through natural language instructions, making professional\nvector graphics creation accessible to all users."
                },
                "authors": [
                    {
                        "name": "Ronghuan Wu"
                    },
                    {
                        "name": "Wanchao Su"
                    },
                    {
                        "name": "Jing Liao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Liao"
                },
                "author": "Jing Liao",
                "arxiv_comment": "Project Page: https://chat2svg.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16594v1",
                "updated": "2024-11-25T17:28:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    17,
                    28,
                    44,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T17:28:44Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    17,
                    28,
                    44,
                    0,
                    330,
                    0
                ],
                "title": "From Generation to Judgment: Opportunities and Challenges of\n  LLM-as-a-judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Generation to Judgment: Opportunities and Challenges of\n  LLM-as-a-judge"
                },
                "summary": "Assessment and evaluation have long been critical challenges in artificial\nintelligence (AI) and natural language processing (NLP). However, traditional\nmethods, whether matching-based or embedding-based, often fall short of judging\nsubtle attributes and delivering satisfactory results. Recent advancements in\nLarge Language Models (LLMs) inspire the \"LLM-as-a-judge\" paradigm, where LLMs\nare leveraged to perform scoring, ranking, or selection across various tasks\nand applications. This paper provides a comprehensive survey of LLM-based\njudgment and assessment, offering an in-depth overview to advance this emerging\nfield. We begin by giving detailed definitions from both input and output\nperspectives. Then we introduce a comprehensive taxonomy to explore\nLLM-as-a-judge from three dimensions: what to judge, how to judge and where to\njudge. Finally, we compile benchmarks for evaluating LLM-as-a-judge and\nhighlight key challenges and promising directions, aiming to provide valuable\ninsights and inspire future research in this promising research area. Paper\nlist and more resources about LLM-as-a-judge can be found at\n\\url{https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge} and\n\\url{https://llm-as-a-judge.github.io}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessment and evaluation have long been critical challenges in artificial\nintelligence (AI) and natural language processing (NLP). However, traditional\nmethods, whether matching-based or embedding-based, often fall short of judging\nsubtle attributes and delivering satisfactory results. Recent advancements in\nLarge Language Models (LLMs) inspire the \"LLM-as-a-judge\" paradigm, where LLMs\nare leveraged to perform scoring, ranking, or selection across various tasks\nand applications. This paper provides a comprehensive survey of LLM-based\njudgment and assessment, offering an in-depth overview to advance this emerging\nfield. We begin by giving detailed definitions from both input and output\nperspectives. Then we introduce a comprehensive taxonomy to explore\nLLM-as-a-judge from three dimensions: what to judge, how to judge and where to\njudge. Finally, we compile benchmarks for evaluating LLM-as-a-judge and\nhighlight key challenges and promising directions, aiming to provide valuable\ninsights and inspire future research in this promising research area. Paper\nlist and more resources about LLM-as-a-judge can be found at\n\\url{https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge} and\n\\url{https://llm-as-a-judge.github.io}."
                },
                "authors": [
                    {
                        "name": "Dawei Li"
                    },
                    {
                        "name": "Bohan Jiang"
                    },
                    {
                        "name": "Liangjie Huang"
                    },
                    {
                        "name": "Alimohammad Beigi"
                    },
                    {
                        "name": "Chengshuai Zhao"
                    },
                    {
                        "name": "Zhen Tan"
                    },
                    {
                        "name": "Amrita Bhattacharjee"
                    },
                    {
                        "name": "Yuxuan Jiang"
                    },
                    {
                        "name": "Canyu Chen"
                    },
                    {
                        "name": "Tianhao Wu"
                    },
                    {
                        "name": "Kai Shu"
                    },
                    {
                        "name": "Lu Cheng"
                    },
                    {
                        "name": "Huan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huan Liu"
                },
                "author": "Huan Liu",
                "arxiv_comment": "32 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16587v1",
                "updated": "2024-11-25T17:22:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    17,
                    22,
                    10,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T17:22:10Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    17,
                    22,
                    10,
                    0,
                    330,
                    0
                ],
                "title": "Large Language Model-based Decision-making for COLREGs and the Control\n  of Autonomous Surface Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-based Decision-making for COLREGs and the Control\n  of Autonomous Surface Vehicles"
                },
                "summary": "In the field of autonomous surface vehicles (ASVs), devising decision-making\nand obstacle avoidance solutions that address maritime COLREGs (Collision\nRegulations), primarily defined for human operators, has long been a pressing\nchallenge. Recent advancements in explainable Artificial Intelligence (AI) and\nmachine learning have shown promise in enabling human-like decision-making.\nNotably, significant developments have occurred in the application of Large\nLanguage Models (LLMs) to the decision-making of complex systems, such as\nself-driving cars. The textual and somewhat ambiguous nature of COLREGs (from\nan algorithmic perspective), however, poses challenges that align well with the\ncapabilities of LLMs, suggesting that LLMs may become increasingly suitable for\nthis application soon. This paper presents and demonstrates the first\napplication of LLM-based decision-making and control for ASVs. The proposed\nmethod establishes a high-level decision-maker that uses online collision risk\nindices and key measurements to make decisions for safe manoeuvres. A tailored\ndesign and runtime structure is developed to support training and real-time\naction generation on a realistic ASV model. Local planning and control\nalgorithms are integrated to execute the commands for waypoint following and\ncollision avoidance at a lower level. To the authors' knowledge, this study\nrepresents the first attempt to apply explainable AI to the dynamic control\nproblem of maritime systems recognising the COLREGs rules, opening new avenues\nfor research in this challenging area. Results obtained across multiple test\nscenarios demonstrate the system's ability to maintain online COLREGs\ncompliance, accurate waypoint tracking, and feasible control, while providing\nhuman-interpretable reasoning for each decision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of autonomous surface vehicles (ASVs), devising decision-making\nand obstacle avoidance solutions that address maritime COLREGs (Collision\nRegulations), primarily defined for human operators, has long been a pressing\nchallenge. Recent advancements in explainable Artificial Intelligence (AI) and\nmachine learning have shown promise in enabling human-like decision-making.\nNotably, significant developments have occurred in the application of Large\nLanguage Models (LLMs) to the decision-making of complex systems, such as\nself-driving cars. The textual and somewhat ambiguous nature of COLREGs (from\nan algorithmic perspective), however, poses challenges that align well with the\ncapabilities of LLMs, suggesting that LLMs may become increasingly suitable for\nthis application soon. This paper presents and demonstrates the first\napplication of LLM-based decision-making and control for ASVs. The proposed\nmethod establishes a high-level decision-maker that uses online collision risk\nindices and key measurements to make decisions for safe manoeuvres. A tailored\ndesign and runtime structure is developed to support training and real-time\naction generation on a realistic ASV model. Local planning and control\nalgorithms are integrated to execute the commands for waypoint following and\ncollision avoidance at a lower level. To the authors' knowledge, this study\nrepresents the first attempt to apply explainable AI to the dynamic control\nproblem of maritime systems recognising the COLREGs rules, opening new avenues\nfor research in this challenging area. Results obtained across multiple test\nscenarios demonstrate the system's ability to maintain online COLREGs\ncompliance, accurate waypoint tracking, and feasible control, while providing\nhuman-interpretable reasoning for each decision."
                },
                "authors": [
                    {
                        "name": "Klinsmann Agyei"
                    },
                    {
                        "name": "Pouria Sarhadi"
                    },
                    {
                        "name": "Wasif Naeem"
                    }
                ],
                "author_detail": {
                    "name": "Wasif Naeem"
                },
                "author": "Wasif Naeem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16579v1",
                "updated": "2024-11-25T17:11:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    17,
                    11,
                    54,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T17:11:54Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    17,
                    11,
                    54,
                    0,
                    330,
                    0
                ],
                "title": "Enhancing LLM Reasoning via Critique Models with Test-Time and\n  Training-Time Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Reasoning via Critique Models with Test-Time and\n  Training-Time Supervision"
                },
                "summary": "Training large language models (LLMs) to spend more time thinking and\nreflection before responding is crucial for effectively solving complex\nreasoning tasks in fields such as science, coding, and mathematics. However,\nthe effectiveness of mechanisms like self-reflection and self-correction\ndepends on the model's capacity to accurately assess its own performance, which\ncan be limited by factors such as initial accuracy, question difficulty, and\nthe lack of external feedback. In this paper, we delve into a two-player\nparadigm that separates the roles of reasoning and critique models, where the\ncritique model provides step-level feedback to supervise the reasoning (actor)\nmodel during both test-time and train-time. We first propose AutoMathCritique,\nan automated and scalable framework for collecting critique data, resulting in\na dataset of $76,321$ responses paired with step-level feedback. Fine-tuning\nlanguage models with this dataset enables them to generate natural language\nfeedback for mathematical reasoning. We demonstrate that the critique models\nconsistently improve the actor's performance on difficult queries at test-time,\nespecially when scaling up inference-time computation. Motivated by these\nfindings, we introduce the critique-based supervision to the actor's\nself-training process, and propose a critique-in-the-loop self-improvement\nmethod. Experiments show that the method improves the actor's exploration\nefficiency and solution diversity, especially on challenging queries, leading\nto a stronger reasoning model. Lastly, we take the preliminary step to explore\ntraining self-talk reasoning models via critique supervision and showcase its\npotential. Our code and datasets are at\n\\href{https://mathcritique.github.io/}{https://mathcritique.github.io/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) to spend more time thinking and\nreflection before responding is crucial for effectively solving complex\nreasoning tasks in fields such as science, coding, and mathematics. However,\nthe effectiveness of mechanisms like self-reflection and self-correction\ndepends on the model's capacity to accurately assess its own performance, which\ncan be limited by factors such as initial accuracy, question difficulty, and\nthe lack of external feedback. In this paper, we delve into a two-player\nparadigm that separates the roles of reasoning and critique models, where the\ncritique model provides step-level feedback to supervise the reasoning (actor)\nmodel during both test-time and train-time. We first propose AutoMathCritique,\nan automated and scalable framework for collecting critique data, resulting in\na dataset of $76,321$ responses paired with step-level feedback. Fine-tuning\nlanguage models with this dataset enables them to generate natural language\nfeedback for mathematical reasoning. We demonstrate that the critique models\nconsistently improve the actor's performance on difficult queries at test-time,\nespecially when scaling up inference-time computation. Motivated by these\nfindings, we introduce the critique-based supervision to the actor's\nself-training process, and propose a critique-in-the-loop self-improvement\nmethod. Experiments show that the method improves the actor's exploration\nefficiency and solution diversity, especially on challenging queries, leading\nto a stronger reasoning model. Lastly, we take the preliminary step to explore\ntraining self-talk reasoning models via critique supervision and showcase its\npotential. Our code and datasets are at\n\\href{https://mathcritique.github.io/}{https://mathcritique.github.io/}."
                },
                "authors": [
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Dingwen Yang"
                    },
                    {
                        "name": "Jixuan Huang"
                    },
                    {
                        "name": "Jiafu Tang"
                    },
                    {
                        "name": "Guanyu Li"
                    },
                    {
                        "name": "Yiwen Ding"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Boyang Hong"
                    },
                    {
                        "name": "Shihan Do"
                    },
                    {
                        "name": "Wenyu Zhan"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Rui Zheng"
                    },
                    {
                        "name": "Tao Ji"
                    },
                    {
                        "name": "Xiaowei Shi"
                    },
                    {
                        "name": "Yitao Zhai"
                    },
                    {
                        "name": "Rongxiang Weng"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Zuxuan Wu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Gang Jiang"
                },
                "author": "Yu-Gang Jiang",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16569v1",
                "updated": "2024-11-25T16:53:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    16,
                    53,
                    22,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T16:53:22Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    16,
                    53,
                    22,
                    0,
                    330,
                    0
                ],
                "title": "Predictive Power of LLMs in Financial Markets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive Power of LLMs in Financial Markets"
                },
                "summary": "Predicting the movement of the stock market and other assets has been\nvaluable over the past few decades. Knowing how the value of a certain sector\nmarket may move in the future provides much information for investors, as they\nuse that information to develop strategies to maximize profit or minimize risk.\nHowever, market data are quite noisy, and it is challenging to choose the right\ndata or the right model to create such predictions. With the rise of large\nlanguage models, there are ways to analyze certain data much more efficiently\nthan before.\n  Our goal is to determine whether the GPT model provides more useful\ninformation compared to other traditional transformer models, such as the BERT\nmodel. We shall use data from the Federal Reserve Beige Book, which provides\nsummaries of economic conditions in different districts in the US. Using such\ndata, we then employ the LLM's to make predictions on the correlations. Using\nthese correlations, we then compare the results with well-known strategies and\ndetermine whether knowing the economic conditions improves investment\ndecisions. We conclude that the Beige Book does contain information regarding\ncorrelations amongst different assets, yet the GPT model has too much\nlook-ahead bias and that traditional models still triumph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting the movement of the stock market and other assets has been\nvaluable over the past few decades. Knowing how the value of a certain sector\nmarket may move in the future provides much information for investors, as they\nuse that information to develop strategies to maximize profit or minimize risk.\nHowever, market data are quite noisy, and it is challenging to choose the right\ndata or the right model to create such predictions. With the rise of large\nlanguage models, there are ways to analyze certain data much more efficiently\nthan before.\n  Our goal is to determine whether the GPT model provides more useful\ninformation compared to other traditional transformer models, such as the BERT\nmodel. We shall use data from the Federal Reserve Beige Book, which provides\nsummaries of economic conditions in different districts in the US. Using such\ndata, we then employ the LLM's to make predictions on the correlations. Using\nthese correlations, we then compare the results with well-known strategies and\ndetermine whether knowing the economic conditions improves investment\ndecisions. We conclude that the Beige Book does contain information regarding\ncorrelations amongst different assets, yet the GPT model has too much\nlook-ahead bias and that traditional models still triumph."
                },
                "authors": [
                    {
                        "name": "Jerick Shi"
                    },
                    {
                        "name": "Burton Hollifield"
                    }
                ],
                "author_detail": {
                    "name": "Burton Hollifield"
                },
                "author": "Burton Hollifield",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.PM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16561v1",
                "updated": "2024-11-25T16:47:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    16,
                    47,
                    10,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T16:47:10Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    16,
                    47,
                    10,
                    0,
                    330,
                    0
                ],
                "title": "EnStack: An Ensemble Stacking Framework of Large Language Models for\n  Enhanced Vulnerability Detection in Source Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EnStack: An Ensemble Stacking Framework of Large Language Models for\n  Enhanced Vulnerability Detection in Source Code"
                },
                "summary": "Automated detection of software vulnerabilities is critical for enhancing\nsecurity, yet existing methods often struggle with the complexity and diversity\nof modern codebases. In this paper, we introduce EnStack, a novel ensemble\nstacking framework that enhances vulnerability detection using natural language\nprocessing (NLP) techniques. Our approach synergizes multiple pre-trained large\nlanguage models (LLMs) specialized in code understanding CodeBERT for semantic\nanalysis, GraphCodeBERT for structural representation, and UniXcoder for\ncross-modal capabilities. By fine-tuning these models on the Draper VDISC\ndataset and integrating their outputs through meta-classifiers such as Logistic\nRegression, Support Vector Machines (SVM), Random Forest, and XGBoost, EnStack\neffectively captures intricate code patterns and vulnerabilities that\nindividual models may overlook. The meta-classifiers consolidate the strengths\nof each LLM, resulting in a comprehensive model that excels in detecting subtle\nand complex vulnerabilities across diverse programming contexts. Experimental\nresults demonstrate that EnStack significantly outperforms existing methods,\nachieving notable improvements in accuracy, precision, recall, and F1-score.\nThis work highlights the potential of ensemble LLM approaches in code analysis\ntasks and offers valuable insights into applying NLP techniques for advancing\nautomated vulnerability detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated detection of software vulnerabilities is critical for enhancing\nsecurity, yet existing methods often struggle with the complexity and diversity\nof modern codebases. In this paper, we introduce EnStack, a novel ensemble\nstacking framework that enhances vulnerability detection using natural language\nprocessing (NLP) techniques. Our approach synergizes multiple pre-trained large\nlanguage models (LLMs) specialized in code understanding CodeBERT for semantic\nanalysis, GraphCodeBERT for structural representation, and UniXcoder for\ncross-modal capabilities. By fine-tuning these models on the Draper VDISC\ndataset and integrating their outputs through meta-classifiers such as Logistic\nRegression, Support Vector Machines (SVM), Random Forest, and XGBoost, EnStack\neffectively captures intricate code patterns and vulnerabilities that\nindividual models may overlook. The meta-classifiers consolidate the strengths\nof each LLM, resulting in a comprehensive model that excels in detecting subtle\nand complex vulnerabilities across diverse programming contexts. Experimental\nresults demonstrate that EnStack significantly outperforms existing methods,\nachieving notable improvements in accuracy, precision, recall, and F1-score.\nThis work highlights the potential of ensemble LLM approaches in code analysis\ntasks and offers valuable insights into applying NLP techniques for advancing\nautomated vulnerability detection."
                },
                "authors": [
                    {
                        "name": "Shahriyar Zaman Ridoy"
                    },
                    {
                        "name": "Md. Shazzad Hossain Shaon"
                    },
                    {
                        "name": "Alfredo Cuzzocrea"
                    },
                    {
                        "name": "Mst Shapna Akter"
                    }
                ],
                "author_detail": {
                    "name": "Mst Shapna Akter"
                },
                "author": "Mst Shapna Akter",
                "arxiv_comment": "Accepted in 2024 IEEE International Conference on Big Data (IEEE\n  BigData 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16554v1",
                "updated": "2024-11-25T16:38:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    16,
                    38,
                    17,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T16:38:17Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    16,
                    38,
                    17,
                    0,
                    330,
                    0
                ],
                "title": "Generating Out-Of-Distribution Scenarios Using Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Out-Of-Distribution Scenarios Using Language Models"
                },
                "summary": "The deployment of autonomous vehicles controlled by machine learning\ntechniques requires extensive testing in diverse real-world environments,\nrobust handling of edge cases and out-of-distribution scenarios, and\ncomprehensive safety validation to ensure that these systems can navigate\nsafely and effectively under unpredictable conditions. Addressing\nOut-Of-Distribution (OOD) driving scenarios is essential for enhancing safety,\nas OOD scenarios help validate the reliability of the models within the\nvehicle's autonomy stack. However, generating OOD scenarios is challenging due\nto their long-tailed distribution and rarity in urban driving dataset.\nRecently, Large Language Models (LLMs) have shown promise in autonomous\ndriving, particularly for their zero-shot generalization and common-sense\nreasoning capabilities. In this paper, we leverage these LLM strengths to\nintroduce a framework for generating diverse OOD driving scenarios. Our\napproach uses LLMs to construct a branching tree, where each branch represents\na unique OOD scenario. These scenarios are then simulated in the CARLA\nsimulator using an automated framework that aligns scene augmentation with the\ncorresponding textual descriptions. We evaluate our framework through extensive\nsimulations, and assess its performance via a diversity metric that measures\nthe richness of the scenarios. Additionally, we introduce a new \"OOD-ness\"\nmetric, which quantifies how much the generated scenarios deviate from typical\nurban driving conditions. Furthermore, we explore the capacity of modern\nVision-Language Models (VLMs) to interpret and safely navigate through the\nsimulated OOD scenarios. Our findings offer valuable insights into the\nreliability of language models in addressing OOD scenarios within the context\nof urban driving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of autonomous vehicles controlled by machine learning\ntechniques requires extensive testing in diverse real-world environments,\nrobust handling of edge cases and out-of-distribution scenarios, and\ncomprehensive safety validation to ensure that these systems can navigate\nsafely and effectively under unpredictable conditions. Addressing\nOut-Of-Distribution (OOD) driving scenarios is essential for enhancing safety,\nas OOD scenarios help validate the reliability of the models within the\nvehicle's autonomy stack. However, generating OOD scenarios is challenging due\nto their long-tailed distribution and rarity in urban driving dataset.\nRecently, Large Language Models (LLMs) have shown promise in autonomous\ndriving, particularly for their zero-shot generalization and common-sense\nreasoning capabilities. In this paper, we leverage these LLM strengths to\nintroduce a framework for generating diverse OOD driving scenarios. Our\napproach uses LLMs to construct a branching tree, where each branch represents\na unique OOD scenario. These scenarios are then simulated in the CARLA\nsimulator using an automated framework that aligns scene augmentation with the\ncorresponding textual descriptions. We evaluate our framework through extensive\nsimulations, and assess its performance via a diversity metric that measures\nthe richness of the scenarios. Additionally, we introduce a new \"OOD-ness\"\nmetric, which quantifies how much the generated scenarios deviate from typical\nurban driving conditions. Furthermore, we explore the capacity of modern\nVision-Language Models (VLMs) to interpret and safely navigate through the\nsimulated OOD scenarios. Our findings offer valuable insights into the\nreliability of language models in addressing OOD scenarios within the context\nof urban driving."
                },
                "authors": [
                    {
                        "name": "Erfan Aasi"
                    },
                    {
                        "name": "Phat Nguyen"
                    },
                    {
                        "name": "Shiva Sreeram"
                    },
                    {
                        "name": "Guy Rosman"
                    },
                    {
                        "name": "Sertac Karaman"
                    },
                    {
                        "name": "Daniela Rus"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Rus"
                },
                "author": "Daniela Rus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16527v1",
                "updated": "2024-11-25T16:14:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    16,
                    14,
                    45,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T16:14:45Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    16,
                    14,
                    45,
                    0,
                    330,
                    0
                ],
                "title": "Profiling Bias in LLMs: Stereotype Dimensions in Contextual Word\n  Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Profiling Bias in LLMs: Stereotype Dimensions in Contextual Word\n  Embeddings"
                },
                "summary": "Large language models (LLMs) are the foundation of the current successes of\nartificial intelligence (AI), however, they are unavoidably biased. To\neffectively communicate the risks and encourage mitigation efforts these models\nneed adequate and intuitive descriptions of their discriminatory properties,\nappropriate for all audiences of AI. We suggest bias profiles with respect to\nstereotype dimensions based on dictionaries from social psychology research.\nAlong these dimensions we investigate gender bias in contextual embeddings,\nacross contexts and layers, and generate stereotype profiles for twelve\ndifferent LLMs, demonstrating their intuition and use case for exposing and\nvisualizing bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are the foundation of the current successes of\nartificial intelligence (AI), however, they are unavoidably biased. To\neffectively communicate the risks and encourage mitigation efforts these models\nneed adequate and intuitive descriptions of their discriminatory properties,\nappropriate for all audiences of AI. We suggest bias profiles with respect to\nstereotype dimensions based on dictionaries from social psychology research.\nAlong these dimensions we investigate gender bias in contextual embeddings,\nacross contexts and layers, and generate stereotype profiles for twelve\ndifferent LLMs, demonstrating their intuition and use case for exposing and\nvisualizing bias."
                },
                "authors": [
                    {
                        "name": "Carolin M. Schuster"
                    },
                    {
                        "name": "Maria-Alexandra Dinisor"
                    },
                    {
                        "name": "Shashwat Ghatiwala"
                    },
                    {
                        "name": "Georg Groh"
                    }
                ],
                "author_detail": {
                    "name": "Georg Groh"
                },
                "author": "Georg Groh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16523v1",
                "updated": "2024-11-25T16:10:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    16,
                    10,
                    5,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T16:10:05Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    16,
                    10,
                    5,
                    0,
                    330,
                    0
                ],
                "title": "LaB-RAG: Label Boosted Retrieval Augmented Generation for Radiology\n  Report Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaB-RAG: Label Boosted Retrieval Augmented Generation for Radiology\n  Report Generation"
                },
                "summary": "In the current paradigm of image captioning, deep learning models are trained\nto generate text from image embeddings of latent features. We challenge the\nassumption that these latent features ought to be high-dimensional vectors\nwhich require model fine tuning to handle. Here we propose Label Boosted\nRetrieval Augmented Generation (LaB-RAG), a text-based approach to image\ncaptioning that leverages image descriptors in the form of categorical labels\nto boost standard retrieval augmented generation (RAG) with pretrained large\nlanguage models (LLMs). We study our method in the context of radiology report\ngeneration (RRG), where the task is to generate a clinician's report detailing\ntheir observations from a set of radiological images, such as X-rays. We argue\nthat simple linear classifiers over extracted image embeddings can effectively\ntransform X-rays into text-space as radiology-specific labels. In combination\nwith standard RAG, we show that these derived text labels can be used with\ngeneral-domain LLMs to generate radiology reports. Without ever training our\ngenerative language model or image feature encoder models, and without ever\ndirectly \"showing\" the LLM an X-ray, we demonstrate that LaB-RAG achieves\nbetter results across natural language and radiology language metrics compared\nwith other retrieval-based RRG methods, while attaining competitive results\ncompared to other fine-tuned vision-language RRG models. We further present\nresults of our experiments with various components of LaB-RAG to better\nunderstand our method. Finally, we critique the use of a popular RRG metric,\narguing it is possible to artificially inflate its results without true\ndata-leakage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the current paradigm of image captioning, deep learning models are trained\nto generate text from image embeddings of latent features. We challenge the\nassumption that these latent features ought to be high-dimensional vectors\nwhich require model fine tuning to handle. Here we propose Label Boosted\nRetrieval Augmented Generation (LaB-RAG), a text-based approach to image\ncaptioning that leverages image descriptors in the form of categorical labels\nto boost standard retrieval augmented generation (RAG) with pretrained large\nlanguage models (LLMs). We study our method in the context of radiology report\ngeneration (RRG), where the task is to generate a clinician's report detailing\ntheir observations from a set of radiological images, such as X-rays. We argue\nthat simple linear classifiers over extracted image embeddings can effectively\ntransform X-rays into text-space as radiology-specific labels. In combination\nwith standard RAG, we show that these derived text labels can be used with\ngeneral-domain LLMs to generate radiology reports. Without ever training our\ngenerative language model or image feature encoder models, and without ever\ndirectly \"showing\" the LLM an X-ray, we demonstrate that LaB-RAG achieves\nbetter results across natural language and radiology language metrics compared\nwith other retrieval-based RRG methods, while attaining competitive results\ncompared to other fine-tuned vision-language RRG models. We further present\nresults of our experiments with various components of LaB-RAG to better\nunderstand our method. Finally, we critique the use of a popular RRG metric,\narguing it is possible to artificially inflate its results without true\ndata-leakage."
                },
                "authors": [
                    {
                        "name": "Steven Song"
                    },
                    {
                        "name": "Anirudh Subramanyam"
                    },
                    {
                        "name": "Irene Madejski"
                    },
                    {
                        "name": "Robert L. Grossman"
                    }
                ],
                "author_detail": {
                    "name": "Robert L. Grossman"
                },
                "author": "Robert L. Grossman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16512v1",
                "updated": "2024-11-25T15:55:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    55,
                    6,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T15:55:06Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    55,
                    6,
                    0,
                    330,
                    0
                ],
                "title": "Guarding the Gate: ConceptGuard Battles Concept-Level Backdoors in\n  Concept Bottleneck Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guarding the Gate: ConceptGuard Battles Concept-Level Backdoors in\n  Concept Bottleneck Models"
                },
                "summary": "The increasing complexity of AI models, especially in deep learning, has\nraised concerns about transparency and accountability, particularly in\nhigh-stakes applications like medical diagnostics, where opaque models can\nundermine trust. Explainable Artificial Intelligence (XAI) aims to address\nthese issues by providing clear, interpretable models. Among XAI techniques,\nConcept Bottleneck Models (CBMs) enhance transparency by using high-level\nsemantic concepts. However, CBMs are vulnerable to concept-level backdoor\nattacks, which inject hidden triggers into these concepts, leading to\nundetectable anomalous behavior. To address this critical security gap, we\nintroduce ConceptGuard, a novel defense framework specifically designed to\nprotect CBMs from concept-level backdoor attacks. ConceptGuard employs a\nmulti-stage approach, including concept clustering based on text distance\nmeasurements and a voting mechanism among classifiers trained on different\nconcept subgroups, to isolate and mitigate potential triggers. Our\ncontributions are threefold: (i) we present ConceptGuard as the first defense\nmechanism tailored for concept-level backdoor attacks in CBMs; (ii) we provide\ntheoretical guarantees that ConceptGuard can effectively defend against such\nattacks within a certain trigger size threshold, ensuring robustness; and (iii)\nwe demonstrate that ConceptGuard maintains the high performance and\ninterpretability of CBMs, crucial for trustworthiness. Through comprehensive\nexperiments and theoretical proofs, we show that ConceptGuard significantly\nenhances the security and trustworthiness of CBMs, paving the way for their\nsecure deployment in critical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of AI models, especially in deep learning, has\nraised concerns about transparency and accountability, particularly in\nhigh-stakes applications like medical diagnostics, where opaque models can\nundermine trust. Explainable Artificial Intelligence (XAI) aims to address\nthese issues by providing clear, interpretable models. Among XAI techniques,\nConcept Bottleneck Models (CBMs) enhance transparency by using high-level\nsemantic concepts. However, CBMs are vulnerable to concept-level backdoor\nattacks, which inject hidden triggers into these concepts, leading to\nundetectable anomalous behavior. To address this critical security gap, we\nintroduce ConceptGuard, a novel defense framework specifically designed to\nprotect CBMs from concept-level backdoor attacks. ConceptGuard employs a\nmulti-stage approach, including concept clustering based on text distance\nmeasurements and a voting mechanism among classifiers trained on different\nconcept subgroups, to isolate and mitigate potential triggers. Our\ncontributions are threefold: (i) we present ConceptGuard as the first defense\nmechanism tailored for concept-level backdoor attacks in CBMs; (ii) we provide\ntheoretical guarantees that ConceptGuard can effectively defend against such\nattacks within a certain trigger size threshold, ensuring robustness; and (iii)\nwe demonstrate that ConceptGuard maintains the high performance and\ninterpretability of CBMs, crucial for trustworthiness. Through comprehensive\nexperiments and theoretical proofs, we show that ConceptGuard significantly\nenhances the security and trustworthiness of CBMs, paving the way for their\nsecure deployment in critical applications."
                },
                "authors": [
                    {
                        "name": "Songning Lai"
                    },
                    {
                        "name": "Yu Huang"
                    },
                    {
                        "name": "Jiayu Yang"
                    },
                    {
                        "name": "Gaoxiang Huang"
                    },
                    {
                        "name": "Wenshuo Chen"
                    },
                    {
                        "name": "Yutao Yue"
                    }
                ],
                "author_detail": {
                    "name": "Yutao Yue"
                },
                "author": "Yutao Yue",
                "arxiv_comment": "17pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16502v1",
                "updated": "2024-11-25T15:37:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    37,
                    27,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T15:37:27Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    37,
                    27,
                    0,
                    330,
                    0
                ],
                "title": "Interpreting Language Reward Models via Contrastive Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting Language Reward Models via Contrastive Explanations"
                },
                "summary": "Reward models (RMs) are a crucial component in the alignment of large\nlanguage models' (LLMs) outputs with human values. RMs approximate human\npreferences over possible LLM responses to the same prompt by predicting and\ncomparing reward scores. However, as they are typically modified versions of\nLLMs with scalar output heads, RMs are large black boxes whose predictions are\nnot explainable. More transparent RMs would enable improved trust in the\nalignment of LLMs. In this work, we propose to use contrastive explanations to\nexplain any binary response comparison made by an RM. Specifically, we generate\na diverse set of new comparisons similar to the original one to characterise\nthe RM's local behaviour. The perturbed responses forming the new comparisons\nare generated to explicitly modify manually specified high-level evaluation\nattributes, on which analyses of RM behaviour are grounded. In quantitative\nexperiments, we validate the effectiveness of our method for finding\nhigh-quality contrastive explanations. We then showcase the qualitative\nusefulness of our method for investigating global sensitivity of RMs to each\nevaluation attribute, and demonstrate how representative examples can be\nautomatically extracted to explain and compare behaviours of different RMs. We\nsee our method as a flexible framework for RM explanation, providing a basis\nfor more interpretable and trustworthy LLM alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models (RMs) are a crucial component in the alignment of large\nlanguage models' (LLMs) outputs with human values. RMs approximate human\npreferences over possible LLM responses to the same prompt by predicting and\ncomparing reward scores. However, as they are typically modified versions of\nLLMs with scalar output heads, RMs are large black boxes whose predictions are\nnot explainable. More transparent RMs would enable improved trust in the\nalignment of LLMs. In this work, we propose to use contrastive explanations to\nexplain any binary response comparison made by an RM. Specifically, we generate\na diverse set of new comparisons similar to the original one to characterise\nthe RM's local behaviour. The perturbed responses forming the new comparisons\nare generated to explicitly modify manually specified high-level evaluation\nattributes, on which analyses of RM behaviour are grounded. In quantitative\nexperiments, we validate the effectiveness of our method for finding\nhigh-quality contrastive explanations. We then showcase the qualitative\nusefulness of our method for investigating global sensitivity of RMs to each\nevaluation attribute, and demonstrate how representative examples can be\nautomatically extracted to explain and compare behaviours of different RMs. We\nsee our method as a flexible framework for RM explanation, providing a basis\nfor more interpretable and trustworthy LLM alignment."
                },
                "authors": [
                    {
                        "name": "Junqi Jiang"
                    },
                    {
                        "name": "Tom Bewley"
                    },
                    {
                        "name": "Saumitra Mishra"
                    },
                    {
                        "name": "Freddy Lecue"
                    },
                    {
                        "name": "Manuela Veloso"
                    }
                ],
                "author_detail": {
                    "name": "Manuela Veloso"
                },
                "author": "Manuela Veloso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16495v1",
                "updated": "2024-11-25T15:35:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    35,
                    51,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T15:35:51Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    35,
                    51,
                    0,
                    330,
                    0
                ],
                "title": "AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous\n  Knowledge Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous\n  Knowledge Reasoning"
                },
                "summary": "Recent advancements in large language models (LLMs) have led to significant\nimprovements in various natural language processing tasks, but it is still\nchallenging for LLMs to perform knowledge-intensive complex question answering\ndue to LLMs' inefficacy in reasoning planning and the hallucination problem. A\ntypical solution is to employ retrieval-augmented generation (RAG) coupled with\nchain-of-thought (CoT) reasoning, which decomposes complex questions into\nchain-like sub-questions and applies iterative RAG at each sub-question.\nHowever, prior works exhibit sub-optimal reasoning planning and overlook\ndynamic knowledge retrieval from heterogeneous sources. In this paper, we\npropose AtomR, a novel heterogeneous knowledge reasoning framework that\nconducts multi-source reasoning at the atomic level. Drawing inspiration from\nthe graph modeling of knowledge, AtomR leverages large language models (LLMs)\nto decompose complex questions into combinations of three atomic knowledge\noperators, significantly enhancing the reasoning process at both the planning\nand execution stages. We also introduce BlendQA, a novel evaluation benchmark\ntailored to assess complex heterogeneous knowledge reasoning. Experiments show\nthat AtomR significantly outperforms state-of-the-art baselines across three\nsingle-source and two multi-source reasoning benchmarks, with notable\nperformance gains of 9.4% on 2WikiMultihop and 9.5% on BlendQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have led to significant\nimprovements in various natural language processing tasks, but it is still\nchallenging for LLMs to perform knowledge-intensive complex question answering\ndue to LLMs' inefficacy in reasoning planning and the hallucination problem. A\ntypical solution is to employ retrieval-augmented generation (RAG) coupled with\nchain-of-thought (CoT) reasoning, which decomposes complex questions into\nchain-like sub-questions and applies iterative RAG at each sub-question.\nHowever, prior works exhibit sub-optimal reasoning planning and overlook\ndynamic knowledge retrieval from heterogeneous sources. In this paper, we\npropose AtomR, a novel heterogeneous knowledge reasoning framework that\nconducts multi-source reasoning at the atomic level. Drawing inspiration from\nthe graph modeling of knowledge, AtomR leverages large language models (LLMs)\nto decompose complex questions into combinations of three atomic knowledge\noperators, significantly enhancing the reasoning process at both the planning\nand execution stages. We also introduce BlendQA, a novel evaluation benchmark\ntailored to assess complex heterogeneous knowledge reasoning. Experiments show\nthat AtomR significantly outperforms state-of-the-art baselines across three\nsingle-source and two multi-source reasoning benchmarks, with notable\nperformance gains of 9.4% on 2WikiMultihop and 9.5% on BlendQA."
                },
                "authors": [
                    {
                        "name": "Amy Xin"
                    },
                    {
                        "name": "Jinxin Liu"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Zhicheng Li"
                    },
                    {
                        "name": "Shulin Cao"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17348v2",
                "updated": "2024-11-25T15:17:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    17,
                    3,
                    0,
                    330,
                    0
                ],
                "published": "2024-09-25T20:49:41Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    20,
                    49,
                    41,
                    2,
                    269,
                    0
                ],
                "title": "Language Grounded Multi-agent Reinforcement Learning with\n  Human-interpretable Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Grounded Multi-agent Reinforcement Learning with\n  Human-interpretable Communication"
                },
                "summary": "Multi-Agent Reinforcement Learning (MARL) methods have shown promise in\nenabling agents to learn a shared communication protocol from scratch and\naccomplish challenging team tasks. However, the learned language is usually not\ninterpretable to humans or other agents not co-trained together, limiting its\napplicability in ad-hoc teamwork scenarios. In this work, we propose a novel\ncomputational pipeline that aligns the communication space between MARL agents\nwith an embedding space of human natural language by grounding agent\ncommunications on synthetic data generated by embodied Large Language Models\n(LLMs) in interactive teamwork scenarios. Our results demonstrate that\nintroducing language grounding not only maintains task performance but also\naccelerates the emergence of communication. Furthermore, the learned\ncommunication protocols exhibit zero-shot generalization capabilities in ad-hoc\nteamwork scenarios with unseen teammates and novel task states. This work\npresents a significant step toward enabling effective communication and\ncollaboration between artificial agents and humans in real-world teamwork\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Reinforcement Learning (MARL) methods have shown promise in\nenabling agents to learn a shared communication protocol from scratch and\naccomplish challenging team tasks. However, the learned language is usually not\ninterpretable to humans or other agents not co-trained together, limiting its\napplicability in ad-hoc teamwork scenarios. In this work, we propose a novel\ncomputational pipeline that aligns the communication space between MARL agents\nwith an embedding space of human natural language by grounding agent\ncommunications on synthetic data generated by embodied Large Language Models\n(LLMs) in interactive teamwork scenarios. Our results demonstrate that\nintroducing language grounding not only maintains task performance but also\naccelerates the emergence of communication. Furthermore, the learned\ncommunication protocols exhibit zero-shot generalization capabilities in ad-hoc\nteamwork scenarios with unseen teammates and novel task states. This work\npresents a significant step toward enabling effective communication and\ncollaboration between artificial agents and humans in real-world teamwork\nsettings."
                },
                "authors": [
                    {
                        "name": "Huao Li"
                    },
                    {
                        "name": "Hossein Nourkhiz Mahjoub"
                    },
                    {
                        "name": "Behdad Chalaki"
                    },
                    {
                        "name": "Vaishnav Tadiparthi"
                    },
                    {
                        "name": "Kwonjoon Lee"
                    },
                    {
                        "name": "Ehsan Moradi-Pari"
                    },
                    {
                        "name": "Charles Michael Lewis"
                    },
                    {
                        "name": "Katia P Sycara"
                    }
                ],
                "author_detail": {
                    "name": "Katia P Sycara"
                },
                "author": "Katia P Sycara",
                "arxiv_comment": "Accepted to Neurips 2024, 19 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16454v1",
                "updated": "2024-11-25T15:01:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    1,
                    25,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T15:01:25Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    1,
                    25,
                    0,
                    330,
                    0
                ],
                "title": "Learning by Analogy: Enhancing Few-Shot Prompting for Math Word Problem\n  Solving with Computational Graph-Based Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning by Analogy: Enhancing Few-Shot Prompting for Math Word Problem\n  Solving with Computational Graph-Based Retrieval"
                },
                "summary": "Large language models (LLMs) are known to struggle with complicated reasoning\ntasks such as math word problems (MWPs). In this paper, we present how analogy\nfrom similarly structured questions can improve LLMs' problem-solving\ncapabilities for MWPs. Specifically, we rely on the retrieval of problems with\nsimilar computational graphs to the given question to serve as exemplars in the\nprompt, providing the correct reasoning path for the generation model to refer\nto. Empirical results across six math word problem datasets demonstrate the\neffectiveness of our proposed method, which achieves a significant improvement\nof up to 6.7 percent on average in absolute value, compared to baseline\nmethods. These results highlight our method's potential in addressing the\nreasoning challenges in current LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are known to struggle with complicated reasoning\ntasks such as math word problems (MWPs). In this paper, we present how analogy\nfrom similarly structured questions can improve LLMs' problem-solving\ncapabilities for MWPs. Specifically, we rely on the retrieval of problems with\nsimilar computational graphs to the given question to serve as exemplars in the\nprompt, providing the correct reasoning path for the generation model to refer\nto. Empirical results across six math word problem datasets demonstrate the\neffectiveness of our proposed method, which achieves a significant improvement\nof up to 6.7 percent on average in absolute value, compared to baseline\nmethods. These results highlight our method's potential in addressing the\nreasoning challenges in current LLMs."
                },
                "authors": [
                    {
                        "name": "Xiaocong Yang"
                    },
                    {
                        "name": "Jiacheng Lin"
                    },
                    {
                        "name": "Ziqi Wang"
                    },
                    {
                        "name": "Chengxiang Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Chengxiang Zhai"
                },
                "author": "Chengxiang Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16441v1",
                "updated": "2024-11-25T14:44:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    14,
                    44,
                    15,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T14:44:15Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    14,
                    44,
                    15,
                    0,
                    330,
                    0
                ],
                "title": "Shortest Path Lengths in Poisson Line Cox Processes: Approximations and\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shortest Path Lengths in Poisson Line Cox Processes: Approximations and\n  Applications"
                },
                "summary": "We derive exact expressions for the shortest path length to a point of a\nPoisson line Cox process (PLCP) from the typical point of the PLCP and from the\ntypical intersection of the underlying Poisson line process (PLP), restricted\nto a single turn. For the two turns case, we derive a bound on the shortest\npath length from the typical point and demonstrate conditions under which the\nbound is tight. We also highlight the line process and point process densities\nfor which the shortest path from the typical intersection under the one turn\nrestriction may be shorter than the shortest path from the typical point under\nthe two turns restriction. Finally, we discuss two applications where our\nresults can be employed for a statistical characterization of system\nperformance: in a re-configurable intelligent surface (RIS) enabled\nvehicle-to-vehicle (V2V) communication system and in electric vehicle charging\npoint deployment planning in urban streets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We derive exact expressions for the shortest path length to a point of a\nPoisson line Cox process (PLCP) from the typical point of the PLCP and from the\ntypical intersection of the underlying Poisson line process (PLP), restricted\nto a single turn. For the two turns case, we derive a bound on the shortest\npath length from the typical point and demonstrate conditions under which the\nbound is tight. We also highlight the line process and point process densities\nfor which the shortest path from the typical intersection under the one turn\nrestriction may be shorter than the shortest path from the typical point under\nthe two turns restriction. Finally, we discuss two applications where our\nresults can be employed for a statistical characterization of system\nperformance: in a re-configurable intelligent surface (RIS) enabled\nvehicle-to-vehicle (V2V) communication system and in electric vehicle charging\npoint deployment planning in urban streets."
                },
                "authors": [
                    {
                        "name": "Gourab Ghatak"
                    },
                    {
                        "name": "Sanjoy Kumar Jhawar"
                    },
                    {
                        "name": "Martin Haenggi"
                    }
                ],
                "author_detail": {
                    "name": "Martin Haenggi"
                },
                "author": "Martin Haenggi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16425v1",
                "updated": "2024-11-25T14:27:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    14,
                    27,
                    55,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T14:27:55Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    14,
                    27,
                    55,
                    0,
                    330,
                    0
                ],
                "title": "TopV-Nav: Unlocking the Top-View Spatial Reasoning Potential of MLLM for\n  Zero-shot Object Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TopV-Nav: Unlocking the Top-View Spatial Reasoning Potential of MLLM for\n  Zero-shot Object Navigation"
                },
                "summary": "The Zero-Shot Object Navigation (ZSON) task requires embodied agents to find\na previously unseen object by navigating in unfamiliar environments. Such a\ngoal-oriented exploration heavily relies on the ability to perceive,\nunderstand, and reason based on the spatial information of the environment.\nHowever, current LLM-based approaches convert visual observations to language\ndescriptions and reason in the linguistic space, leading to the loss of spatial\ninformation. In this paper, we introduce TopV-Nav, a MLLM-based method that\ndirectly reasons on the top-view map with complete spatial information. To\nfully unlock the MLLM's spatial reasoning potential in top-view perspective, we\npropose the Adaptive Visual Prompt Generation (AVPG) method to adaptively\nconstruct semantically-rich top-view map. It enables the agent to directly\nutilize spatial information contained in the top-view map to conduct thorough\nreasoning. Besides, we design a Dynamic Map Scaling (DMS) mechanism to\ndynamically zoom top-view map at preferred scales, enhancing local fine-grained\nreasoning. Additionally, we devise a Target-Guided Navigation (TGN) mechanism\nto predict and to utilize target locations, facilitating global and human-like\nexploration. Experiments on MP3D and HM3D benchmarks demonstrate the\nsuperiority of our TopV-Nav, e.g., $+3.9\\%$ SR and $+2.0\\%$ SPL absolute\nimprovements on HM3D.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Zero-Shot Object Navigation (ZSON) task requires embodied agents to find\na previously unseen object by navigating in unfamiliar environments. Such a\ngoal-oriented exploration heavily relies on the ability to perceive,\nunderstand, and reason based on the spatial information of the environment.\nHowever, current LLM-based approaches convert visual observations to language\ndescriptions and reason in the linguistic space, leading to the loss of spatial\ninformation. In this paper, we introduce TopV-Nav, a MLLM-based method that\ndirectly reasons on the top-view map with complete spatial information. To\nfully unlock the MLLM's spatial reasoning potential in top-view perspective, we\npropose the Adaptive Visual Prompt Generation (AVPG) method to adaptively\nconstruct semantically-rich top-view map. It enables the agent to directly\nutilize spatial information contained in the top-view map to conduct thorough\nreasoning. Besides, we design a Dynamic Map Scaling (DMS) mechanism to\ndynamically zoom top-view map at preferred scales, enhancing local fine-grained\nreasoning. Additionally, we devise a Target-Guided Navigation (TGN) mechanism\nto predict and to utilize target locations, facilitating global and human-like\nexploration. Experiments on MP3D and HM3D benchmarks demonstrate the\nsuperiority of our TopV-Nav, e.g., $+3.9\\%$ SR and $+2.0\\%$ SPL absolute\nimprovements on HM3D."
                },
                "authors": [
                    {
                        "name": "Linqing Zhong"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Zihan Ding"
                    },
                    {
                        "name": "Yue Liao"
                    },
                    {
                        "name": "Si Liu"
                    }
                ],
                "author_detail": {
                    "name": "Si Liu"
                },
                "author": "Si Liu",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16416v1",
                "updated": "2024-11-25T14:19:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    14,
                    19,
                    35,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T14:19:35Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    14,
                    19,
                    35,
                    0,
                    330,
                    0
                ],
                "title": "A Multi-agent Framework for Materials Laws Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-agent Framework for Materials Laws Discovery"
                },
                "summary": "Uncovering the underlying laws governing correlations between different\nmaterials properties, and the structure-composition-property relationship, is\nessential for advancing materials theory and enabling efficient materials\ndesign. With recent advances in artificial intelligence (AI), particularly in\nlarge language models (LLMs), symbolic regression has emerged as a powerful\nmethod for deriving explicit formulas for materials laws. LLMs, with their\npre-trained, cross-disciplinary knowledge, present a promising direction in \"AI\nfor Materials\". In this work, we introduce a multi-agent framework based on\nLLMs specifically designed for symbolic regression in materials science. We\ndemonstrate the effectiveness of the framework using the glass-forming ability\n(GFA) of metallic glasses as a case study, employing three characteristic\ntemperatures as independent variables. Our framework derived an interpretable\nformula to describe GFA, achieving a correlation coefficient of up to 0.948\nwith low formula complexity. This approach outperforms standard packages such\nas GPlearn and demonstrates a ~30% improvement over random generation methods,\nowing to integrated memory and reflection mechanisms. The proposed framework\ncan be extended to discover laws in various materials applications, supporting\nnew materials design and enhancing the interpretation of experimental and\nsimulation data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering the underlying laws governing correlations between different\nmaterials properties, and the structure-composition-property relationship, is\nessential for advancing materials theory and enabling efficient materials\ndesign. With recent advances in artificial intelligence (AI), particularly in\nlarge language models (LLMs), symbolic regression has emerged as a powerful\nmethod for deriving explicit formulas for materials laws. LLMs, with their\npre-trained, cross-disciplinary knowledge, present a promising direction in \"AI\nfor Materials\". In this work, we introduce a multi-agent framework based on\nLLMs specifically designed for symbolic regression in materials science. We\ndemonstrate the effectiveness of the framework using the glass-forming ability\n(GFA) of metallic glasses as a case study, employing three characteristic\ntemperatures as independent variables. Our framework derived an interpretable\nformula to describe GFA, achieving a correlation coefficient of up to 0.948\nwith low formula complexity. This approach outperforms standard packages such\nas GPlearn and demonstrates a ~30% improvement over random generation methods,\nowing to integrated memory and reflection mechanisms. The proposed framework\ncan be extended to discover laws in various materials applications, supporting\nnew materials design and enhancing the interpretation of experimental and\nsimulation data."
                },
                "authors": [
                    {
                        "name": "Bo Hu"
                    },
                    {
                        "name": "Siyu Liu"
                    },
                    {
                        "name": "Beilin Ye"
                    },
                    {
                        "name": "Yun Hao"
                    },
                    {
                        "name": "Tongqi Wen"
                    }
                ],
                "author_detail": {
                    "name": "Tongqi Wen"
                },
                "author": "Tongqi Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16410v1",
                "updated": "2024-11-25T14:16:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    14,
                    16,
                    55,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T14:16:55Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    14,
                    16,
                    55,
                    0,
                    330,
                    0
                ],
                "title": "Ultrahigh-fidelity spatial mode quantum gates in high-dimensional space\n  by diffractive deep neural networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultrahigh-fidelity spatial mode quantum gates in high-dimensional space\n  by diffractive deep neural networks"
                },
                "summary": "While the spatial mode of photons is widely used in quantum cryptography, its\npotential for quantum computation remains largely unexplored. Here, we showcase\nthe use of the multi-dimensional spatial mode of photons to construct a series\nof high-dimensional quantum gates, achieved through the use of diffractive deep\nneural networks (D2NNs). Notably, our gates demonstrate high fidelity of up to\n99.6(2)%, as characterized by quantum process tomography. Our experimental\nimplementation of these gates involves a programmable array of phase layers in\na compact and scalable device, capable of performing complex operations or even\nquantum circuits. We also demonstrate the efficacy of the D2NN gates by\nsuccessfully implementing the Deutsch algorithm and propose an intelligent\ndeployment protocol that involves self-configuration and self-optimization.\nMoreover, we conduct a comparative analysis of the D2NN gate's performance to\nthe wave-front matching approach. Overall, our work opens a door for designing\nspecific quantum gates using deep learning, with the potential for reliable\nexecution of quantum computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the spatial mode of photons is widely used in quantum cryptography, its\npotential for quantum computation remains largely unexplored. Here, we showcase\nthe use of the multi-dimensional spatial mode of photons to construct a series\nof high-dimensional quantum gates, achieved through the use of diffractive deep\nneural networks (D2NNs). Notably, our gates demonstrate high fidelity of up to\n99.6(2)%, as characterized by quantum process tomography. Our experimental\nimplementation of these gates involves a programmable array of phase layers in\na compact and scalable device, capable of performing complex operations or even\nquantum circuits. We also demonstrate the efficacy of the D2NN gates by\nsuccessfully implementing the Deutsch algorithm and propose an intelligent\ndeployment protocol that involves self-configuration and self-optimization.\nMoreover, we conduct a comparative analysis of the D2NN gate's performance to\nthe wave-front matching approach. Overall, our work opens a door for designing\nspecific quantum gates using deep learning, with the potential for reliable\nexecution of quantum computation."
                },
                "authors": [
                    {
                        "name": "Qianke Wang"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Dawei Lyu"
                    },
                    {
                        "name": "Jian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Wang"
                },
                "author": "Jian Wang",
                "arxiv_doi": "10.1038/s41377-023-01336-7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41377-023-01336-7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.16410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17348v2",
                "updated": "2024-11-25T14:04:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    14,
                    4,
                    35,
                    0,
                    330,
                    0
                ],
                "published": "2024-07-24T15:17:55Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    15,
                    17,
                    55,
                    2,
                    206,
                    0
                ],
                "title": "DexGANGrasp: Dexterous Generative Adversarial Grasping Synthesis for\n  Task-Oriented Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DexGANGrasp: Dexterous Generative Adversarial Grasping Synthesis for\n  Task-Oriented Manipulation"
                },
                "summary": "We introduce DexGanGrasp, a dexterous grasping synthesis method that\ngenerates and evaluates grasps with single view in real time. DexGanGrasp\ncomprises a Conditional Generative Adversarial Networks (cGANs)-based\nDexGenerator to generate dexterous grasps and a discriminator-like DexEvalautor\nto assess the stability of these grasps. Extensive simulation and real-world\nexpriments showcases the effectiveness of our proposed method, outperforming\nthe baseline FFHNet with an 18.57% higher success rate in real-world\nevaluation. We further extend DexGanGrasp to DexAfford-Prompt, an\nopen-vocabulary affordance grounding pipeline for dexterous grasping leveraging\nMultimodal Large Language Models (MLLMs) and Vision Language Models (VLMs), to\nachieve task-oriented grasping with successful real-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce DexGanGrasp, a dexterous grasping synthesis method that\ngenerates and evaluates grasps with single view in real time. DexGanGrasp\ncomprises a Conditional Generative Adversarial Networks (cGANs)-based\nDexGenerator to generate dexterous grasps and a discriminator-like DexEvalautor\nto assess the stability of these grasps. Extensive simulation and real-world\nexpriments showcases the effectiveness of our proposed method, outperforming\nthe baseline FFHNet with an 18.57% higher success rate in real-world\nevaluation. We further extend DexGanGrasp to DexAfford-Prompt, an\nopen-vocabulary affordance grounding pipeline for dexterous grasping leveraging\nMultimodal Large Language Models (MLLMs) and Vision Language Models (VLMs), to\nachieve task-oriented grasping with successful real-world deployments."
                },
                "authors": [
                    {
                        "name": "Qian Feng"
                    },
                    {
                        "name": "David S. Martinez Lema"
                    },
                    {
                        "name": "Mohammadhossein Malmir"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Jianxiang Feng"
                    },
                    {
                        "name": "Zhaopeng Chen"
                    },
                    {
                        "name": "Alois Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois Knoll"
                },
                "author": "Alois Knoll",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16387v1",
                "updated": "2024-11-25T13:49:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    49,
                    45,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T13:49:45Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    49,
                    45,
                    0,
                    330,
                    0
                ],
                "title": "FineWeb-zhtw: Scalable Curation of Traditional Chinese Text Data from\n  the Web",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineWeb-zhtw: Scalable Curation of Traditional Chinese Text Data from\n  the Web"
                },
                "summary": "The quality and size of a pretraining dataset significantly influence the\nperformance of large language models (LLMs). While there have been numerous\nefforts in the curation of such a dataset for English users, there is a\nrelative lack of similar initiatives for Traditional Chinese. Building upon\nthis foundation of FineWeb, we introduce FineWeb-zhtw, a dataset tailored\nspecifically for Traditional Chinese users. We came up with multiple stages of\nmeticulously designed filters to cater to the linguistic difference between\nEnglish and Traditional Chinese, to ensure comprehensiveness and quality. We\ndetermined effectiveness from querying dataset samples with three main\nobjectives. Our code and datasets are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quality and size of a pretraining dataset significantly influence the\nperformance of large language models (LLMs). While there have been numerous\nefforts in the curation of such a dataset for English users, there is a\nrelative lack of similar initiatives for Traditional Chinese. Building upon\nthis foundation of FineWeb, we introduce FineWeb-zhtw, a dataset tailored\nspecifically for Traditional Chinese users. We came up with multiple stages of\nmeticulously designed filters to cater to the linguistic difference between\nEnglish and Traditional Chinese, to ensure comprehensiveness and quality. We\ndetermined effectiveness from querying dataset samples with three main\nobjectives. Our code and datasets are publicly available."
                },
                "authors": [
                    {
                        "name": "Cheng-Wei Lin"
                    },
                    {
                        "name": "Wan-Hsuan Hsieh"
                    },
                    {
                        "name": "Kai-Xin Guan"
                    },
                    {
                        "name": "Chan-Jan Hsu"
                    },
                    {
                        "name": "Chia-Chen Kuo"
                    },
                    {
                        "name": "Chuan-Lin Lai"
                    },
                    {
                        "name": "Chung-Wei Chung"
                    },
                    {
                        "name": "Ming-Jen Wang"
                    },
                    {
                        "name": "Da-Shan Shiu"
                    }
                ],
                "author_detail": {
                    "name": "Da-Shan Shiu"
                },
                "author": "Da-Shan Shiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12977v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12977v2",
                "updated": "2024-11-25T13:17:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    17,
                    1,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-20T02:10:44Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    10,
                    44,
                    2,
                    325,
                    0
                ],
                "title": "MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong\n  Collaborative Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong\n  Collaborative Learning"
                },
                "summary": "Contemporary embodied agents, such as Voyager in Minecraft, have demonstrated\npromising capabilities in open-ended individual learning. However, when powered\nwith open large language models (LLMs), these agents often struggle with\nrudimentary tasks, even when fine-tuned on domain-specific knowledge. Inspired\nby human cultural learning, we present \\collabvoyager, a novel framework that\nenhances Voyager with lifelong collaborative learning through explicit\nperspective-taking. \\collabvoyager introduces three key innovations: (1) theory\nof mind representations linking percepts, beliefs, desires, and actions; (2)\nnatural language communication between agents; and (3) semantic memory of task\nand environment knowledge and episodic memory of collaboration episodes. These\nadvancements enable agents to reason about their and others' mental states,\nempirically addressing two prevalent failure modes: false beliefs and faulty\ntask executions. In mixed-expertise Minecraft experiments, \\collabvoyager\nagents outperform Voyager counterparts, significantly improving task completion\nrate by $66.6\\% (+39.4\\%)$ for collecting one block of dirt and $70.8\\%\n(+20.8\\%)$ for collecting one wood block. They exhibit emergent behaviors like\nknowledge transfer from expert to novice agents and collaborative code\ncorrection. \\collabvoyager agents also demonstrate the ability to adapt to\nout-of-distribution tasks by using their previous experiences and beliefs\nobtained through collaboration. In this open-ended social learning paradigm,\n\\collabvoyager paves the way for the democratic development of embodied AI,\nwhere agents learn in deployment from both peer and environmental feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary embodied agents, such as Voyager in Minecraft, have demonstrated\npromising capabilities in open-ended individual learning. However, when powered\nwith open large language models (LLMs), these agents often struggle with\nrudimentary tasks, even when fine-tuned on domain-specific knowledge. Inspired\nby human cultural learning, we present \\collabvoyager, a novel framework that\nenhances Voyager with lifelong collaborative learning through explicit\nperspective-taking. \\collabvoyager introduces three key innovations: (1) theory\nof mind representations linking percepts, beliefs, desires, and actions; (2)\nnatural language communication between agents; and (3) semantic memory of task\nand environment knowledge and episodic memory of collaboration episodes. These\nadvancements enable agents to reason about their and others' mental states,\nempirically addressing two prevalent failure modes: false beliefs and faulty\ntask executions. In mixed-expertise Minecraft experiments, \\collabvoyager\nagents outperform Voyager counterparts, significantly improving task completion\nrate by $66.6\\% (+39.4\\%)$ for collecting one block of dirt and $70.8\\%\n(+20.8\\%)$ for collecting one wood block. They exhibit emergent behaviors like\nknowledge transfer from expert to novice agents and collaborative code\ncorrection. \\collabvoyager agents also demonstrate the ability to adapt to\nout-of-distribution tasks by using their previous experiences and beliefs\nobtained through collaboration. In this open-ended social learning paradigm,\n\\collabvoyager paves the way for the democratic development of embodied AI,\nwhere agents learn in deployment from both peer and environmental feedback."
                },
                "authors": [
                    {
                        "name": "Mircea Lic"
                    },
                    {
                        "name": "Ojas Shirekar"
                    },
                    {
                        "name": "Baptiste Colle"
                    },
                    {
                        "name": "Chirag Raman"
                    }
                ],
                "author_detail": {
                    "name": "Chirag Raman"
                },
                "author": "Chirag Raman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12977v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12977v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16353v1",
                "updated": "2024-11-25T13:04:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    4,
                    28,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T13:04:28Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    4,
                    28,
                    0,
                    330,
                    0
                ],
                "title": "The Two-Hop Curse: LLMs trained on A->B, B->C fail to learn A-->C",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Two-Hop Curse: LLMs trained on A->B, B->C fail to learn A-->C"
                },
                "summary": "While LLMs excel at multi-hop questions (e.g. \"Who is the spouse of the\nperformer of Imagine?\") when using chain-of-thought reasoning (CoT), they\nstruggle when forced to reason internally (without CoT). Previous work on the\nsize and nature of this gap produced mixed evidence with inconclusive results.\nIn this paper, we introduce a controlled setting for investigating two-hop\nreasoning in LLMs, where the above-chance performance constitutes undeniable\nevidence for latent reasoning. We fine-tune LLMs (including Llama 3 8B Instruct\nand GPT-4o) on fictional facts and confirm that they generalize to answering\ntwo-hop questions about them using CoT. We find that models can perform latent\nreasoning when facts appear together during training or in the prompt. However,\nto our surprise, models completely fail at two-hop reasoning without CoT when\nlearned facts only appear in different documents, achieving chance-level\naccuracy and chance-level test loss. We call this complete failure to compose\nseparately learned facts the Two-Hop Curse. Moreover, we evaluate 9 frontier\nLLMs on real-world facts, finding that models completely fail at two-hop no-CoT\nreasoning for over half of question categories while maintaining partial\nsuccess with CoT across most categories. These results suggest that LLMs lack a\ngeneral capability for latent multi-hop reasoning independent of the question\ntype.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While LLMs excel at multi-hop questions (e.g. \"Who is the spouse of the\nperformer of Imagine?\") when using chain-of-thought reasoning (CoT), they\nstruggle when forced to reason internally (without CoT). Previous work on the\nsize and nature of this gap produced mixed evidence with inconclusive results.\nIn this paper, we introduce a controlled setting for investigating two-hop\nreasoning in LLMs, where the above-chance performance constitutes undeniable\nevidence for latent reasoning. We fine-tune LLMs (including Llama 3 8B Instruct\nand GPT-4o) on fictional facts and confirm that they generalize to answering\ntwo-hop questions about them using CoT. We find that models can perform latent\nreasoning when facts appear together during training or in the prompt. However,\nto our surprise, models completely fail at two-hop reasoning without CoT when\nlearned facts only appear in different documents, achieving chance-level\naccuracy and chance-level test loss. We call this complete failure to compose\nseparately learned facts the Two-Hop Curse. Moreover, we evaluate 9 frontier\nLLMs on real-world facts, finding that models completely fail at two-hop no-CoT\nreasoning for over half of question categories while maintaining partial\nsuccess with CoT across most categories. These results suggest that LLMs lack a\ngeneral capability for latent multi-hop reasoning independent of the question\ntype."
                },
                "authors": [
                    {
                        "name": "Mikita Balesni"
                    },
                    {
                        "name": "Tomek Korbak"
                    },
                    {
                        "name": "Owain Evans"
                    }
                ],
                "author_detail": {
                    "name": "Owain Evans"
                },
                "author": "Owain Evans",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01775v2",
                "updated": "2024-11-25T12:45:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    45,
                    19,
                    0,
                    330,
                    0
                ],
                "published": "2024-06-03T20:37:27Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    20,
                    37,
                    27,
                    0,
                    155,
                    0
                ],
                "title": "OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models"
                },
                "summary": "The advent of large language models (LLMs) has revolutionized natural\nlanguage processing, enabling unprecedented capabilities in understanding and\ngenerating human-like text. However, the computational cost and convergence\ntimes associated with fine-tuning these models remain significant challenges.\nLow-Rank Adaptation (LoRA) has emerged as a promising method to mitigate these\nissues by introducing efficient fine-tuning techniques with a reduced number of\ntrainable parameters. In this paper, we present OLoRA, an enhancement to the\nLoRA method that leverages orthonormal matrix initialization through QR\ndecomposition. OLoRA significantly accelerates the convergence of LLM training\nwhile preserving the efficiency benefits of LoRA, such as the number of\ntrainable parameters and GPU memory footprint. Our empirical evaluations\ndemonstrate that OLoRA not only converges faster but also exhibits improved\nperformance compared to standard LoRA across a variety of language modeling\ntasks. This advancement opens new avenues for more efficient and accessible\nfine-tuning of LLMs, potentially enabling broader adoption and innovation in\nnatural language applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of large language models (LLMs) has revolutionized natural\nlanguage processing, enabling unprecedented capabilities in understanding and\ngenerating human-like text. However, the computational cost and convergence\ntimes associated with fine-tuning these models remain significant challenges.\nLow-Rank Adaptation (LoRA) has emerged as a promising method to mitigate these\nissues by introducing efficient fine-tuning techniques with a reduced number of\ntrainable parameters. In this paper, we present OLoRA, an enhancement to the\nLoRA method that leverages orthonormal matrix initialization through QR\ndecomposition. OLoRA significantly accelerates the convergence of LLM training\nwhile preserving the efficiency benefits of LoRA, such as the number of\ntrainable parameters and GPU memory footprint. Our empirical evaluations\ndemonstrate that OLoRA not only converges faster but also exhibits improved\nperformance compared to standard LoRA across a variety of language modeling\ntasks. This advancement opens new avenues for more efficient and accessible\nfine-tuning of LLMs, potentially enabling broader adoption and innovation in\nnatural language applications."
                },
                "authors": [
                    {
                        "name": "Kerim Bykakyz"
                    }
                ],
                "author_detail": {
                    "name": "Kerim Bykakyz"
                },
                "author": "Kerim Bykakyz",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16345v1",
                "updated": "2024-11-25T12:44:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    44,
                    2,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T12:44:02Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    44,
                    2,
                    0,
                    330,
                    0
                ],
                "title": "Preference Optimization for Reasoning with Pseudo Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference Optimization for Reasoning with Pseudo Feedback"
                },
                "summary": "Preference optimization techniques, such as Direct Preference Optimization\n(DPO), are frequently employed to enhance the reasoning capabilities of large\nlanguage models (LLMs) in domains like mathematical reasoning and coding,\ntypically following supervised fine-tuning. These methods rely on high-quality\nlabels for reasoning tasks to generate preference pairs; however, the\navailability of reasoning datasets with human-verified labels is limited. In\nthis study, we introduce a novel approach to generate pseudo feedback for\nreasoning tasks by framing the labeling of solutions to reason problems as an\nevaluation against associated test cases. We explore two forms of pseudo\nfeedback based on test cases: one generated by frontier LLMs and the other by\nextending self-consistency to multi-test-case. We conduct experiments on both\nmathematical reasoning and coding tasks using pseudo feedback for preference\noptimization, and observe improvements across both tasks. Specifically, using\nMathstral-7B as our base model, we improve MATH results from 58.3 to 68.6,\nsurpassing both NuminaMath-72B and GPT-4-Turbo-1106-preview. In GSM8K and\nCollege Math, our scores increase from 85.6 to 90.3 and from 34.3 to 42.3,\nrespectively. Building on Deepseek-coder-7B-v1.5, we achieve a score of 24.6 on\nLiveCodeBench (from 21.1), surpassing Claude-3-Haiku.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference optimization techniques, such as Direct Preference Optimization\n(DPO), are frequently employed to enhance the reasoning capabilities of large\nlanguage models (LLMs) in domains like mathematical reasoning and coding,\ntypically following supervised fine-tuning. These methods rely on high-quality\nlabels for reasoning tasks to generate preference pairs; however, the\navailability of reasoning datasets with human-verified labels is limited. In\nthis study, we introduce a novel approach to generate pseudo feedback for\nreasoning tasks by framing the labeling of solutions to reason problems as an\nevaluation against associated test cases. We explore two forms of pseudo\nfeedback based on test cases: one generated by frontier LLMs and the other by\nextending self-consistency to multi-test-case. We conduct experiments on both\nmathematical reasoning and coding tasks using pseudo feedback for preference\noptimization, and observe improvements across both tasks. Specifically, using\nMathstral-7B as our base model, we improve MATH results from 58.3 to 68.6,\nsurpassing both NuminaMath-72B and GPT-4-Turbo-1106-preview. In GSM8K and\nCollege Math, our scores increase from 85.6 to 90.3 and from 34.3 to 42.3,\nrespectively. Building on Deepseek-coder-7B-v1.5, we achieve a score of 24.6 on\nLiveCodeBench (from 21.1), surpassing Claude-3-Haiku."
                },
                "authors": [
                    {
                        "name": "Fangkai Jiao"
                    },
                    {
                        "name": "Geyang Guo"
                    },
                    {
                        "name": "Xingxing Zhang"
                    },
                    {
                        "name": "Nancy F. Chen"
                    },
                    {
                        "name": "Shafiq Joty"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "28 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16341v1",
                "updated": "2024-11-25T12:37:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    37,
                    7,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T12:37:07Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    37,
                    7,
                    0,
                    330,
                    0
                ],
                "title": "From CISC to RISC: language-model guided assembly transpilation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From CISC to RISC: language-model guided assembly transpilation"
                },
                "summary": "The transition from x86 to ARM architecture is becoming increasingly common\nacross various domains, primarily driven by ARM's energy efficiency and\nimproved performance across traditional sectors. However, this ISA shift poses\nsignificant challenges, mainly due to the extensive legacy ecosystem of x86\nsoftware and lack of portability across proprietary ecosystems and software\nstacks. This paper introduces CRT, a lightweight LLM-based transpiler that\nautomatically converts x86 assembly to ARM assembly. Our approach bridges the\nfundamental architectural gap between x86's CISC-based and ARM's RISC-based\ncomputing paradigms while preserving program semantics and optimizing\nperformance. We evaluate CRT on diverse real-world applications, achieving\n79.25% translation accuracy from x86 to ARMv5 on our comprehensive test suite,\nand an 88.68% accuracy from x86 to RISC-V. In practical deployments on Apple M2\nhardware (ARMv8), our transpiled code achieves 1.73$\\times$ speedup compared to\nApple's Rosetta 2 virtualization engine, while delivering 2.41$\\times$ memory\nefficiency and 1.47$\\times$ better energy consumption. Through testing and\nanalysis, we show that CRT successfully navigates the CISC/RISC divide and\ngenerates correctly executable RISC code despite machine ``language'' barriers.\nWe release our code, models, training datasets, and benchmarks at:\n\\url{https://ahmedheakl.github.io/asm2asm/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transition from x86 to ARM architecture is becoming increasingly common\nacross various domains, primarily driven by ARM's energy efficiency and\nimproved performance across traditional sectors. However, this ISA shift poses\nsignificant challenges, mainly due to the extensive legacy ecosystem of x86\nsoftware and lack of portability across proprietary ecosystems and software\nstacks. This paper introduces CRT, a lightweight LLM-based transpiler that\nautomatically converts x86 assembly to ARM assembly. Our approach bridges the\nfundamental architectural gap between x86's CISC-based and ARM's RISC-based\ncomputing paradigms while preserving program semantics and optimizing\nperformance. We evaluate CRT on diverse real-world applications, achieving\n79.25% translation accuracy from x86 to ARMv5 on our comprehensive test suite,\nand an 88.68% accuracy from x86 to RISC-V. In practical deployments on Apple M2\nhardware (ARMv8), our transpiled code achieves 1.73$\\times$ speedup compared to\nApple's Rosetta 2 virtualization engine, while delivering 2.41$\\times$ memory\nefficiency and 1.47$\\times$ better energy consumption. Through testing and\nanalysis, we show that CRT successfully navigates the CISC/RISC divide and\ngenerates correctly executable RISC code despite machine ``language'' barriers.\nWe release our code, models, training datasets, and benchmarks at:\n\\url{https://ahmedheakl.github.io/asm2asm/}."
                },
                "authors": [
                    {
                        "name": "Ahmed Heakl"
                    },
                    {
                        "name": "Chaimaa Abi"
                    },
                    {
                        "name": "Rania Hossam"
                    },
                    {
                        "name": "Abdulrahman Mahmoud"
                    }
                ],
                "author_detail": {
                    "name": "Abdulrahman Mahmoud"
                },
                "author": "Abdulrahman Mahmoud",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16337v1",
                "updated": "2024-11-25T12:33:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    33,
                    14,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T12:33:14Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    33,
                    14,
                    0,
                    330,
                    0
                ],
                "title": "Can AI grade your essays? A comparative analysis of large language\n  models and teacher ratings in multidimensional essay scoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can AI grade your essays? A comparative analysis of large language\n  models and teacher ratings in multidimensional essay scoring"
                },
                "summary": "The manual assessment and grading of student writing is a time-consuming yet\ncritical task for teachers. Recent developments in generative AI, such as large\nlanguage models, offer potential solutions to facilitate essay-scoring tasks\nfor teachers. In our study, we evaluate the performance and reliability of both\nopen-source and closed-source LLMs in assessing German student essays,\ncomparing their evaluations to those of 37 teachers across 10 pre-defined\ncriteria (i.e., plot logic, expression). A corpus of 20 real-world essays from\nYear 7 and 8 students was analyzed using five LLMs: GPT-3.5, GPT-4, o1, LLaMA\n3-70B, and Mixtral 8x7B, aiming to provide in-depth insights into LLMs' scoring\ncapabilities. Closed-source GPT models outperform open-source models in both\ninternal consistency and alignment with human ratings, particularly excelling\nin language-related criteria. The novel o1 model outperforms all other LLMs,\nachieving Spearman's $r = .74$ with human assessments in the overall score, and\nan internal consistency of $ICC=.80$. These findings indicate that LLM-based\nassessment can be a useful tool to reduce teacher workload by supporting the\nevaluation of essays, especially with regard to language-related criteria.\nHowever, due to their tendency for higher scores, the models require further\nrefinement to better capture aspects of content quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The manual assessment and grading of student writing is a time-consuming yet\ncritical task for teachers. Recent developments in generative AI, such as large\nlanguage models, offer potential solutions to facilitate essay-scoring tasks\nfor teachers. In our study, we evaluate the performance and reliability of both\nopen-source and closed-source LLMs in assessing German student essays,\ncomparing their evaluations to those of 37 teachers across 10 pre-defined\ncriteria (i.e., plot logic, expression). A corpus of 20 real-world essays from\nYear 7 and 8 students was analyzed using five LLMs: GPT-3.5, GPT-4, o1, LLaMA\n3-70B, and Mixtral 8x7B, aiming to provide in-depth insights into LLMs' scoring\ncapabilities. Closed-source GPT models outperform open-source models in both\ninternal consistency and alignment with human ratings, particularly excelling\nin language-related criteria. The novel o1 model outperforms all other LLMs,\nachieving Spearman's $r = .74$ with human assessments in the overall score, and\nan internal consistency of $ICC=.80$. These findings indicate that LLM-based\nassessment can be a useful tool to reduce teacher workload by supporting the\nevaluation of essays, especially with regard to language-related criteria.\nHowever, due to their tendency for higher scores, the models require further\nrefinement to better capture aspects of content quality."
                },
                "authors": [
                    {
                        "name": "Kathrin Seler"
                    },
                    {
                        "name": "Maurice Frstenberg"
                    },
                    {
                        "name": "Babette Bhler"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Enkelejda Kasneci"
                },
                "author": "Enkelejda Kasneci",
                "arxiv_comment": "Accepted at LAK '25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11581v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11581v3",
                "updated": "2024-11-25T12:16:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    16,
                    0,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-18T13:57:35Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    57,
                    35,
                    0,
                    323,
                    0
                ],
                "title": "OASIS: Open Agent Social Interaction Simulations with One Million Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS: Open Agent Social Interaction Simulations with One Million Agents"
                },
                "summary": "There has been a growing interest in enhancing rule-based agent-based models\n(ABMs) for social media platforms (i.e., X, Reddit) with more realistic large\nlanguage model (LLM) agents, thereby allowing for a more nuanced study of\ncomplex systems. As a result, several LLM-based ABMs have been proposed in the\npast year. While they hold promise, each simulator is specifically designed to\nstudy a particular scenario, making it time-consuming and resource-intensive to\nexplore other phenomena using the same ABM. Additionally, these models simulate\nonly a limited number of agents, whereas real-world social media platforms\ninvolve millions of users. To this end, we propose OASIS, a generalizable and\nscalable social media simulator. OASIS is designed based on real-world social\nmedia platforms, incorporating dynamically updated environments (i.e., dynamic\nsocial networks and post information), diverse action spaces (i.e., following,\ncommenting), and recommendation systems (i.e., interest-based and\nhot-score-based). Additionally, OASIS supports large-scale user simulations,\ncapable of modeling up to one million users. With these features, OASIS can be\neasily extended to different social media platforms to study large-scale group\nphenomena and behaviors. We replicate various social phenomena, including\ninformation spreading, group polarization, and herd effects across X and Reddit\nplatforms. Moreover, we provide observations of social phenomena at different\nagent group scales. We observe that the larger agent group scale leads to more\nenhanced group dynamics and more diverse and helpful agents' opinions. These\nfindings demonstrate OASIS's potential as a powerful tool for studying complex\nsystems in digital environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been a growing interest in enhancing rule-based agent-based models\n(ABMs) for social media platforms (i.e., X, Reddit) with more realistic large\nlanguage model (LLM) agents, thereby allowing for a more nuanced study of\ncomplex systems. As a result, several LLM-based ABMs have been proposed in the\npast year. While they hold promise, each simulator is specifically designed to\nstudy a particular scenario, making it time-consuming and resource-intensive to\nexplore other phenomena using the same ABM. Additionally, these models simulate\nonly a limited number of agents, whereas real-world social media platforms\ninvolve millions of users. To this end, we propose OASIS, a generalizable and\nscalable social media simulator. OASIS is designed based on real-world social\nmedia platforms, incorporating dynamically updated environments (i.e., dynamic\nsocial networks and post information), diverse action spaces (i.e., following,\ncommenting), and recommendation systems (i.e., interest-based and\nhot-score-based). Additionally, OASIS supports large-scale user simulations,\ncapable of modeling up to one million users. With these features, OASIS can be\neasily extended to different social media platforms to study large-scale group\nphenomena and behaviors. We replicate various social phenomena, including\ninformation spreading, group polarization, and herd effects across X and Reddit\nplatforms. Moreover, we provide observations of social phenomena at different\nagent group scales. We observe that the larger agent group scale leads to more\nenhanced group dynamics and more diverse and helpful agents' opinions. These\nfindings demonstrate OASIS's potential as a powerful tool for studying complex\nsystems in digital environments."
                },
                "authors": [
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Zaibin Zhang"
                    },
                    {
                        "name": "Zirui Zheng"
                    },
                    {
                        "name": "Yuxian Jiang"
                    },
                    {
                        "name": "Ziyue Gan"
                    },
                    {
                        "name": "Zhiyu Wang"
                    },
                    {
                        "name": "Zijian Ling"
                    },
                    {
                        "name": "Jinsong Chen"
                    },
                    {
                        "name": "Martz Ma"
                    },
                    {
                        "name": "Bowen Dong"
                    },
                    {
                        "name": "Prateek Gupta"
                    },
                    {
                        "name": "Shuyue Hu"
                    },
                    {
                        "name": "Zhenfei Yin"
                    },
                    {
                        "name": "Guohao Li"
                    },
                    {
                        "name": "Xu Jia"
                    },
                    {
                        "name": "Lijun Wang"
                    },
                    {
                        "name": "Bernard Ghanem"
                    },
                    {
                        "name": "Huchuan Lu"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11581v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11581v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16313v1",
                "updated": "2024-11-25T12:05:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    5,
                    49,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T12:05:49Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    5,
                    49,
                    0,
                    330,
                    0
                ],
                "title": "CATP-LLM: Empowering Large Language Models for Cost-Aware Tool Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CATP-LLM: Empowering Large Language Models for Cost-Aware Tool Planning"
                },
                "summary": "Utilizing large language models (LLMs) for tool planning has emerged as a\npromising avenue for developing general AI systems, where LLMs automatically\nschedule external tools (e.g. vision models) to tackle complex tasks based on\ntask descriptions. To push this paradigm toward practical applications, it is\ncrucial for LLMs to consider tool execution costs (e.g. execution time) for\ntool planning. Unfortunately, prior studies overlook the tool execution costs,\nleading to the generation of expensive plans of which the costs outweigh task\nperformance. To fill this gap, we propose the Cost-Aware Tool Planning with\nLLMs (CATP-LLM) framework, which for the first time provides a coherent design\nto empower LLMs for cost-aware tool planning. Specifically, CATP-LLM\nincorporates a tool planning language to enhance the LLM to generate\nnon-sequential plans of multiple branches for efficient concurrent tool\nexecution and cost reduction. Moreover, it further designs a cost-aware offline\nreinforcement learning algorithm to fine-tune the LLM to optimize the\nperformance-cost trade-off in tool planning. In lack of public cost-related\ndatasets, we further present OpenCATP, the first platform for cost-aware\nplanning evaluation. Experiments on OpenCATP show that CATP-LLM outperforms\nGPT-4 even when using Llama2-7B as its backbone, with the average improvement\nof 28.2%-30.2% higher plan performance and 24.7%-45.8% lower costs even on the\nchallenging planning tasks. The codes of CATP-LLM and OpenCATP will be publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing large language models (LLMs) for tool planning has emerged as a\npromising avenue for developing general AI systems, where LLMs automatically\nschedule external tools (e.g. vision models) to tackle complex tasks based on\ntask descriptions. To push this paradigm toward practical applications, it is\ncrucial for LLMs to consider tool execution costs (e.g. execution time) for\ntool planning. Unfortunately, prior studies overlook the tool execution costs,\nleading to the generation of expensive plans of which the costs outweigh task\nperformance. To fill this gap, we propose the Cost-Aware Tool Planning with\nLLMs (CATP-LLM) framework, which for the first time provides a coherent design\nto empower LLMs for cost-aware tool planning. Specifically, CATP-LLM\nincorporates a tool planning language to enhance the LLM to generate\nnon-sequential plans of multiple branches for efficient concurrent tool\nexecution and cost reduction. Moreover, it further designs a cost-aware offline\nreinforcement learning algorithm to fine-tune the LLM to optimize the\nperformance-cost trade-off in tool planning. In lack of public cost-related\ndatasets, we further present OpenCATP, the first platform for cost-aware\nplanning evaluation. Experiments on OpenCATP show that CATP-LLM outperforms\nGPT-4 even when using Llama2-7B as its backbone, with the average improvement\nof 28.2%-30.2% higher plan performance and 24.7%-45.8% lower costs even on the\nchallenging planning tasks. The codes of CATP-LLM and OpenCATP will be publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Duo Wu"
                    },
                    {
                        "name": "Jinghe Wang"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Yanning Zhang"
                    },
                    {
                        "name": "Le Sun"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "arxiv_comment": "In submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00208v2",
                "updated": "2024-11-25T12:04:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    4,
                    18,
                    0,
                    330,
                    0
                ],
                "published": "2024-10-31T21:07:58Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    21,
                    7,
                    58,
                    3,
                    305,
                    0
                ],
                "title": "Using Large Language Models for a standard assessment mapping for\n  sustainable communities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models for a standard assessment mapping for\n  sustainable communities"
                },
                "summary": "This paper presents a new approach to urban sustainability assessment through\nthe use of Large Language Models (LLMs) to streamline the use of the ISO 37101\nframework to automate and standardise the assessment of urban initiatives\nagainst the six \"sustainability purposes\" and twelve \"issues\" outlined in the\nstandard. The methodology includes the development of a custom prompt based on\nthe standard definitions and its application to two different datasets: 527\nprojects from the Paris Participatory Budget and 398 activities from the\nPROBONO Horizon 2020 project. The results show the effectiveness of LLMs in\nquickly and consistently categorising different urban initiatives according to\nsustainability criteria. The approach is particularly promising when it comes\nto breaking down silos in urban planning by providing a holistic view of the\nimpact of projects. The paper discusses the advantages of this method over\ntraditional human-led assessments, including significant time savings and\nimproved consistency. However, it also points out the importance of human\nexpertise in interpreting results and ethical considerations. This study\nhopefully can contribute to the growing body of work on AI applications in\nurban planning and provides a novel method for operationalising standardised\nsustainability frameworks in different urban contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a new approach to urban sustainability assessment through\nthe use of Large Language Models (LLMs) to streamline the use of the ISO 37101\nframework to automate and standardise the assessment of urban initiatives\nagainst the six \"sustainability purposes\" and twelve \"issues\" outlined in the\nstandard. The methodology includes the development of a custom prompt based on\nthe standard definitions and its application to two different datasets: 527\nprojects from the Paris Participatory Budget and 398 activities from the\nPROBONO Horizon 2020 project. The results show the effectiveness of LLMs in\nquickly and consistently categorising different urban initiatives according to\nsustainability criteria. The approach is particularly promising when it comes\nto breaking down silos in urban planning by providing a holistic view of the\nimpact of projects. The paper discusses the advantages of this method over\ntraditional human-led assessments, including significant time savings and\nimproved consistency. However, it also points out the importance of human\nexpertise in interpreting results and ethical considerations. This study\nhopefully can contribute to the growing body of work on AI applications in\nurban planning and provides a novel method for operationalising standardised\nsustainability frameworks in different urban contexts."
                },
                "authors": [
                    {
                        "name": "Luc Jonveaux"
                    }
                ],
                "author_detail": {
                    "name": "Luc Jonveaux"
                },
                "author": "Luc Jonveaux",
                "arxiv_comment": "8 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.4.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06870v2",
                "updated": "2024-11-25T11:58:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    11,
                    58,
                    44,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-11T11:10:39Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    11,
                    10,
                    39,
                    0,
                    316,
                    0
                ],
                "title": "AI-Native Multi-Access Future Networks -- The REASON Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Native Multi-Access Future Networks -- The REASON Architecture"
                },
                "summary": "The development of the sixth generation of communication networks (6G) has\nbeen gaining momentum over the past years, with a target of being introduced by\n2030. Several initiatives worldwide are developing innovative solutions and\nsetting the direction for the key features of these networks. Some common\nemerging themes are the tight integration of AI, the convergence of multiple\naccess technologies and sustainable operation, aiming to meet stringent\nperformance and societal requirements. To that end, we are introducing REASON -\nRealising Enabling Architectures and Solutions for Open Networks. The REASON\nproject aims to address technical challenges in future network deployments,\nsuch as E2E service orchestration, sustainability, security and trust\nmanagement, and policy management, utilising AI-native principles, considering\nmultiple access technologies and cloud-native solutions.\n  This paper presents REASON's architecture and the identified requirements for\nfuture networks. The architecture is meticulously designed for modularity,\ninteroperability, scalability, simplified troubleshooting, flexibility, and\nenhanced security, taking into consideration current and future standardisation\nefforts, and the ease of implementation and training. It is structured into\nfour horizontal layers: Physical Infrastructure, Network Service, Knowledge,\nand End-User Application, complemented by two vertical layers: Management and\nOrchestration, and E2E Security. This layered approach ensures a robust,\nadaptable framework to support the diverse and evolving requirements of 6G\nnetworks, fostering innovation and facilitating seamless integration of\nadvanced technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of the sixth generation of communication networks (6G) has\nbeen gaining momentum over the past years, with a target of being introduced by\n2030. Several initiatives worldwide are developing innovative solutions and\nsetting the direction for the key features of these networks. Some common\nemerging themes are the tight integration of AI, the convergence of multiple\naccess technologies and sustainable operation, aiming to meet stringent\nperformance and societal requirements. To that end, we are introducing REASON -\nRealising Enabling Architectures and Solutions for Open Networks. The REASON\nproject aims to address technical challenges in future network deployments,\nsuch as E2E service orchestration, sustainability, security and trust\nmanagement, and policy management, utilising AI-native principles, considering\nmultiple access technologies and cloud-native solutions.\n  This paper presents REASON's architecture and the identified requirements for\nfuture networks. The architecture is meticulously designed for modularity,\ninteroperability, scalability, simplified troubleshooting, flexibility, and\nenhanced security, taking into consideration current and future standardisation\nefforts, and the ease of implementation and training. It is structured into\nfour horizontal layers: Physical Infrastructure, Network Service, Knowledge,\nand End-User Application, complemented by two vertical layers: Management and\nOrchestration, and E2E Security. This layered approach ensures a robust,\nadaptable framework to support the diverse and evolving requirements of 6G\nnetworks, fostering innovation and facilitating seamless integration of\nadvanced technologies."
                },
                "authors": [
                    {
                        "name": "Konstantinos Katsaros"
                    },
                    {
                        "name": "Ioannis Mavromatis"
                    },
                    {
                        "name": "Kostantinos Antonakoglou"
                    },
                    {
                        "name": "Saptarshi Ghosh"
                    },
                    {
                        "name": "Dritan Kaleshi"
                    },
                    {
                        "name": "Toktam Mahmoodi"
                    },
                    {
                        "name": "Hamid Asgari"
                    },
                    {
                        "name": "Anastasios Karousos"
                    },
                    {
                        "name": "Iman Tavakkolnia"
                    },
                    {
                        "name": "Hossein Safi"
                    },
                    {
                        "name": "Harald Hass"
                    },
                    {
                        "name": "Constantinos Vrontos"
                    },
                    {
                        "name": "Amin Emami"
                    },
                    {
                        "name": "Juan Parra Ullauri"
                    },
                    {
                        "name": "Shadi Moazzeni"
                    },
                    {
                        "name": "Dimitra Simeonidou"
                    }
                ],
                "author_detail": {
                    "name": "Dimitra Simeonidou"
                },
                "author": "Dimitra Simeonidou",
                "arxiv_comment": "Accepted for publication at IEEE Access",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12788v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12788v2",
                "updated": "2024-11-25T11:54:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    11,
                    54,
                    3,
                    0,
                    330,
                    0
                ],
                "published": "2024-10-16T17:59:32Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    59,
                    32,
                    2,
                    290,
                    0
                ],
                "title": "Meta-Chunking: Learning Efficient Text Segmentation via Logical\n  Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-Chunking: Learning Efficient Text Segmentation via Logical\n  Perception"
                },
                "summary": "Retrieval-Augmented Generation (RAG), while serving as a viable complement to\nlarge language models (LLMs), often overlooks the crucial aspect of text\nchunking within its pipeline, which impacts the quality of knowledge-intensive\ntasks. This paper introduces the concept of Meta-Chunking, which refers to a\ngranularity between sentences and paragraphs, consisting of a collection of\nsentences within a paragraph that have deep linguistic logical connections. To\nimplement Meta-Chunking, we designed Perplexity (PPL) Chunking, which balances\nperformance and speed, and precisely identifies the boundaries of text chunks\nby analyzing the characteristics of context perplexity distribution.\nAdditionally, considering the inherent complexity of different texts, we\npropose a strategy that combines PPL Chunking with dynamic merging to achieve a\nbalance between fine-grained and coarse-grained text chunking. Experiments\nconducted on eleven datasets demonstrate that Meta-Chunking can more\nefficiently improve the performance of single-hop and multi-hop question\nanswering based on RAG. For instance, on the 2WikiMultihopQA dataset, it\noutperforms similarity chunking by 1.32 while only consuming 45.8% of the time.\nFurthermore, through the analysis of models of various scales and types, we\nobserved that PPL Chunking exhibits notable flexibility and adaptability. Our\ncode is available at https://github.com/IAAR-Shanghai/Meta-Chunking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG), while serving as a viable complement to\nlarge language models (LLMs), often overlooks the crucial aspect of text\nchunking within its pipeline, which impacts the quality of knowledge-intensive\ntasks. This paper introduces the concept of Meta-Chunking, which refers to a\ngranularity between sentences and paragraphs, consisting of a collection of\nsentences within a paragraph that have deep linguistic logical connections. To\nimplement Meta-Chunking, we designed Perplexity (PPL) Chunking, which balances\nperformance and speed, and precisely identifies the boundaries of text chunks\nby analyzing the characteristics of context perplexity distribution.\nAdditionally, considering the inherent complexity of different texts, we\npropose a strategy that combines PPL Chunking with dynamic merging to achieve a\nbalance between fine-grained and coarse-grained text chunking. Experiments\nconducted on eleven datasets demonstrate that Meta-Chunking can more\nefficiently improve the performance of single-hop and multi-hop question\nanswering based on RAG. For instance, on the 2WikiMultihopQA dataset, it\noutperforms similarity chunking by 1.32 while only consuming 45.8% of the time.\nFurthermore, through the analysis of models of various scales and types, we\nobserved that PPL Chunking exhibits notable flexibility and adaptability. Our\ncode is available at https://github.com/IAAR-Shanghai/Meta-Chunking."
                },
                "authors": [
                    {
                        "name": "Jihao Zhao"
                    },
                    {
                        "name": "Zhiyuan Ji"
                    },
                    {
                        "name": "Yuchen Feng"
                    },
                    {
                        "name": "Pengnian Qi"
                    },
                    {
                        "name": "Simin Niu"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12788v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12788v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16300v1",
                "updated": "2024-11-25T11:35:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    11,
                    35,
                    8,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T11:35:08Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    11,
                    35,
                    8,
                    0,
                    330,
                    0
                ],
                "title": "BayLing 2: A Multilingual Large Language Model with Efficient Language\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BayLing 2: A Multilingual Large Language Model with Efficient Language\n  Alignment"
                },
                "summary": "Large language models (LLMs), with their powerful generative capabilities and\nvast knowledge, empower various tasks in everyday life. However, these\nabilities are primarily concentrated in high-resource languages, leaving\nlow-resource languages with weaker generative capabilities and relatively\nlimited knowledge. Enhancing the multilingual capabilities of LLMs is therefore\ncrucial for serving over 100 linguistic communities worldwide. An intuitive\napproach to enhance the multilingual capabilities would be to construct\ninstruction data for various languages, but constructing instruction data for\nover 100 languages is prohibitively costly. In this paper, we introduce BayLing\n2, which efficiently transfers generative capabilities and knowledge from\nhigh-resource languages to low-resource languages through language alignment.\nTo achieve this, we constructed a dataset of 3.2 million instructions,\ncomprising high-resource language instructions (Chinese and English) and\ncross-lingual instructions for 100+ languages and performed instruction tuning\nbased on the dataset to facilitate the capability transfer between languages.\nUsing Llama as the foundation model, we developed BayLing-2-7B, BayLing-2-13B,\nand BayLing-3-8B, and conducted a comprehensive evaluation of BayLing. For\nmultilingual translation across 100+ languages, BayLing shows superior\nperformance compared to open-source models of similar scale. For multilingual\nknowledge and understanding benchmarks, BayLing achieves significant\nimprovements across over 20 low-resource languages, demonstrating its\ncapability of effective knowledge transfer from high-resource to low-resource\nlanguages. Furthermore, results on English benchmarks indicate that BayLing\nmaintains high performance in highresource languages while enhancing the\nperformance in low-resource languages. Demo, homepage, code and models of\nBayLing are available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), with their powerful generative capabilities and\nvast knowledge, empower various tasks in everyday life. However, these\nabilities are primarily concentrated in high-resource languages, leaving\nlow-resource languages with weaker generative capabilities and relatively\nlimited knowledge. Enhancing the multilingual capabilities of LLMs is therefore\ncrucial for serving over 100 linguistic communities worldwide. An intuitive\napproach to enhance the multilingual capabilities would be to construct\ninstruction data for various languages, but constructing instruction data for\nover 100 languages is prohibitively costly. In this paper, we introduce BayLing\n2, which efficiently transfers generative capabilities and knowledge from\nhigh-resource languages to low-resource languages through language alignment.\nTo achieve this, we constructed a dataset of 3.2 million instructions,\ncomprising high-resource language instructions (Chinese and English) and\ncross-lingual instructions for 100+ languages and performed instruction tuning\nbased on the dataset to facilitate the capability transfer between languages.\nUsing Llama as the foundation model, we developed BayLing-2-7B, BayLing-2-13B,\nand BayLing-3-8B, and conducted a comprehensive evaluation of BayLing. For\nmultilingual translation across 100+ languages, BayLing shows superior\nperformance compared to open-source models of similar scale. For multilingual\nknowledge and understanding benchmarks, BayLing achieves significant\nimprovements across over 20 low-resource languages, demonstrating its\ncapability of effective knowledge transfer from high-resource to low-resource\nlanguages. Furthermore, results on English benchmarks indicate that BayLing\nmaintains high performance in highresource languages while enhancing the\nperformance in low-resource languages. Demo, homepage, code and models of\nBayLing are available."
                },
                "authors": [
                    {
                        "name": "Shaolei Zhang"
                    },
                    {
                        "name": "Kehao Zhang"
                    },
                    {
                        "name": "Qingkai Fang"
                    },
                    {
                        "name": "Shoutao Guo"
                    },
                    {
                        "name": "Yan Zhou"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Yang Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yang Feng"
                },
                "author": "Yang Feng",
                "arxiv_comment": "BayLing 2's online demo: http://nlp.ict.ac.cn/bayling/demo. BayLing\n  2's code and models: https://github.com/ictnlp/BayLing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12564v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12564v2",
                "updated": "2024-11-25T11:24:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    11,
                    24,
                    23,
                    0,
                    330,
                    0
                ],
                "published": "2024-10-16T13:38:31Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    38,
                    31,
                    2,
                    290,
                    0
                ],
                "title": "FTII-Bench: A Comprehensive Multimodal Benchmark for Flow Text with\n  Image Insertion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FTII-Bench: A Comprehensive Multimodal Benchmark for Flow Text with\n  Image Insertion"
                },
                "summary": "Benefiting from the revolutionary advances in large language models (LLMs)\nand foundational vision models, large vision-language models (LVLMs) have also\nmade significant progress. However, current benchmarks focus on tasks that\nevaluating only a single aspect of LVLM capabilities (e.g., recognition,\ndetection, understanding). These tasks fail to fully demonstrate LVLMs'\npotential in complex application scenarios. To comprehensively assess the\nperformance of existing LVLMs, we propose a more challenging task called the\nFlow Text with Image Insertion task (FTII). This task requires LVLMs to\nsimultaneously possess outstanding abilities in image comprehension,\ninstruction understanding, and long-text interpretation. Specifically, given\nseveral text paragraphs and a set of candidate images, as the text paragraphs\naccumulate, the LVLMs are required to select the most suitable image from the\ncandidates to insert after the corresponding paragraph. Constructing a\nbenchmark for such a task is highly challenging, particularly in determining\nthe sequence of flowing text and images. To address this challenge, we turn to\nprofessional news reports, which naturally contain a gold standard for\nimage-text sequences. Based on this, we introduce the Flow Text with Image\nInsertion Benchmark (FTII-Bench), which includes 318 high-quality Chinese\nimage-text news articles and 307 high-quality English image-text news articles,\ncovering 10 different news domains. Using these 625 high-quality articles, we\nconstruct problems of two different types with multiple levels of difficulty.\nFurthermore, we establish two different evaluation pipelines based on the CLIP\nmodel and existing LVLMs. We evaluate 9 open-source and 2 closed-source LVLMs\nas well as 2 CLIP-based models. Results indicate that even the most advanced\nmodels (e.g., GPT-4o) face significant challenges when tackling the FTII task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benefiting from the revolutionary advances in large language models (LLMs)\nand foundational vision models, large vision-language models (LVLMs) have also\nmade significant progress. However, current benchmarks focus on tasks that\nevaluating only a single aspect of LVLM capabilities (e.g., recognition,\ndetection, understanding). These tasks fail to fully demonstrate LVLMs'\npotential in complex application scenarios. To comprehensively assess the\nperformance of existing LVLMs, we propose a more challenging task called the\nFlow Text with Image Insertion task (FTII). This task requires LVLMs to\nsimultaneously possess outstanding abilities in image comprehension,\ninstruction understanding, and long-text interpretation. Specifically, given\nseveral text paragraphs and a set of candidate images, as the text paragraphs\naccumulate, the LVLMs are required to select the most suitable image from the\ncandidates to insert after the corresponding paragraph. Constructing a\nbenchmark for such a task is highly challenging, particularly in determining\nthe sequence of flowing text and images. To address this challenge, we turn to\nprofessional news reports, which naturally contain a gold standard for\nimage-text sequences. Based on this, we introduce the Flow Text with Image\nInsertion Benchmark (FTII-Bench), which includes 318 high-quality Chinese\nimage-text news articles and 307 high-quality English image-text news articles,\ncovering 10 different news domains. Using these 625 high-quality articles, we\nconstruct problems of two different types with multiple levels of difficulty.\nFurthermore, we establish two different evaluation pipelines based on the CLIP\nmodel and existing LVLMs. We evaluate 9 open-source and 2 closed-source LVLMs\nas well as 2 CLIP-based models. Results indicate that even the most advanced\nmodels (e.g., GPT-4o) face significant challenges when tackling the FTII task."
                },
                "authors": [
                    {
                        "name": "Jiacheng Ruan"
                    },
                    {
                        "name": "Yebin Yang"
                    },
                    {
                        "name": "Zehao Lin"
                    },
                    {
                        "name": "Yuchen Feng"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zeyun Tang"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "arxiv_comment": "Work in progress. 9 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12564v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12564v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14986v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14986v2",
                "updated": "2024-11-25T11:10:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    11,
                    10,
                    34,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-22T14:47:00Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    47,
                    0,
                    4,
                    327,
                    0
                ],
                "title": "Generative AI may backfire for counterspeech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI may backfire for counterspeech"
                },
                "summary": "Online hate speech poses a serious threat to individual well-being and\nsocietal cohesion. A promising solution to curb online hate speech is\ncounterspeech. Counterspeech is aimed at encouraging users to reconsider\nhateful posts by direct replies. However, current methods lack scalability due\nto the need for human intervention or fail to adapt to the specific context of\nthe post. A potential remedy is the use of generative AI, specifically large\nlanguage models (LLMs), to write tailored counterspeech messages. In this\npaper, we analyze whether contextualized counterspeech generated by\nstate-of-the-art LLMs is effective in curbing online hate speech. To do so, we\nconducted a large-scale, pre-registered field experiment (N=2,664) on the\nsocial media platform Twitter/X. Our experiment followed a 2x2 between-subjects\ndesign and, additionally, a control condition with no counterspeech. On the one\nhand, users posting hateful content on Twitter/X were randomly assigned to\nreceive either (a) contextualized counterspeech or (b) non-contextualized\ncounterspeech. Here, the former is generated through LLMs, while the latter\nrelies on predefined, generic messages. On the other hand, we tested two\ncounterspeech strategies: (a) promoting empathy and (b) warning about the\nconsequences of online misbehavior. We then measured whether users deleted\ntheir initial hateful posts and whether their behavior changed after the\ncounterspeech intervention (e.g., whether users adopted a less toxic language).\nWe find that non-contextualized counterspeech employing a\nwarning-of-consequence strategy significantly reduces online hate speech.\nHowever, contextualized counterspeech generated by LLMs proves ineffective and\nmay even backfire.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online hate speech poses a serious threat to individual well-being and\nsocietal cohesion. A promising solution to curb online hate speech is\ncounterspeech. Counterspeech is aimed at encouraging users to reconsider\nhateful posts by direct replies. However, current methods lack scalability due\nto the need for human intervention or fail to adapt to the specific context of\nthe post. A potential remedy is the use of generative AI, specifically large\nlanguage models (LLMs), to write tailored counterspeech messages. In this\npaper, we analyze whether contextualized counterspeech generated by\nstate-of-the-art LLMs is effective in curbing online hate speech. To do so, we\nconducted a large-scale, pre-registered field experiment (N=2,664) on the\nsocial media platform Twitter/X. Our experiment followed a 2x2 between-subjects\ndesign and, additionally, a control condition with no counterspeech. On the one\nhand, users posting hateful content on Twitter/X were randomly assigned to\nreceive either (a) contextualized counterspeech or (b) non-contextualized\ncounterspeech. Here, the former is generated through LLMs, while the latter\nrelies on predefined, generic messages. On the other hand, we tested two\ncounterspeech strategies: (a) promoting empathy and (b) warning about the\nconsequences of online misbehavior. We then measured whether users deleted\ntheir initial hateful posts and whether their behavior changed after the\ncounterspeech intervention (e.g., whether users adopted a less toxic language).\nWe find that non-contextualized counterspeech employing a\nwarning-of-consequence strategy significantly reduces online hate speech.\nHowever, contextualized counterspeech generated by LLMs proves ineffective and\nmay even backfire."
                },
                "authors": [
                    {
                        "name": "Dominik Br"
                    },
                    {
                        "name": "Abdurahman Maarouf"
                    },
                    {
                        "name": "Stefan Feuerriegel"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Feuerriegel"
                },
                "author": "Stefan Feuerriegel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14986v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14986v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2211.11483v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2211.11483v5",
                "updated": "2024-11-25T10:55:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    10,
                    55,
                    29,
                    0,
                    330,
                    0
                ],
                "published": "2022-11-21T14:18:25Z",
                "published_parsed": [
                    2022,
                    11,
                    21,
                    14,
                    18,
                    25,
                    0,
                    325,
                    0
                ],
                "title": "Deanthropomorphising NLP: Can a Language Model Be Conscious?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deanthropomorphising NLP: Can a Language Model Be Conscious?"
                },
                "summary": "This work is intended as a voice in the discussion over previous claims that\na pretrained large language model (LLM) based on the Transformer model\narchitecture can be sentient. Such claims have been made concerning the LaMDA\nmodel and also concerning the current wave of LLM-powered chatbots, such as\nChatGPT. This claim, if confirmed, would have serious ramifications in the\nNatural Language Processing (NLP) community due to wide-spread use of similar\nmodels. However, here we take the position that such a large language model\ncannot be sentient, or conscious, and that LaMDA in particular exhibits no\nadvances over other similar models that would qualify it. We justify this by\nanalysing the Transformer architecture through Integrated Information Theory of\nconsciousness. We see the claims of sentience as part of a wider tendency to\nuse anthropomorphic language in NLP reporting. Regardless of the veracity of\nthe claims, we consider this an opportune moment to take stock of progress in\nlanguage modelling and consider the ethical implications of the task. In order\nto make this work helpful for readers outside the NLP community, we also\npresent the necessary background in language modelling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work is intended as a voice in the discussion over previous claims that\na pretrained large language model (LLM) based on the Transformer model\narchitecture can be sentient. Such claims have been made concerning the LaMDA\nmodel and also concerning the current wave of LLM-powered chatbots, such as\nChatGPT. This claim, if confirmed, would have serious ramifications in the\nNatural Language Processing (NLP) community due to wide-spread use of similar\nmodels. However, here we take the position that such a large language model\ncannot be sentient, or conscious, and that LaMDA in particular exhibits no\nadvances over other similar models that would qualify it. We justify this by\nanalysing the Transformer architecture through Integrated Information Theory of\nconsciousness. We see the claims of sentience as part of a wider tendency to\nuse anthropomorphic language in NLP reporting. Regardless of the veracity of\nthe claims, we consider this an opportune moment to take stock of progress in\nlanguage modelling and consider the ethical implications of the task. In order\nto make this work helpful for readers outside the NLP community, we also\npresent the necessary background in language modelling."
                },
                "authors": [
                    {
                        "name": "Matthew Shardlow"
                    },
                    {
                        "name": "Piotr Przybya"
                    }
                ],
                "author_detail": {
                    "name": "Piotr Przybya"
                },
                "author": "Piotr Przybya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2211.11483v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2211.11483v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16260v1",
                "updated": "2024-11-25T10:23:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    10,
                    23,
                    11,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T10:23:11Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    10,
                    23,
                    11,
                    0,
                    330,
                    0
                ],
                "title": "Unraveling Arithmetic in Large Language Models: The Role of Algebraic\n  Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unraveling Arithmetic in Large Language Models: The Role of Algebraic\n  Structures"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable mathematical\ncapabilities, largely driven by chain-of-thought (CoT) prompting, which\ndecomposes complex reasoning into step-by-step solutions. This approach has\nenabled significant advancements, as evidenced by performance on benchmarks\nlike GSM8K and MATH. However, the mechanisms underlying LLMs' ability to\nperform arithmetic in a single step of CoT remain poorly understood. Existing\nstudies debate whether LLMs encode numerical values or rely on symbolic\nreasoning, while others explore attention and multi-layered processing in\narithmetic tasks. In this work, we propose that LLMs learn arithmetic by\ncapturing algebraic structures, such as \\emph{Commutativity} and\n\\emph{Identity} properties. Since these structures are observable through\ninput-output relationships, they can generalize to unseen data. We empirically\ndemonstrate that LLMs can learn algebraic structures using a custom dataset of\narithmetic problems. Our findings indicate that leveraging algebraic structures\ncan enhance the LLMs' arithmetic capabilities, offering insights into improving\ntheir arithmetic performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable mathematical\ncapabilities, largely driven by chain-of-thought (CoT) prompting, which\ndecomposes complex reasoning into step-by-step solutions. This approach has\nenabled significant advancements, as evidenced by performance on benchmarks\nlike GSM8K and MATH. However, the mechanisms underlying LLMs' ability to\nperform arithmetic in a single step of CoT remain poorly understood. Existing\nstudies debate whether LLMs encode numerical values or rely on symbolic\nreasoning, while others explore attention and multi-layered processing in\narithmetic tasks. In this work, we propose that LLMs learn arithmetic by\ncapturing algebraic structures, such as \\emph{Commutativity} and\n\\emph{Identity} properties. Since these structures are observable through\ninput-output relationships, they can generalize to unseen data. We empirically\ndemonstrate that LLMs can learn algebraic structures using a custom dataset of\narithmetic problems. Our findings indicate that leveraging algebraic structures\ncan enhance the LLMs' arithmetic capabilities, offering insights into improving\ntheir arithmetic performance."
                },
                "authors": [
                    {
                        "name": "Fu-Chieh Chang"
                    },
                    {
                        "name": "Pei-Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Pei-Yuan Wu"
                },
                "author": "Pei-Yuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16252v1",
                "updated": "2024-11-25T10:12:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    10,
                    12,
                    27,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T10:12:27Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    10,
                    12,
                    27,
                    0,
                    330,
                    0
                ],
                "title": "NormXLogit: The Head-on-Top Never Lies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NormXLogit: The Head-on-Top Never Lies"
                },
                "summary": "The Transformer architecture has emerged as the dominant choice for building\nlarge language models (LLMs). However, with new LLMs emerging on a frequent\nbasis, it is important to consider the potential value of architecture-agnostic\napproaches that can provide interpretability across a variety of architectures.\nDespite recent successes in the interpretability of LLMs, many existing\napproaches rely on complex methods that are often tied to a specific model\ndesign and come with a significant computational cost. To address these\nlimitations, we propose a novel technique, called NormXLogit, for assessing the\nsignificance of individual input tokens. This method operates based on the\ninput and output representations associated with each token. First, we\ndemonstrate that during the pre-training of LLMs, the norms of word embeddings\ncapture the importance of input tokens. Second, we reveal a significant\nrelationship between a token's importance and the extent to which its\nrepresentation can resemble the model's final prediction. Through extensive\nanalysis, we show that our approach consistently outperforms existing\ngradient-based methods in terms of faithfulness. Additionally, our method\nachieves better performance in layer-wise explanations compared to the most\nprominent architecture-specific methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Transformer architecture has emerged as the dominant choice for building\nlarge language models (LLMs). However, with new LLMs emerging on a frequent\nbasis, it is important to consider the potential value of architecture-agnostic\napproaches that can provide interpretability across a variety of architectures.\nDespite recent successes in the interpretability of LLMs, many existing\napproaches rely on complex methods that are often tied to a specific model\ndesign and come with a significant computational cost. To address these\nlimitations, we propose a novel technique, called NormXLogit, for assessing the\nsignificance of individual input tokens. This method operates based on the\ninput and output representations associated with each token. First, we\ndemonstrate that during the pre-training of LLMs, the norms of word embeddings\ncapture the importance of input tokens. Second, we reveal a significant\nrelationship between a token's importance and the extent to which its\nrepresentation can resemble the model's final prediction. Through extensive\nanalysis, we show that our approach consistently outperforms existing\ngradient-based methods in terms of faithfulness. Additionally, our method\nachieves better performance in layer-wise explanations compared to the most\nprominent architecture-specific methods."
                },
                "authors": [
                    {
                        "name": "Sina Abbasi"
                    },
                    {
                        "name": "Mohammad Reza Modarres"
                    },
                    {
                        "name": "Mohammad Taher Pilehvar"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Taher Pilehvar"
                },
                "author": "Mohammad Taher Pilehvar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16239v1",
                "updated": "2024-11-25T09:54:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    54,
                    42,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T09:54:42Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    54,
                    42,
                    0,
                    330,
                    0
                ],
                "title": "CS-Eval: A Comprehensive Large Language Model Benchmark for\n  CyberSecurity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CS-Eval: A Comprehensive Large Language Model Benchmark for\n  CyberSecurity"
                },
                "summary": "Over the past year, there has been a notable rise in the use of large\nlanguage models (LLMs) for academic research and industrial practices within\nthe cybersecurity field. However, it remains a lack of comprehensive and\npublicly accessible benchmarks to evaluate the performance of LLMs on\ncybersecurity tasks. To address this gap, we introduce CS-Eval, a publicly\naccessible, comprehensive and bilingual LLM benchmark specifically designed for\ncybersecurity. CS-Eval synthesizes the research hotspots from academia and\npractical applications from industry, curating a diverse set of high-quality\nquestions across 42 categories within cybersecurity, systematically organized\ninto three cognitive levels: knowledge, ability, and application. Through an\nextensive evaluation of a wide range of LLMs using CS-Eval, we have uncovered\nvaluable insights. For instance, while GPT-4 generally excels overall, other\nmodels may outperform it in certain specific subcategories. Additionally, by\nconducting evaluations over several months, we observed significant\nimprovements in many LLMs' abilities to solve cybersecurity tasks. The\nbenchmarks are now publicly available at https://github.com/CS-EVAL/CS-Eval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past year, there has been a notable rise in the use of large\nlanguage models (LLMs) for academic research and industrial practices within\nthe cybersecurity field. However, it remains a lack of comprehensive and\npublicly accessible benchmarks to evaluate the performance of LLMs on\ncybersecurity tasks. To address this gap, we introduce CS-Eval, a publicly\naccessible, comprehensive and bilingual LLM benchmark specifically designed for\ncybersecurity. CS-Eval synthesizes the research hotspots from academia and\npractical applications from industry, curating a diverse set of high-quality\nquestions across 42 categories within cybersecurity, systematically organized\ninto three cognitive levels: knowledge, ability, and application. Through an\nextensive evaluation of a wide range of LLMs using CS-Eval, we have uncovered\nvaluable insights. For instance, while GPT-4 generally excels overall, other\nmodels may outperform it in certain specific subcategories. Additionally, by\nconducting evaluations over several months, we observed significant\nimprovements in many LLMs' abilities to solve cybersecurity tasks. The\nbenchmarks are now publicly available at https://github.com/CS-EVAL/CS-Eval."
                },
                "authors": [
                    {
                        "name": "Zhengmin Yu"
                    },
                    {
                        "name": "Jiutian Zeng"
                    },
                    {
                        "name": "Siyi Chen"
                    },
                    {
                        "name": "Wenhan Xu"
                    },
                    {
                        "name": "Dandan Xu"
                    },
                    {
                        "name": "Xiangyu Liu"
                    },
                    {
                        "name": "Zonghao Ying"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16238v1",
                "updated": "2024-11-25T09:53:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    53,
                    35,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T09:53:35Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    53,
                    35,
                    0,
                    330,
                    0
                ],
                "title": "UVLLM: An Automated Universal RTL Verification Framework using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UVLLM: An Automated Universal RTL Verification Framework using LLMs"
                },
                "summary": "Verifying hardware designs in embedded systems is crucial but often\nlabor-intensive and time-consuming. While existing solutions have improved\nautomation, they frequently rely on unrealistic assumptions. To address these\nchallenges, we introduce a novel framework, UVLLM, which combines Large\nLanguage Models (LLMs) with the Universal Verification Methodology (UVM) to\nrelax these assumptions. UVLLM significantly enhances the automation of testing\nand repairing error-prone Register Transfer Level (RTL) codes, a critical\naspect of verification development. Unlike existing methods, UVLLM ensures that\nall errors are triggered during verification, achieving a syntax error fix rate\nof 86.99% and a functional error fix rate of 71.92% on our proposed benchmark.\nThese results demonstrate a substantial improvement in verification efficiency.\nAdditionally, our study highlights the current limitations of LLM applications,\nparticularly their reliance on extensive training data. We emphasize the\ntransformative potential of LLMs in hardware design verification and suggest\npromising directions for future research in AI-driven hardware design\nmethodologies. The Repo. of dataset and code:\nhttps://anonymous.4open.science/r/UVLLM/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verifying hardware designs in embedded systems is crucial but often\nlabor-intensive and time-consuming. While existing solutions have improved\nautomation, they frequently rely on unrealistic assumptions. To address these\nchallenges, we introduce a novel framework, UVLLM, which combines Large\nLanguage Models (LLMs) with the Universal Verification Methodology (UVM) to\nrelax these assumptions. UVLLM significantly enhances the automation of testing\nand repairing error-prone Register Transfer Level (RTL) codes, a critical\naspect of verification development. Unlike existing methods, UVLLM ensures that\nall errors are triggered during verification, achieving a syntax error fix rate\nof 86.99% and a functional error fix rate of 71.92% on our proposed benchmark.\nThese results demonstrate a substantial improvement in verification efficiency.\nAdditionally, our study highlights the current limitations of LLM applications,\nparticularly their reliance on extensive training data. We emphasize the\ntransformative potential of LLMs in hardware design verification and suggest\npromising directions for future research in AI-driven hardware design\nmethodologies. The Repo. of dataset and code:\nhttps://anonymous.4open.science/r/UVLLM/."
                },
                "authors": [
                    {
                        "name": "Yuchen Hu"
                    },
                    {
                        "name": "Junhao Ye"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Jialin Sun"
                    },
                    {
                        "name": "Shiyue Zhang"
                    },
                    {
                        "name": "Xinyao Jiao"
                    },
                    {
                        "name": "Dingrong Pan"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Ning Wang"
                    },
                    {
                        "name": "Weiwei Shan"
                    },
                    {
                        "name": "Xinwei Fang"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Nan Guan"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16232v1",
                "updated": "2024-11-25T09:46:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    46,
                    41,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T09:46:41Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    46,
                    41,
                    0,
                    330,
                    0
                ],
                "title": "Poster: Could Large Language Models Perform Network Management?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Poster: Could Large Language Models Perform Network Management?"
                },
                "summary": "Modern wireless communication systems have become increasingly complex due to\nthe proliferation of wireless devices, increasing performance standards, and\ngrowing security threats. Managing these networks is becoming more challenging,\nrequiring the use of advanced network management methods and tools. AI-driven\nnetwork management systems such as Self-Optimizing Networks (SONs) are gaining\nattention. On the other hand, Large Language Models (LLMs) have been\ndemonstrating exceptional zero-shot learning and generalization capabilities\nacross several domains. In this paper, we leverage the potential of LLMs with\nSONs to enhance future network management systems. Specifically, we benchmark\nthe use of various LLMs such as GPT-4, Llama, and Falcon, in a zero-shot\nsetting based on their real-time network configuration recommendations. Our\nresults indicate promising prospects for integrating LLMs into future network\nmanagement systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern wireless communication systems have become increasingly complex due to\nthe proliferation of wireless devices, increasing performance standards, and\ngrowing security threats. Managing these networks is becoming more challenging,\nrequiring the use of advanced network management methods and tools. AI-driven\nnetwork management systems such as Self-Optimizing Networks (SONs) are gaining\nattention. On the other hand, Large Language Models (LLMs) have been\ndemonstrating exceptional zero-shot learning and generalization capabilities\nacross several domains. In this paper, we leverage the potential of LLMs with\nSONs to enhance future network management systems. Specifically, we benchmark\nthe use of various LLMs such as GPT-4, Llama, and Falcon, in a zero-shot\nsetting based on their real-time network configuration recommendations. Our\nresults indicate promising prospects for integrating LLMs into future network\nmanagement systems."
                },
                "authors": [
                    {
                        "name": "Zine el abidine Kherroubi"
                    },
                    {
                        "name": "Monika Prakash"
                    },
                    {
                        "name": "Jean-Pierre Giacalone"
                    },
                    {
                        "name": "Michael Baddeley"
                    }
                ],
                "author_detail": {
                    "name": "Michael Baddeley"
                },
                "author": "Michael Baddeley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16213v1",
                "updated": "2024-11-25T09:22:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    22,
                    13,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T09:22:13Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    22,
                    13,
                    0,
                    330,
                    0
                ],
                "title": "SAVEn-Vid: Synergistic Audio-Visual Integration for Enhanced\n  Understanding in Long Video Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAVEn-Vid: Synergistic Audio-Visual Integration for Enhanced\n  Understanding in Long Video Context"
                },
                "summary": "Endeavors have been made to explore Large Language Models for video analysis\n(Video-LLMs), particularly in understanding and interpreting long videos.\nHowever, existing Video-LLMs still face challenges in effectively integrating\nthe rich and diverse audio-visual information inherent in long videos, which is\ncrucial for comprehensive understanding. This raises the question: how can we\nleverage embedded audio-visual information to enhance long video understanding?\nTherefore, (i) we introduce SAVEn-Vid, the first-ever long audio-visual video\ndataset comprising over 58k audio-visual instructions. (ii) From the model\nperspective, we propose a time-aware Audio-Visual Large Language Model\n(AV-LLM), SAVEnVideo, fine-tuned on SAVEn-Vid. (iii) Besides, we present\nAVBench, a benchmark containing 2,500 QAs designed to evaluate models on\nenhanced audio-visual comprehension tasks within long video, challenging their\nability to handle intricate audio-visual interactions. Experiments on AVBench\nreveal the limitations of current AV-LLMs. Experiments also demonstrate that\nSAVEnVideo outperforms the best Video-LLM by 3.61% on the zero-shot long video\ntask (Video-MME) and surpasses the leading audio-visual LLM by 1.29% on the\nzero-shot audio-visual task (Music-AVQA). Consequently, at the 7B parameter\nscale, SAVEnVideo can achieve state-of-the-art performance. Our dataset and\ncode will be released at https://ljungang.github.io/SAVEn-Vid/ upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Endeavors have been made to explore Large Language Models for video analysis\n(Video-LLMs), particularly in understanding and interpreting long videos.\nHowever, existing Video-LLMs still face challenges in effectively integrating\nthe rich and diverse audio-visual information inherent in long videos, which is\ncrucial for comprehensive understanding. This raises the question: how can we\nleverage embedded audio-visual information to enhance long video understanding?\nTherefore, (i) we introduce SAVEn-Vid, the first-ever long audio-visual video\ndataset comprising over 58k audio-visual instructions. (ii) From the model\nperspective, we propose a time-aware Audio-Visual Large Language Model\n(AV-LLM), SAVEnVideo, fine-tuned on SAVEn-Vid. (iii) Besides, we present\nAVBench, a benchmark containing 2,500 QAs designed to evaluate models on\nenhanced audio-visual comprehension tasks within long video, challenging their\nability to handle intricate audio-visual interactions. Experiments on AVBench\nreveal the limitations of current AV-LLMs. Experiments also demonstrate that\nSAVEnVideo outperforms the best Video-LLM by 3.61% on the zero-shot long video\ntask (Video-MME) and surpasses the leading audio-visual LLM by 1.29% on the\nzero-shot audio-visual task (Music-AVQA). Consequently, at the 7B parameter\nscale, SAVEnVideo can achieve state-of-the-art performance. Our dataset and\ncode will be released at https://ljungang.github.io/SAVEn-Vid/ upon acceptance."
                },
                "authors": [
                    {
                        "name": "Jungang Li"
                    },
                    {
                        "name": "Sicheng Tao"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Xiaojie Gu"
                    },
                    {
                        "name": "Haodong Xu"
                    },
                    {
                        "name": "Xu Zheng"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16205v1",
                "updated": "2024-11-25T09:05:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    5,
                    36,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T09:05:36Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    5,
                    36,
                    0,
                    330,
                    0
                ],
                "title": "MH-MoE:Multi-Head Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MH-MoE:Multi-Head Mixture-of-Experts"
                },
                "summary": "Multi-Head Mixture-of-Experts (MH-MoE) demonstrates superior performance by\nusing the multi-head mechanism to collectively attend to information from\nvarious representation spaces within different experts. In this paper, we\npresent a novel implementation of MH-MoE that maintains both FLOPs and\nparameter parity with sparse Mixture of Experts models. Experimental results on\nlanguage models show that the new implementation yields quality improvements\nover both vanilla MoE and fine-grained MoE models. Additionally, our\nexperiments demonstrate that MH-MoE is compatible with 1-bit Large Language\nModels (LLMs) such as BitNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head Mixture-of-Experts (MH-MoE) demonstrates superior performance by\nusing the multi-head mechanism to collectively attend to information from\nvarious representation spaces within different experts. In this paper, we\npresent a novel implementation of MH-MoE that maintains both FLOPs and\nparameter parity with sparse Mixture of Experts models. Experimental results on\nlanguage models show that the new implementation yields quality improvements\nover both vanilla MoE and fine-grained MoE models. Additionally, our\nexperiments demonstrate that MH-MoE is compatible with 1-bit Large Language\nModels (LLMs) such as BitNet."
                },
                "authors": [
                    {
                        "name": "Shaohan Huang"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "7 pages, 0 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16196v1",
                "updated": "2024-11-25T08:52:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    52,
                    46,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T08:52:46Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    52,
                    46,
                    0,
                    330,
                    0
                ],
                "title": "Learn from Foundation Model: Fruit Detection Model without Manual\n  Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learn from Foundation Model: Fruit Detection Model without Manual\n  Annotation"
                },
                "summary": "Recent breakthroughs in large foundation models have enabled the possibility\nof transferring knowledge pre-trained on vast datasets to domains with limited\ndata availability. Agriculture is one of the domains that lacks sufficient\ndata. This study proposes a framework to train effective, domain-specific,\nsmall models from foundation models without manual annotation. Our approach\nbegins with SDM (Segmentation-Description-Matching), a stage that leverages two\nfoundation models: SAM2 (Segment Anything in Images and Videos) for\nsegmentation and OpenCLIP (Open Contrastive Language-Image Pretraining) for\nzero-shot open-vocabulary classification. In the second stage, a novel\nknowledge distillation mechanism is utilized to distill compact,\nedge-deployable models from SDM, enhancing both inference speed and perception\naccuracy. The complete method, termed SDM-D\n(Segmentation-Description-Matching-Distilling), demonstrates strong performance\nacross various fruit detection tasks object detection, semantic segmentation,\nand instance segmentation) without manual annotation. It nearly matches the\nperformance of models trained with abundant labels. Notably, SDM-D outperforms\nopen-set detection methods such as Grounding SAM and YOLO-World on all tested\nfruit detection datasets. Additionally, we introduce MegaFruits, a\ncomprehensive fruit segmentation dataset encompassing over 25,000 images, and\nall code and datasets are made publicly available at\nhttps://github.com/AgRoboticsResearch/SDM-D.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in large foundation models have enabled the possibility\nof transferring knowledge pre-trained on vast datasets to domains with limited\ndata availability. Agriculture is one of the domains that lacks sufficient\ndata. This study proposes a framework to train effective, domain-specific,\nsmall models from foundation models without manual annotation. Our approach\nbegins with SDM (Segmentation-Description-Matching), a stage that leverages two\nfoundation models: SAM2 (Segment Anything in Images and Videos) for\nsegmentation and OpenCLIP (Open Contrastive Language-Image Pretraining) for\nzero-shot open-vocabulary classification. In the second stage, a novel\nknowledge distillation mechanism is utilized to distill compact,\nedge-deployable models from SDM, enhancing both inference speed and perception\naccuracy. The complete method, termed SDM-D\n(Segmentation-Description-Matching-Distilling), demonstrates strong performance\nacross various fruit detection tasks object detection, semantic segmentation,\nand instance segmentation) without manual annotation. It nearly matches the\nperformance of models trained with abundant labels. Notably, SDM-D outperforms\nopen-set detection methods such as Grounding SAM and YOLO-World on all tested\nfruit detection datasets. Additionally, we introduce MegaFruits, a\ncomprehensive fruit segmentation dataset encompassing over 25,000 images, and\nall code and datasets are made publicly available at\nhttps://github.com/AgRoboticsResearch/SDM-D.git."
                },
                "authors": [
                    {
                        "name": "Yanan Wang"
                    },
                    {
                        "name": "Zhenghao Fei"
                    },
                    {
                        "name": "Ruichen Li"
                    },
                    {
                        "name": "Yibin Ying"
                    }
                ],
                "author_detail": {
                    "name": "Yibin Ying"
                },
                "author": "Yibin Ying",
                "arxiv_comment": "17 pages, 12 figures, conference or other essential info",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04269v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04269v2",
                "updated": "2024-11-25T08:44:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    44,
                    32,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-06T21:22:46Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    21,
                    22,
                    46,
                    2,
                    311,
                    0
                ],
                "title": "Increasing the scalability of graph convolution for FPGA-implemented\n  event-based vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Increasing the scalability of graph convolution for FPGA-implemented\n  event-based vision"
                },
                "summary": "Event cameras are becoming increasingly popular as an alternative to\ntraditional frame-based vision sensors, especially in mobile robotics. Taking\nfull advantage of their high temporal resolution, high dynamic range, low power\nconsumption and sparsity of event data, which only reflects changes in the\nobserved scene, requires both an efficient algorithm and a specialised hardware\nplatform. A recent trend involves using Graph Convolutional Neural Networks\n(GCNNs) implemented on a heterogeneous SoC FPGA. In this paper we focus on\noptimising hardware modules for graph convolution to allow flexible selection\nof the FPGA resource (BlockRAM, DSP and LUT) for their implementation. We\npropose a ''two-step convolution'' approach that utilises additional BRAM\nbuffers in order to reduce up to 94% of LUT usage for multiplications. This\nmethod significantly improves the scalability of GCNNs, enabling the deployment\nof models with more layers, larger graphs sizes and their application for more\ndynamic scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event cameras are becoming increasingly popular as an alternative to\ntraditional frame-based vision sensors, especially in mobile robotics. Taking\nfull advantage of their high temporal resolution, high dynamic range, low power\nconsumption and sparsity of event data, which only reflects changes in the\nobserved scene, requires both an efficient algorithm and a specialised hardware\nplatform. A recent trend involves using Graph Convolutional Neural Networks\n(GCNNs) implemented on a heterogeneous SoC FPGA. In this paper we focus on\noptimising hardware modules for graph convolution to allow flexible selection\nof the FPGA resource (BlockRAM, DSP and LUT) for their implementation. We\npropose a ''two-step convolution'' approach that utilises additional BRAM\nbuffers in order to reduce up to 94% of LUT usage for multiplications. This\nmethod significantly improves the scalability of GCNNs, enabling the deployment\nof models with more layers, larger graphs sizes and their application for more\ndynamic scenarios."
                },
                "authors": [
                    {
                        "name": "Piotr Wzorek"
                    },
                    {
                        "name": "Kamil Jeziorek"
                    },
                    {
                        "name": "Tomasz Kryjak"
                    },
                    {
                        "name": "Andrea Pinna"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Pinna"
                },
                "author": "Andrea Pinna",
                "arxiv_comment": "Accepted for the PhD forum during FPT 2024 (International Conference\n  on Field Programmable Technology), 10-12 December 2024, Sydney, Australia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04269v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04269v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16189v1",
                "updated": "2024-11-25T08:42:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    42,
                    33,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T08:42:33Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    42,
                    33,
                    0,
                    330,
                    0
                ],
                "title": "Enhancing Multi-Agent Consensus through Third-Party LLM Integration:\n  Analyzing Uncertainty and Mitigating Hallucinations in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Multi-Agent Consensus through Third-Party LLM Integration:\n  Analyzing Uncertainty and Mitigating Hallucinations in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) still face challenges when dealing with complex\nreasoning tasks, often resulting in hallucinations, which limit the practical\napplication of LLMs. To alleviate this issue, this paper proposes a new method\nthat integrates different LLMs to expand the knowledge boundary, reduce\ndependence on a single model, and promote in-depth debate among agents. The\nmain contributions include: 1) Introducing third-party LLMs to adjust the\nattention weights of agents through uncertainty estimation and confidence\nanalysis, optimizing consensus formation in multi-agent systems; 2) Experiments\non arithmetic datasets have validated the effectiveness of the method,\nsurpassing traditional multi-agent baselines. This research provides a new\nperspective for large models to alleviate hallucination phenomena when dealing\nwith complex tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) still face challenges when dealing with complex\nreasoning tasks, often resulting in hallucinations, which limit the practical\napplication of LLMs. To alleviate this issue, this paper proposes a new method\nthat integrates different LLMs to expand the knowledge boundary, reduce\ndependence on a single model, and promote in-depth debate among agents. The\nmain contributions include: 1) Introducing third-party LLMs to adjust the\nattention weights of agents through uncertainty estimation and confidence\nanalysis, optimizing consensus formation in multi-agent systems; 2) Experiments\non arithmetic datasets have validated the effectiveness of the method,\nsurpassing traditional multi-agent baselines. This research provides a new\nperspective for large models to alleviate hallucination phenomena when dealing\nwith complex tasks."
                },
                "authors": [
                    {
                        "name": "Zhihua Duan"
                    },
                    {
                        "name": "Jialin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jialin Wang"
                },
                "author": "Jialin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07745v2",
                "updated": "2024-11-25T08:38:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    38,
                    49,
                    0,
                    330,
                    0
                ],
                "published": "2024-10-10T09:23:26Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    9,
                    23,
                    26,
                    3,
                    284,
                    0
                ],
                "title": "StepTool: A Step-grained Reinforcement Learning Framework for Tool\n  Learning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StepTool: A Step-grained Reinforcement Learning Framework for Tool\n  Learning in LLMs"
                },
                "summary": "Despite having powerful reasoning and inference capabilities, Large Language\nModels (LLMs) still need external tools to acquire real-time information\nretrieval or domain-specific expertise to solve complex tasks, which is\nreferred to as tool learning. Existing tool learning methods primarily rely on\ntuning with expert trajectories, focusing on token-sequence learning from a\nlinguistic perspective. However, there are several challenges: 1) imitating\nstatic trajectories limits their ability to generalize to new tasks. 2) even\nexpert trajectories can be suboptimal, and better solution paths may exist. In\nthis work, we introduce StepTool, a novel step-grained reinforcement learning\nframework to improve tool learning in LLMs. It consists of two components:\nStep-grained Reward Shaping, which assigns rewards at each tool interaction\nbased on tool invocation success and its contribution to the task, and\nStep-grained Optimization, which uses policy gradient methods to optimize the\nmodel in a multi-step manner. Experimental results demonstrate that StepTool\nsignificantly outperforms existing methods in multi-step, tool-based tasks,\nproviding a robust solution for complex task environments. Codes are available\nat https://github.com/yuyq18/StepTool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite having powerful reasoning and inference capabilities, Large Language\nModels (LLMs) still need external tools to acquire real-time information\nretrieval or domain-specific expertise to solve complex tasks, which is\nreferred to as tool learning. Existing tool learning methods primarily rely on\ntuning with expert trajectories, focusing on token-sequence learning from a\nlinguistic perspective. However, there are several challenges: 1) imitating\nstatic trajectories limits their ability to generalize to new tasks. 2) even\nexpert trajectories can be suboptimal, and better solution paths may exist. In\nthis work, we introduce StepTool, a novel step-grained reinforcement learning\nframework to improve tool learning in LLMs. It consists of two components:\nStep-grained Reward Shaping, which assigns rewards at each tool interaction\nbased on tool invocation success and its contribution to the task, and\nStep-grained Optimization, which uses policy gradient methods to optimize the\nmodel in a multi-step manner. Experimental results demonstrate that StepTool\nsignificantly outperforms existing methods in multi-step, tool-based tasks,\nproviding a robust solution for complex task environments. Codes are available\nat https://github.com/yuyq18/StepTool."
                },
                "authors": [
                    {
                        "name": "Yuanqing Yu"
                    },
                    {
                        "name": "Zhefan Wang"
                    },
                    {
                        "name": "Weizhi Ma"
                    },
                    {
                        "name": "Zhicheng Guo"
                    },
                    {
                        "name": "Jingtao Zhan"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Chuhan Wu"
                    },
                    {
                        "name": "Zhiqiang Guo"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Ongoning Work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02801v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02801v3",
                "updated": "2024-11-25T08:32:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    32,
                    13,
                    0,
                    330,
                    0
                ],
                "published": "2024-05-05T03:15:52Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    3,
                    15,
                    52,
                    6,
                    126,
                    0
                ],
                "title": "Mozart's Touch: A Lightweight Multi-modal Music Generation Framework\n  Based on Pre-Trained Large Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mozart's Touch: A Lightweight Multi-modal Music Generation Framework\n  Based on Pre-Trained Large Models"
                },
                "summary": "In recent years, AI-Generated Content (AIGC) has witnessed rapid\nadvancements, facilitating the creation of music, images, and other artistic\nforms across a wide range of industries. However, current models for image- and\nvideo-to-music synthesis struggle to capture the nuanced emotions and\natmosphere conveyed by visual content. To fill this gap, we propose Mozart's\nTouch, a multi-modal music generation framework capable of generating music\naligned with cross-modal inputs such as images, videos, and text. The framework\nconsists of three key components: Multi-modal Captioning Module, Large Language\nModel (LLM) understanding \\& Bridging Module, and Music Generation Module.\nUnlike traditional end-to-end methods, Mozart's Touch uses LLMs to accurately\ninterpret visual elements without requiring the training or fine-tuning of\nmusic generation models, providing efficiency and transparency through clear,\ninterpretable prompts. We also introduce the \"LLM-Bridge\" method to resolve the\nheterogeneous representation challenges between descriptive texts from\ndifferent modalities. Through a series of objective and subjective evaluations,\nwe demonstrate that Mozart's Touch outperforms current state-of-the-art models.\nOur code and examples are available at\nhttps://github.com/TiffanyBlews/MozartsTouch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, AI-Generated Content (AIGC) has witnessed rapid\nadvancements, facilitating the creation of music, images, and other artistic\nforms across a wide range of industries. However, current models for image- and\nvideo-to-music synthesis struggle to capture the nuanced emotions and\natmosphere conveyed by visual content. To fill this gap, we propose Mozart's\nTouch, a multi-modal music generation framework capable of generating music\naligned with cross-modal inputs such as images, videos, and text. The framework\nconsists of three key components: Multi-modal Captioning Module, Large Language\nModel (LLM) understanding \\& Bridging Module, and Music Generation Module.\nUnlike traditional end-to-end methods, Mozart's Touch uses LLMs to accurately\ninterpret visual elements without requiring the training or fine-tuning of\nmusic generation models, providing efficiency and transparency through clear,\ninterpretable prompts. We also introduce the \"LLM-Bridge\" method to resolve the\nheterogeneous representation challenges between descriptive texts from\ndifferent modalities. Through a series of objective and subjective evaluations,\nwe demonstrate that Mozart's Touch outperforms current state-of-the-art models.\nOur code and examples are available at\nhttps://github.com/TiffanyBlews/MozartsTouch."
                },
                "authors": [
                    {
                        "name": "Jiajun Li"
                    },
                    {
                        "name": "Tianze Xu"
                    },
                    {
                        "name": "Xuesong Chen"
                    },
                    {
                        "name": "Xinrui Yao"
                    },
                    {
                        "name": "Shuchang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shuchang Liu"
                },
                "author": "Shuchang Liu",
                "arxiv_comment": "10 pages, 2 figures, submitted to AIGC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02801v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02801v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14303v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14303v2",
                "updated": "2024-11-25T08:31:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    31,
                    0,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-21T16:56:33Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    56,
                    33,
                    3,
                    326,
                    0
                ],
                "title": "BugSpotter: Automated Generation of Code Debugging Exercises",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BugSpotter: Automated Generation of Code Debugging Exercises"
                },
                "summary": "Debugging is an essential skill when learning to program, yet its instruction\nand emphasis often vary widely across introductory courses. In the era of\ncode-generating large language models (LLMs), the ability for students to\nreason about code and identify errors is increasingly important. However,\nstudents frequently resort to trial-and-error methods to resolve bugs without\nfully understanding the underlying issues. Developing the ability to identify\nand hypothesize the cause of bugs is crucial but can be time-consuming to teach\neffectively through traditional means. This paper introduces BugSpotter, an\ninnovative tool that leverages an LLM to generate buggy code from a problem\ndescription and verify the synthesized bugs via a test suite. Students interact\nwith BugSpotter by designing failing test cases, where the buggy code's output\ndiffers from the expected result as defined by the problem specification. This\nnot only provides opportunities for students to enhance their debugging skills,\nbut also to practice reading and understanding problem specifications. We\ndeployed BugSpotter in a large classroom setting and compared the debugging\nexercises it generated to exercises hand-crafted by an instructor for the same\nproblems. We found that the LLM-generated exercises produced by BugSpotter\nvaried in difficulty and were well-matched to the problem specifications.\nImportantly, the LLM-generated exercises were comparable to those manually\ncreated by instructors with respect to student performance, suggesting that\nBugSpotter could be an effective and efficient aid for learning debugging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debugging is an essential skill when learning to program, yet its instruction\nand emphasis often vary widely across introductory courses. In the era of\ncode-generating large language models (LLMs), the ability for students to\nreason about code and identify errors is increasingly important. However,\nstudents frequently resort to trial-and-error methods to resolve bugs without\nfully understanding the underlying issues. Developing the ability to identify\nand hypothesize the cause of bugs is crucial but can be time-consuming to teach\neffectively through traditional means. This paper introduces BugSpotter, an\ninnovative tool that leverages an LLM to generate buggy code from a problem\ndescription and verify the synthesized bugs via a test suite. Students interact\nwith BugSpotter by designing failing test cases, where the buggy code's output\ndiffers from the expected result as defined by the problem specification. This\nnot only provides opportunities for students to enhance their debugging skills,\nbut also to practice reading and understanding problem specifications. We\ndeployed BugSpotter in a large classroom setting and compared the debugging\nexercises it generated to exercises hand-crafted by an instructor for the same\nproblems. We found that the LLM-generated exercises produced by BugSpotter\nvaried in difficulty and were well-matched to the problem specifications.\nImportantly, the LLM-generated exercises were comparable to those manually\ncreated by instructors with respect to student performance, suggesting that\nBugSpotter could be an effective and efficient aid for learning debugging."
                },
                "authors": [
                    {
                        "name": "Victor-Alexandru Pdurean"
                    },
                    {
                        "name": "Paul Denny"
                    },
                    {
                        "name": "Adish Singla"
                    }
                ],
                "author_detail": {
                    "name": "Adish Singla"
                },
                "author": "Adish Singla",
                "arxiv_comment": "Preprint of the SIGCSE'25 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14303v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14303v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04449v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04449v4",
                "updated": "2024-11-25T08:18:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    18,
                    1,
                    0,
                    330,
                    0
                ],
                "published": "2024-08-08T13:19:37Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    13,
                    19,
                    37,
                    3,
                    221,
                    0
                ],
                "title": "EAIRiskBench: Towards Evaluating Physical Risk Awareness for Task\n  Planning of Foundation Model-based Embodied AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EAIRiskBench: Towards Evaluating Physical Risk Awareness for Task\n  Planning of Foundation Model-based Embodied AI Agents"
                },
                "summary": "Embodied artificial intelligence (EAI) integrates advanced AI models into\nphysical entities for real-world interaction. The emergence of foundation\nmodels as the \"brain\" of EAI agents for high-level task planning has shown\npromising results. However, the deployment of these agents in physical\nenvironments presents significant safety challenges. For instance, a\nhousekeeping robot lacking sufficient risk awareness might place a metal\ncontainer in a microwave, potentially causing a fire. To address these critical\nsafety concerns, comprehensive pre-deployment risk assessments are imperative.\nThis study introduces EAIRiskBench, a novel framework for automated physical\nrisk assessment in EAI scenarios. EAIRiskBench employs a multi-agent\ncooperative system that leverages various foundation models to generate safety\nguidelines, create risk-prone scenarios, make task planning, and evaluate\nsafety systematically. Utilizing this framework, we construct EAIRiskDataset,\ncomprising diverse test cases across various domains, encompassing both textual\nand visual scenarios. Our comprehensive evaluation of state-of-the-art\nfoundation models reveals alarming results: all models exhibit high task risk\nrates (TRR), with an average of 95.75% across all evaluated models. To address\nthese challenges, we further propose two prompting-based risk mitigation\nstrategies. While these strategies demonstrate some efficacy in reducing TRR,\nthe improvements are limited, still indicating substantial safety concerns.\nThis study provides the first large-scale assessment of physical risk awareness\nin EAI agents. Our findings underscore the critical need for enhanced safety\nmeasures in EAI systems and provide valuable insights for future research\ndirections in developing safer embodied artificial intelligence system. Data\nand code are available at https://github.com/zihao-ai/EAIRiskBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied artificial intelligence (EAI) integrates advanced AI models into\nphysical entities for real-world interaction. The emergence of foundation\nmodels as the \"brain\" of EAI agents for high-level task planning has shown\npromising results. However, the deployment of these agents in physical\nenvironments presents significant safety challenges. For instance, a\nhousekeeping robot lacking sufficient risk awareness might place a metal\ncontainer in a microwave, potentially causing a fire. To address these critical\nsafety concerns, comprehensive pre-deployment risk assessments are imperative.\nThis study introduces EAIRiskBench, a novel framework for automated physical\nrisk assessment in EAI scenarios. EAIRiskBench employs a multi-agent\ncooperative system that leverages various foundation models to generate safety\nguidelines, create risk-prone scenarios, make task planning, and evaluate\nsafety systematically. Utilizing this framework, we construct EAIRiskDataset,\ncomprising diverse test cases across various domains, encompassing both textual\nand visual scenarios. Our comprehensive evaluation of state-of-the-art\nfoundation models reveals alarming results: all models exhibit high task risk\nrates (TRR), with an average of 95.75% across all evaluated models. To address\nthese challenges, we further propose two prompting-based risk mitigation\nstrategies. While these strategies demonstrate some efficacy in reducing TRR,\nthe improvements are limited, still indicating substantial safety concerns.\nThis study provides the first large-scale assessment of physical risk awareness\nin EAI agents. Our findings underscore the critical need for enhanced safety\nmeasures in EAI systems and provide valuable insights for future research\ndirections in developing safer embodied artificial intelligence system. Data\nand code are available at https://github.com/zihao-ai/EAIRiskBench."
                },
                "authors": [
                    {
                        "name": "Zihao Zhu"
                    },
                    {
                        "name": "Bingzhe Wu"
                    },
                    {
                        "name": "Zhengyou Zhang"
                    },
                    {
                        "name": "Lei Han"
                    },
                    {
                        "name": "Baoyuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Baoyuan Wu"
                },
                "author": "Baoyuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04449v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04449v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07965v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07965v2",
                "updated": "2024-11-25T08:13:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    13,
                    0,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-12T17:41:16Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    17,
                    41,
                    16,
                    1,
                    317,
                    0
                ],
                "title": "From General to Specific: Utilizing General Hallucination to Benchmark\n  Specific Role-Playing Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From General to Specific: Utilizing General Hallucination to Benchmark\n  Specific Role-Playing Agents"
                },
                "summary": "The advanced role-playing capabilities of Large Language Models (LLMs) have\npaved the way for developing Role-Playing Agents (RPAs). However, existing\nbenchmarks in this domain, such as HPD and SocialBench face limitations like\npoor generalizability, implicit and inaccurate judgments, and the risk of model\nforgetting. To address the above issues, we propose an automatic, scalable, and\ngeneralizable paradigm. Specifically, we construct a benchmark, SHARP, by\nextracting relations from a general knowledge graph and leveraging the inherent\nhallucination properties of RPAs to simulate interactions across roles. We\nemploy ChatGPT for stance detection and define relationship hallucination along\nwith three related metrics based on stance transfer. Extensive experiments\nvalidate the effectiveness and stability of our paradigm. Our findings further\nexplore the factors influencing these metrics and discuss the trade-off between\nblind loyalty to relationships and adherence to facts in RPAs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advanced role-playing capabilities of Large Language Models (LLMs) have\npaved the way for developing Role-Playing Agents (RPAs). However, existing\nbenchmarks in this domain, such as HPD and SocialBench face limitations like\npoor generalizability, implicit and inaccurate judgments, and the risk of model\nforgetting. To address the above issues, we propose an automatic, scalable, and\ngeneralizable paradigm. Specifically, we construct a benchmark, SHARP, by\nextracting relations from a general knowledge graph and leveraging the inherent\nhallucination properties of RPAs to simulate interactions across roles. We\nemploy ChatGPT for stance detection and define relationship hallucination along\nwith three related metrics based on stance transfer. Extensive experiments\nvalidate the effectiveness and stability of our paradigm. Our findings further\nexplore the factors influencing these metrics and discuss the trade-off between\nblind loyalty to relationships and adherence to facts in RPAs."
                },
                "authors": [
                    {
                        "name": "Chuyi Kong"
                    },
                    {
                        "name": "Ziyang Luo"
                    },
                    {
                        "name": "Hongzhan Lin"
                    },
                    {
                        "name": "Zhiyuan Fan"
                    },
                    {
                        "name": "Yaxin Fan"
                    },
                    {
                        "name": "Yuxi Sun"
                    },
                    {
                        "name": "Jing Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jing Ma"
                },
                "author": "Jing Ma",
                "arxiv_comment": "Revise three typos in the abstract and methodology sections of the\n  introduction",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07965v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07965v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16173v1",
                "updated": "2024-11-25T08:04:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    4,
                    47,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T08:04:47Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    4,
                    47,
                    0,
                    330,
                    0
                ],
                "title": "SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval\n  and Routing in Long-Form Video Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval\n  and Routing in Long-Form Video Analysis"
                },
                "summary": "Despite advances in Large Multi-modal Models, applying them to long and\nuntrimmed video content remains challenging due to limitations in context\nlength and substantial memory overhead. These constraints often lead to\nsignificant information loss and reduced relevance in the model responses. With\nthe exponential growth of video data across web platforms, understanding\nlong-form video is crucial for advancing generalized intelligence. In this\npaper, we introduce SALOVA: Segment-Augmented LOng Video Assistant, a novel\nvideo-LLM framework designed to enhance the comprehension of lengthy video\ncontent through targeted retrieval process. We address two main challenges to\nachieve it: (i) We present the SceneWalk dataset, a high-quality collection of\n87.8K long videos, each densely captioned at the segment level to enable models\nto capture scene continuity and maintain rich descriptive context. (ii) We\ndevelop robust architectural designs integrating dynamic routing mechanism and\nspatio-temporal projector to efficiently retrieve and process relevant video\nsegments based on user queries. Our framework mitigates the limitations of\ncurrent video-LMMs by allowing for precise identification and retrieval of\nrelevant video segments in response to queries, thereby improving the\ncontextual relevance of the generated responses. Through extensive experiments,\nSALOVA demonstrates enhanced capability in processing complex long-form videos,\nshowing significant capability to maintain contextual integrity across extended\nsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advances in Large Multi-modal Models, applying them to long and\nuntrimmed video content remains challenging due to limitations in context\nlength and substantial memory overhead. These constraints often lead to\nsignificant information loss and reduced relevance in the model responses. With\nthe exponential growth of video data across web platforms, understanding\nlong-form video is crucial for advancing generalized intelligence. In this\npaper, we introduce SALOVA: Segment-Augmented LOng Video Assistant, a novel\nvideo-LLM framework designed to enhance the comprehension of lengthy video\ncontent through targeted retrieval process. We address two main challenges to\nachieve it: (i) We present the SceneWalk dataset, a high-quality collection of\n87.8K long videos, each densely captioned at the segment level to enable models\nto capture scene continuity and maintain rich descriptive context. (ii) We\ndevelop robust architectural designs integrating dynamic routing mechanism and\nspatio-temporal projector to efficiently retrieve and process relevant video\nsegments based on user queries. Our framework mitigates the limitations of\ncurrent video-LMMs by allowing for precise identification and retrieval of\nrelevant video segments in response to queries, thereby improving the\ncontextual relevance of the generated responses. Through extensive experiments,\nSALOVA demonstrates enhanced capability in processing complex long-form videos,\nshowing significant capability to maintain contextual integrity across extended\nsequences."
                },
                "authors": [
                    {
                        "name": "Junho Kim"
                    },
                    {
                        "name": "Hyunjun Kim"
                    },
                    {
                        "name": "Hosu Lee"
                    },
                    {
                        "name": "Yong Man Ro"
                    }
                ],
                "author_detail": {
                    "name": "Yong Man Ro"
                },
                "author": "Yong Man Ro",
                "arxiv_comment": "Project page: https://ivy-lvlm.github.io/SALOVA/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16159v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16159v1",
                "updated": "2024-11-25T07:35:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    7,
                    35,
                    56,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T07:35:56Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    7,
                    35,
                    56,
                    0,
                    330,
                    0
                ],
                "title": "Static and Dynamic Routing, Fiber, Modulation Format, and Spectrum\n  Allocation in Hybrid ULL Fiber-SSMF Elastic Optical Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static and Dynamic Routing, Fiber, Modulation Format, and Spectrum\n  Allocation in Hybrid ULL Fiber-SSMF Elastic Optical Networks"
                },
                "summary": "Traditional standard single-mode fibers (SSMF) are unable to satisfy the\nfuture long-distance and high-speed optical channel transmission requirement\ndue to their relatively large signal losses. To address this issue, the\nultra-low loss and large effective area (ULL) fibers are successfully\nmanufactured and expected to deployed in the existing optical networks. For\nsuch ULL fiber deployment, network operators prefer adding ULL fibers to each\nlink rather than replace existing SSMFs, resulting in a scenario where both of\nSSMF and ULL fiber coexist on the same link. In this paper, we investigated the\nrouting, fiber, modulation format, and spectrum allocation (RFMSA) problem in\nthe context of an elastic optical network (EON) where ULL fiber and SSMF\ncoexisting on each link under both the static and dynamic traffic demands. We\nformulated this RFMSA problem as a node-arc based Mixed Integer Linear\nProgramming (MILP) model and developed Spectrum Window Plane (SWP)-based\nheuristic algorithms based on different fiber selection strategies, including\nspectrum usage based (SU), optical signal-to-noise ratio (OSNR) aware, ULL\nfiber first (UFF), and random strategies. Simulation results show that in the\nstatic traffic demand situation, the RFMSA algorithm based on the OSNR-aware\n(OA) strategy exhibits optimal performance, attaining a performance similar to\nthat of the MILP model regarding the maximum number of frequency slots (FSs)\nused in the entire network. Moreover, in the dynamic traffic demand scenario,\nthe SU strategy remarkably surpasses the other strategies in terms of the\nlightpath blocking probability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional standard single-mode fibers (SSMF) are unable to satisfy the\nfuture long-distance and high-speed optical channel transmission requirement\ndue to their relatively large signal losses. To address this issue, the\nultra-low loss and large effective area (ULL) fibers are successfully\nmanufactured and expected to deployed in the existing optical networks. For\nsuch ULL fiber deployment, network operators prefer adding ULL fibers to each\nlink rather than replace existing SSMFs, resulting in a scenario where both of\nSSMF and ULL fiber coexist on the same link. In this paper, we investigated the\nrouting, fiber, modulation format, and spectrum allocation (RFMSA) problem in\nthe context of an elastic optical network (EON) where ULL fiber and SSMF\ncoexisting on each link under both the static and dynamic traffic demands. We\nformulated this RFMSA problem as a node-arc based Mixed Integer Linear\nProgramming (MILP) model and developed Spectrum Window Plane (SWP)-based\nheuristic algorithms based on different fiber selection strategies, including\nspectrum usage based (SU), optical signal-to-noise ratio (OSNR) aware, ULL\nfiber first (UFF), and random strategies. Simulation results show that in the\nstatic traffic demand situation, the RFMSA algorithm based on the OSNR-aware\n(OA) strategy exhibits optimal performance, attaining a performance similar to\nthat of the MILP model regarding the maximum number of frequency slots (FSs)\nused in the entire network. Moreover, in the dynamic traffic demand scenario,\nthe SU strategy remarkably surpasses the other strategies in terms of the\nlightpath blocking probability."
                },
                "authors": [
                    {
                        "name": "Kangao Ouyang"
                    },
                    {
                        "name": "Fengxian Tang"
                    },
                    {
                        "name": "Zhilin Yuan"
                    },
                    {
                        "name": "Jun Li"
                    },
                    {
                        "name": "Yongcheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongcheng Li"
                },
                "author": "Yongcheng Li",
                "arxiv_comment": "12 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16159v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16158v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16158v1",
                "updated": "2024-11-25T07:34:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    7,
                    34,
                    53,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T07:34:53Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    7,
                    34,
                    53,
                    0,
                    330,
                    0
                ],
                "title": "MixPE: Quantization and Hardware Co-design for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MixPE: Quantization and Hardware Co-design for Efficient LLM Inference"
                },
                "summary": "Transformer-based large language models (LLMs) have achieved remarkable\nsuccess as model sizes continue to grow, yet their deployment remains\nchallenging due to significant computational and memory demands. Quantization\nhas emerged as a promising solution, and state-of-the-art quantization\nalgorithms for LLMs introduce the need for mixed-precision matrix\nmultiplication (mpGEMM), where lower-precision weights are multiplied with\nhigher-precision activations. Despite its benefits, current hardware\naccelerators such as GPUs and TPUs lack native support for efficient mpGEMM,\nleading to inefficient dequantization operations in the main sequential loop.\nTo address this limitation, we introduce MixPE, a specialized mixed-precision\nprocessing element designed for efficient low-bit quantization in LLM\ninference. MixPE leverages two key innovations to minimize dequantization\noverhead and unlock the full potential of low-bit quantization. First,\nrecognizing that scale and zero point are shared within each quantization\ngroup, we propose performing dequantization after per-group mpGEMM,\nsignificantly reducing dequantization overhead. Second, instead of relying on\nconventional multipliers, MixPE utilizes efficient shift\\&add operations for\nmultiplication, optimizing both computation and energy efficiency. Our\nexperimental results demonstrate that MixPE surpasses the state-of-the-art\nquantization accelerators by $2.6\\times$ speedup and $1.4\\times$ energy\nreduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have achieved remarkable\nsuccess as model sizes continue to grow, yet their deployment remains\nchallenging due to significant computational and memory demands. Quantization\nhas emerged as a promising solution, and state-of-the-art quantization\nalgorithms for LLMs introduce the need for mixed-precision matrix\nmultiplication (mpGEMM), where lower-precision weights are multiplied with\nhigher-precision activations. Despite its benefits, current hardware\naccelerators such as GPUs and TPUs lack native support for efficient mpGEMM,\nleading to inefficient dequantization operations in the main sequential loop.\nTo address this limitation, we introduce MixPE, a specialized mixed-precision\nprocessing element designed for efficient low-bit quantization in LLM\ninference. MixPE leverages two key innovations to minimize dequantization\noverhead and unlock the full potential of low-bit quantization. First,\nrecognizing that scale and zero point are shared within each quantization\ngroup, we propose performing dequantization after per-group mpGEMM,\nsignificantly reducing dequantization overhead. Second, instead of relying on\nconventional multipliers, MixPE utilizes efficient shift\\&add operations for\nmultiplication, optimizing both computation and energy efficiency. Our\nexperimental results demonstrate that MixPE surpasses the state-of-the-art\nquantization accelerators by $2.6\\times$ speedup and $1.4\\times$ energy\nreduction."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Lancheng Zou"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Bei Yu"
                },
                "author": "Bei Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16158v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16156v1",
                "updated": "2024-11-25T07:32:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    7,
                    32,
                    2,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T07:32:02Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    7,
                    32,
                    2,
                    0,
                    330,
                    0
                ],
                "title": "VideoOrion: Tokenizing Object Dynamics in Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoOrion: Tokenizing Object Dynamics in Videos"
                },
                "summary": "We present VideoOrion, a Video Large Language Model (Video-LLM) that\nexplicitly captures the key semantic information in videos--the\nspatial-temporal dynamics of objects throughout the videos. VideoOrion employs\nexpert vision models to extract object dynamics through a detect-segment-track\npipeline, encoding them into a set of object tokens by aggregating\nspatial-temporal object features. Our method addresses the persistent challenge\nin Video-LLMs of efficiently compressing high-dimensional video data into\nsemantic tokens that are comprehensible to LLMs. Compared to prior methods\nwhich resort to downsampling the original video or aggregating visual tokens\nusing resamplers, leading to information loss and entangled semantics,\nVideoOrion not only offers a more natural and efficient way to derive compact,\ndisentangled semantic representations but also enables explicit object modeling\nof video content with minimal computational cost. Moreover, the introduced\nobject tokens naturally allow VideoOrion to accomplish video-based referring\ntasks. Experimental results show that VideoOrion can learn to make good use of\nthe object tokens, and achieves competitive results on both general video\nquestion answering and video-based referring benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present VideoOrion, a Video Large Language Model (Video-LLM) that\nexplicitly captures the key semantic information in videos--the\nspatial-temporal dynamics of objects throughout the videos. VideoOrion employs\nexpert vision models to extract object dynamics through a detect-segment-track\npipeline, encoding them into a set of object tokens by aggregating\nspatial-temporal object features. Our method addresses the persistent challenge\nin Video-LLMs of efficiently compressing high-dimensional video data into\nsemantic tokens that are comprehensible to LLMs. Compared to prior methods\nwhich resort to downsampling the original video or aggregating visual tokens\nusing resamplers, leading to information loss and entangled semantics,\nVideoOrion not only offers a more natural and efficient way to derive compact,\ndisentangled semantic representations but also enables explicit object modeling\nof video content with minimal computational cost. Moreover, the introduced\nobject tokens naturally allow VideoOrion to accomplish video-based referring\ntasks. Experimental results show that VideoOrion can learn to make good use of\nthe object tokens, and achieves competitive results on both general video\nquestion answering and video-based referring benchmarks."
                },
                "authors": [
                    {
                        "name": "Yicheng Feng"
                    },
                    {
                        "name": "Yijiang Li"
                    },
                    {
                        "name": "Wanpeng Zhang"
                    },
                    {
                        "name": "Sipeng Zheng"
                    },
                    {
                        "name": "Zongqing Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zongqing Lu"
                },
                "author": "Zongqing Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05547v2",
                "updated": "2024-11-25T07:18:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    7,
                    18,
                    33,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-08T13:09:14Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    9,
                    14,
                    4,
                    313,
                    0
                ],
                "title": "Assessing the Answerability of Queries in Retrieval-Augmented Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Answerability of Queries in Retrieval-Augmented Code\n  Generation"
                },
                "summary": "Thanks to unprecedented language understanding and generation capabilities of\nlarge language model (LLM), Retrieval-augmented Code Generation (RaCG) has\nrecently been widely utilized among software developers. While this has\nincreased productivity, there are still frequent instances of incorrect codes\nbeing provided. In particular, there are cases where plausible yet incorrect\ncodes are generated for queries from users that cannot be answered with the\ngiven queries and API descriptions. This study proposes a task for evaluating\nanswerability, which assesses whether valid answers can be generated based on\nusers' queries and retrieved APIs in RaCG. Additionally, we build a benchmark\ndataset called Retrieval-augmented Code Generability Evaluation (RaCGEval) to\nevaluate the performance of models performing this task. Experimental results\nshow that this task remains at a very challenging level, with baseline models\nexhibiting a low performance of 46.7%. Furthermore, this study discusses\nmethods that could significantly improve performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thanks to unprecedented language understanding and generation capabilities of\nlarge language model (LLM), Retrieval-augmented Code Generation (RaCG) has\nrecently been widely utilized among software developers. While this has\nincreased productivity, there are still frequent instances of incorrect codes\nbeing provided. In particular, there are cases where plausible yet incorrect\ncodes are generated for queries from users that cannot be answered with the\ngiven queries and API descriptions. This study proposes a task for evaluating\nanswerability, which assesses whether valid answers can be generated based on\nusers' queries and retrieved APIs in RaCG. Additionally, we build a benchmark\ndataset called Retrieval-augmented Code Generability Evaluation (RaCGEval) to\nevaluate the performance of models performing this task. Experimental results\nshow that this task remains at a very challenging level, with baseline models\nexhibiting a low performance of 46.7%. Furthermore, this study discusses\nmethods that could significantly improve performance."
                },
                "authors": [
                    {
                        "name": "Geonmin Kim"
                    },
                    {
                        "name": "Jaeyeon Kim"
                    },
                    {
                        "name": "Hancheol Park"
                    },
                    {
                        "name": "Wooksu Shin"
                    },
                    {
                        "name": "Tae-Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Tae-Ho Kim"
                },
                "author": "Tae-Ho Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16710v2",
                "updated": "2024-11-25T07:12:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    7,
                    12,
                    35,
                    0,
                    330,
                    0
                ],
                "published": "2024-09-25T07:55:36Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    55,
                    36,
                    2,
                    269,
                    0
                ],
                "title": "Beyond Turing Test: Can GPT-4 Sway Experts' Decisions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Turing Test: Can GPT-4 Sway Experts' Decisions?"
                },
                "summary": "In the post-Turing era, evaluating large language models (LLMs) involves\nassessing generated text based on readers' reactions rather than merely its\nindistinguishability from human-produced content. This paper explores how\nLLM-generated text impacts readers' decisions, focusing on both amateur and\nexpert audiences. Our findings indicate that GPT-4 can generate persuasive\nanalyses affecting the decisions of both amateurs and professionals.\nFurthermore, we evaluate the generated text from the aspects of grammar,\nconvincingness, logical coherence, and usefulness. The results highlight a high\ncorrelation between real-world evaluation through audience reactions and the\ncurrent multi-dimensional evaluators commonly used for generative models.\nOverall, this paper shows the potential and risk of using generated text to\nsway human decisions and also points out a new direction for evaluating\ngenerated text, i.e., leveraging the reactions and decisions of readers. We\nrelease our dataset to assist future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the post-Turing era, evaluating large language models (LLMs) involves\nassessing generated text based on readers' reactions rather than merely its\nindistinguishability from human-produced content. This paper explores how\nLLM-generated text impacts readers' decisions, focusing on both amateur and\nexpert audiences. Our findings indicate that GPT-4 can generate persuasive\nanalyses affecting the decisions of both amateurs and professionals.\nFurthermore, we evaluate the generated text from the aspects of grammar,\nconvincingness, logical coherence, and usefulness. The results highlight a high\ncorrelation between real-world evaluation through audience reactions and the\ncurrent multi-dimensional evaluators commonly used for generative models.\nOverall, this paper shows the potential and risk of using generated text to\nsway human decisions and also points out a new direction for evaluating\ngenerated text, i.e., leveraging the reactions and decisions of readers. We\nrelease our dataset to assist future research."
                },
                "authors": [
                    {
                        "name": "Takehiro Takayanagi"
                    },
                    {
                        "name": "Hiroya Takamura"
                    },
                    {
                        "name": "Kiyoshi Izumi"
                    },
                    {
                        "name": "Chung-Chi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chung-Chi Chen"
                },
                "author": "Chung-Chi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.02684v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.02684v3",
                "updated": "2024-11-25T07:07:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    7,
                    7,
                    45,
                    0,
                    330,
                    0
                ],
                "published": "2023-11-05T15:48:29Z",
                "published_parsed": [
                    2023,
                    11,
                    5,
                    15,
                    48,
                    29,
                    6,
                    309,
                    0
                ],
                "title": "Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE"
                },
                "summary": "Recent studies have demonstrated Large Language Models (LLMs) can extend\ntheir zero-shot generalization capabilities to multimodal learning through\ninstruction tuning. As more modalities and downstream tasks are introduced,\nnegative conflicts and interference may have a worse impact on performance.\nWhile this phenomenon has been overlooked in previous work, we propose a novel\nand extensible framework, called Octavius, for comprehensive studies and\nexperimentation on multimodal learning with Multimodal Large Language Models\n(MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and\none of the representative PEFT techniques, i.e., LoRA, designing a novel\nLLM-based decoder, called LoRA-MoE, for multimodal learning. To the best of our\nknowledge, we are one of the pioneering efforts to introduce MoE into MLLMs to\naddress this problem. The experimental results (about 20% improvement) have\nshown the effectiveness and versatility of our design in various 2D and 3D\ndownstream tasks. Code and datasets are available at\nhttps://openlamm.github.io/tutorial/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have demonstrated Large Language Models (LLMs) can extend\ntheir zero-shot generalization capabilities to multimodal learning through\ninstruction tuning. As more modalities and downstream tasks are introduced,\nnegative conflicts and interference may have a worse impact on performance.\nWhile this phenomenon has been overlooked in previous work, we propose a novel\nand extensible framework, called Octavius, for comprehensive studies and\nexperimentation on multimodal learning with Multimodal Large Language Models\n(MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and\none of the representative PEFT techniques, i.e., LoRA, designing a novel\nLLM-based decoder, called LoRA-MoE, for multimodal learning. To the best of our\nknowledge, we are one of the pioneering efforts to introduce MoE into MLLMs to\naddress this problem. The experimental results (about 20% improvement) have\nshown the effectiveness and versatility of our design in various 2D and 3D\ndownstream tasks. Code and datasets are available at\nhttps://openlamm.github.io/tutorial/."
                },
                "authors": [
                    {
                        "name": "Zeren Chen"
                    },
                    {
                        "name": "Ziqin Wang"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Huayang Liu"
                    },
                    {
                        "name": "Zhenfei Yin"
                    },
                    {
                        "name": "Si Liu"
                    },
                    {
                        "name": "Lu Sheng"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "arxiv_comment": "22 pages, 12 figures. Accepted in ICLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.02684v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.02684v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13990v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13990v2",
                "updated": "2024-11-25T06:57:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    6,
                    57,
                    25,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-21T10:00:52Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    0,
                    52,
                    3,
                    326,
                    0
                ],
                "title": "Repository-level Code Translation Benchmark Targeting Rust",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repository-level Code Translation Benchmark Targeting Rust"
                },
                "summary": "Recent advances in large language models (LLMs) have shown significant\ncapabilities in code translation, often evaluated using benchmarks like\nCodeTransOcean. However, these evaluations typically focus on simple,\nfunction-level translations without considering dependencies, which does not\nreflect the complexities of real-world software development. Further, their\neffectiveness in translating to newer, lower-resource languages like Rust in\nrealistic scenarios is still under-explored. To address this gap, we introduce\nfirst repository-level code translation benchmark comprising 375 tasks\ntargeting Rust, complete with relevant dependencies. Using this benchmark, we\nstudy four state-of-the-art LLMs, analyzing their erroneous outputs to\nunderstand their performance in more complex translation scenarios. Our\nfindings reveal that LLMs exhibit substantially worse performance (41.5%-56.2%\nPass@1 drop of GPT-4) on repository-level translations compared to simpler\ntasks, highlighting limitations in existing evaluation methods. The model that\nperformed the best is Claude-3.5, demonstrating the strongest translation\ncapabilities in both basic functionality accuracy and several relevant\nadditional abilities. Additionally, we discover that LLMs struggle with\nidentifying language differences in complex tasks, and that increased\ndependencies correlate with greater translation difficulty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have shown significant\ncapabilities in code translation, often evaluated using benchmarks like\nCodeTransOcean. However, these evaluations typically focus on simple,\nfunction-level translations without considering dependencies, which does not\nreflect the complexities of real-world software development. Further, their\neffectiveness in translating to newer, lower-resource languages like Rust in\nrealistic scenarios is still under-explored. To address this gap, we introduce\nfirst repository-level code translation benchmark comprising 375 tasks\ntargeting Rust, complete with relevant dependencies. Using this benchmark, we\nstudy four state-of-the-art LLMs, analyzing their erroneous outputs to\nunderstand their performance in more complex translation scenarios. Our\nfindings reveal that LLMs exhibit substantially worse performance (41.5%-56.2%\nPass@1 drop of GPT-4) on repository-level translations compared to simpler\ntasks, highlighting limitations in existing evaluation methods. The model that\nperformed the best is Claude-3.5, demonstrating the strongest translation\ncapabilities in both basic functionality accuracy and several relevant\nadditional abilities. Additionally, we discover that LLMs struggle with\nidentifying language differences in complex tasks, and that increased\ndependencies correlate with greater translation difficulty."
                },
                "authors": [
                    {
                        "name": "Guangsheng Ou"
                    },
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Xing Peng"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13990v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13990v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14449v2",
                "updated": "2024-11-25T06:51:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    6,
                    51,
                    39,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-10T07:01:53Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    7,
                    1,
                    53,
                    6,
                    315,
                    0
                ],
                "title": "Unlearn to Relearn Backdoors: Deferred Backdoor Functionality Attacks on\n  Deep Learning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlearn to Relearn Backdoors: Deferred Backdoor Functionality Attacks on\n  Deep Learning Models"
                },
                "summary": "Deep learning models are vulnerable to backdoor attacks, where adversaries\ninject malicious functionality during training that activates on trigger inputs\nat inference time. Extensive research has focused on developing stealthy\nbackdoor attacks to evade detection and defense mechanisms. However, these\napproaches still have limitations that leave the door open for detection and\nmitigation due to their inherent design to cause malicious behavior in the\npresence of a trigger. To address this limitation, we introduce Deferred\nActivated Backdoor Functionality (DABF), a new paradigm in backdoor attacks.\nUnlike conventional attacks, DABF initially conceals its backdoor, producing\nbenign outputs even when triggered. This stealthy behavior allows DABF to\nbypass multiple detection and defense methods, remaining undetected during\ninitial inspections. The backdoor functionality is strategically activated only\nafter the model undergoes subsequent updates, such as retraining on benign\ndata. DABF attacks exploit the common practice in the life cycle of machine\nlearning models to perform model updates and fine-tuning after initial\ndeployment. To implement DABF attacks, we approach the problem by making the\nunlearning of the backdoor fragile, allowing it to be easily cancelled and\nsubsequently reactivate the backdoor functionality. To achieve this, we propose\na novel two-stage training scheme, called DeferBad. Our extensive experiments\nacross various fine-tuning scenarios, backdoor attack types, datasets, and\nmodel architectures demonstrate the effectiveness and stealthiness of DeferBad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models are vulnerable to backdoor attacks, where adversaries\ninject malicious functionality during training that activates on trigger inputs\nat inference time. Extensive research has focused on developing stealthy\nbackdoor attacks to evade detection and defense mechanisms. However, these\napproaches still have limitations that leave the door open for detection and\nmitigation due to their inherent design to cause malicious behavior in the\npresence of a trigger. To address this limitation, we introduce Deferred\nActivated Backdoor Functionality (DABF), a new paradigm in backdoor attacks.\nUnlike conventional attacks, DABF initially conceals its backdoor, producing\nbenign outputs even when triggered. This stealthy behavior allows DABF to\nbypass multiple detection and defense methods, remaining undetected during\ninitial inspections. The backdoor functionality is strategically activated only\nafter the model undergoes subsequent updates, such as retraining on benign\ndata. DABF attacks exploit the common practice in the life cycle of machine\nlearning models to perform model updates and fine-tuning after initial\ndeployment. To implement DABF attacks, we approach the problem by making the\nunlearning of the backdoor fragile, allowing it to be easily cancelled and\nsubsequently reactivate the backdoor functionality. To achieve this, we propose\na novel two-stage training scheme, called DeferBad. Our extensive experiments\nacross various fine-tuning scenarios, backdoor attack types, datasets, and\nmodel architectures demonstrate the effectiveness and stealthiness of DeferBad."
                },
                "authors": [
                    {
                        "name": "Jeongjin Shin"
                    },
                    {
                        "name": "Sangdon Park"
                    }
                ],
                "author_detail": {
                    "name": "Sangdon Park"
                },
                "author": "Sangdon Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16133v1",
                "updated": "2024-11-25T06:48:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    6,
                    48,
                    38,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T06:48:38Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    6,
                    48,
                    38,
                    0,
                    330,
                    0
                ],
                "title": "Context Awareness Gate For Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Awareness Gate For Retrieval Augmented Generation"
                },
                "summary": "Retrieval Augmented Generation (RAG) has emerged as a widely adopted approach\nto mitigate the limitations of large language models (LLMs) in answering\ndomain-specific questions. Previous research has predominantly focused on\nimproving the accuracy and quality of retrieved data chunks to enhance the\noverall performance of the generation pipeline. However, despite ongoing\nadvancements, the critical issue of retrieving irrelevant information -- which\ncan impair the ability of the model to utilize its internal knowledge\neffectively -- has received minimal attention. In this work, we investigate the\nimpact of retrieving irrelevant information in open-domain question answering,\nhighlighting its significant detrimental effect on the quality of LLM outputs.\nTo address this challenge, we propose the Context Awareness Gate (CAG)\narchitecture, a novel mechanism that dynamically adjusts the LLMs' input prompt\nbased on whether the user query necessitates external context retrieval.\nAdditionally, we introduce the Vector Candidates method, a core mathematical\ncomponent of CAG that is statistical, LLM-independent, and highly scalable. We\nfurther examine the distributions of relationships between contexts and\nquestions, presenting a statistical analysis of these distributions. This\nanalysis can be leveraged to enhance the context retrieval process in Retrieval\nAugmented Generation (RAG) systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) has emerged as a widely adopted approach\nto mitigate the limitations of large language models (LLMs) in answering\ndomain-specific questions. Previous research has predominantly focused on\nimproving the accuracy and quality of retrieved data chunks to enhance the\noverall performance of the generation pipeline. However, despite ongoing\nadvancements, the critical issue of retrieving irrelevant information -- which\ncan impair the ability of the model to utilize its internal knowledge\neffectively -- has received minimal attention. In this work, we investigate the\nimpact of retrieving irrelevant information in open-domain question answering,\nhighlighting its significant detrimental effect on the quality of LLM outputs.\nTo address this challenge, we propose the Context Awareness Gate (CAG)\narchitecture, a novel mechanism that dynamically adjusts the LLMs' input prompt\nbased on whether the user query necessitates external context retrieval.\nAdditionally, we introduce the Vector Candidates method, a core mathematical\ncomponent of CAG that is statistical, LLM-independent, and highly scalable. We\nfurther examine the distributions of relationships between contexts and\nquestions, presenting a statistical analysis of these distributions. This\nanalysis can be leveraged to enhance the context retrieval process in Retrieval\nAugmented Generation (RAG) systems."
                },
                "authors": [
                    {
                        "name": "Mohammad Hassan Heydari"
                    },
                    {
                        "name": "Arshia Hemmat"
                    },
                    {
                        "name": "Erfan Naman"
                    },
                    {
                        "name": "Afsaneh Fatemi"
                    }
                ],
                "author_detail": {
                    "name": "Afsaneh Fatemi"
                },
                "author": "Afsaneh Fatemi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16116v1",
                "updated": "2024-11-25T06:00:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    6,
                    0,
                    42,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T06:00:42Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    6,
                    0,
                    42,
                    0,
                    330,
                    0
                ],
                "title": "LLM Augmentations to support Analytical Reasoning over Multiple\n  Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Augmentations to support Analytical Reasoning over Multiple\n  Documents"
                },
                "summary": "Building on their demonstrated ability to perform a variety of tasks, we\ninvestigate the application of large language models (LLMs) to enhance in-depth\nanalytical reasoning within the context of intelligence analysis. Intelligence\nanalysts typically work with massive dossiers to draw connections between\nseemingly unrelated entities, and uncover adversaries' plans and motives. We\nexplore if and how LLMs can be helpful to analysts for this task and develop an\narchitecture to augment the capabilities of an LLM with a memory module called\ndynamic evidence trees (DETs) to develop and track multiple investigation\nthreads. Through extensive experiments on multiple datasets, we highlight how\nLLMs, as-is, are still inadequate to support intelligence analysts and offer\nrecommendations to improve LLMs for such intricate reasoning applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building on their demonstrated ability to perform a variety of tasks, we\ninvestigate the application of large language models (LLMs) to enhance in-depth\nanalytical reasoning within the context of intelligence analysis. Intelligence\nanalysts typically work with massive dossiers to draw connections between\nseemingly unrelated entities, and uncover adversaries' plans and motives. We\nexplore if and how LLMs can be helpful to analysts for this task and develop an\narchitecture to augment the capabilities of an LLM with a memory module called\ndynamic evidence trees (DETs) to develop and track multiple investigation\nthreads. Through extensive experiments on multiple datasets, we highlight how\nLLMs, as-is, are still inadequate to support intelligence analysts and offer\nrecommendations to improve LLMs for such intricate reasoning applications."
                },
                "authors": [
                    {
                        "name": "Raquib Bin Yousuf"
                    },
                    {
                        "name": "Nicholas Defelice"
                    },
                    {
                        "name": "Mandar Sharma"
                    },
                    {
                        "name": "Shengzhe Xu"
                    },
                    {
                        "name": "Naren Ramakrishnan"
                    }
                ],
                "author_detail": {
                    "name": "Naren Ramakrishnan"
                },
                "author": "Naren Ramakrishnan",
                "arxiv_comment": "2024 IEEE International Conference on Big Data (IEEE BigData 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16111v1",
                "updated": "2024-11-25T05:54:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    5,
                    54,
                    6,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T05:54:06Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    5,
                    54,
                    6,
                    0,
                    330,
                    0
                ],
                "title": "LLMPirate: LLMs for Black-box Hardware IP Piracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMPirate: LLMs for Black-box Hardware IP Piracy"
                },
                "summary": "The rapid advancement of large language models (LLMs) has enabled the ability\nto effectively analyze and generate code nearly instantaneously, resulting in\ntheir widespread adoption in software development. Following this advancement,\nresearchers and companies have begun integrating LLMs across the hardware\ndesign and verification process. However, these highly potent LLMs can also\ninduce new attack scenarios upon security vulnerabilities across the hardware\ndevelopment process. One such attack vector that has not been explored is\nintellectual property (IP) piracy. Given that this attack can manifest as\nrewriting hardware designs to evade piracy detection, it is essential to\nthoroughly evaluate LLM capabilities in performing this task and assess the\nmitigation abilities of current IP piracy detection tools.\n  Therefore, in this work, we propose LLMPirate, the first LLM-based technique\nable to generate pirated variations of circuit designs that successfully evade\ndetection across multiple state-of-the-art piracy detection tools. We devise\nthree solutions to overcome challenges related to integration of LLMs for\nhardware circuit designs, scalability to large circuits, and effectiveness,\nresulting in an end-to-end automated, efficient, and practical formulation. We\nperform an extensive experimental evaluation of LLMPirate using eight LLMs of\nvarying sizes and capabilities and assess their performance in pirating various\ncircuit designs against four state-of-the-art, widely-used piracy detection\ntools. Our experiments demonstrate that LLMPirate is able to consistently evade\ndetection on 100% of tested circuits across every detection tool. Additionally,\nwe showcase the ramifications of LLMPirate using case studies on IBEX and\nMOR1KX processors and a GPS module, that we successfully pirate. We envision\nthat our work motivates and fosters the development of better IP piracy\ndetection tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has enabled the ability\nto effectively analyze and generate code nearly instantaneously, resulting in\ntheir widespread adoption in software development. Following this advancement,\nresearchers and companies have begun integrating LLMs across the hardware\ndesign and verification process. However, these highly potent LLMs can also\ninduce new attack scenarios upon security vulnerabilities across the hardware\ndevelopment process. One such attack vector that has not been explored is\nintellectual property (IP) piracy. Given that this attack can manifest as\nrewriting hardware designs to evade piracy detection, it is essential to\nthoroughly evaluate LLM capabilities in performing this task and assess the\nmitigation abilities of current IP piracy detection tools.\n  Therefore, in this work, we propose LLMPirate, the first LLM-based technique\nable to generate pirated variations of circuit designs that successfully evade\ndetection across multiple state-of-the-art piracy detection tools. We devise\nthree solutions to overcome challenges related to integration of LLMs for\nhardware circuit designs, scalability to large circuits, and effectiveness,\nresulting in an end-to-end automated, efficient, and practical formulation. We\nperform an extensive experimental evaluation of LLMPirate using eight LLMs of\nvarying sizes and capabilities and assess their performance in pirating various\ncircuit designs against four state-of-the-art, widely-used piracy detection\ntools. Our experiments demonstrate that LLMPirate is able to consistently evade\ndetection on 100% of tested circuits across every detection tool. Additionally,\nwe showcase the ramifications of LLMPirate using case studies on IBEX and\nMOR1KX processors and a GPS module, that we successfully pirate. We envision\nthat our work motivates and fosters the development of better IP piracy\ndetection tools."
                },
                "authors": [
                    {
                        "name": "Vasudev Gohil"
                    },
                    {
                        "name": "Matthew DeLorenzo"
                    },
                    {
                        "name": "Veera Vishwa Achuta Sai Venkat Nallam"
                    },
                    {
                        "name": "Joey See"
                    },
                    {
                        "name": "Jeyavijayan Rajendran"
                    }
                ],
                "author_detail": {
                    "name": "Jeyavijayan Rajendran"
                },
                "author": "Jeyavijayan Rajendran",
                "arxiv_comment": "Accepted by NDSS Symposium 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16105v1",
                "updated": "2024-11-25T05:32:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    5,
                    32,
                    34,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T05:32:34Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    5,
                    32,
                    34,
                    0,
                    330,
                    0
                ],
                "title": "Adaptive Circuit Behavior and Generalization in Mechanistic\n  Interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Circuit Behavior and Generalization in Mechanistic\n  Interpretability"
                },
                "summary": "Mechanistic interpretability aims to understand the inner workings of large\nneural networks by identifying circuits, or minimal subgraphs within the model\nthat implement algorithms responsible for performing specific tasks. These\ncircuits are typically discovered and analyzed using a narrowly defined prompt\nformat. However, given the abilities of large language models (LLMs) to\ngeneralize across various prompt formats for the same task, it remains unclear\nhow well these circuits generalize. For instance, it is unclear whether the\nmodels generalization results from reusing the same circuit components, the\ncomponents behaving differently, or the use of entirely different components.\nIn this paper, we investigate the generality of the indirect object\nidentification (IOI) circuit in GPT-2 small, which is well-studied and believed\nto implement a simple, interpretable algorithm. We evaluate its performance on\nprompt variants that challenge the assumptions of this algorithm. Our findings\nreveal that the circuit generalizes surprisingly well, reusing all of its\ncomponents and mechanisms while only adding additional input edges. Notably,\nthe circuit generalizes even to prompt variants where the original algorithm\nshould fail; we discover a mechanism that explains this which we term S2\nHacking. Our findings indicate that circuits within LLMs may be more flexible\nand general than previously recognized, underscoring the importance of studying\ncircuit generalization to better understand the broader capabilities of these\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mechanistic interpretability aims to understand the inner workings of large\nneural networks by identifying circuits, or minimal subgraphs within the model\nthat implement algorithms responsible for performing specific tasks. These\ncircuits are typically discovered and analyzed using a narrowly defined prompt\nformat. However, given the abilities of large language models (LLMs) to\ngeneralize across various prompt formats for the same task, it remains unclear\nhow well these circuits generalize. For instance, it is unclear whether the\nmodels generalization results from reusing the same circuit components, the\ncomponents behaving differently, or the use of entirely different components.\nIn this paper, we investigate the generality of the indirect object\nidentification (IOI) circuit in GPT-2 small, which is well-studied and believed\nto implement a simple, interpretable algorithm. We evaluate its performance on\nprompt variants that challenge the assumptions of this algorithm. Our findings\nreveal that the circuit generalizes surprisingly well, reusing all of its\ncomponents and mechanisms while only adding additional input edges. Notably,\nthe circuit generalizes even to prompt variants where the original algorithm\nshould fail; we discover a mechanism that explains this which we term S2\nHacking. Our findings indicate that circuits within LLMs may be more flexible\nand general than previously recognized, underscoring the importance of studying\ncircuit generalization to better understand the broader capabilities of these\nmodels."
                },
                "authors": [
                    {
                        "name": "Jatin Nainani"
                    },
                    {
                        "name": "Sankaran Vaidyanathan"
                    },
                    {
                        "name": "AJ Yeung"
                    },
                    {
                        "name": "Kartik Gupta"
                    },
                    {
                        "name": "David Jensen"
                    }
                ],
                "author_detail": {
                    "name": "David Jensen"
                },
                "author": "David Jensen",
                "arxiv_comment": "10 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16789v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16789v3",
                "updated": "2024-11-25T05:27:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    5,
                    27,
                    13,
                    0,
                    330,
                    0
                ],
                "published": "2024-04-25T17:38:57Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    17,
                    38,
                    57,
                    3,
                    116,
                    0
                ],
                "title": "Continual Learning of Large Language Models: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Learning of Large Language Models: A Comprehensive Survey"
                },
                "summary": "The recent success of large language models (LLMs) trained on static,\npre-collected, general datasets has sparked numerous research directions and\napplications. One such direction addresses the non-trivial challenge of\nintegrating pre-trained LLMs into dynamic data distributions, task structures,\nand user preferences. Pre-trained LLMs, when tailored for specific needs, often\nexperience significant performance degradation in previous knowledge domains --\na phenomenon known as \"catastrophic forgetting\". While extensively studied in\nthe continual learning (CL) community, it presents new manifestations in the\nrealm of LLMs. In this survey, we provide a comprehensive overview of the\ncurrent research progress on LLMs within the context of CL. This survey is\nstructured into four main sections: we first describe an overview of\ncontinually learning LLMs, consisting of two directions of continuity: vertical\ncontinuity (or vertical continual learning), i.e., continual adaptation from\ngeneral to specific capabilities, and horizontal continuity (or horizontal\ncontinual learning), i.e., continual adaptation across time and domains\n(Section 3). We then summarize three stages of learning LLMs in the context of\nmodern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP),\nand Continual Fine-Tuning (CFT) (Section 4). Then we provide an overview of\nevaluation protocols for continual learning with LLMs, along with the current\navailable data sources (Section 5). Finally, we discuss intriguing questions\npertaining to continual learning for LLMs (Section 6). The full list of papers\nexamined in this survey is available at\nhttps://github.com/Wang-ML-Lab/llm-continual-learning-survey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent success of large language models (LLMs) trained on static,\npre-collected, general datasets has sparked numerous research directions and\napplications. One such direction addresses the non-trivial challenge of\nintegrating pre-trained LLMs into dynamic data distributions, task structures,\nand user preferences. Pre-trained LLMs, when tailored for specific needs, often\nexperience significant performance degradation in previous knowledge domains --\na phenomenon known as \"catastrophic forgetting\". While extensively studied in\nthe continual learning (CL) community, it presents new manifestations in the\nrealm of LLMs. In this survey, we provide a comprehensive overview of the\ncurrent research progress on LLMs within the context of CL. This survey is\nstructured into four main sections: we first describe an overview of\ncontinually learning LLMs, consisting of two directions of continuity: vertical\ncontinuity (or vertical continual learning), i.e., continual adaptation from\ngeneral to specific capabilities, and horizontal continuity (or horizontal\ncontinual learning), i.e., continual adaptation across time and domains\n(Section 3). We then summarize three stages of learning LLMs in the context of\nmodern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP),\nand Continual Fine-Tuning (CFT) (Section 4). Then we provide an overview of\nevaluation protocols for continual learning with LLMs, along with the current\navailable data sources (Section 5). Finally, we discuss intriguing questions\npertaining to continual learning for LLMs (Section 6). The full list of papers\nexamined in this survey is available at\nhttps://github.com/Wang-ML-Lab/llm-continual-learning-survey."
                },
                "authors": [
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Zihao Xu"
                    },
                    {
                        "name": "Hengyi Wang"
                    },
                    {
                        "name": "Weiyi Qin"
                    },
                    {
                        "name": "Wenyuan Wang"
                    },
                    {
                        "name": "Yibin Wang"
                    },
                    {
                        "name": "Zifeng Wang"
                    },
                    {
                        "name": "Sayna Ebrahimi"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "arxiv_comment": "44 pages, 2 figures, 4 tables; Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16789v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16789v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14790v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14790v2",
                "updated": "2024-11-25T04:51:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    4,
                    51,
                    57,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-22T08:21:03Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    8,
                    21,
                    3,
                    4,
                    327,
                    0
                ],
                "title": "KBAlign: Efficient Self Adaptation on Specific Knowledge Bases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KBAlign: Efficient Self Adaptation on Specific Knowledge Bases"
                },
                "summary": "Humans can utilize techniques to quickly acquire knowledge from specific\nmaterials in advance, such as creating self-assessment questions, enabling us\nto achieving related tasks more efficiently. In contrast, large language models\n(LLMs) usually relies on retrieval-augmented generation to exploit knowledge\nmaterials in an instant manner, or requires external signals such as human\npreference data and stronger LLM annotations to conduct knowledge adaptation.\nTo unleash the self-learning potential of LLMs, we propose KBAlign, an approach\ndesigned for efficient adaptation to downstream tasks involving knowledge\nbases. Our method utilizes iterative training with self-annotated data such as\nQ&A pairs and revision suggestions, enabling the model to grasp the knowledge\ncontent efficiently. Experimental results on multiple datasets demonstrate the\neffectiveness of our approach, significantly boosting model performance in\ndownstream tasks that require specific knowledge at a low cost. Notably, our\napproach achieves over 90% of the performance improvement that can be obtained\nby using GPT-4-turbo annotation, while relying entirely on self-supervision. We\nrelease our experimental data, models, and process analyses to the community\nfor further exploration (https://github.com/thunlp/KBAlign).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans can utilize techniques to quickly acquire knowledge from specific\nmaterials in advance, such as creating self-assessment questions, enabling us\nto achieving related tasks more efficiently. In contrast, large language models\n(LLMs) usually relies on retrieval-augmented generation to exploit knowledge\nmaterials in an instant manner, or requires external signals such as human\npreference data and stronger LLM annotations to conduct knowledge adaptation.\nTo unleash the self-learning potential of LLMs, we propose KBAlign, an approach\ndesigned for efficient adaptation to downstream tasks involving knowledge\nbases. Our method utilizes iterative training with self-annotated data such as\nQ&A pairs and revision suggestions, enabling the model to grasp the knowledge\ncontent efficiently. Experimental results on multiple datasets demonstrate the\neffectiveness of our approach, significantly boosting model performance in\ndownstream tasks that require specific knowledge at a low cost. Notably, our\napproach achieves over 90% of the performance improvement that can be obtained\nby using GPT-4-turbo annotation, while relying entirely on self-supervision. We\nrelease our experimental data, models, and process analyses to the community\nfor further exploration (https://github.com/thunlp/KBAlign)."
                },
                "authors": [
                    {
                        "name": "Zheni Zeng"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Shi Yu"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Zhenghao Liu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14790v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14790v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16084v1",
                "updated": "2024-11-25T04:35:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    4,
                    35,
                    56,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T04:35:56Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    4,
                    35,
                    56,
                    0,
                    330,
                    0
                ],
                "title": "Deciphering genomic codes using advanced NLP techniques: a scoping\n  review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deciphering genomic codes using advanced NLP techniques: a scoping\n  review"
                },
                "summary": "Objectives: The vast and complex nature of human genomic sequencing data\npresents challenges for effective analysis. This review aims to investigate the\napplication of Natural Language Processing (NLP) techniques, particularly Large\nLanguage Models (LLMs) and transformer architectures, in deciphering genomic\ncodes, focusing on tokenization, transformer models, and regulatory annotation\nprediction. The goal of this review is to assess data and model accessibility\nin the most recent literature, gaining a better understanding of the existing\ncapabilities and constraints of these tools in processing genomic sequencing\ndata.\n  Methods: Following Preferred Reporting Items for Systematic Reviews and\nMeta-Analyses (PRISMA) guidelines, our scoping review was conducted across\nPubMed, Medline, Scopus, Web of Science, Embase, and ACM Digital Library.\nStudies were included if they focused on NLP methodologies applied to genomic\nsequencing data analysis, without restrictions on publication date or article\ntype.\n  Results: A total of 26 studies published between 2021 and April 2024 were\nselected for review. The review highlights that tokenization and transformer\nmodels enhance the processing and understanding of genomic data, with\napplications in predicting regulatory annotations like transcription-factor\nbinding sites and chromatin accessibility.\n  Discussion: The application of NLP and LLMs to genomic sequencing data\ninterpretation is a promising field that can help streamline the processing of\nlarge-scale genomic data while also providing a better understanding of its\ncomplex structures. It has the potential to drive advancements in personalized\nmedicine by offering more efficient and scalable solutions for genomic\nanalysis. Further research is also needed to discuss and overcome current\nlimitations, enhancing model transparency and applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objectives: The vast and complex nature of human genomic sequencing data\npresents challenges for effective analysis. This review aims to investigate the\napplication of Natural Language Processing (NLP) techniques, particularly Large\nLanguage Models (LLMs) and transformer architectures, in deciphering genomic\ncodes, focusing on tokenization, transformer models, and regulatory annotation\nprediction. The goal of this review is to assess data and model accessibility\nin the most recent literature, gaining a better understanding of the existing\ncapabilities and constraints of these tools in processing genomic sequencing\ndata.\n  Methods: Following Preferred Reporting Items for Systematic Reviews and\nMeta-Analyses (PRISMA) guidelines, our scoping review was conducted across\nPubMed, Medline, Scopus, Web of Science, Embase, and ACM Digital Library.\nStudies were included if they focused on NLP methodologies applied to genomic\nsequencing data analysis, without restrictions on publication date or article\ntype.\n  Results: A total of 26 studies published between 2021 and April 2024 were\nselected for review. The review highlights that tokenization and transformer\nmodels enhance the processing and understanding of genomic data, with\napplications in predicting regulatory annotations like transcription-factor\nbinding sites and chromatin accessibility.\n  Discussion: The application of NLP and LLMs to genomic sequencing data\ninterpretation is a promising field that can help streamline the processing of\nlarge-scale genomic data while also providing a better understanding of its\ncomplex structures. It has the potential to drive advancements in personalized\nmedicine by offering more efficient and scalable solutions for genomic\nanalysis. Further research is also needed to discuss and overcome current\nlimitations, enhancing model transparency and applicability."
                },
                "authors": [
                    {
                        "name": "Shuyan Cheng"
                    },
                    {
                        "name": "Yishu Wei"
                    },
                    {
                        "name": "Yiliang Zhou"
                    },
                    {
                        "name": "Zihan Xu"
                    },
                    {
                        "name": "Drew N Wright"
                    },
                    {
                        "name": "Jinze Liu"
                    },
                    {
                        "name": "Yifan Peng"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Peng"
                },
                "author": "Yifan Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16077v1",
                "updated": "2024-11-25T04:07:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    4,
                    7,
                    16,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T04:07:16Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    4,
                    7,
                    16,
                    0,
                    330,
                    0
                ],
                "title": "SAGEval: The frontiers of Satisfactory Agent based NLG Evaluation for\n  reference-free open-ended text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAGEval: The frontiers of Satisfactory Agent based NLG Evaluation for\n  reference-free open-ended text"
                },
                "summary": "Large Language Model (LLM) integrations into applications like Microsoft365\nsuite and Google Workspace for creating/processing documents, emails,\npresentations, etc. has led to considerable enhancements in productivity and\ntime savings. But as these integrations become more more complex, it is\nparamount to ensure that the quality of output from the LLM-integrated\napplications are relevant and appropriate for use. Identifying the need to\ndevelop robust evaluation approaches for natural language generation, wherein\nreferences/ground labels doesn't exist or isn't amply available, this paper\nintroduces a novel framework called \"SAGEval\" which utilizes a critiquing Agent\nto provide feedback on scores generated by LLM evaluators. We show that the\ncritiquing Agent is able to rectify scores from LLM evaluators, in absence of\nreferences/ground-truth labels, thereby reducing the need for labeled data even\nfor complex NLG evaluation scenarios, like the generation of JSON-structured\nforms/surveys with responses in different styles like multiple choice, likert\nratings, single choice questions, etc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) integrations into applications like Microsoft365\nsuite and Google Workspace for creating/processing documents, emails,\npresentations, etc. has led to considerable enhancements in productivity and\ntime savings. But as these integrations become more more complex, it is\nparamount to ensure that the quality of output from the LLM-integrated\napplications are relevant and appropriate for use. Identifying the need to\ndevelop robust evaluation approaches for natural language generation, wherein\nreferences/ground labels doesn't exist or isn't amply available, this paper\nintroduces a novel framework called \"SAGEval\" which utilizes a critiquing Agent\nto provide feedback on scores generated by LLM evaluators. We show that the\ncritiquing Agent is able to rectify scores from LLM evaluators, in absence of\nreferences/ground-truth labels, thereby reducing the need for labeled data even\nfor complex NLG evaluation scenarios, like the generation of JSON-structured\nforms/surveys with responses in different styles like multiple choice, likert\nratings, single choice questions, etc."
                },
                "authors": [
                    {
                        "name": "Reshmi Ghosh"
                    },
                    {
                        "name": "Tianyi Yao"
                    },
                    {
                        "name": "Lizzy Chen"
                    },
                    {
                        "name": "Sadid Hasan"
                    },
                    {
                        "name": "Tianwei Chen"
                    },
                    {
                        "name": "Dario Bernal"
                    },
                    {
                        "name": "Huitian Jiao"
                    },
                    {
                        "name": "H M Sajjad Hossain"
                    }
                ],
                "author_detail": {
                    "name": "H M Sajjad Hossain"
                },
                "author": "H M Sajjad Hossain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06209v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06209v7",
                "updated": "2024-11-25T02:39:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    2,
                    39,
                    3,
                    0,
                    330,
                    0
                ],
                "published": "2024-10-08T17:11:24Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    17,
                    11,
                    24,
                    1,
                    282,
                    0
                ],
                "title": "LeanAgent: Lifelong Learning for Formal Theorem Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeanAgent: Lifelong Learning for Formal Theorem Proving"
                },
                "summary": "Large Language Models (LLMs) have been successful in mathematical reasoning\ntasks such as formal theorem proving when integrated with interactive proof\nassistants like Lean. Existing approaches involve training or fine-tuning an\nLLM on a specific dataset to perform well on particular domains, such as\nundergraduate-level mathematics. These methods struggle with generalizability\nto advanced mathematics. A fundamental limitation is that these approaches\noperate on static domains, failing to capture how mathematicians often work\nacross multiple domains and projects simultaneously or cyclically. We present\nLeanAgent, a novel lifelong learning framework for formal theorem proving that\ncontinuously generalizes to and improves on ever-expanding mathematical\nknowledge without forgetting previously learned knowledge. LeanAgent introduces\nseveral key innovations, including a curriculum learning strategy that\noptimizes the learning trajectory in terms of mathematical difficulty, a\ndynamic database for efficient management of evolving mathematical knowledge,\nand progressive training to balance stability and plasticity. LeanAgent\nsuccessfully proves 155 theorems previously unproved formally by humans across\n23 diverse Lean repositories, many from advanced mathematics. It performs\nsignificantly better than the static LLM baseline, proving challenging theorems\nin domains like abstract algebra and algebraic topology while showcasing a\nclear progression of learning from basic concepts to advanced topics. In\naddition, we analyze LeanAgent's superior performance on key lifelong learning\nmetrics. LeanAgent achieves exceptional scores in stability and backward\ntransfer, where learning new tasks improves performance on previously learned\ntasks. This emphasizes LeanAgent's continuous generalizability and improvement,\nexplaining its superior theorem-proving performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been successful in mathematical reasoning\ntasks such as formal theorem proving when integrated with interactive proof\nassistants like Lean. Existing approaches involve training or fine-tuning an\nLLM on a specific dataset to perform well on particular domains, such as\nundergraduate-level mathematics. These methods struggle with generalizability\nto advanced mathematics. A fundamental limitation is that these approaches\noperate on static domains, failing to capture how mathematicians often work\nacross multiple domains and projects simultaneously or cyclically. We present\nLeanAgent, a novel lifelong learning framework for formal theorem proving that\ncontinuously generalizes to and improves on ever-expanding mathematical\nknowledge without forgetting previously learned knowledge. LeanAgent introduces\nseveral key innovations, including a curriculum learning strategy that\noptimizes the learning trajectory in terms of mathematical difficulty, a\ndynamic database for efficient management of evolving mathematical knowledge,\nand progressive training to balance stability and plasticity. LeanAgent\nsuccessfully proves 155 theorems previously unproved formally by humans across\n23 diverse Lean repositories, many from advanced mathematics. It performs\nsignificantly better than the static LLM baseline, proving challenging theorems\nin domains like abstract algebra and algebraic topology while showcasing a\nclear progression of learning from basic concepts to advanced topics. In\naddition, we analyze LeanAgent's superior performance on key lifelong learning\nmetrics. LeanAgent achieves exceptional scores in stability and backward\ntransfer, where learning new tasks improves performance on previously learned\ntasks. This emphasizes LeanAgent's continuous generalizability and improvement,\nexplaining its superior theorem-proving performance."
                },
                "authors": [
                    {
                        "name": "Adarsh Kumarappan"
                    },
                    {
                        "name": "Mo Tiwari"
                    },
                    {
                        "name": "Peiyang Song"
                    },
                    {
                        "name": "Robert Joseph George"
                    },
                    {
                        "name": "Chaowei Xiao"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06209v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06209v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16044v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16044v1",
                "updated": "2024-11-25T02:15:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    2,
                    15,
                    30,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T02:15:30Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    2,
                    15,
                    30,
                    0,
                    330,
                    0
                ],
                "title": "ZoomEye: Enhancing Multimodal LLMs with Human-Like Zooming Capabilities\n  through Tree-Based Image Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZoomEye: Enhancing Multimodal LLMs with Human-Like Zooming Capabilities\n  through Tree-Based Image Exploration"
                },
                "summary": "An image, especially with high-resolution, typically consists of numerous\nvisual elements, ranging from dominant large objects to fine-grained detailed\nobjects. When perceiving such images, multimodal large language models~(MLLMs)\nface limitations due to the restricted input resolution of the pretrained\nvision encoder and the cluttered, dense context of the image, resulting in a\nfocus on primary objects while easily overlooking detailed ones. In this paper,\nwe propose Zoom Eye, a tree search algorithm designed to navigate the\nhierarchical and visual nature of images to capture relevant information. Zoom\nEye conceptualizes an image as a tree, with each children node representing a\nzoomed sub-patch of the parent node and the root represents the overall image.\nMoreover, Zoom Eye is model-agnostic and training-free, so it enables any MLLMs\nto simulate human zooming actions by searching along the image tree from root\nto leaf nodes, seeking out pertinent information, and accurately responding to\nrelated queries. We experiment on a series of elaborate high-resolution\nbenchmarks and the results demonstrate that Zoom Eye not only consistently\nimproves the performance of a series base MLLMs with large margin~(e.g.,\nLLaVA-v1.5-7B increases by 34.57\\% on $V^*$ Bench and 17.88\\% on HR-Bench), but\nalso enables small 7B MLLMs to outperform strong large models such as GPT-4o.\nOur code is available at\n\\href{https://github.com/om-ai-lab/ZoomEye}{https://github.com/om-ai-lab/ZoomEye}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An image, especially with high-resolution, typically consists of numerous\nvisual elements, ranging from dominant large objects to fine-grained detailed\nobjects. When perceiving such images, multimodal large language models~(MLLMs)\nface limitations due to the restricted input resolution of the pretrained\nvision encoder and the cluttered, dense context of the image, resulting in a\nfocus on primary objects while easily overlooking detailed ones. In this paper,\nwe propose Zoom Eye, a tree search algorithm designed to navigate the\nhierarchical and visual nature of images to capture relevant information. Zoom\nEye conceptualizes an image as a tree, with each children node representing a\nzoomed sub-patch of the parent node and the root represents the overall image.\nMoreover, Zoom Eye is model-agnostic and training-free, so it enables any MLLMs\nto simulate human zooming actions by searching along the image tree from root\nto leaf nodes, seeking out pertinent information, and accurately responding to\nrelated queries. We experiment on a series of elaborate high-resolution\nbenchmarks and the results demonstrate that Zoom Eye not only consistently\nimproves the performance of a series base MLLMs with large margin~(e.g.,\nLLaVA-v1.5-7B increases by 34.57\\% on $V^*$ Bench and 17.88\\% on HR-Bench), but\nalso enables small 7B MLLMs to outperform strong large models such as GPT-4o.\nOur code is available at\n\\href{https://github.com/om-ai-lab/ZoomEye}{https://github.com/om-ai-lab/ZoomEye}."
                },
                "authors": [
                    {
                        "name": "Haozhan Shen"
                    },
                    {
                        "name": "Kangjia Zhao"
                    },
                    {
                        "name": "Tiancheng Zhao"
                    },
                    {
                        "name": "Ruochen Xu"
                    },
                    {
                        "name": "Zilun Zhang"
                    },
                    {
                        "name": "Mingwei Zhu"
                    },
                    {
                        "name": "Jianwei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Jianwei Yin"
                },
                "author": "Jianwei Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16044v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16035v1",
                "updated": "2024-11-25T01:48:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    1,
                    48,
                    9,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T01:48:09Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    1,
                    48,
                    9,
                    0,
                    330,
                    0
                ],
                "title": "Predicting Emergent Capabilities by Finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting Emergent Capabilities by Finetuning"
                },
                "summary": "A fundamental open challenge in modern LLM scaling is the lack of\nunderstanding around emergent capabilities. In particular, language model\npretraining loss is known to be highly predictable as a function of compute.\nHowever, downstream capabilities are far less predictable -- sometimes even\nexhibiting emergent jumps -- which makes it challenging to anticipate the\ncapabilities of future models. In this work, we first pose the task of\nemergence prediction: given access to current LLMs that have random few-shot\naccuracy on a task, can we predict whether future models (GPT-N+1) will have\nnon-trivial accuracy on that task? We then discover a simple insight for this\nproblem: finetuning LLMs on a given task can shift the point in scaling at\nwhich emergence occurs towards less capable models. To operationalize this\ninsight, we can finetune LLMs with varying amounts of data and fit a parametric\nfunction that predicts when emergence will occur (i.e., \"emergence laws\"). We\nvalidate this approach using four standard NLP benchmarks where large-scale\nopen-source LLMs already demonstrate emergence (MMLU, GSM8K, CommonsenseQA, and\nCoLA). Using only small-scale LLMs, we find that, in some cases, we can\naccurately predict whether models trained with up to 4x more compute have\nemerged. Finally, we present a case study of two realistic uses for emergence\nprediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fundamental open challenge in modern LLM scaling is the lack of\nunderstanding around emergent capabilities. In particular, language model\npretraining loss is known to be highly predictable as a function of compute.\nHowever, downstream capabilities are far less predictable -- sometimes even\nexhibiting emergent jumps -- which makes it challenging to anticipate the\ncapabilities of future models. In this work, we first pose the task of\nemergence prediction: given access to current LLMs that have random few-shot\naccuracy on a task, can we predict whether future models (GPT-N+1) will have\nnon-trivial accuracy on that task? We then discover a simple insight for this\nproblem: finetuning LLMs on a given task can shift the point in scaling at\nwhich emergence occurs towards less capable models. To operationalize this\ninsight, we can finetune LLMs with varying amounts of data and fit a parametric\nfunction that predicts when emergence will occur (i.e., \"emergence laws\"). We\nvalidate this approach using four standard NLP benchmarks where large-scale\nopen-source LLMs already demonstrate emergence (MMLU, GSM8K, CommonsenseQA, and\nCoLA). Using only small-scale LLMs, we find that, in some cases, we can\naccurately predict whether models trained with up to 4x more compute have\nemerged. Finally, we present a case study of two realistic uses for emergence\nprediction."
                },
                "authors": [
                    {
                        "name": "Charlie Snell"
                    },
                    {
                        "name": "Eric Wallace"
                    },
                    {
                        "name": "Dan Klein"
                    },
                    {
                        "name": "Sergey Levine"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Levine"
                },
                "author": "Sergey Levine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07756v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07756v2",
                "updated": "2024-11-25T01:36:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    1,
                    36,
                    31,
                    0,
                    330,
                    0
                ],
                "published": "2024-09-12T05:18:57Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    5,
                    18,
                    57,
                    3,
                    256,
                    0
                ],
                "title": "DiTAS: Quantizing Diffusion Transformers via Enhanced Activation\n  Smoothing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiTAS: Quantizing Diffusion Transformers via Enhanced Activation\n  Smoothing"
                },
                "summary": "Diffusion Transformers (DiTs) have recently attracted significant interest\nfrom both industry and academia due to their enhanced capabilities in visual\ngeneration, surpassing the performance of traditional diffusion models that\nemploy U-Net. However, the improved performance of DiTs comes at the expense of\nhigher parameter counts and implementation costs, which significantly limits\ntheir deployment on resource-constrained devices like mobile phones. We propose\nDiTAS, a data-free post-training quantization (PTQ) method for efficient DiT\ninference. DiTAS relies on the proposed temporal-aggregated smoothing\ntechniques to mitigate the impact of the channel-wise outliers within the input\nactivations, leading to much lower quantization error under extremely low\nbitwidth. To further enhance the performance of the quantized DiT, we adopt the\nlayer-wise grid search strategy to optimize the smoothing factor. Moreover, we\nintegrate a training-free LoRA module for weight quantization, leveraging\nalternating optimization to minimize quantization errors without additional\nfine-tuning. Experimental results demonstrate that our approach enables 4-bit\nweight, 8-bit activation (W4A8) quantization for DiTs while maintaining\ncomparable performance as the full-precision model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have recently attracted significant interest\nfrom both industry and academia due to their enhanced capabilities in visual\ngeneration, surpassing the performance of traditional diffusion models that\nemploy U-Net. However, the improved performance of DiTs comes at the expense of\nhigher parameter counts and implementation costs, which significantly limits\ntheir deployment on resource-constrained devices like mobile phones. We propose\nDiTAS, a data-free post-training quantization (PTQ) method for efficient DiT\ninference. DiTAS relies on the proposed temporal-aggregated smoothing\ntechniques to mitigate the impact of the channel-wise outliers within the input\nactivations, leading to much lower quantization error under extremely low\nbitwidth. To further enhance the performance of the quantized DiT, we adopt the\nlayer-wise grid search strategy to optimize the smoothing factor. Moreover, we\nintegrate a training-free LoRA module for weight quantization, leveraging\nalternating optimization to minimize quantization errors without additional\nfine-tuning. Experimental results demonstrate that our approach enables 4-bit\nweight, 8-bit activation (W4A8) quantization for DiTs while maintaining\ncomparable performance as the full-precision model."
                },
                "authors": [
                    {
                        "name": "Zhenyuan Dong"
                    },
                    {
                        "name": "Sai Qian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Sai Qian Zhang"
                },
                "author": "Sai Qian Zhang",
                "arxiv_comment": "Accepted at WACV 2025. Code is available at\n  https://github.com/DZY122/DiTAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07756v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07756v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16031v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16031v1",
                "updated": "2024-11-25T01:18:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    1,
                    18,
                    49,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T01:18:49Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    1,
                    18,
                    49,
                    0,
                    330,
                    0
                ],
                "title": "Agent-Based Modelling Meets Generative AI in Social Network Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-Based Modelling Meets Generative AI in Social Network Simulations"
                },
                "summary": "Agent-Based Modelling (ABM) has emerged as an essential tool for simulating\nsocial networks, encompassing diverse phenomena such as information\ndissemination, influence dynamics, and community formation. However, manually\nconfiguring varied agent interactions and information flow dynamics poses\nchallenges, often resulting in oversimplified models that lack real-world\ngeneralizability. Integrating modern Large Language Models (LLMs) with ABM\npresents a promising avenue to address these challenges and enhance simulation\nfidelity, leveraging LLMs' human-like capabilities in sensing, reasoning, and\nbehavior. In this paper, we propose a novel framework utilizing LLM-empowered\nagents to simulate social network users based on their interests and\npersonality traits. The framework allows for customizable agent interactions\nresembling various social network platforms, including mechanisms for content\nresharing and personalized recommendations. We validate our framework using a\ncomprehensive Twitter dataset from the 2020 US election, demonstrating that\nLLM-agents accurately replicate real users' behaviors, including linguistic\npatterns and political inclinations. These agents form homogeneous ideological\nclusters and retain the main themes of their community. Notably,\npreference-based recommendations significantly influence agent behavior,\npromoting increased engagement, network homophily and the formation of echo\nchambers. Overall, our findings underscore the potential of LLM-agents in\nadvancing social media simulations and unraveling intricate online dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-Based Modelling (ABM) has emerged as an essential tool for simulating\nsocial networks, encompassing diverse phenomena such as information\ndissemination, influence dynamics, and community formation. However, manually\nconfiguring varied agent interactions and information flow dynamics poses\nchallenges, often resulting in oversimplified models that lack real-world\ngeneralizability. Integrating modern Large Language Models (LLMs) with ABM\npresents a promising avenue to address these challenges and enhance simulation\nfidelity, leveraging LLMs' human-like capabilities in sensing, reasoning, and\nbehavior. In this paper, we propose a novel framework utilizing LLM-empowered\nagents to simulate social network users based on their interests and\npersonality traits. The framework allows for customizable agent interactions\nresembling various social network platforms, including mechanisms for content\nresharing and personalized recommendations. We validate our framework using a\ncomprehensive Twitter dataset from the 2020 US election, demonstrating that\nLLM-agents accurately replicate real users' behaviors, including linguistic\npatterns and political inclinations. These agents form homogeneous ideological\nclusters and retain the main themes of their community. Notably,\npreference-based recommendations significantly influence agent behavior,\npromoting increased engagement, network homophily and the formation of echo\nchambers. Overall, our findings underscore the potential of LLM-agents in\nadvancing social media simulations and unraveling intricate online dynamics."
                },
                "authors": [
                    {
                        "name": "Antonino Ferraro"
                    },
                    {
                        "name": "Antonio Galli"
                    },
                    {
                        "name": "Valerio La Gatta"
                    },
                    {
                        "name": "Marco Postiglione"
                    },
                    {
                        "name": "Gian Marco Orlando"
                    },
                    {
                        "name": "Diego Russo"
                    },
                    {
                        "name": "Giuseppe Riccio"
                    },
                    {
                        "name": "Antonio Romano"
                    },
                    {
                        "name": "Vincenzo Moscato"
                    }
                ],
                "author_detail": {
                    "name": "Vincenzo Moscato"
                },
                "author": "Vincenzo Moscato",
                "arxiv_comment": "Accepted at ASONAM2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16031v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16031v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.00563v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.00563v2",
                "updated": "2024-11-25T01:10:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    1,
                    10,
                    44,
                    0,
                    330,
                    0
                ],
                "published": "2023-12-31T18:47:33Z",
                "published_parsed": [
                    2023,
                    12,
                    31,
                    18,
                    47,
                    33,
                    6,
                    365,
                    0
                ],
                "title": "KernelGPT: Enhanced Kernel Fuzzing via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KernelGPT: Enhanced Kernel Fuzzing via Large Language Models"
                },
                "summary": "Bugs in operating system kernels can affect billions of devices and users all\nover the world. As a result, a large body of research has been focused on\nkernel fuzzing, i.e., automatically generating syscall (system call) sequences\nto detect potential kernel bugs or vulnerabilities. Kernel fuzzing aims to\ngenerate valid syscall sequences guided by syscall specifications that define\nboth the syntax and semantics of syscalls. While there has been existing work\ntrying to automate syscall specification generation, this remains largely\nmanual work, and a large number of important syscalls are still uncovered.\n  In this paper, we propose KernelGPT, the first approach to automatically\nsynthesizing syscall specifications via Large Language Models (LLMs) for\nenhanced kernel fuzzing. Our key insight is that LLMs have seen massive kernel\ncode, documentation, and use cases during pre-training, and thus can\nautomatically distill the necessary information for making valid syscalls. More\nspecifically, KernelGPT leverages an iterative approach to automatically infer\nthe specifications, and further debug and repair them based on the validation\nfeedback. Our results demonstrate that KernelGPT can generate more new and\nvalid specifications and achieve higher coverage than state-of-the-art\ntechniques. So far, by using newly generated specifications, KernelGPT has\nalready detected 24 new unique bugs in Linux kernel, with 12 fixed and 11\nassigned with CVE numbers. Moreover, a number of specifications generated by\nKernelGPT have already been merged into the kernel fuzzer Syzkaller, following\nthe request from its development team.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bugs in operating system kernels can affect billions of devices and users all\nover the world. As a result, a large body of research has been focused on\nkernel fuzzing, i.e., automatically generating syscall (system call) sequences\nto detect potential kernel bugs or vulnerabilities. Kernel fuzzing aims to\ngenerate valid syscall sequences guided by syscall specifications that define\nboth the syntax and semantics of syscalls. While there has been existing work\ntrying to automate syscall specification generation, this remains largely\nmanual work, and a large number of important syscalls are still uncovered.\n  In this paper, we propose KernelGPT, the first approach to automatically\nsynthesizing syscall specifications via Large Language Models (LLMs) for\nenhanced kernel fuzzing. Our key insight is that LLMs have seen massive kernel\ncode, documentation, and use cases during pre-training, and thus can\nautomatically distill the necessary information for making valid syscalls. More\nspecifically, KernelGPT leverages an iterative approach to automatically infer\nthe specifications, and further debug and repair them based on the validation\nfeedback. Our results demonstrate that KernelGPT can generate more new and\nvalid specifications and achieve higher coverage than state-of-the-art\ntechniques. So far, by using newly generated specifications, KernelGPT has\nalready detected 24 new unique bugs in Linux kernel, with 12 fixed and 11\nassigned with CVE numbers. Moreover, a number of specifications generated by\nKernelGPT have already been merged into the kernel fuzzer Syzkaller, following\nthe request from its development team."
                },
                "authors": [
                    {
                        "name": "Chenyuan Yang"
                    },
                    {
                        "name": "Zijie Zhao"
                    },
                    {
                        "name": "Lingming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lingming Zhang"
                },
                "author": "Lingming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.00563v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.00563v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16020v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16020v1",
                "updated": "2024-11-25T00:32:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    0,
                    32,
                    20,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T00:32:20Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    0,
                    32,
                    20,
                    0,
                    330,
                    0
                ],
                "title": "TransCompressor: LLM-Powered Multimodal Data Compression for Smart\n  Transportation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransCompressor: LLM-Powered Multimodal Data Compression for Smart\n  Transportation"
                },
                "summary": "The incorporation of Large Language Models (LLMs) into smart transportation\nsystems has paved the way for improving data management and operational\nefficiency. This study introduces TransCompressor, a novel framework that\nleverages LLMs for efficient compression and decompression of multimodal\ntransportation sensor data. TransCompressor has undergone thorough evaluation\nwith diverse sensor data types, including barometer, speed, and altitude\nmeasurements, across various transportation modes like buses, taxis, and MTRs.\nComprehensive evaluation illustrates the effectiveness of TransCompressor in\nreconstructing transportation sensor data at different compression ratios. The\nresults highlight that, with well-crafted prompts, LLMs can utilize their vast\nknowledge base to contribute to data compression processes, enhancing data\nstorage, analysis, and retrieval in smart transportation settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The incorporation of Large Language Models (LLMs) into smart transportation\nsystems has paved the way for improving data management and operational\nefficiency. This study introduces TransCompressor, a novel framework that\nleverages LLMs for efficient compression and decompression of multimodal\ntransportation sensor data. TransCompressor has undergone thorough evaluation\nwith diverse sensor data types, including barometer, speed, and altitude\nmeasurements, across various transportation modes like buses, taxis, and MTRs.\nComprehensive evaluation illustrates the effectiveness of TransCompressor in\nreconstructing transportation sensor data at different compression ratios. The\nresults highlight that, with well-crafted prompts, LLMs can utilize their vast\nknowledge base to contribute to data compression processes, enhancing data\nstorage, analysis, and retrieval in smart transportation settings."
                },
                "authors": [
                    {
                        "name": "Huanqi Yang"
                    },
                    {
                        "name": "Rucheng Wu"
                    },
                    {
                        "name": "Weitao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Weitao Xu"
                },
                "author": "Weitao Xu",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16020v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14985v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14985v3",
                "updated": "2024-11-24T23:25:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    23,
                    25,
                    33,
                    6,
                    329,
                    0
                ],
                "published": "2024-07-20T21:24:40Z",
                "published_parsed": [
                    2024,
                    7,
                    20,
                    21,
                    24,
                    40,
                    5,
                    202,
                    0
                ],
                "title": "Generalization v.s. Memorization: Tracing Language Models' Capabilities\n  Back to Pretraining Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalization v.s. Memorization: Tracing Language Models' Capabilities\n  Back to Pretraining Data"
                },
                "summary": "The impressive capabilities of large language models (LLMs) have sparked\ndebate over whether these models genuinely generalize to unseen tasks or\npredominantly rely on memorizing vast amounts of pretraining data. To explore\nthis issue, we introduce an extended concept of memorization, distributional\nmemorization, which measures the correlation between the LLM output\nprobabilities and the pretraining data frequency. To effectively capture\ntask-specific pretraining data frequency, we propose a novel task-gram language\nmodel, which is built by counting the co-occurrence of semantically related\n$n$-gram pairs from task inputs and outputs in the pretraining corpus. Using\nthe Pythia models trained on the Pile dataset, we evaluate four distinct tasks:\nmachine translation, factual question answering, world knowledge understanding,\nand math reasoning. Our findings reveal varying levels of memorization, with\nthe strongest effect observed in factual question answering. Furthermore, while\nmodel performance improves across all tasks as LLM size increases, only factual\nquestion answering shows an increase in memorization, whereas machine\ntranslation and reasoning tasks exhibit greater generalization, producing more\nnovel outputs. This study demonstrates that memorization plays a larger role in\nsimpler, knowledge-intensive tasks, while generalization is the key for harder,\nreasoning-based tasks, providing a scalable method for analyzing large\npretraining corpora in greater depth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impressive capabilities of large language models (LLMs) have sparked\ndebate over whether these models genuinely generalize to unseen tasks or\npredominantly rely on memorizing vast amounts of pretraining data. To explore\nthis issue, we introduce an extended concept of memorization, distributional\nmemorization, which measures the correlation between the LLM output\nprobabilities and the pretraining data frequency. To effectively capture\ntask-specific pretraining data frequency, we propose a novel task-gram language\nmodel, which is built by counting the co-occurrence of semantically related\n$n$-gram pairs from task inputs and outputs in the pretraining corpus. Using\nthe Pythia models trained on the Pile dataset, we evaluate four distinct tasks:\nmachine translation, factual question answering, world knowledge understanding,\nand math reasoning. Our findings reveal varying levels of memorization, with\nthe strongest effect observed in factual question answering. Furthermore, while\nmodel performance improves across all tasks as LLM size increases, only factual\nquestion answering shows an increase in memorization, whereas machine\ntranslation and reasoning tasks exhibit greater generalization, producing more\nnovel outputs. This study demonstrates that memorization plays a larger role in\nsimpler, knowledge-intensive tasks, while generalization is the key for harder,\nreasoning-based tasks, providing a scalable method for analyzing large\npretraining corpora in greater depth."
                },
                "authors": [
                    {
                        "name": "Xinyi Wang"
                    },
                    {
                        "name": "Antonis Antoniades"
                    },
                    {
                        "name": "Yanai Elazar"
                    },
                    {
                        "name": "Alfonso Amayuelas"
                    },
                    {
                        "name": "Alon Albalak"
                    },
                    {
                        "name": "Kexun Zhang"
                    },
                    {
                        "name": "William Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "William Yang Wang"
                },
                "author": "William Yang Wang",
                "arxiv_comment": "updated 10-page version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14985v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14985v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16003v1",
                "updated": "2024-11-24T22:50:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    22,
                    50,
                    2,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T22:50:02Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    22,
                    50,
                    2,
                    6,
                    329,
                    0
                ],
                "title": "eFedLLM: Efficient LLM Inference Based on Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "eFedLLM: Efficient LLM Inference Based on Federated Learning"
                },
                "summary": "Large Language Models (LLMs) herald a transformative era in artificial\nintelligence (AI). However, the expansive scale of data and parameters of LLMs\nrequires high-demand computational and memory resources, restricting their\naccessibility to a broader range of users and researchers. This paper\nintroduces an effective approach that enhances the operational efficiency and\naffordability of LLM inference. By utilizing transformer-based federated\nlearning (FL) with model-parallel distributed training, our model efficiently\ndistributes the computational loads and memory requirements across a network of\nparticipants. This strategy permits users, especially those with limited\nresources to train state-of-the-art LLMs collaboratively. We also innovate an\nincentive mechanism within the FL framework, rewarding constructive\ncontributions and filtering out malicious activities, thereby safeguarding the\nintegrity and reliability of the training process. Concurrently, we leverage\nmemory hierarchy strategies and Singular Value Decomposition (SVD) on weight\nmatrices to boost computational and memory efficiencies further. Our results,\nderived from formulaic analyses and numerical calculations, demonstrate\nsignificant optimization of resource use and democratize access to cutting-edge\nLLMs, ensuring that a wide scale of users can both contribute to and benefit\nfrom these advanced models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) herald a transformative era in artificial\nintelligence (AI). However, the expansive scale of data and parameters of LLMs\nrequires high-demand computational and memory resources, restricting their\naccessibility to a broader range of users and researchers. This paper\nintroduces an effective approach that enhances the operational efficiency and\naffordability of LLM inference. By utilizing transformer-based federated\nlearning (FL) with model-parallel distributed training, our model efficiently\ndistributes the computational loads and memory requirements across a network of\nparticipants. This strategy permits users, especially those with limited\nresources to train state-of-the-art LLMs collaboratively. We also innovate an\nincentive mechanism within the FL framework, rewarding constructive\ncontributions and filtering out malicious activities, thereby safeguarding the\nintegrity and reliability of the training process. Concurrently, we leverage\nmemory hierarchy strategies and Singular Value Decomposition (SVD) on weight\nmatrices to boost computational and memory efficiencies further. Our results,\nderived from formulaic analyses and numerical calculations, demonstrate\nsignificant optimization of resource use and democratize access to cutting-edge\nLLMs, ensuring that a wide scale of users can both contribute to and benefit\nfrom these advanced models."
                },
                "authors": [
                    {
                        "name": "Shengwen Ding"
                    },
                    {
                        "name": "Chenhui Hu"
                    }
                ],
                "author_detail": {
                    "name": "Chenhui Hu"
                },
                "author": "Chenhui Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15999v1",
                "updated": "2024-11-24T22:37:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    22,
                    37,
                    59,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T22:37:59Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    22,
                    37,
                    59,
                    6,
                    329,
                    0
                ],
                "title": "Multi-ToM: Evaluating Multilingual Theory of Mind Capabilities in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-ToM: Evaluating Multilingual Theory of Mind Capabilities in Large\n  Language Models"
                },
                "summary": "Theory of Mind (ToM) refers to the cognitive ability to infer and attribute\nmental states to oneself and others. As large language models (LLMs) are\nincreasingly evaluated for social and cognitive capabilities, it remains\nunclear to what extent these models demonstrate ToM across diverse languages\nand cultural contexts. In this paper, we introduce a comprehensive study of\nmultilingual ToM capabilities aimed at addressing this gap. Our approach\nincludes two key components: (1) We translate existing ToM datasets into\nmultiple languages, effectively creating a multilingual ToM dataset and (2) We\nenrich these translations with culturally specific elements to reflect the\nsocial and cognitive scenarios relevant to diverse populations. We conduct\nextensive evaluations of six state-of-the-art LLMs to measure their ToM\nperformance across both the translated and culturally adapted datasets. The\nresults highlight the influence of linguistic and cultural diversity on the\nmodels' ability to exhibit ToM, and questions their social reasoning\ncapabilities. This work lays the groundwork for future research into enhancing\nLLMs' cross-cultural social cognition and contributes to the development of\nmore culturally aware and socially intelligent AI systems. All our data and\ncode are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory of Mind (ToM) refers to the cognitive ability to infer and attribute\nmental states to oneself and others. As large language models (LLMs) are\nincreasingly evaluated for social and cognitive capabilities, it remains\nunclear to what extent these models demonstrate ToM across diverse languages\nand cultural contexts. In this paper, we introduce a comprehensive study of\nmultilingual ToM capabilities aimed at addressing this gap. Our approach\nincludes two key components: (1) We translate existing ToM datasets into\nmultiple languages, effectively creating a multilingual ToM dataset and (2) We\nenrich these translations with culturally specific elements to reflect the\nsocial and cognitive scenarios relevant to diverse populations. We conduct\nextensive evaluations of six state-of-the-art LLMs to measure their ToM\nperformance across both the translated and culturally adapted datasets. The\nresults highlight the influence of linguistic and cultural diversity on the\nmodels' ability to exhibit ToM, and questions their social reasoning\ncapabilities. This work lays the groundwork for future research into enhancing\nLLMs' cross-cultural social cognition and contributes to the development of\nmore culturally aware and socially intelligent AI systems. All our data and\ncode are publicly available."
                },
                "authors": [
                    {
                        "name": "Jayanta Sadhu"
                    },
                    {
                        "name": "Ayan Antik Khan"
                    },
                    {
                        "name": "Noshin Nawal"
                    },
                    {
                        "name": "Sanju Basak"
                    },
                    {
                        "name": "Abhik Bhattacharjee"
                    },
                    {
                        "name": "Rifat Shahriyar"
                    }
                ],
                "author_detail": {
                    "name": "Rifat Shahriyar"
                },
                "author": "Rifat Shahriyar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15998v1",
                "updated": "2024-11-24T22:36:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    22,
                    36,
                    34,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T22:36:34Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    22,
                    36,
                    34,
                    6,
                    329,
                    0
                ],
                "title": "PIANIST: Learning Partially Observable World Models with LLMs for\n  Multi-Agent Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIANIST: Learning Partially Observable World Models with LLMs for\n  Multi-Agent Decision Making"
                },
                "summary": "Effective extraction of the world knowledge in LLMs for complex\ndecision-making tasks remains a challenge. We propose a framework PIANIST for\ndecomposing the world model into seven intuitive components conducive to\nzero-shot LLM generation. Given only the natural language description of the\ngame and how input observations are formatted, our method can generate a\nworking world model for fast and efficient MCTS simulation. We show that our\nmethod works well on two different games that challenge the planning and\ndecision making skills of the agent for both language and non-language based\naction taking, without any training on domain-specific training data or\nexplicitly defined world model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective extraction of the world knowledge in LLMs for complex\ndecision-making tasks remains a challenge. We propose a framework PIANIST for\ndecomposing the world model into seven intuitive components conducive to\nzero-shot LLM generation. Given only the natural language description of the\ngame and how input observations are formatted, our method can generate a\nworking world model for fast and efficient MCTS simulation. We show that our\nmethod works well on two different games that challenge the planning and\ndecision making skills of the agent for both language and non-language based\naction taking, without any training on domain-specific training data or\nexplicitly defined world model."
                },
                "authors": [
                    {
                        "name": "Jonathan Light"
                    },
                    {
                        "name": "Sixue Xing"
                    },
                    {
                        "name": "Yuanzhe Liu"
                    },
                    {
                        "name": "Weiqin Chen"
                    },
                    {
                        "name": "Min Cai"
                    },
                    {
                        "name": "Xiusi Chen"
                    },
                    {
                        "name": "Guanzhi Wang"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Yisong Yue"
                    },
                    {
                        "name": "Ziniu Hu"
                    }
                ],
                "author_detail": {
                    "name": "Ziniu Hu"
                },
                "author": "Ziniu Hu",
                "arxiv_comment": "Published at Language Gamification Workshop 2024 @ NeurIPS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15997v1",
                "updated": "2024-11-24T22:35:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    22,
                    35,
                    44,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T22:35:44Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    22,
                    35,
                    44,
                    6,
                    329,
                    0
                ],
                "title": "Ensuring Fair LLM Serving Amid Diverse Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring Fair LLM Serving Amid Diverse Applications"
                },
                "summary": "In a multi-tenant large language model (LLM) serving platform hosting diverse\napplications, some users may submit an excessive number of requests, causing\nthe service to become unavailable to other users and creating unfairness.\nExisting fairness approaches do not account for variations in token lengths\nacross applications and multiple LLM calls, making them unsuitable for such\nplatforms. To address the fairness challenge, this paper analyzes millions of\nrequests from thousands of users on MS CoPilot, a real-world multi-tenant LLM\nplatform hosted by Microsoft. Our analysis confirms the inadequacy of existing\nmethods and guides the development of FairServe, a system that ensures fair LLM\naccess across diverse applications. FairServe proposes\napplication-characteristic aware request throttling coupled with a weighted\nservice counter based scheduling technique to curb abusive behavior and ensure\nfairness. Our experimental results on real-world traces demonstrate FairServe's\nsuperior performance compared to the state-of-the-art method in ensuring\nfairness. We are actively working on deploying our system in production,\nexpecting to benefit millions of customers world-wide.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a multi-tenant large language model (LLM) serving platform hosting diverse\napplications, some users may submit an excessive number of requests, causing\nthe service to become unavailable to other users and creating unfairness.\nExisting fairness approaches do not account for variations in token lengths\nacross applications and multiple LLM calls, making them unsuitable for such\nplatforms. To address the fairness challenge, this paper analyzes millions of\nrequests from thousands of users on MS CoPilot, a real-world multi-tenant LLM\nplatform hosted by Microsoft. Our analysis confirms the inadequacy of existing\nmethods and guides the development of FairServe, a system that ensures fair LLM\naccess across diverse applications. FairServe proposes\napplication-characteristic aware request throttling coupled with a weighted\nservice counter based scheduling technique to curb abusive behavior and ensure\nfairness. Our experimental results on real-world traces demonstrate FairServe's\nsuperior performance compared to the state-of-the-art method in ensuring\nfairness. We are actively working on deploying our system in production,\nexpecting to benefit millions of customers world-wide."
                },
                "authors": [
                    {
                        "name": "Redwan Ibne Seraj Khan"
                    },
                    {
                        "name": "Kunal Jain"
                    },
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Ankur Mallick"
                    },
                    {
                        "name": "Anjaly Parayil"
                    },
                    {
                        "name": "Anoop Kulkarni"
                    },
                    {
                        "name": "Steve Kofsky"
                    },
                    {
                        "name": "Pankhuri Choudhary"
                    },
                    {
                        "name": "Rene St. Amant"
                    },
                    {
                        "name": "Rujia Wang"
                    },
                    {
                        "name": "Yue Cheng"
                    },
                    {
                        "name": "Ali R. Butt"
                    },
                    {
                        "name": "Victor Rhle"
                    },
                    {
                        "name": "Chetan Bansal"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15993v1",
                "updated": "2024-11-24T22:06:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    22,
                    6,
                    26,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T22:06:26Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    22,
                    6,
                    26,
                    6,
                    329,
                    0
                ],
                "title": "Investigating Factuality in Long-Form Text Generation: The Roles of\n  Self-Known and Self-Unknown",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Factuality in Long-Form Text Generation: The Roles of\n  Self-Known and Self-Unknown"
                },
                "summary": "Large language models (LLMs) have demonstrated strong capabilities in text\nunderstanding and generation. However, they often lack factuality, producing a\nmixture of true and false information, especially in long-form generation. In\nthis work, we investigates the factuality of long-form text generation across\nvarious large language models (LLMs), including GPT-4, Gemini-1.5-Pro,\nClaude-3-Opus, Llama-3-70B, and Mistral. Our analysis reveals that factuality\nscores tend to decline in later sentences of the generated text, accompanied by\na rise in the number of unsupported claims. Furthermore, we explore the\neffectiveness of different evaluation settings to assess whether LLMs can\naccurately judge the correctness of their own outputs: Self-Known (the\npercentage of supported atomic claims, decomposed from LLM outputs, that the\ncorresponding LLMs judge as correct) and Self-Unknown (the percentage of\nunsupported atomic claims that the corresponding LLMs judge as incorrect). The\nresults indicate that even advanced models like GPT-4 and Gemini-1.5-Pro fail\nto achieve perfect Self-Known scores, while their Self-Unknown scores remain\nnotably above zero, reflecting ongoing uncertainty in their self-assessments.\nMoreover, we find a correlation between higher Self-Known scores and improved\nfactuality, while higher Self-Unknown scores are associated with lower\nfactuality. Interestingly, even without significant changes in the models'\nself-judgment (Self-Known and Self-Unknown), the number of unsupported claims\ncan increases, likely as an artifact of long-form generation. These findings\nshow the limitations of current LLMs in long-form generation, and provide\nvaluable insights for improving factuality in long-form text generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong capabilities in text\nunderstanding and generation. However, they often lack factuality, producing a\nmixture of true and false information, especially in long-form generation. In\nthis work, we investigates the factuality of long-form text generation across\nvarious large language models (LLMs), including GPT-4, Gemini-1.5-Pro,\nClaude-3-Opus, Llama-3-70B, and Mistral. Our analysis reveals that factuality\nscores tend to decline in later sentences of the generated text, accompanied by\na rise in the number of unsupported claims. Furthermore, we explore the\neffectiveness of different evaluation settings to assess whether LLMs can\naccurately judge the correctness of their own outputs: Self-Known (the\npercentage of supported atomic claims, decomposed from LLM outputs, that the\ncorresponding LLMs judge as correct) and Self-Unknown (the percentage of\nunsupported atomic claims that the corresponding LLMs judge as incorrect). The\nresults indicate that even advanced models like GPT-4 and Gemini-1.5-Pro fail\nto achieve perfect Self-Known scores, while their Self-Unknown scores remain\nnotably above zero, reflecting ongoing uncertainty in their self-assessments.\nMoreover, we find a correlation between higher Self-Known scores and improved\nfactuality, while higher Self-Unknown scores are associated with lower\nfactuality. Interestingly, even without significant changes in the models'\nself-judgment (Self-Known and Self-Unknown), the number of unsupported claims\ncan increases, likely as an artifact of long-form generation. These findings\nshow the limitations of current LLMs in long-form generation, and provide\nvaluable insights for improving factuality in long-form text generation."
                },
                "authors": [
                    {
                        "name": "Lifu Tu"
                    },
                    {
                        "name": "Rui Meng"
                    },
                    {
                        "name": "Shafiq Joty"
                    },
                    {
                        "name": "Yingbo Zhou"
                    },
                    {
                        "name": "Semih Yavuz"
                    }
                ],
                "author_detail": {
                    "name": "Semih Yavuz"
                },
                "author": "Semih Yavuz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13352v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13352v3",
                "updated": "2024-11-24T22:04:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    22,
                    4,
                    23,
                    6,
                    329,
                    0
                ],
                "published": "2024-06-19T08:55:56Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    8,
                    55,
                    56,
                    2,
                    171,
                    0
                ],
                "title": "AgentDojo: A Dynamic Environment to Evaluate Prompt Injection Attacks\n  and Defenses for LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentDojo: A Dynamic Environment to Evaluate Prompt Injection Attacks\n  and Defenses for LLM Agents"
                },
                "summary": "AI agents aim to solve complex tasks by combining text-based reasoning with\nexternal tool calls. Unfortunately, AI agents are vulnerable to prompt\ninjection attacks where data returned by external tools hijacks the agent to\nexecute malicious tasks. To measure the adversarial robustness of AI agents, we\nintroduce AgentDojo, an evaluation framework for agents that execute tools over\nuntrusted data. To capture the evolving nature of attacks and defenses,\nAgentDojo is not a static test suite, but rather an extensible environment for\ndesigning and evaluating new agent tasks, defenses, and adaptive attacks. We\npopulate the environment with 97 realistic tasks (e.g., managing an email\nclient, navigating an e-banking website, or making travel bookings), 629\nsecurity test cases, and various attack and defense paradigms from the\nliterature. We find that AgentDojo poses a challenge for both attacks and\ndefenses: state-of-the-art LLMs fail at many tasks (even in the absence of\nattacks), and existing prompt injection attacks break some security properties\nbut not all. We hope that AgentDojo can foster research on new design\nprinciples for AI agents that solve common tasks in a reliable and robust\nmanner.. We release the code for AgentDojo at\nhttps://github.com/ethz-spylab/agentdojo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents aim to solve complex tasks by combining text-based reasoning with\nexternal tool calls. Unfortunately, AI agents are vulnerable to prompt\ninjection attacks where data returned by external tools hijacks the agent to\nexecute malicious tasks. To measure the adversarial robustness of AI agents, we\nintroduce AgentDojo, an evaluation framework for agents that execute tools over\nuntrusted data. To capture the evolving nature of attacks and defenses,\nAgentDojo is not a static test suite, but rather an extensible environment for\ndesigning and evaluating new agent tasks, defenses, and adaptive attacks. We\npopulate the environment with 97 realistic tasks (e.g., managing an email\nclient, navigating an e-banking website, or making travel bookings), 629\nsecurity test cases, and various attack and defense paradigms from the\nliterature. We find that AgentDojo poses a challenge for both attacks and\ndefenses: state-of-the-art LLMs fail at many tasks (even in the absence of\nattacks), and existing prompt injection attacks break some security properties\nbut not all. We hope that AgentDojo can foster research on new design\nprinciples for AI agents that solve common tasks in a reliable and robust\nmanner.. We release the code for AgentDojo at\nhttps://github.com/ethz-spylab/agentdojo."
                },
                "authors": [
                    {
                        "name": "Edoardo Debenedetti"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Mislav Balunovi"
                    },
                    {
                        "name": "Luca Beurer-Kellner"
                    },
                    {
                        "name": "Marc Fischer"
                    },
                    {
                        "name": "Florian Tramr"
                    }
                ],
                "author_detail": {
                    "name": "Florian Tramr"
                },
                "author": "Florian Tramr",
                "arxiv_comment": "Updated version after fixing a bug in the Llama implementation and\n  updating the travel suite",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13352v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13352v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17011v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17011v3",
                "updated": "2024-11-24T21:49:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    21,
                    49,
                    20,
                    6,
                    329,
                    0
                ],
                "published": "2024-09-25T15:15:57Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    15,
                    57,
                    2,
                    269,
                    0
                ],
                "title": "AutoLLM-CARD: Towards a Description and Landscape of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoLLM-CARD: Towards a Description and Landscape of Large Language\n  Models"
                },
                "summary": "With the rapid growth of the Natural Language Processing (NLP) field, a vast\nvariety of Large Language Models (LLMs) continue to emerge for diverse NLP\ntasks. As more papers are published, researchers and developers face the\nchallenge of information overload. Thus, developing a system that can\nautomatically extract and organise key information about LLMs from academic\npapers is particularly important. The standard format for documenting\ninformation about LLMs is the LLM model card (\\textbf{LLM-Card}). We propose a\nmethod for automatically generating LLM model cards from scientific\npublications. We use Named Entity Recognition (\\textbf{NER}) and Relation\nExtraction (\\textbf{RE}) methods that automatically extract key information\nabout LLMs from the papers, helping researchers to access information about\nLLMs efficiently. These features include model \\textit{licence}, model\n\\textit{name}, and model \\textit{application}. With these features, we can form\na model card for each paper. We processed 106 academic papers by defining three\ndictionaries -- LLM's name, licence, and application. 11,051 sentences were\nextracted through dictionary lookup, and the dataset was constructed through\nmanual review of the final selection of 129 sentences with a link between the\nname and the \\textit{licence}, and 106 sentences with a link between the model\nname and the \\textit{application}. The resulting resource is relevant for LLM\ncard illustrations using relational knowledge graphs. Our code and findings can\ncontribute to automatic LLM card generation. Data and code in\n\\textsc{autoLLM-Card} will be shared and freely available at\n\\url{https://github.com/shengwei-tian/dependency-parser-visualization}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of the Natural Language Processing (NLP) field, a vast\nvariety of Large Language Models (LLMs) continue to emerge for diverse NLP\ntasks. As more papers are published, researchers and developers face the\nchallenge of information overload. Thus, developing a system that can\nautomatically extract and organise key information about LLMs from academic\npapers is particularly important. The standard format for documenting\ninformation about LLMs is the LLM model card (\\textbf{LLM-Card}). We propose a\nmethod for automatically generating LLM model cards from scientific\npublications. We use Named Entity Recognition (\\textbf{NER}) and Relation\nExtraction (\\textbf{RE}) methods that automatically extract key information\nabout LLMs from the papers, helping researchers to access information about\nLLMs efficiently. These features include model \\textit{licence}, model\n\\textit{name}, and model \\textit{application}. With these features, we can form\na model card for each paper. We processed 106 academic papers by defining three\ndictionaries -- LLM's name, licence, and application. 11,051 sentences were\nextracted through dictionary lookup, and the dataset was constructed through\nmanual review of the final selection of 129 sentences with a link between the\nname and the \\textit{licence}, and 106 sentences with a link between the model\nname and the \\textit{application}. The resulting resource is relevant for LLM\ncard illustrations using relational knowledge graphs. Our code and findings can\ncontribute to automatic LLM card generation. Data and code in\n\\textsc{autoLLM-Card} will be shared and freely available at\n\\url{https://github.com/shengwei-tian/dependency-parser-visualization}"
                },
                "authors": [
                    {
                        "name": "Shengwei Tian"
                    },
                    {
                        "name": "Lifeng Han"
                    },
                    {
                        "name": "Goran Nenadic"
                    }
                ],
                "author_detail": {
                    "name": "Goran Nenadic"
                },
                "author": "Goran Nenadic",
                "arxiv_comment": "ongoing work, technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17011v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17011v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15982v1",
                "updated": "2024-11-24T20:59:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    20,
                    59,
                    39,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T20:59:39Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    20,
                    59,
                    39,
                    6,
                    329,
                    0
                ],
                "title": "Anda: Unlocking Efficient LLM Inference with a Variable-Length Grouped\n  Activation Data Format",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anda: Unlocking Efficient LLM Inference with a Variable-Length Grouped\n  Activation Data Format"
                },
                "summary": "The widely-used, weight-only quantized large language models (LLMs), which\nleverage low-bit integer (INT) weights and retain floating-point (FP)\nactivations, reduce storage requirements while maintaining accuracy. However,\nthis shifts the energy and latency bottlenecks towards the FP activations that\nare associated with costly memory accesses and computations. Existing LLM\naccelerators focus primarily on computation optimizations, overlooking the\npotential of jointly optimizing FP computations and data movement, particularly\nfor the dominant FP-INT GeMM operations in LLM inference.\n  To address these challenges, we investigate the sensitivity of activation\nprecision across various LLM modules and its impact on overall model accuracy.\nBased on our findings, we first propose the Anda data type: an adaptive data\nformat with group-shared exponent bits and dynamic mantissa bit allocation.\nSecondly, we develop an iterative post-training adaptive precision search\nalgorithm that optimizes the bit-width for different LLM modules to balance\nmodel accuracy, energy efficiency, and inference speed. Lastly, a suite of\nhardware optimization techniques is proposed to maximally exploit the benefits\nof the Anda format. These include a bit-plane-based data organization scheme,\nAnda-enhanced processing units with bit-serial computation, and a runtime\nbit-plane Anda compressor to simultaneously optimize storage, computation, and\nmemory footprints. Our evaluations on FPINT GeMM operations show that Anda\nachieves a 2.4x speedup, 4.0x area efficiency, and 3.1x energy efficiency\nimprovement on average for popular LLMs including OPT, LLaMA, and LLaMA-2\nseries over the GPU-like FP-FP baseline. Anda demonstrates strong adaptability\nacross various application scenarios, accuracy requirements, and system\nperformance, enabling efficient LLM inference across a wide range of deployment\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widely-used, weight-only quantized large language models (LLMs), which\nleverage low-bit integer (INT) weights and retain floating-point (FP)\nactivations, reduce storage requirements while maintaining accuracy. However,\nthis shifts the energy and latency bottlenecks towards the FP activations that\nare associated with costly memory accesses and computations. Existing LLM\naccelerators focus primarily on computation optimizations, overlooking the\npotential of jointly optimizing FP computations and data movement, particularly\nfor the dominant FP-INT GeMM operations in LLM inference.\n  To address these challenges, we investigate the sensitivity of activation\nprecision across various LLM modules and its impact on overall model accuracy.\nBased on our findings, we first propose the Anda data type: an adaptive data\nformat with group-shared exponent bits and dynamic mantissa bit allocation.\nSecondly, we develop an iterative post-training adaptive precision search\nalgorithm that optimizes the bit-width for different LLM modules to balance\nmodel accuracy, energy efficiency, and inference speed. Lastly, a suite of\nhardware optimization techniques is proposed to maximally exploit the benefits\nof the Anda format. These include a bit-plane-based data organization scheme,\nAnda-enhanced processing units with bit-serial computation, and a runtime\nbit-plane Anda compressor to simultaneously optimize storage, computation, and\nmemory footprints. Our evaluations on FPINT GeMM operations show that Anda\nachieves a 2.4x speedup, 4.0x area efficiency, and 3.1x energy efficiency\nimprovement on average for popular LLMs including OPT, LLaMA, and LLaMA-2\nseries over the GPU-like FP-FP baseline. Anda demonstrates strong adaptability\nacross various application scenarios, accuracy requirements, and system\nperformance, enabling efficient LLM inference across a wide range of deployment\nscenarios."
                },
                "authors": [
                    {
                        "name": "Chao Fang"
                    },
                    {
                        "name": "Man Shi"
                    },
                    {
                        "name": "Robin Geens"
                    },
                    {
                        "name": "Arne Symons"
                    },
                    {
                        "name": "Zhongfeng Wang"
                    },
                    {
                        "name": "Marian Verhelst"
                    }
                ],
                "author_detail": {
                    "name": "Marian Verhelst"
                },
                "author": "Marian Verhelst",
                "arxiv_comment": "To appear in 2025 IEEE International Symposium on High-Performance\n  Computer Architecture (HPCA 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16886v2",
                "updated": "2024-11-24T20:54:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    20,
                    54,
                    48,
                    6,
                    329,
                    0
                ],
                "published": "2024-08-29T20:19:10Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    20,
                    19,
                    10,
                    3,
                    242,
                    0
                ],
                "title": "LV-UNet: A Lightweight and Vanilla Model for Medical Image Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LV-UNet: A Lightweight and Vanilla Model for Medical Image Segmentation"
                },
                "summary": "While large models have achieved significant progress in computer vision,\nchallenges such as optimization complexity, the intricacy of transformer\narchitectures, computational constraints, and practical application demands\nhighlight the importance of simpler model designs in medical image\nsegmentation. This need is particularly pronounced in mobile medical devices,\nwhich require lightweight, deployable models with real-time performance.\nHowever, existing lightweight models often suffer from poor robustness across\ndatasets, limiting their widespread adoption. To address these challenges, this\npaper introduces LV-UNet, a lightweight and vanilla model that leverages\npre-trained MobileNetv3-Large backbones and incorporates fusible modules.\nLV-UNet employs an enhanced deep training strategy and switches to a deployment\nmode during inference by re-parametrization, significantly reducing parameter\ncount and computational overhead. Experimental results on ISIC 2016, BUSI,\nCVC-ClinicDB, CVC-ColonDB, and Kvair-SEG datasets demonstrate a better\ntrade-off between performance and the computational load. The code will be\nreleased at \\url{https://github.com/juntaoJianggavin/LV-UNet}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large models have achieved significant progress in computer vision,\nchallenges such as optimization complexity, the intricacy of transformer\narchitectures, computational constraints, and practical application demands\nhighlight the importance of simpler model designs in medical image\nsegmentation. This need is particularly pronounced in mobile medical devices,\nwhich require lightweight, deployable models with real-time performance.\nHowever, existing lightweight models often suffer from poor robustness across\ndatasets, limiting their widespread adoption. To address these challenges, this\npaper introduces LV-UNet, a lightweight and vanilla model that leverages\npre-trained MobileNetv3-Large backbones and incorporates fusible modules.\nLV-UNet employs an enhanced deep training strategy and switches to a deployment\nmode during inference by re-parametrization, significantly reducing parameter\ncount and computational overhead. Experimental results on ISIC 2016, BUSI,\nCVC-ClinicDB, CVC-ColonDB, and Kvair-SEG datasets demonstrate a better\ntrade-off between performance and the computational load. The code will be\nreleased at \\url{https://github.com/juntaoJianggavin/LV-UNet}."
                },
                "authors": [
                    {
                        "name": "Juntao Jiang"
                    },
                    {
                        "name": "Mengmeng Wang"
                    },
                    {
                        "name": "Huizhong Tian"
                    },
                    {
                        "name": "Lingbo Cheng"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "Accepted by IEEE BIBM2024 ML4BMI workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01109v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01109v6",
                "updated": "2024-11-24T20:09:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    20,
                    9,
                    55,
                    6,
                    329,
                    0
                ],
                "published": "2024-02-02T02:56:50Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    2,
                    56,
                    50,
                    4,
                    33,
                    0
                ],
                "title": "Vaccine: Perturbation-aware Alignment for Large Language Models against\n  Harmful Fine-tuning Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vaccine: Perturbation-aware Alignment for Large Language Models against\n  Harmful Fine-tuning Attack"
                },
                "summary": "The new paradigm of finetuning-as-a-service introduces a new attack surface\nfor Large Language Models (LLMs): a few harmful data uploaded by users can\neasily trick the finetuning to produce an alignment-broken model. We conduct an\nempirical analysis and uncover a \\textit{harmful embedding drift} phenomenon,\nshowing a probable cause of the alignment-broken effect. Inspired by our\nfindings, we propose Vaccine, a perturbation-aware alignment technique to\nmitigate the security risk of users finetuning. The core idea of Vaccine is to\nproduce invariant hidden embeddings by progressively adding crafted\nperturbation to them in the alignment phase. This enables the embeddings to\nwithstand harmful perturbation from un-sanitized user data in the finetuning\nphase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna)\ndemonstrate that Vaccine can boost the robustness of alignment against harmful\nprompts induced embedding drift while reserving reasoning ability towards\nbenign prompts. Our code is available at\n\\url{https://github.com/git-disl/Vaccine}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new paradigm of finetuning-as-a-service introduces a new attack surface\nfor Large Language Models (LLMs): a few harmful data uploaded by users can\neasily trick the finetuning to produce an alignment-broken model. We conduct an\nempirical analysis and uncover a \\textit{harmful embedding drift} phenomenon,\nshowing a probable cause of the alignment-broken effect. Inspired by our\nfindings, we propose Vaccine, a perturbation-aware alignment technique to\nmitigate the security risk of users finetuning. The core idea of Vaccine is to\nproduce invariant hidden embeddings by progressively adding crafted\nperturbation to them in the alignment phase. This enables the embeddings to\nwithstand harmful perturbation from un-sanitized user data in the finetuning\nphase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna)\ndemonstrate that Vaccine can boost the robustness of alignment against harmful\nprompts induced embedding drift while reserving reasoning ability towards\nbenign prompts. Our code is available at\n\\url{https://github.com/git-disl/Vaccine}."
                },
                "authors": [
                    {
                        "name": "Tiansheng Huang"
                    },
                    {
                        "name": "Sihao Hu"
                    },
                    {
                        "name": "Ling Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ling Liu"
                },
                "author": "Ling Liu",
                "arxiv_comment": "Rejected by ICML2024. Accepted by NeurIPS2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01109v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01109v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10313v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10313v2",
                "updated": "2024-11-24T18:44:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    18,
                    44,
                    27,
                    6,
                    329,
                    0
                ],
                "published": "2024-05-16T17:59:02Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    17,
                    59,
                    2,
                    3,
                    137,
                    0
                ],
                "title": "How Far Are We From AGI: Are LLMs All We Need?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Far Are We From AGI: Are LLMs All We Need?"
                },
                "summary": "The evolution of artificial intelligence (AI) has profoundly impacted human\nsociety, driving significant advancements in multiple sectors. AGI,\ndistinguished by its ability to execute diverse real-world tasks with\nefficiency and effectiveness comparable to human intelligence, reflects a\nparamount milestone in AI evolution. While existing studies have reviewed\nspecific advancements in AI and proposed potential paths to AGI, such as large\nlanguage models (LLMs), they fall short of providing a thorough exploration of\nAGI's definitions, objectives, and developmental trajectories. Unlike previous\nsurvey papers, this work goes beyond summarizing LLMs by addressing key\nquestions about our progress toward AGI and outlining the strategies essential\nfor its realization through comprehensive analysis, in-depth discussions, and\nnovel insights. We start by articulating the requisite capability frameworks\nfor AGI, integrating the internal, interface, and system dimensions. As the\nrealization of AGI requires more advanced capabilities and adherence to\nstringent constraints, we further discuss necessary AGI alignment technologies\nto harmonize these factors. Notably, we emphasize the importance of approaching\nAGI responsibly by first defining the key levels of AGI progression, followed\nby the evaluation framework that situates the status quo, and finally giving\nour roadmap of how to reach the pinnacle of AGI. Moreover, to give tangible\ninsights into the ubiquitous impact of the integration of AI, we outline\nexisting challenges and potential pathways toward AGI in multiple domains. In\nsum, serving as a pioneering exploration into the current state and future\ntrajectory of AGI, this paper aims to foster a collective comprehension and\ncatalyze broader public discussions among researchers and practitioners on AGI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of artificial intelligence (AI) has profoundly impacted human\nsociety, driving significant advancements in multiple sectors. AGI,\ndistinguished by its ability to execute diverse real-world tasks with\nefficiency and effectiveness comparable to human intelligence, reflects a\nparamount milestone in AI evolution. While existing studies have reviewed\nspecific advancements in AI and proposed potential paths to AGI, such as large\nlanguage models (LLMs), they fall short of providing a thorough exploration of\nAGI's definitions, objectives, and developmental trajectories. Unlike previous\nsurvey papers, this work goes beyond summarizing LLMs by addressing key\nquestions about our progress toward AGI and outlining the strategies essential\nfor its realization through comprehensive analysis, in-depth discussions, and\nnovel insights. We start by articulating the requisite capability frameworks\nfor AGI, integrating the internal, interface, and system dimensions. As the\nrealization of AGI requires more advanced capabilities and adherence to\nstringent constraints, we further discuss necessary AGI alignment technologies\nto harmonize these factors. Notably, we emphasize the importance of approaching\nAGI responsibly by first defining the key levels of AGI progression, followed\nby the evaluation framework that situates the status quo, and finally giving\nour roadmap of how to reach the pinnacle of AGI. Moreover, to give tangible\ninsights into the ubiquitous impact of the integration of AI, we outline\nexisting challenges and potential pathways toward AGI in multiple domains. In\nsum, serving as a pioneering exploration into the current state and future\ntrajectory of AGI, this paper aims to foster a collective comprehension and\ncatalyze broader public discussions among researchers and practitioners on AGI."
                },
                "authors": [
                    {
                        "name": "Tao Feng"
                    },
                    {
                        "name": "Chuanyang Jin"
                    },
                    {
                        "name": "Jingyu Liu"
                    },
                    {
                        "name": "Kunlun Zhu"
                    },
                    {
                        "name": "Haoqin Tu"
                    },
                    {
                        "name": "Zirui Cheng"
                    },
                    {
                        "name": "Guanyu Lin"
                    },
                    {
                        "name": "Jiaxuan You"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxuan You"
                },
                "author": "Jiaxuan You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10313v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10313v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.12815v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.12815v4",
                "updated": "2024-11-24T18:14:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    18,
                    14,
                    20,
                    6,
                    329,
                    0
                ],
                "published": "2023-10-19T15:12:09Z",
                "published_parsed": [
                    2023,
                    10,
                    19,
                    15,
                    12,
                    9,
                    3,
                    292,
                    0
                ],
                "title": "Formalizing and Benchmarking Prompt Injection Attacks and Defenses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formalizing and Benchmarking Prompt Injection Attacks and Defenses"
                },
                "summary": "A prompt injection attack aims to inject malicious instruction/data into the\ninput of an LLM-Integrated Application such that it produces results as an\nattacker desires. Existing works are limited to case studies. As a result, the\nliterature lacks a systematic understanding of prompt injection attacks and\ntheir defenses. We aim to bridge the gap in this work. In particular, we\npropose a framework to formalize prompt injection attacks. Existing attacks are\nspecial cases in our framework. Moreover, based on our framework, we design a\nnew attack by combining existing ones. Using our framework, we conduct a\nsystematic evaluation on 5 prompt injection attacks and 10 defenses with 10\nLLMs and 7 tasks. Our work provides a common benchmark for quantitatively\nevaluating future prompt injection attacks and defenses. To facilitate research\non this topic, we make our platform public at\nhttps://github.com/liu00222/Open-Prompt-Injection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A prompt injection attack aims to inject malicious instruction/data into the\ninput of an LLM-Integrated Application such that it produces results as an\nattacker desires. Existing works are limited to case studies. As a result, the\nliterature lacks a systematic understanding of prompt injection attacks and\ntheir defenses. We aim to bridge the gap in this work. In particular, we\npropose a framework to formalize prompt injection attacks. Existing attacks are\nspecial cases in our framework. Moreover, based on our framework, we design a\nnew attack by combining existing ones. Using our framework, we conduct a\nsystematic evaluation on 5 prompt injection attacks and 10 defenses with 10\nLLMs and 7 tasks. Our work provides a common benchmark for quantitatively\nevaluating future prompt injection attacks and defenses. To facilitate research\non this topic, we make our platform public at\nhttps://github.com/liu00222/Open-Prompt-Injection."
                },
                "authors": [
                    {
                        "name": "Yupei Liu"
                    },
                    {
                        "name": "Yuqi Jia"
                    },
                    {
                        "name": "Runpeng Geng"
                    },
                    {
                        "name": "Jinyuan Jia"
                    },
                    {
                        "name": "Neil Zhenqiang Gong"
                    }
                ],
                "author_detail": {
                    "name": "Neil Zhenqiang Gong"
                },
                "author": "Neil Zhenqiang Gong",
                "arxiv_comment": "Published in USENIX Security Symposium 2024; the model sizes for\n  closed-source models are from blog posts",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.12815v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.12815v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07921v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07921v3",
                "updated": "2024-11-24T18:03:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    18,
                    3,
                    43,
                    6,
                    329,
                    0
                ],
                "published": "2024-04-11T17:05:50Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    17,
                    5,
                    50,
                    3,
                    102,
                    0
                ],
                "title": "AmpleGCG: Learning a Universal and Transferable Generative Model of\n  Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AmpleGCG: Learning a Universal and Transferable Generative Model of\n  Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs"
                },
                "summary": "As large language models (LLMs) become increasingly prevalent and integrated\ninto autonomous systems, ensuring their safety is imperative. Despite\nsignificant strides toward safety alignment, recent work\nGCG~\\citep{zou2023universal} proposes a discrete token optimization algorithm\nand selects the single suffix with the lowest loss to successfully jailbreak\naligned LLMs. In this work, we first discuss the drawbacks of solely picking\nthe suffix with the lowest loss during GCG optimization for jailbreaking and\nuncover the missed successful suffixes during the intermediate steps. Moreover,\nwe utilize those successful suffixes as training data to learn a generative\nmodel, named AmpleGCG, which captures the distribution of adversarial suffixes\ngiven a harmful query and enables the rapid generation of hundreds of suffixes\nfor any harmful queries in seconds. AmpleGCG achieves near 100\\% attack success\nrate (ASR) on two aligned LLMs (Llama-2-7B-chat and Vicuna-7B), surpassing two\nstrongest attack baselines. More interestingly, AmpleGCG also transfers\nseamlessly to attack different models, including closed-source LLMs, achieving\na 99\\% ASR on the latest GPT-3.5. To summarize, our work amplifies the impact\nof GCG by training a generative model of adversarial suffixes that is universal\nto any harmful queries and transferable from attacking open-source LLMs to\nclosed-source LLMs. In addition, it can generate 200 adversarial suffixes for\none harmful query in only 4 seconds, rendering it more challenging to defend.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become increasingly prevalent and integrated\ninto autonomous systems, ensuring their safety is imperative. Despite\nsignificant strides toward safety alignment, recent work\nGCG~\\citep{zou2023universal} proposes a discrete token optimization algorithm\nand selects the single suffix with the lowest loss to successfully jailbreak\naligned LLMs. In this work, we first discuss the drawbacks of solely picking\nthe suffix with the lowest loss during GCG optimization for jailbreaking and\nuncover the missed successful suffixes during the intermediate steps. Moreover,\nwe utilize those successful suffixes as training data to learn a generative\nmodel, named AmpleGCG, which captures the distribution of adversarial suffixes\ngiven a harmful query and enables the rapid generation of hundreds of suffixes\nfor any harmful queries in seconds. AmpleGCG achieves near 100\\% attack success\nrate (ASR) on two aligned LLMs (Llama-2-7B-chat and Vicuna-7B), surpassing two\nstrongest attack baselines. More interestingly, AmpleGCG also transfers\nseamlessly to attack different models, including closed-source LLMs, achieving\na 99\\% ASR on the latest GPT-3.5. To summarize, our work amplifies the impact\nof GCG by training a generative model of adversarial suffixes that is universal\nto any harmful queries and transferable from attacking open-source LLMs to\nclosed-source LLMs. In addition, it can generate 200 adversarial suffixes for\none harmful query in only 4 seconds, rendering it more challenging to defend."
                },
                "authors": [
                    {
                        "name": "Zeyi Liao"
                    },
                    {
                        "name": "Huan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Huan Sun"
                },
                "author": "Huan Sun",
                "arxiv_comment": "Published as a conference paper at COLM 2024\n  (https://colmweb.org/index.html)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07921v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07921v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15942v1",
                "updated": "2024-11-24T18:01:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    18,
                    1,
                    13,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T18:01:13Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    18,
                    1,
                    13,
                    6,
                    329,
                    0
                ],
                "title": "Cross-organ Deployment of EOS Detection AI without Retraining:\n  Feasibility and Limitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-organ Deployment of EOS Detection AI without Retraining:\n  Feasibility and Limitation"
                },
                "summary": "Chronic rhinosinusitis (CRS) is characterized by persistent inflammation in\nthe paranasal sinuses, leading to typical symptoms of nasal congestion, facial\npressure, olfactory dysfunction, and discolored nasal drainage, which can\nsignificantly impact quality-of-life. Eosinophils (Eos), a crucial component in\nthe mucosal immune response, have been linked to disease severity in CRS. The\ndiagnosis of eosinophilic CRS typically uses a threshold of 10-20 eos per\nhigh-power field (HPF). However, manually counting Eos in histological samples\nis laborious and time-intensive, making the use of AI-driven methods for\nautomated evaluations highly desirable. Interestingly, eosinophils are\npredominantly located in the gastrointestinal (GI) tract, which has prompted\nthe release of numerous deep learning models trained on GI data. This study\nleverages a CircleSnake model initially trained on upper-GI data to segment Eos\ncells in whole slide images (WSIs) of nasal tissues. It aims to determine the\nextent to which Eos segmentation models developed for the GI tract can be\nadapted to nasal applications without retraining. The experimental results show\npromising accuracy in some WSIs, although, unsurprisingly, the performance\nvaries across cases. This paper details these performance outcomes, delves into\nthe reasons for such variations, and aims to provide insights that could guide\nfuture development of deep learning models for eosinophilic CRS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chronic rhinosinusitis (CRS) is characterized by persistent inflammation in\nthe paranasal sinuses, leading to typical symptoms of nasal congestion, facial\npressure, olfactory dysfunction, and discolored nasal drainage, which can\nsignificantly impact quality-of-life. Eosinophils (Eos), a crucial component in\nthe mucosal immune response, have been linked to disease severity in CRS. The\ndiagnosis of eosinophilic CRS typically uses a threshold of 10-20 eos per\nhigh-power field (HPF). However, manually counting Eos in histological samples\nis laborious and time-intensive, making the use of AI-driven methods for\nautomated evaluations highly desirable. Interestingly, eosinophils are\npredominantly located in the gastrointestinal (GI) tract, which has prompted\nthe release of numerous deep learning models trained on GI data. This study\nleverages a CircleSnake model initially trained on upper-GI data to segment Eos\ncells in whole slide images (WSIs) of nasal tissues. It aims to determine the\nextent to which Eos segmentation models developed for the GI tract can be\nadapted to nasal applications without retraining. The experimental results show\npromising accuracy in some WSIs, although, unsurprisingly, the performance\nvaries across cases. This paper details these performance outcomes, delves into\nthe reasons for such variations, and aims to provide insights that could guide\nfuture development of deep learning models for eosinophilic CRS."
                },
                "authors": [
                    {
                        "name": "Yifei Wu"
                    },
                    {
                        "name": "Juming Xiong"
                    },
                    {
                        "name": "Tianyuan Yao"
                    },
                    {
                        "name": "Ruining Deng"
                    },
                    {
                        "name": "Junlin Guo"
                    },
                    {
                        "name": "Jialin Yue"
                    },
                    {
                        "name": "Naweed Chowdhury"
                    },
                    {
                        "name": "Yuankai Huo"
                    }
                ],
                "author_detail": {
                    "name": "Yuankai Huo"
                },
                "author": "Yuankai Huo",
                "arxiv_comment": "8 pages, 5 figures. Accepted by SPIE Medical Imaging 2025 on October\n  28, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15936v1",
                "updated": "2024-11-24T17:49:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    17,
                    49,
                    14,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T17:49:14Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    17,
                    49,
                    14,
                    6,
                    329,
                    0
                ],
                "title": "Internet Measurement of Quantum-Resistant IKEv2 in Constrained Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internet Measurement of Quantum-Resistant IKEv2 in Constrained Networks"
                },
                "summary": "Within 1-2 decades, quantum computers are expected to obsolesce current\npublic-key cryptography, driving authorities such as IETF and NIST to push for\nadopting quantum-resistant cryptography (QRC) in ecosystems like Internet\nProtocol Security (IPsec). However, IPsec struggles to adopt QRC, primarily due\nto the limited ability of Internet Key Exchange Protocol Version 2 (IKEv2),\nwhich establishes IPsec connections, to tolerate the large public keys and\ndigital signatures of QRC. Many solutions (e.g., IETF RFCs) are proposed to\nintegrate QRC into IKEv2, but remain largely untested in practice. In this\npaper, we measure the performance of these proposals over the Internet by\ndesigning and implementing a novel, scalable, and flexible testbed for\nquantum-resistant IPsec, and we expose the serious shortcomings of existing\nproposals for quantum-resistant IKEv2 when deployed in constrained (e.g.,\nlossy, rate-limited) networks. Through experimental deployments ranging from\ncloud-based virtual networks to hardware-in-the-loop wireless links between\nsoftware-defined radios, as well as deployment on the international FABRIC\ntestbed for next-generation networks, we show that today's solutions for\nquantum-resistant IPsec are insufficient, necessitating development of better\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Within 1-2 decades, quantum computers are expected to obsolesce current\npublic-key cryptography, driving authorities such as IETF and NIST to push for\nadopting quantum-resistant cryptography (QRC) in ecosystems like Internet\nProtocol Security (IPsec). However, IPsec struggles to adopt QRC, primarily due\nto the limited ability of Internet Key Exchange Protocol Version 2 (IKEv2),\nwhich establishes IPsec connections, to tolerate the large public keys and\ndigital signatures of QRC. Many solutions (e.g., IETF RFCs) are proposed to\nintegrate QRC into IKEv2, but remain largely untested in practice. In this\npaper, we measure the performance of these proposals over the Internet by\ndesigning and implementing a novel, scalable, and flexible testbed for\nquantum-resistant IPsec, and we expose the serious shortcomings of existing\nproposals for quantum-resistant IKEv2 when deployed in constrained (e.g.,\nlossy, rate-limited) networks. Through experimental deployments ranging from\ncloud-based virtual networks to hardware-in-the-loop wireless links between\nsoftware-defined radios, as well as deployment on the international FABRIC\ntestbed for next-generation networks, we show that today's solutions for\nquantum-resistant IPsec are insufficient, necessitating development of better\napproaches."
                },
                "authors": [
                    {
                        "name": "Geoff Twardokus"
                    },
                    {
                        "name": "William Joslin"
                    },
                    {
                        "name": "Benjamin Carini"
                    },
                    {
                        "name": "Hanif Rahbari"
                    },
                    {
                        "name": "William Layton"
                    }
                ],
                "author_detail": {
                    "name": "William Layton"
                },
                "author": "William Layton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15933v1",
                "updated": "2024-11-24T17:39:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    17,
                    39,
                    39,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T17:39:39Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    17,
                    39,
                    39,
                    6,
                    329,
                    0
                ],
                "title": "Segment to Recognize Robustly -- Enhancing Recognition by Image\n  Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Segment to Recognize Robustly -- Enhancing Recognition by Image\n  Decomposition"
                },
                "summary": "In image recognition, both foreground (FG) and background (BG) play an\nimportant role; however, standard deep image recognition often leads to\nunintended over-reliance on the BG, limiting model robustness in real-world\ndeployment settings. Current solutions mainly suppress the BG, sacrificing BG\ninformation for improved generalization. We propose \"Segment to Recognize\nRobustly\" (S2R^2), a novel recognition approach which decouples the FG and BG\nmodelling and combines them in a simple, robust, and interpretable manner.\nS2R^2 leverages recent advances in zero-shot segmentation to isolate the FG and\nthe BG before or during recognition. By combining FG and BG, potentially also\nwith a standard full-image classifier, S2R^2 achieves state-of-the-art results\non in-domain data while maintaining robustness to BG shifts. The results\nconfirm that segmentation before recognition is now possible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In image recognition, both foreground (FG) and background (BG) play an\nimportant role; however, standard deep image recognition often leads to\nunintended over-reliance on the BG, limiting model robustness in real-world\ndeployment settings. Current solutions mainly suppress the BG, sacrificing BG\ninformation for improved generalization. We propose \"Segment to Recognize\nRobustly\" (S2R^2), a novel recognition approach which decouples the FG and BG\nmodelling and combines them in a simple, robust, and interpretable manner.\nS2R^2 leverages recent advances in zero-shot segmentation to isolate the FG and\nthe BG before or during recognition. By combining FG and BG, potentially also\nwith a standard full-image classifier, S2R^2 achieves state-of-the-art results\non in-domain data while maintaining robustness to BG shifts. The results\nconfirm that segmentation before recognition is now possible."
                },
                "authors": [
                    {
                        "name": "Klara Janouskova"
                    },
                    {
                        "name": "Cristian Gavrus"
                    },
                    {
                        "name": "Jiri Matas"
                    }
                ],
                "author_detail": {
                    "name": "Jiri Matas"
                },
                "author": "Jiri Matas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15898v1",
                "updated": "2024-11-24T16:14:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    16,
                    14,
                    32,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T16:14:32Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    16,
                    14,
                    32,
                    6,
                    329,
                    0
                ],
                "title": "Towards the LLM-Based Generation of Formal Specifications from\n  Natural-Language Contracts: Early Experiments with Symboleo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards the LLM-Based Generation of Formal Specifications from\n  Natural-Language Contracts: Early Experiments with Symboleo"
                },
                "summary": "Over the past decade, different domain-specific languages (DSLs) were\nproposed to formally specify requirements stated in legal contracts, mainly for\nanalysis but also for code generation. Symboleo is a promising language in that\narea. However, writing formal specifications from natural-language contracts is\na complex task, especial for legal experts who do not have formal language\nexpertise. This paper reports on an exploratory experiment targeting the\nautomated generation of Symboleo specifications from business contracts in\nEnglish using Large Language Models (LLMs). Combinations (38) of prompt\ncomponents are investigated (with/without the grammar, semantics explanations,\n0 to 3 examples, and emotional prompts), mainly on GPT-4o but also to a lesser\nextent on 4 other LLMs. The generated specifications are manually assessed\nagainst 16 error types grouped into 3 severity levels. Early results on all\nLLMs show promising outcomes (even for a little-known DSL) that will likely\naccelerate the specification of legal contracts. However, several observed\nissues, especially around grammar/syntax adherence and environment variable\nidentification (49%), suggest many areas where potential improvements should be\ninvestigated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past decade, different domain-specific languages (DSLs) were\nproposed to formally specify requirements stated in legal contracts, mainly for\nanalysis but also for code generation. Symboleo is a promising language in that\narea. However, writing formal specifications from natural-language contracts is\na complex task, especial for legal experts who do not have formal language\nexpertise. This paper reports on an exploratory experiment targeting the\nautomated generation of Symboleo specifications from business contracts in\nEnglish using Large Language Models (LLMs). Combinations (38) of prompt\ncomponents are investigated (with/without the grammar, semantics explanations,\n0 to 3 examples, and emotional prompts), mainly on GPT-4o but also to a lesser\nextent on 4 other LLMs. The generated specifications are manually assessed\nagainst 16 error types grouped into 3 severity levels. Early results on all\nLLMs show promising outcomes (even for a little-known DSL) that will likely\naccelerate the specification of legal contracts. However, several observed\nissues, especially around grammar/syntax adherence and environment variable\nidentification (49%), suggest many areas where potential improvements should be\ninvestigated."
                },
                "authors": [
                    {
                        "name": "Mounira Nihad Zitouni"
                    },
                    {
                        "name": "Amal Ahmed Anda"
                    },
                    {
                        "name": "Sahil Rajpal"
                    },
                    {
                        "name": "Daniel Amyot"
                    },
                    {
                        "name": "John Mylopoulos"
                    }
                ],
                "author_detail": {
                    "name": "John Mylopoulos"
                },
                "author": "John Mylopoulos",
                "arxiv_comment": "9 pages, 1 figure, 2 tables, submitted to a workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15891v1",
                "updated": "2024-11-24T15:57:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    15,
                    57,
                    53,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T15:57:53Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    15,
                    57,
                    53,
                    6,
                    329,
                    0
                ],
                "title": "From Laws to Motivation: Guiding Exploration through Law-Based Reasoning\n  and Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Laws to Motivation: Guiding Exploration through Law-Based Reasoning\n  and Rewards"
                },
                "summary": "Large Language Models (LLMs) and Reinforcement Learning (RL) are two powerful\napproaches for building autonomous agents. However, due to limited\nunderstanding of the game environment, agents often resort to inefficient\nexploration and trial-and-error, struggling to develop long-term strategies or\nmake decisions. We propose a method that extracts experience from interaction\nrecords to model the underlying laws of the game environment, using these\nexperience as internal motivation to guide agents. These experience, expressed\nin language, are highly flexible and can either assist agents in reasoning\ndirectly or be transformed into rewards for guiding training. Our evaluation\nresults in Crafter demonstrate that both RL and LLM agents benefit from these\nexperience, leading to improved overall performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and Reinforcement Learning (RL) are two powerful\napproaches for building autonomous agents. However, due to limited\nunderstanding of the game environment, agents often resort to inefficient\nexploration and trial-and-error, struggling to develop long-term strategies or\nmake decisions. We propose a method that extracts experience from interaction\nrecords to model the underlying laws of the game environment, using these\nexperience as internal motivation to guide agents. These experience, expressed\nin language, are highly flexible and can either assist agents in reasoning\ndirectly or be transformed into rewards for guiding training. Our evaluation\nresults in Crafter demonstrate that both RL and LLM agents benefit from these\nexperience, leading to improved overall performance."
                },
                "authors": [
                    {
                        "name": "Ziyu Chen"
                    },
                    {
                        "name": "Zhiqing Xiao"
                    },
                    {
                        "name": "Xinbei Jiang"
                    },
                    {
                        "name": "Junbo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Junbo Zhao"
                },
                "author": "Junbo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15888v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15888v1",
                "updated": "2024-11-24T15:51:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    15,
                    51,
                    56,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T15:51:56Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    15,
                    51,
                    56,
                    6,
                    329,
                    0
                ],
                "title": "Evaluating Large Language Models for Causal Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models for Causal Modeling"
                },
                "summary": "In this paper, we consider the process of transforming causal domain\nknowledge into a representation that aligns more closely with guidelines from\ncausal data science. To this end, we introduce two novel tasks related to\ndistilling causal domain knowledge into causal variables and detecting\ninteraction entities using LLMs. We have determined that contemporary LLMs are\nhelpful tools for conducting causal modeling tasks in collaboration with human\nexperts, as they can provide a wider perspective. Specifically, LLMs, such as\nGPT-4-turbo and Llama3-70b, perform better in distilling causal domain\nknowledge into causal variables compared to sparse expert models, such as\nMixtral-8x22b. On the contrary, sparse expert models such as Mixtral-8x22b\nstand out as the most effective in identifying interaction entities. Finally,\nwe highlight the dependency between the domain where the entities are generated\nand the performance of the chosen LLM for causal modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we consider the process of transforming causal domain\nknowledge into a representation that aligns more closely with guidelines from\ncausal data science. To this end, we introduce two novel tasks related to\ndistilling causal domain knowledge into causal variables and detecting\ninteraction entities using LLMs. We have determined that contemporary LLMs are\nhelpful tools for conducting causal modeling tasks in collaboration with human\nexperts, as they can provide a wider perspective. Specifically, LLMs, such as\nGPT-4-turbo and Llama3-70b, perform better in distilling causal domain\nknowledge into causal variables compared to sparse expert models, such as\nMixtral-8x22b. On the contrary, sparse expert models such as Mixtral-8x22b\nstand out as the most effective in identifying interaction entities. Finally,\nwe highlight the dependency between the domain where the entities are generated\nand the performance of the chosen LLM for causal modeling."
                },
                "authors": [
                    {
                        "name": "Houssam Razouk"
                    },
                    {
                        "name": "Leonie Benischke"
                    },
                    {
                        "name": "Georg Niess"
                    },
                    {
                        "name": "Roman Kern"
                    }
                ],
                "author_detail": {
                    "name": "Roman Kern"
                },
                "author": "Roman Kern",
                "arxiv_comment": "13 pages, 6 figutrd, 4 tabels",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15888v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12830v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12830v3",
                "updated": "2024-11-24T15:45:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    15,
                    45,
                    35,
                    6,
                    329,
                    0
                ],
                "published": "2024-10-02T14:05:21Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    14,
                    5,
                    21,
                    2,
                    276,
                    0
                ],
                "title": "Incorporating Metabolic Information into LLMs for Anomaly Detection in\n  Clinical Time-Series",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating Metabolic Information into LLMs for Anomaly Detection in\n  Clinical Time-Series"
                },
                "summary": "Anomaly detection in clinical time-series holds significant potential in\nidentifying suspicious patterns in different biological parameters. In this\npaper, we propose a targeted method that incorporates the clinical domain\nknowledge into LLMs to improve their ability to detect anomalies. We introduce\nthe Metabolism Pathway-driven Prompting (MPP) method, which integrates the\ninformation about metabolic pathways to better capture the structural and\ntemporal changes in biological samples. We applied our method for doping\ndetection in sports, focusing on steroid metabolism, and evaluated using\nreal-world data from athletes. The results show that our method improves\nanomaly detection performance by leveraging metabolic context, providing a more\nnuanced and accurate prediction of suspicious samples in athletes' profiles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anomaly detection in clinical time-series holds significant potential in\nidentifying suspicious patterns in different biological parameters. In this\npaper, we propose a targeted method that incorporates the clinical domain\nknowledge into LLMs to improve their ability to detect anomalies. We introduce\nthe Metabolism Pathway-driven Prompting (MPP) method, which integrates the\ninformation about metabolic pathways to better capture the structural and\ntemporal changes in biological samples. We applied our method for doping\ndetection in sports, focusing on steroid metabolism, and evaluated using\nreal-world data from athletes. The results show that our method improves\nanomaly detection performance by leveraging metabolic context, providing a more\nnuanced and accurate prediction of suspicious samples in athletes' profiles."
                },
                "authors": [
                    {
                        "name": "Maxx Richard Rahman"
                    },
                    {
                        "name": "Ruoxuan Liu"
                    },
                    {
                        "name": "Wolfgang Maass"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang Maass"
                },
                "author": "Wolfgang Maass",
                "arxiv_journal_ref": "NeurIPS 2024 Workshop on Time Series in the Age of Large Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12830v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12830v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]