[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.09775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09775v2",
                "updated": "2025-04-16T17:34:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    17,
                    34,
                    4,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-14T00:29:49Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    0,
                    29,
                    49,
                    0,
                    104,
                    0
                ],
                "title": "Understanding and Optimizing Multi-Stage AI Inference Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Optimizing Multi-Stage AI Inference Pipelines"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads."
                },
                "authors": [
                    {
                        "name": "Abhimanyu Rajeshkumar Bambhaniya"
                    },
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "name": "Sudarshan Srinivasan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Amir Yazdanbakhsh"
                    },
                    {
                        "name": "Midhilesh Elavazhagan"
                    },
                    {
                        "name": "Madhu Kumar"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "arxiv_comment": "Inference System Design for Multi-Stage AI Inference Pipelines. 13\n  Pages, 15 Figues, 3 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12240v1",
                "updated": "2025-04-16T16:45:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T16:45:19Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cobra: Efficient Line Art COlorization with BRoAder References"
                },
                "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/."
                },
                "authors": [
                    {
                        "name": "Junhao Zhuang"
                    },
                    {
                        "name": "Lingen Li"
                    },
                    {
                        "name": "Xuan Ju"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project page with code: https://zhuang2002.github.io/Cobra/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11816v1",
                "updated": "2025-04-16T07:02:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    2,
                    38,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T07:02:38Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    2,
                    38,
                    2,
                    106,
                    0
                ],
                "title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading"
                },
                "summary": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads."
                },
                "authors": [
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Hyunsun Chung"
                    },
                    {
                        "name": "Myung-Hoon Cha"
                    },
                    {
                        "name": "Hong-Yeon Kim"
                    },
                    {
                        "name": "Youngjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Kim"
                },
                "author": "Youngjae Kim",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08334v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08334v3",
                "updated": "2025-04-16T05:57:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    57,
                    8,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-11T07:59:06Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    59,
                    6,
                    4,
                    101,
                    0
                ],
                "title": "Efficient Architecture for RISC-V Vector Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Architecture for RISC-V Vector Memory Access"
                },
                "summary": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors."
                },
                "authors": [
                    {
                        "name": "Hongyi Guan"
                    },
                    {
                        "name": "Yichuan Gao"
                    },
                    {
                        "name": "Chenlu Miao"
                    },
                    {
                        "name": "Haoyang Wu"
                    },
                    {
                        "name": "Hang Zhu"
                    },
                    {
                        "name": "Mingfeng Lin"
                    },
                    {
                        "name": "Huayue Liang"
                    }
                ],
                "author_detail": {
                    "name": "Huayue Liang"
                },
                "author": "Huayue Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08334v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08334v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11765v1",
                "updated": "2025-04-16T04:59:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    59,
                    18,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T04:59:18Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    59,
                    18,
                    2,
                    106,
                    0
                ],
                "title": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference\n  in RAG-Powered LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference\n  in RAG-Powered LLMs"
                },
                "summary": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration."
                },
                "authors": [
                    {
                        "name": "Hyungwoo Lee"
                    },
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Jungmin So"
                    },
                    {
                        "name": "Myung-Hoon Cha"
                    },
                    {
                        "name": "Hong-Yeon Kim"
                    },
                    {
                        "name": "James J. Kim"
                    },
                    {
                        "name": "Youngjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Kim"
                },
                "arxiv_affiliation": "Dept. of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea",
                "author": "Youngjae Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11729v1",
                "updated": "2025-04-16T03:07:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    7,
                    7,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T03:07:07Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    7,
                    7,
                    2,
                    106,
                    0
                ],
                "title": "EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G\n  Networks"
                },
                "summary": "As sixth-generation (6G) networks advance, large language models (LLMs) are\nincreasingly integrated into 6G infrastructure to enhance network management\nand intelligence. However, traditional LLMs architecture struggle to meet the\nstringent latency and security requirements of 6G, especially as the increasing\nin sequence length leads to greater task complexity. This paper proposes\nEdge-Prompt, a cloud-edge collaborative framework based on a hierarchical\nattention splicing mechanism. EdgePrompt employs distributed key-value (KV)\npair optimization techniques to accelerate inference and adapt to network\nconditions. Additionally, to reduce the risk of data leakage, EdgePrompt\nincorporates a privacy preserving strategy by isolating sensitive information\nduring processing. Experiments on public dataset show that EdgePrompt\neffectively improves the inference throughput and reduces the latency, which\nprovides a reliable solution for LLMs deployment in 6G environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As sixth-generation (6G) networks advance, large language models (LLMs) are\nincreasingly integrated into 6G infrastructure to enhance network management\nand intelligence. However, traditional LLMs architecture struggle to meet the\nstringent latency and security requirements of 6G, especially as the increasing\nin sequence length leads to greater task complexity. This paper proposes\nEdge-Prompt, a cloud-edge collaborative framework based on a hierarchical\nattention splicing mechanism. EdgePrompt employs distributed key-value (KV)\npair optimization techniques to accelerate inference and adapt to network\nconditions. Additionally, to reduce the risk of data leakage, EdgePrompt\nincorporates a privacy preserving strategy by isolating sensitive information\nduring processing. Experiments on public dataset show that EdgePrompt\neffectively improves the inference throughput and reduces the latency, which\nprovides a reliable solution for LLMs deployment in 6G environments."
                },
                "authors": [
                    {
                        "name": "Jiahong Ning"
                    },
                    {
                        "name": "Pengyan Zhu"
                    },
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Gary Lee"
                    },
                    {
                        "name": "Sumei Sun"
                    },
                    {
                        "name": "Tingting Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Yang"
                },
                "author": "Tingting Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11652v1",
                "updated": "2025-04-15T22:38:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    22,
                    38,
                    54,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T22:38:54Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    22,
                    38,
                    54,
                    1,
                    105,
                    0
                ],
                "title": "Engineering MultiQueues: Fast Relaxed Concurrent Priority Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engineering MultiQueues: Fast Relaxed Concurrent Priority Queues"
                },
                "summary": "Priority queues are used in a wide range of applications, including\nprioritized online scheduling, discrete event simulation, and greedy\nalgorithms. In parallel settings, classical priority queues often become a\nsevere bottleneck, resulting in low throughput. Consequently, there has been\nsignificant interest in concurrent priority queues with relaxed semantics. In\nthis article, we present the MultiQueue, a flexible approach to relaxed\npriority queues that uses multiple internal sequential priority queues. The\nscalability of the MultiQueue is enhanced by buffering elements, batching\noperations on the internal queues, and optimizing access patterns for high\ncache locality. We investigate the complementary quality criteria of rank\nerror, which measures how close deleted elements are to the global minimum, and\ndelay, which quantifies how many smaller elements were deleted before a given\nelement. Extensive experimental evaluation shows that the MultiQueue\noutperforms competing approaches across several benchmarks. This includes\nshortest-path and branch-and-bound benchmarks that resemble real applications.\nMoreover, the MultiQueue can be configured easily to balance throughput and\nquality according to the application's requirements. We employ a seemingly\nparadoxical technique of wait-free locking that might be of broader interest\nfor converting sequential data structures into relaxed concurrent data\nstructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Priority queues are used in a wide range of applications, including\nprioritized online scheduling, discrete event simulation, and greedy\nalgorithms. In parallel settings, classical priority queues often become a\nsevere bottleneck, resulting in low throughput. Consequently, there has been\nsignificant interest in concurrent priority queues with relaxed semantics. In\nthis article, we present the MultiQueue, a flexible approach to relaxed\npriority queues that uses multiple internal sequential priority queues. The\nscalability of the MultiQueue is enhanced by buffering elements, batching\noperations on the internal queues, and optimizing access patterns for high\ncache locality. We investigate the complementary quality criteria of rank\nerror, which measures how close deleted elements are to the global minimum, and\ndelay, which quantifies how many smaller elements were deleted before a given\nelement. Extensive experimental evaluation shows that the MultiQueue\noutperforms competing approaches across several benchmarks. This includes\nshortest-path and branch-and-bound benchmarks that resemble real applications.\nMoreover, the MultiQueue can be configured easily to balance throughput and\nquality according to the application's requirements. We employ a seemingly\nparadoxical technique of wait-free locking that might be of broader interest\nfor converting sequential data structures into relaxed concurrent data\nstructures."
                },
                "authors": [
                    {
                        "name": "Marvin Williams"
                    },
                    {
                        "name": "Peter Sanders"
                    }
                ],
                "author_detail": {
                    "name": "Peter Sanders"
                },
                "author": "Peter Sanders",
                "arxiv_comment": "40 pages, extended journal version of arXiv:2107.01350",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11320v1",
                "updated": "2025-04-15T16:00:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:00:21Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "title": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints"
                },
                "summary": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints."
                },
                "authors": [
                    {
                        "name": "Ruicheng Ao"
                    },
                    {
                        "name": "Gan Luo"
                    },
                    {
                        "name": "David Simchi-Levi"
                    },
                    {
                        "name": "Xinshang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinshang Wang"
                },
                "author": "Xinshang Wang",
                "arxiv_comment": "42 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v3",
                "updated": "2025-04-15T15:40:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    40,
                    25,
                    1,
                    105,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13195v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13195v5",
                "updated": "2025-04-15T15:37:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    37,
                    58,
                    1,
                    105,
                    0
                ],
                "published": "2024-04-19T22:06:14Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    22,
                    6,
                    14,
                    4,
                    110,
                    0
                ],
                "title": "Automatic BLAS Offloading on Unified Memory Architecture: A Study on\n  NVIDIA Grace-Hopper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic BLAS Offloading on Unified Memory Architecture: A Study on\n  NVIDIA Grace-Hopper"
                },
                "summary": "Porting codes to GPU often requires major efforts. While several tools exist\nfor automatically offload numerical libraries such as BLAS and LAPACK, they\noften prove impractical due to the high cost of mandatory data transfer. The\nnew unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth\ncache-coherent memory access of all memory from both CPU and GPU, potentially\neliminating bottleneck faced in conventional architecture. This breakthrough\nopens up new avenues for application development and porting strategies. In\nthis study, we introduce a new tool for automatic BLAS offload, the tool\nleverages the high speed cache coherent NVLink C2C interconnect in\nGrace-Hopper, and enables performant GPU offload for BLAS heavy applications\nwith no code changes or recompilation. The tool was tested on two quantum\nchemistry or physics codes, great performance benefits were observed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Porting codes to GPU often requires major efforts. While several tools exist\nfor automatically offload numerical libraries such as BLAS and LAPACK, they\noften prove impractical due to the high cost of mandatory data transfer. The\nnew unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth\ncache-coherent memory access of all memory from both CPU and GPU, potentially\neliminating bottleneck faced in conventional architecture. This breakthrough\nopens up new avenues for application development and porting strategies. In\nthis study, we introduce a new tool for automatic BLAS offload, the tool\nleverages the high speed cache coherent NVLink C2C interconnect in\nGrace-Hopper, and enables performant GPU offload for BLAS heavy applications\nwith no code changes or recompilation. The tool was tested on two quantum\nchemistry or physics codes, great performance benefits were observed."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Yinzhi Wang"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Hang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Liu"
                },
                "author": "Hang Liu",
                "arxiv_doi": "10.1145/3626203.3670561",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3626203.3670561",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.13195v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13195v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11208v1",
                "updated": "2025-04-15T14:11:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    11,
                    38,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T14:11:38Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    11,
                    38,
                    1,
                    105,
                    0
                ],
                "title": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye"
                },
                "summary": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively."
                },
                "authors": [
                    {
                        "name": "Bradley Morgan"
                    },
                    {
                        "name": "Gal Horowitz"
                    },
                    {
                        "name": "Sioli O'Connell"
                    },
                    {
                        "name": "Stephan van Schaik"
                    },
                    {
                        "name": "Chitchanok Chuengsatiansup"
                    },
                    {
                        "name": "Daniel Genkin"
                    },
                    {
                        "name": "Olaf Maennel"
                    },
                    {
                        "name": "Paul Montague"
                    },
                    {
                        "name": "Eyal Ronen"
                    },
                    {
                        "name": "Yuval Yarom"
                    }
                ],
                "author_detail": {
                    "name": "Yuval Yarom"
                },
                "author": "Yuval Yarom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11067v1",
                "updated": "2025-04-15T11:02:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    2,
                    34,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T11:02:34Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    2,
                    34,
                    1,
                    105,
                    0
                ],
                "title": "Morphing-based Compression for Data-centric ML Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Morphing-based Compression for Data-centric ML Pipelines"
                },
                "summary": "Data-centric ML pipelines extend traditional machine learning (ML) pipelines\n-- of feature transformations and ML model training -- by outer loops for data\ncleaning, augmentation, and feature engineering to create high-quality input\ndata. Existing lossless matrix compression applies lightweight compression\nschemes to numeric matrices and performs linear algebra operations such as\nmatrix-vector multiplications directly on the compressed representation but\nstruggles to efficiently rediscover structural data redundancy. Compressed\noperations are effective at fitting data in available memory, reducing I/O\nacross the storage-memory-cache hierarchy, and improving instruction\nparallelism. The applied data cleaning, augmentation, and feature\ntransformations provide a rich source of information about data characteristics\nsuch as distinct items, column sparsity, and column correlations. In this\npaper, we introduce BWARE -- an extension of AWARE for workload-aware lossless\nmatrix compression -- that pushes compression through feature transformations\nand engineering to leverage information about structural transformations.\nBesides compressed feature transformations, we introduce a novel technique for\nlightweight morphing of a compressed representation into workload-optimized\ncompressed representations without decompression. BWARE shows substantial\nend-to-end runtime improvements, reducing the execution time for training\ndata-centric ML pipelines from days to hours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-centric ML pipelines extend traditional machine learning (ML) pipelines\n-- of feature transformations and ML model training -- by outer loops for data\ncleaning, augmentation, and feature engineering to create high-quality input\ndata. Existing lossless matrix compression applies lightweight compression\nschemes to numeric matrices and performs linear algebra operations such as\nmatrix-vector multiplications directly on the compressed representation but\nstruggles to efficiently rediscover structural data redundancy. Compressed\noperations are effective at fitting data in available memory, reducing I/O\nacross the storage-memory-cache hierarchy, and improving instruction\nparallelism. The applied data cleaning, augmentation, and feature\ntransformations provide a rich source of information about data characteristics\nsuch as distinct items, column sparsity, and column correlations. In this\npaper, we introduce BWARE -- an extension of AWARE for workload-aware lossless\nmatrix compression -- that pushes compression through feature transformations\nand engineering to leverage information about structural transformations.\nBesides compressed feature transformations, we introduce a novel technique for\nlightweight morphing of a compressed representation into workload-optimized\ncompressed representations without decompression. BWARE shows substantial\nend-to-end runtime improvements, reducing the execution time for training\ndata-centric ML pipelines from days to hours."
                },
                "authors": [
                    {
                        "name": "Sebastian Baunsgaard"
                    },
                    {
                        "name": "Matthias Boehm"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Boehm"
                },
                "author": "Matthias Boehm",
                "arxiv_comment": "20 pages, 28 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10326v1",
                "updated": "2025-04-14T15:34:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:34:26Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference"
                },
                "summary": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks."
                },
                "authors": [
                    {
                        "name": "Yangshen Deng"
                    },
                    {
                        "name": "Zhengxin You"
                    },
                    {
                        "name": "Long Xiang"
                    },
                    {
                        "name": "Qilong Li"
                    },
                    {
                        "name": "Peiqi Yuan"
                    },
                    {
                        "name": "Zhaoyang Hong"
                    },
                    {
                        "name": "Yitao Zheng"
                    },
                    {
                        "name": "Wanting Li"
                    },
                    {
                        "name": "Runzhong Li"
                    },
                    {
                        "name": "Haotian Liu"
                    },
                    {
                        "name": "Kyriakos Mouratidis"
                    },
                    {
                        "name": "Man Lung Yiu"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Qiaomu Shen"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Bo Tang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Tang"
                },
                "author": "Bo Tang",
                "arxiv_comment": "14 pages, 12 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.1; H.3.2; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10318v1",
                "updated": "2025-04-14T15:27:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    27,
                    32,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:27:32Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    27,
                    32,
                    0,
                    104,
                    0
                ],
                "title": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing\n  Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing\n  Obfuscation"
                },
                "summary": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead."
                },
                "authors": [
                    {
                        "name": "Kartik Ramkrishnan"
                    },
                    {
                        "name": "Antonia Zhai"
                    },
                    {
                        "name": "Stephen McCamant"
                    },
                    {
                        "name": "Pen Chung Yew"
                    }
                ],
                "author_detail": {
                    "name": "Pen Chung Yew"
                },
                "author": "Pen Chung Yew",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10181v1",
                "updated": "2025-04-14T12:34:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    34,
                    20,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:34:20Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    34,
                    20,
                    0,
                    104,
                    0
                ],
                "title": "A New Paradigm in IBR Modeling for Power Flow and Short Circuit Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Paradigm in IBR Modeling for Power Flow and Short Circuit Analysis"
                },
                "summary": "The fault characteristics of inverter-based resources (IBRs) are different\nfrom conventional synchronous generators. The fault response of IBRs is\nnon-linear due to saturation states and mainly determined by fault ride through\n(FRT) strategies of the associated voltage source converter (VSC). This results\nin prohibitively large solution times for power flows considering these short\ncircuit characteristics, especially when the power system states change fast\ndue to uncertainty in IBR generations. To overcome this, a phasor-domain steady\nstate (SS) short circuit (SC) solver for IBR dominated power systems is\nproposed in this paper, and subsequently the developed IBR models are\nincorporated with a novel Jacobian-based Power Flow (PF) solver. In this\nmultiphase PF solver, any power system components can be modeled by considering\ntheir original non-linear or linear mathematical representations. Moreover, two\nnovel FRT strategies are proposed to fully utilize the converter capacity and\nto comply with IEEE-2800 2022 std and German grid code. The results are\ncompared with the Electromagnetic Transient (EMT) simulation on the IEEE 34\ntest network and the 120 kV EPRI benchmark system. The developed IBR sequence\ndomain PF model demonstrates more accurate behavior compared to the classical\nIBR generator model. The error in calculating the short circuit current with\nthe proposed SC solver is less than 3%, while achieving significant speed\nimprovements of three order of magnitudes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fault characteristics of inverter-based resources (IBRs) are different\nfrom conventional synchronous generators. The fault response of IBRs is\nnon-linear due to saturation states and mainly determined by fault ride through\n(FRT) strategies of the associated voltage source converter (VSC). This results\nin prohibitively large solution times for power flows considering these short\ncircuit characteristics, especially when the power system states change fast\ndue to uncertainty in IBR generations. To overcome this, a phasor-domain steady\nstate (SS) short circuit (SC) solver for IBR dominated power systems is\nproposed in this paper, and subsequently the developed IBR models are\nincorporated with a novel Jacobian-based Power Flow (PF) solver. In this\nmultiphase PF solver, any power system components can be modeled by considering\ntheir original non-linear or linear mathematical representations. Moreover, two\nnovel FRT strategies are proposed to fully utilize the converter capacity and\nto comply with IEEE-2800 2022 std and German grid code. The results are\ncompared with the Electromagnetic Transient (EMT) simulation on the IEEE 34\ntest network and the 120 kV EPRI benchmark system. The developed IBR sequence\ndomain PF model demonstrates more accurate behavior compared to the classical\nIBR generator model. The error in calculating the short circuit current with\nthe proposed SC solver is less than 3%, while achieving significant speed\nimprovements of three order of magnitudes."
                },
                "authors": [
                    {
                        "name": "Zahid Javid"
                    },
                    {
                        "name": "Firdous Ul Nazir"
                    },
                    {
                        "name": "Wentao Zhu"
                    },
                    {
                        "name": "Diptargha Chakravorty"
                    },
                    {
                        "name": "Ahmed Aboushady"
                    },
                    {
                        "name": "Mohamed Galeela"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Galeela"
                },
                "author": "Mohamed Galeela",
                "arxiv_comment": "12 Pages, First Revision Submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00601v3",
                "updated": "2025-04-14T11:20:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    20,
                    56,
                    0,
                    104,
                    0
                ],
                "published": "2024-11-01T14:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    3,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Diversity in Network-Friendly Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in Network-Friendly Recommendations"
                },
                "summary": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms."
                },
                "authors": [
                    {
                        "name": "Evangelia Tzimpimpaki"
                    },
                    {
                        "name": "Thrasyvoulos Spyropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Thrasyvoulos Spyropoulos"
                },
                "author": "Thrasyvoulos Spyropoulos",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09984v1",
                "updated": "2025-04-14T08:51:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    8,
                    51,
                    35,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T08:51:35Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    8,
                    51,
                    35,
                    0,
                    104,
                    0
                ],
                "title": "On Precomputation and Caching in Information Retrieval Experiments with\n  Pipeline Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Precomputation and Caching in Information Retrieval Experiments with\n  Pipeline Architectures"
                },
                "summary": "Modern information retrieval systems often rely on multiple components\nexecuted in a pipeline. In a research setting, this can lead to substantial\nredundant computations (e.g., retrieving the same query multiple times for\nevaluating different downstream rerankers). To overcome this, researchers take\ncached \"result\" files as inputs, which represent the output of another\npipeline. However, these result files can be brittle and can cause a disconnect\nbetween the conceptual design of the pipeline and its logical implementation.\nTo overcome both the redundancy problem (when executing complete pipelines) and\nthe disconnect problem (when relying on intermediate result files), we describe\nour recent efforts to improve the caching capabilities in the open-source\nPyTerrier IR platform. We focus on two main directions: (1) automatic implicit\ncaching of common pipeline prefixes when comparing systems and (2) explicit\ncaching of operations through a new extension package, pyterrier-caching. These\napproaches allow for the best of both worlds: pipelines can be fully expressed\nend-to-end, while also avoiding redundant computations between pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern information retrieval systems often rely on multiple components\nexecuted in a pipeline. In a research setting, this can lead to substantial\nredundant computations (e.g., retrieving the same query multiple times for\nevaluating different downstream rerankers). To overcome this, researchers take\ncached \"result\" files as inputs, which represent the output of another\npipeline. However, these result files can be brittle and can cause a disconnect\nbetween the conceptual design of the pipeline and its logical implementation.\nTo overcome both the redundancy problem (when executing complete pipelines) and\nthe disconnect problem (when relying on intermediate result files), we describe\nour recent efforts to improve the caching capabilities in the open-source\nPyTerrier IR platform. We focus on two main directions: (1) automatic implicit\ncaching of common pipeline prefixes when comparing systems and (2) explicit\ncaching of operations through a new extension package, pyterrier-caching. These\napproaches allow for the best of both worlds: pipelines can be fully expressed\nend-to-end, while also avoiding redundant computations between pipelines."
                },
                "authors": [
                    {
                        "name": "Sean MacAvaney"
                    },
                    {
                        "name": "Craig Macdonald"
                    }
                ],
                "author_detail": {
                    "name": "Craig Macdonald"
                },
                "author": "Craig Macdonald",
                "arxiv_comment": "WOWS @ ECIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09952v1",
                "updated": "2025-04-14T07:30:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    7,
                    30,
                    3,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T07:30:03Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    7,
                    30,
                    3,
                    0,
                    104,
                    0
                ],
                "title": "Secrecy and Privacy in Multi-Access Combinatorial Topology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secrecy and Privacy in Multi-Access Combinatorial Topology"
                },
                "summary": "In this work, we consider the multi-access combinatorial topology with $C$\ncaches where each user accesses a unique set of $r$ caches. For this setup, we\nconsider secrecy, where each user should not know anything about the files it\ndid not request, and demand privacy, where each user's demand must be kept\nprivate from other non-colluding users. We propose a scheme satisfying both\nconditions and derive a lower bound based on cut-set arguments. Also, we prove\nthat our scheme is optimal when $r\\geq C-1$, and it is order-optimal when the\ncache memory size $M$ is greater than or equal to a certain threshold for\n$r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same\nrate as the one given by the secretive scheme for the dedicated cache setup by\nRavindrakumar et al. ( 'Private Coded Caching,' in \\textit{IEEE Transactions on\nInformation Forensics and Security}, 2018), while satisfying both secrecy and\ndemand privacy conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we consider the multi-access combinatorial topology with $C$\ncaches where each user accesses a unique set of $r$ caches. For this setup, we\nconsider secrecy, where each user should not know anything about the files it\ndid not request, and demand privacy, where each user's demand must be kept\nprivate from other non-colluding users. We propose a scheme satisfying both\nconditions and derive a lower bound based on cut-set arguments. Also, we prove\nthat our scheme is optimal when $r\\geq C-1$, and it is order-optimal when the\ncache memory size $M$ is greater than or equal to a certain threshold for\n$r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same\nrate as the one given by the secretive scheme for the dedicated cache setup by\nRavindrakumar et al. ( 'Private Coded Caching,' in \\textit{IEEE Transactions on\nInformation Forensics and Security}, 2018), while satisfying both secrecy and\ndemand privacy conditions."
                },
                "authors": [
                    {
                        "name": "Mallikharjuna Chinnapadamala"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "11 pages and 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09936v1",
                "updated": "2025-04-14T06:58:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    6,
                    58,
                    0,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T06:58:00Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    6,
                    58,
                    0,
                    0,
                    104,
                    0
                ],
                "title": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for\n  Efficient LLMs Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for\n  Efficient LLMs Inference"
                },
                "summary": "Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets."
                },
                "authors": [
                    {
                        "name": "Yuxuan Tian"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Yebo Peng"
                    },
                    {
                        "name": "Aomufei Yuan"
                    },
                    {
                        "name": "Zhiming Wang"
                    },
                    {
                        "name": "Bairen Yi"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12280v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12280v2",
                "updated": "2025-04-13T14:17:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    17,
                    57,
                    6,
                    103,
                    0
                ],
                "published": "2024-02-19T16:47:04Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    16,
                    47,
                    4,
                    0,
                    50,
                    0
                ],
                "title": "Plato: Plan to Efficiently Decode for Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plato: Plan to Efficiently Decode for Large Language Model Inference"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage tasks, but their inference incurs substantial computational and memory\noverhead. To improve efficiency, parallel decoding methods like\nSkeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent\nprocessing. However, these methods significantly compromise answer quality by\ntreating semantically linked sub-problems as independent. We propose Plato, a\nnovel approach that co-designs algorithms and systems for semantic-aware\nparallel decoding. Plato leverages LLMs to organize sub-problems into a\ndependency graph based on logical and causal relationships, enabling concurrent\ndecoding of non-dependent nodes while preserving answer coherence and quality.\nTo further enhance efficiency, Plato pipelines planning and node decoding\nstages, implements a global context cache, and carefully structures node\ninference prompts to maximize key-value cache reuse and minimize overhead. Our\nevaluations show that Plato improves throughput by 68% over autoregressive\ndecoding while achieving a 40% net win rate in answer quality. Compared to SoT,\nPlato demonstrates a remarkable 90% quality net-win rate. Ablation studies\nreveal that our pipeline design improves speedup by 29%, while our KV cache\nreuse optimization reduces overhead by 75%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage tasks, but their inference incurs substantial computational and memory\noverhead. To improve efficiency, parallel decoding methods like\nSkeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent\nprocessing. However, these methods significantly compromise answer quality by\ntreating semantically linked sub-problems as independent. We propose Plato, a\nnovel approach that co-designs algorithms and systems for semantic-aware\nparallel decoding. Plato leverages LLMs to organize sub-problems into a\ndependency graph based on logical and causal relationships, enabling concurrent\ndecoding of non-dependent nodes while preserving answer coherence and quality.\nTo further enhance efficiency, Plato pipelines planning and node decoding\nstages, implements a global context cache, and carefully structures node\ninference prompts to maximize key-value cache reuse and minimize overhead. Our\nevaluations show that Plato improves throughput by 68% over autoregressive\ndecoding while achieving a 40% net win rate in answer quality. Compared to SoT,\nPlato demonstrates a remarkable 90% quality net-win rate. Ablation studies\nreveal that our pipeline design improves speedup by 29%, while our KV cache\nreuse optimization reduces overhead by 75%."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Haizhong Zheng"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Atul Prakash"
                    },
                    {
                        "name": "Matthew Lentz"
                    },
                    {
                        "name": "Danyang Zhuo"
                    },
                    {
                        "name": "Feng Qian"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12280v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12280v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09590v1",
                "updated": "2025-04-13T14:16:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    16,
                    57,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T14:16:57Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    16,
                    57,
                    6,
                    103,
                    0
                ],
                "title": "Efficient LLM Serving on Hybrid Real-time and Best-effort Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Serving on Hybrid Real-time and Best-effort Requests"
                },
                "summary": "Recent breakthroughs in large Language Models (LLMs) have enabled various\ngenerative tasks on a single model. Real-world services (e.g., OpenAI's ChatGPT\n[27]) powered by an LLM often concurrently support latency-critical requests\nfor interactive applications (e.g., question-answering systems, referred to as\nreal-time or RT requests) and throughput-oriented requests for back-of-house\nprocessing (e.g., documents batch processing [28], referred to best-effort or\nBE requests), with complex hybrid inference workloads to the underlying model.\nState-of-the-art (SOTA) LLM serving systems dedicate machines to each type of\nrequest, towards either low inference latency or high serving throughput,\nrespectively. This practice simplifies request scheduling and management but\nsuffers from poor resource utilization. We propose BROS, a hybrid LLM serving\nsystem that aims to collocate RT/BE requests, meeting RT requests' latency\nrequirements while maintaining BE requests' throughput. BROS formulates the\nproblem of hybrid RT/BE request scheduling and solves it with a dynamic\npriority-based algorithm. BROS designs a bidirectional KV cache management\nmechanism, allowing RT requests to share KV memory with BE requests to remove\nthe scheduling restrictions caused by insufficient KV memory and improve\nutilization. Extensive experiments validate that BROS achieves a good trade-off\nwhen serving hybrid RT and BE requests. It significantly reduces the latency of\nRT requests (up to 74.20%), improving their fine-grained service level\nobjectives (SLOs) attainments (up to 36.38x), with negligible throughput\nreduction for BE requests, showing significant advantages over SOTA systems\nlike vLLM and TGI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in large Language Models (LLMs) have enabled various\ngenerative tasks on a single model. Real-world services (e.g., OpenAI's ChatGPT\n[27]) powered by an LLM often concurrently support latency-critical requests\nfor interactive applications (e.g., question-answering systems, referred to as\nreal-time or RT requests) and throughput-oriented requests for back-of-house\nprocessing (e.g., documents batch processing [28], referred to best-effort or\nBE requests), with complex hybrid inference workloads to the underlying model.\nState-of-the-art (SOTA) LLM serving systems dedicate machines to each type of\nrequest, towards either low inference latency or high serving throughput,\nrespectively. This practice simplifies request scheduling and management but\nsuffers from poor resource utilization. We propose BROS, a hybrid LLM serving\nsystem that aims to collocate RT/BE requests, meeting RT requests' latency\nrequirements while maintaining BE requests' throughput. BROS formulates the\nproblem of hybrid RT/BE request scheduling and solves it with a dynamic\npriority-based algorithm. BROS designs a bidirectional KV cache management\nmechanism, allowing RT requests to share KV memory with BE requests to remove\nthe scheduling restrictions caused by insufficient KV memory and improve\nutilization. Extensive experiments validate that BROS achieves a good trade-off\nwhen serving hybrid RT and BE requests. It significantly reduces the latency of\nRT requests (up to 74.20%), improving their fine-grained service level\nobjectives (SLOs) attainments (up to 36.38x), with negligible throughput\nreduction for BE requests, showing significant advantages over SOTA systems\nlike vLLM and TGI."
                },
                "authors": [
                    {
                        "name": "Wan Borui"
                    },
                    {
                        "name": "Zhao Juntao"
                    },
                    {
                        "name": "Jiang Chenyu"
                    },
                    {
                        "name": "Guo Chuanxiong"
                    },
                    {
                        "name": "Wu Chuan"
                    }
                ],
                "author_detail": {
                    "name": "Wu Chuan"
                },
                "author": "Wu Chuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v5",
                "updated": "2025-04-13T14:02:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    2,
                    47,
                    6,
                    103,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient Prefilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient Prefilling"
                },
                "summary": "We introduce Block-attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context in an\nauto-regressive manner. Instead, Block-attention divides retrieved documents\ninto discrete blocks, with each block independently calculating key-value (KV)\nstates except for the final block. In RAG scenarios, by defining each passage\nas a block, Block-attention enables us to reuse the KV states of passages that\nhave been seen before, thereby significantly reducing the latency and the\ncomputation overhead during inference. The implementation of Block-attention\ninvolves block segmentation, position re-encoding, and fine-tuning the LLM to\nadapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks,\nincluding RAG, ICL, and general domains, demonstrate that after block\nfine-tuning, the Block-attention model not only achieves performance comparable\nto that of full-attention models, but can also seamlessly switch between the\nblock and full attention modes without any performance loss. Notably,\nBlock-attention significantly reduces the time to first token (TTFT) and\nfloating point operations (FLOPs) to a very low level. It only takes 45 ms to\noutput the first token for an input sequence with a total length of 32K.\nCompared to the full-attention models, the TTFT and corresponding FLOPs are\nreduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we\nelaborate on how Block-attention is applied in Game AI scenario and the\nsubstantial potential benefits it entails. We strongly suggest researchers in\nthe gaming field not to overlook this section.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context in an\nauto-regressive manner. Instead, Block-attention divides retrieved documents\ninto discrete blocks, with each block independently calculating key-value (KV)\nstates except for the final block. In RAG scenarios, by defining each passage\nas a block, Block-attention enables us to reuse the KV states of passages that\nhave been seen before, thereby significantly reducing the latency and the\ncomputation overhead during inference. The implementation of Block-attention\ninvolves block segmentation, position re-encoding, and fine-tuning the LLM to\nadapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks,\nincluding RAG, ICL, and general domains, demonstrate that after block\nfine-tuning, the Block-attention model not only achieves performance comparable\nto that of full-attention models, but can also seamlessly switch between the\nblock and full attention modes without any performance loss. Notably,\nBlock-attention significantly reduces the time to first token (TTFT) and\nfloating point operations (FLOPs) to a very low level. It only takes 45 ms to\noutput the first token for an input sequence with a total length of 32K.\nCompared to the full-attention models, the TTFT and corresponding FLOPs are\nreduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we\nelaborate on how Block-attention is applied in Game AI scenario and the\nsubstantial potential benefits it entails. We strongly suggest researchers in\nthe gaming field not to overlook this section."
                },
                "authors": [
                    {
                        "name": "Dongyang Ma"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10540v1",
                "updated": "2025-04-13T08:29:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    8,
                    29,
                    58,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T08:29:58Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    8,
                    29,
                    58,
                    6,
                    103,
                    0
                ],
                "title": "AB-Cache: Training-Free Acceleration of Diffusion Models via\n  Adams-Bashforth Cached Feature Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AB-Cache: Training-Free Acceleration of Diffusion Models via\n  Adams-Bashforth Cached Feature Reuse"
                },
                "summary": "Diffusion models have demonstrated remarkable success in generative tasks,\nyet their iterative denoising process results in slow inference, limiting their\npracticality. While existing acceleration methods exploit the well-known\nU-shaped similarity pattern between adjacent steps through caching mechanisms,\nthey lack theoretical foundation and rely on simplistic computation reuse,\noften leading to performance degradation. In this work, we provide a\ntheoretical understanding by analyzing the denoising process through the\nsecond-order Adams-Bashforth method, revealing a linear relationship between\nthe outputs of consecutive steps. This analysis explains why the outputs of\nadjacent steps exhibit a U-shaped pattern. Furthermore, extending\nAdams-Bashforth method to higher order, we propose a novel caching-based\nacceleration approach for diffusion models, instead of directly reusing cached\nresults, with a truncation error bound of only \\(O(h^k)\\) where $h$ is the step\nsize. Extensive validation across diverse image and video diffusion models\n(including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates\nour method's effectiveness in achieving nearly $3\\times$ speedup while\nmaintaining original performance levels, offering a practical real-time\nsolution without compromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated remarkable success in generative tasks,\nyet their iterative denoising process results in slow inference, limiting their\npracticality. While existing acceleration methods exploit the well-known\nU-shaped similarity pattern between adjacent steps through caching mechanisms,\nthey lack theoretical foundation and rely on simplistic computation reuse,\noften leading to performance degradation. In this work, we provide a\ntheoretical understanding by analyzing the denoising process through the\nsecond-order Adams-Bashforth method, revealing a linear relationship between\nthe outputs of consecutive steps. This analysis explains why the outputs of\nadjacent steps exhibit a U-shaped pattern. Furthermore, extending\nAdams-Bashforth method to higher order, we propose a novel caching-based\nacceleration approach for diffusion models, instead of directly reusing cached\nresults, with a truncation error bound of only \\(O(h^k)\\) where $h$ is the step\nsize. Extensive validation across diverse image and video diffusion models\n(including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates\nour method's effectiveness in achieving nearly $3\\times$ speedup while\nmaintaining original performance levels, offering a practical real-time\nsolution without compromising generation quality."
                },
                "authors": [
                    {
                        "name": "Zichao Yu"
                    },
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Guojiang Shao"
                    },
                    {
                        "name": "Chengwei Zhang"
                    },
                    {
                        "name": "Shengze Xu"
                    },
                    {
                        "name": "Jie Huang"
                    },
                    {
                        "name": "Feng Zhao"
                    },
                    {
                        "name": "Xiaodong Cun"
                    },
                    {
                        "name": "Wenyi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenyi Zhang"
                },
                "author": "Wenyi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09431v1",
                "updated": "2025-04-13T04:46:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    4,
                    46,
                    2,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T04:46:02Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    4,
                    46,
                    2,
                    6,
                    103,
                    0
                ],
                "title": "Sub-nanosecond in-plane magnetization switching induced by field-like\n  spin-orbit torques from ferromagnets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-nanosecond in-plane magnetization switching induced by field-like\n  spin-orbit torques from ferromagnets"
                },
                "summary": "Spin-orbit torques (SOTs) generated in SOT-material/ferromagnet structures\nare classified as damping-like (DL) and field-like (FL) torques for\ncurrent-driven magnetization switching. It is well known that both DL- and\nFL-SOTs originate from the SOT-material and DL-SOT dominates the current-driven\nswitching process while FL-SOT contributes limitedly, resulting in an\nincubation time (several nanoseconds) during collinear magnetization switching\nwith the spin polarization because of the DL attributes. Here we report a\nFL-SOT originated from the ferromagnet, different from the origin of DL-SOT,\nand demonstrate that it dominates the collinear magnetization switching. We\nshow that the FL-SOT and resultant collinear switching can be modulated, one\norder of magnitude and sign reversal, by controlling the ferromagnet. Because\nof no incubation time and higher charge-to-spin efficiencies in the FL\nswitching, we further show that the switching time can be down to 200 ps with\none order lower critical switching current density compared to DL switching.\nThese results indicate that the FL switching may provide a practical solution\nfor magnetic memory in speed-priority cache applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spin-orbit torques (SOTs) generated in SOT-material/ferromagnet structures\nare classified as damping-like (DL) and field-like (FL) torques for\ncurrent-driven magnetization switching. It is well known that both DL- and\nFL-SOTs originate from the SOT-material and DL-SOT dominates the current-driven\nswitching process while FL-SOT contributes limitedly, resulting in an\nincubation time (several nanoseconds) during collinear magnetization switching\nwith the spin polarization because of the DL attributes. Here we report a\nFL-SOT originated from the ferromagnet, different from the origin of DL-SOT,\nand demonstrate that it dominates the collinear magnetization switching. We\nshow that the FL-SOT and resultant collinear switching can be modulated, one\norder of magnitude and sign reversal, by controlling the ferromagnet. Because\nof no incubation time and higher charge-to-spin efficiencies in the FL\nswitching, we further show that the switching time can be down to 200 ps with\none order lower critical switching current density compared to DL switching.\nThese results indicate that the FL switching may provide a practical solution\nfor magnetic memory in speed-priority cache applications."
                },
                "authors": [
                    {
                        "name": "Hanying Zhang"
                    },
                    {
                        "name": "Ziqian Cui"
                    },
                    {
                        "name": "Baiqing Jiang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "C. Bi"
                    }
                ],
                "author_detail": {
                    "name": "C. Bi"
                },
                "author": "C. Bi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09261v1",
                "updated": "2025-04-12T15:42:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    12,
                    15,
                    42,
                    17,
                    5,
                    102,
                    0
                ],
                "published": "2025-04-12T15:42:17Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    15,
                    42,
                    17,
                    5,
                    102,
                    0
                ],
                "title": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive\n  Modeling"
                },
                "summary": "Visual Autoregressive (VAR) models have emerged as a powerful approach for\nmulti-modal content creation, offering high efficiency and quality across\ndiverse multimedia applications. However, they face significant memory\nbottlenecks due to extensive KV cache accumulation during inference. Existing\nKV cache compression techniques for large language models are suboptimal for\nVAR models due to, as we identify in this paper, two distinct categories of\nattention heads in VAR models: Structural Heads, which preserve spatial\ncoherence through diagonal attention patterns, and Contextual Heads, which\nmaintain semantic consistency through vertical attention patterns. These\ndifferences render single-strategy KV compression techniques ineffective for\nVAR models. To address this, we propose HACK, a training-free Head-Aware\nCompression method for KV cache. HACK allocates asymmetric cache budgets and\nemploys pattern-specific compression strategies tailored to the essential\ncharacteristics of each head category. Experiments on Infinity-2B, Infinity-8B,\nand VAR-d30 demonstrate its effectiveness in text-to-image and\nclass-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of\ncache with minimal performance degradation for VAR-d30 and Infinity-8B,\nrespectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and\nInfinity-8B, HACK still maintains high-quality generation while reducing memory\nusage by 44.2\\% and 58.9\\%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) models have emerged as a powerful approach for\nmulti-modal content creation, offering high efficiency and quality across\ndiverse multimedia applications. However, they face significant memory\nbottlenecks due to extensive KV cache accumulation during inference. Existing\nKV cache compression techniques for large language models are suboptimal for\nVAR models due to, as we identify in this paper, two distinct categories of\nattention heads in VAR models: Structural Heads, which preserve spatial\ncoherence through diagonal attention patterns, and Contextual Heads, which\nmaintain semantic consistency through vertical attention patterns. These\ndifferences render single-strategy KV compression techniques ineffective for\nVAR models. To address this, we propose HACK, a training-free Head-Aware\nCompression method for KV cache. HACK allocates asymmetric cache budgets and\nemploys pattern-specific compression strategies tailored to the essential\ncharacteristics of each head category. Experiments on Infinity-2B, Infinity-8B,\nand VAR-d30 demonstrate its effectiveness in text-to-image and\nclass-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of\ncache with minimal performance degradation for VAR-d30 and Infinity-8B,\nrespectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and\nInfinity-8B, HACK still maintains high-quality generation while reducing memory\nusage by 44.2\\% and 58.9\\%, respectively."
                },
                "authors": [
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Youru Lv"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Zeren Zhang"
                    },
                    {
                        "name": "Danping Zou"
                    },
                    {
                        "name": "Weiyao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weiyao Lin"
                },
                "author": "Weiyao Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17459v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17459v3",
                "updated": "2025-04-11T12:31:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    12,
                    31,
                    7,
                    4,
                    101,
                    0
                ],
                "published": "2024-11-26T14:23:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model"
                },
                "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE."
                },
                "authors": [
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17459v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17459v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08378v1",
                "updated": "2025-04-11T09:26:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T09:26:47Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "title": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash"
                },
                "summary": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods."
                },
                "authors": [
                    {
                        "name": "Fucheng Jia"
                    },
                    {
                        "name": "Zewen Wu"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Ju Ren"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Ting Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ting Cao"
                },
                "author": "Ting Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08204v1",
                "updated": "2025-04-11T02:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    10,
                    2,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T02:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    10,
                    2,
                    4,
                    101,
                    0
                ],
                "title": "II-NVM: Enhancing Map Accuracy and Consistency with Normal\n  Vector-Assisted Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "II-NVM: Enhancing Map Accuracy and Consistency with Normal\n  Vector-Assisted Mapping"
                },
                "summary": "SLAM technology plays a crucial role in indoor mapping and localization. A\ncommon challenge in indoor environments is the \"double-sided mapping issue\",\nwhere closely positioned walls, doors, and other surfaces are mistakenly\nidentified as a single plane, significantly hindering map accuracy and\nconsistency. To address this issue this paper introduces a SLAM approach that\nensures accurate mapping using normal vector consistency. We enhance the voxel\nmap structure to store both point cloud data and normal vector information,\nenabling the system to evaluate consistency during nearest neighbor searches\nand map updates. This process distinguishes between the front and back sides of\nsurfaces, preventing incorrect point-to-plane constraints. Moreover, we\nimplement an adaptive radius KD-tree search method that dynamically adjusts the\nsearch radius based on the local density of the point cloud, thereby enhancing\nthe accuracy of normal vector calculations. To further improve realtime\nperformance and storage efficiency, we incorporate a Least Recently Used (LRU)\ncache strategy, which facilitates efficient incremental updates of the voxel\nmap. The code is released as open-source and validated in both simulated\nenvironments and real indoor scenarios. Experimental results demonstrate that\nthis approach effectively resolves the \"double-sided mapping issue\" and\nsignificantly improves mapping precision. Additionally, we have developed and\nopen-sourced the first simulation and real world dataset specifically tailored\nfor the \"double-sided mapping issue\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLAM technology plays a crucial role in indoor mapping and localization. A\ncommon challenge in indoor environments is the \"double-sided mapping issue\",\nwhere closely positioned walls, doors, and other surfaces are mistakenly\nidentified as a single plane, significantly hindering map accuracy and\nconsistency. To address this issue this paper introduces a SLAM approach that\nensures accurate mapping using normal vector consistency. We enhance the voxel\nmap structure to store both point cloud data and normal vector information,\nenabling the system to evaluate consistency during nearest neighbor searches\nand map updates. This process distinguishes between the front and back sides of\nsurfaces, preventing incorrect point-to-plane constraints. Moreover, we\nimplement an adaptive radius KD-tree search method that dynamically adjusts the\nsearch radius based on the local density of the point cloud, thereby enhancing\nthe accuracy of normal vector calculations. To further improve realtime\nperformance and storage efficiency, we incorporate a Least Recently Used (LRU)\ncache strategy, which facilitates efficient incremental updates of the voxel\nmap. The code is released as open-source and validated in both simulated\nenvironments and real indoor scenarios. Experimental results demonstrate that\nthis approach effectively resolves the \"double-sided mapping issue\" and\nsignificantly improves mapping precision. Additionally, we have developed and\nopen-sourced the first simulation and real world dataset specifically tailored\nfor the \"double-sided mapping issue\"."
                },
                "authors": [
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Yina Jian"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Linji Wang"
                    },
                    {
                        "name": "Yongxin Ma"
                    },
                    {
                        "name": "Xinglai Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xinglai Jin"
                },
                "author": "Xinglai Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07596v2",
                "updated": "2025-04-11T02:05:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    5,
                    1,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-10T09:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    48,
                    56,
                    3,
                    100,
                    0
                ],
                "title": "Boosting Universal LLM Reward Design through Heuristic Reward\n  Observation Space Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Universal LLM Reward Design through Heuristic Reward\n  Observation Space Evolution"
                },
                "summary": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward."
                },
                "authors": [
                    {
                        "name": "Zen Kit Heng"
                    },
                    {
                        "name": "Zimeng Zhao"
                    },
                    {
                        "name": "Tianhao Wu"
                    },
                    {
                        "name": "Yuanfei Wang"
                    },
                    {
                        "name": "Mingdong Wu"
                    },
                    {
                        "name": "Yangang Wang"
                    },
                    {
                        "name": "Hao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Hao Dong"
                },
                "author": "Hao Dong",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07815v1",
                "updated": "2025-04-10T14:52:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    52,
                    3,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T14:52:03Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    52,
                    3,
                    3,
                    100,
                    0
                ],
                "title": "Siren Federate: Bridging document, relational, and graph models for\n  exploratory graph analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Siren Federate: Bridging document, relational, and graph models for\n  exploratory graph analysis"
                },
                "summary": "Investigative workflows require interactive exploratory analysis on large\nheterogeneous knowledge graphs. Current databases show limitations in enabling\nsuch task. This paper discusses the architecture of Siren Federate, a system\nthat efficiently supports exploratory graph analysis by bridging\ndocument-oriented, relational and graph models. Technical contributions include\ndistributed join algorithms, adaptive query planning, query plan folding,\nsemantic caching, and semi-join decomposition for path query. Semi-join\ndecomposition addresses the exponential growth of intermediate results in\npath-based queries. Experiments show that Siren Federate exhibits low latency\nand scales well with the amount of data, the number of users, and the number of\ncomputing nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigative workflows require interactive exploratory analysis on large\nheterogeneous knowledge graphs. Current databases show limitations in enabling\nsuch task. This paper discusses the architecture of Siren Federate, a system\nthat efficiently supports exploratory graph analysis by bridging\ndocument-oriented, relational and graph models. Technical contributions include\ndistributed join algorithms, adaptive query planning, query plan folding,\nsemantic caching, and semi-join decomposition for path query. Semi-join\ndecomposition addresses the exponential growth of intermediate results in\npath-based queries. Experiments show that Siren Federate exhibits low latency\nand scales well with the amount of data, the number of users, and the number of\ncomputing nodes."
                },
                "authors": [
                    {
                        "name": "Georgeta Bordea"
                    },
                    {
                        "name": "Stephane Campinas"
                    },
                    {
                        "name": "Matteo Catena"
                    },
                    {
                        "name": "Renaud Delbru"
                    }
                ],
                "author_detail": {
                    "name": "Renaud Delbru"
                },
                "author": "Renaud Delbru",
                "arxiv_comment": "36 pages, 16 figures, submitted to the ComSIS journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.11; E.1; H.2.4; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07642v1",
                "updated": "2025-04-10T10:43:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    43,
                    42,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T10:43:42Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    43,
                    42,
                    3,
                    100,
                    0
                ],
                "title": "Cache-a-lot: Pushing the Limits of Unsatisfiable Core Reuse in SMT-Based\n  Program Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-a-lot: Pushing the Limits of Unsatisfiable Core Reuse in SMT-Based\n  Program Analysis"
                },
                "summary": "Satisfiability Modulo Theories (SMT) solvers are integral to program analysis\ntechniques like concolic and symbolic execution, where they help assess the\nsatisfiability of logical formulae to explore execution paths of the program\nunder test. However, frequent solver invocations are still the main performance\nbottleneck of these techniques. One way to mitigate this challenge is through\noptimizations such as caching and reusing solver results. While current methods\ntypically focus on reusing results from fully equivalent or closely related\nformulas, they often miss broader opportunities for reuse. In this paper, we\npropose a novel approach, Cache-a-lot, that extends the reuse of unsatisfiable\n(unsat) results by systematically considering all possible variable\nsubstitutions. This enables more extensive reuse of results, thereby reducing\nthe number of SMT solver invocations and improving the overall efficiency of\nconcolic and symbolic execution. Our evaluation, conducted against the\nstate-of-the-art Utopia solution using two benchmark sets, shows significant\nimprovements, particularly with more complex formulas. Our method achieves up\nto 74% unsat core reuse, compared to Utopia's 41%, and significant increase in\nthe time savings. These results demonstrate that, despite the additional\ncomputational complexity, the broader reuse of unsat results significantly\nenhances performance, offering valuable advancements for formal verification\nand program analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satisfiability Modulo Theories (SMT) solvers are integral to program analysis\ntechniques like concolic and symbolic execution, where they help assess the\nsatisfiability of logical formulae to explore execution paths of the program\nunder test. However, frequent solver invocations are still the main performance\nbottleneck of these techniques. One way to mitigate this challenge is through\noptimizations such as caching and reusing solver results. While current methods\ntypically focus on reusing results from fully equivalent or closely related\nformulas, they often miss broader opportunities for reuse. In this paper, we\npropose a novel approach, Cache-a-lot, that extends the reuse of unsatisfiable\n(unsat) results by systematically considering all possible variable\nsubstitutions. This enables more extensive reuse of results, thereby reducing\nthe number of SMT solver invocations and improving the overall efficiency of\nconcolic and symbolic execution. Our evaluation, conducted against the\nstate-of-the-art Utopia solution using two benchmark sets, shows significant\nimprovements, particularly with more complex formulas. Our method achieves up\nto 74% unsat core reuse, compared to Utopia's 41%, and significant increase in\nthe time savings. These results demonstrate that, despite the additional\ncomputational complexity, the broader reuse of unsat results significantly\nenhances performance, offering valuable advancements for formal verification\nand program analysis."
                },
                "authors": [
                    {
                        "name": "Rustam Sadykov"
                    },
                    {
                        "name": "Azat Abdullin"
                    },
                    {
                        "name": "Marat Akhin"
                    }
                ],
                "author_detail": {
                    "name": "Marat Akhin"
                },
                "author": "Marat Akhin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07494v1",
                "updated": "2025-04-10T06:51:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    51,
                    23,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:51:23Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    51,
                    23,
                    3,
                    100,
                    0
                ],
                "title": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\n  Inference Serving"
                },
                "summary": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems."
                },
                "authors": [
                    {
                        "name": "Shihong Gao"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Yanyan Shen"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_doi": "10.1145/3725394",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725394",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.07494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07479v1",
                "updated": "2025-04-10T06:13:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    13,
                    30,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:13:30Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    13,
                    30,
                    3,
                    100,
                    0
                ],
                "title": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\n  Pruning for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\n  Pruning for Efficient Long-Context LLM Inference"
                },
                "summary": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference."
                },
                "authors": [
                    {
                        "name": "Weikai Xu"
                    },
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Qianqian Huang"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Ru Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ru Huang"
                },
                "author": "Ru Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v3",
                "updated": "2025-04-10T05:06:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    6,
                    29,
                    3,
                    100,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "arxiv_comment": "MLSys 2025 camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07056v2",
                "updated": "2025-04-09T21:47:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    21,
                    47,
                    31,
                    2,
                    99,
                    0
                ],
                "published": "2025-01-13T04:31:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "MAGNUS: Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGNUS: Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs"
                },
                "summary": "Sparse general matrix-matrix multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines, including Intel MKL, for a variety of different matrices on three\nIntel architectures. For matrices from the SuiteSparse collection, MAGNUS is\nfaster than all the baselines in most cases and is often an order of magnitude\nfaster than at least one baseline. For massive random matrices, MAGNUS scales\nto the largest matrix sizes, while the baselines do not. Furthermore, MAGNUS is\nclose to the optimal bound for these matrices, regardless of the matrix size,\nstructure, and density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse general matrix-matrix multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines, including Intel MKL, for a variety of different matrices on three\nIntel architectures. For matrices from the SuiteSparse collection, MAGNUS is\nfaster than all the baselines in most cases and is often an order of magnitude\nfaster than at least one baseline. For massive random matrices, MAGNUS scales\nto the largest matrix sizes, while the baselines do not. Furthermore, MAGNUS is\nclose to the optimal bound for these matrices, regardless of the matrix size,\nstructure, and density."
                },
                "authors": [
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Petrini"
                },
                "author": "Fabrizio Petrini",
                "arxiv_comment": "Accepted to ICS25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v3",
                "updated": "2025-04-09T20:51:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    20,
                    51,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "31 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06261v2",
                "updated": "2025-04-09T17:56:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    56,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-08T17:59:41Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    41,
                    1,
                    98,
                    0
                ],
                "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention"
                },
                "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning."
                },
                "authors": [
                    {
                        "name": "Gleb Rodionov"
                    },
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "George Yakushev"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Anton Sinitsin"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04514v2",
                "updated": "2025-04-09T14:36:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    36,
                    19,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-06T15:15:07Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    15,
                    15,
                    7,
                    6,
                    96,
                    0
                ],
                "title": "Saliency-driven Dynamic Token Pruning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saliency-driven Dynamic Token Pruning for Large Language Models"
                },
                "summary": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression."
                },
                "authors": [
                    {
                        "name": "Yao Tao"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Yun Wang"
                    },
                    {
                        "name": "Mingjian Zhu"
                    },
                    {
                        "name": "Hailin Hu"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06813v1",
                "updated": "2025-04-09T12:07:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    7,
                    26,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T12:07:26Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    7,
                    26,
                    2,
                    99,
                    0
                ],
                "title": "Introducing the Arm-membench Throughput Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introducing the Arm-membench Throughput Benchmark"
                },
                "summary": "Application performance of modern day processors is often limited by the\nmemory subsystem rather than actual compute capabilities. Therefore, data\nthroughput specifications play a key role in modeling application performance\nand determining possible bottlenecks. However, while peak instruction\nthroughputs and bandwidths for local caches are often documented, the\nachievable throughput can also depend on the relation between memory access and\ncompute instructions. In this paper, we present an Arm version of the well\nestablished x86-membench throughput benchmark, which we have adapted to support\nall current SIMD extensions of the Armv8 instruction set architecture. We\ndescribe aspects of the Armv8 ISA that need to be considered in the portable\ndesign of this benchmark. We use the benchmark to analyze the memory subsystem\nat a fine spatial granularity and to unveil microarchitectural details of three\nprocessors: Fujitsu A64FX, Ampere Altra and Cavium ThunderX2. Based on the\nresulting performance information, we show that instruction fetch and decoder\nwidths become a potential bottleneck for cache-bandwidth-sensitive workloads\ndue to the load-store concept of the Arm ISA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application performance of modern day processors is often limited by the\nmemory subsystem rather than actual compute capabilities. Therefore, data\nthroughput specifications play a key role in modeling application performance\nand determining possible bottlenecks. However, while peak instruction\nthroughputs and bandwidths for local caches are often documented, the\nachievable throughput can also depend on the relation between memory access and\ncompute instructions. In this paper, we present an Arm version of the well\nestablished x86-membench throughput benchmark, which we have adapted to support\nall current SIMD extensions of the Armv8 instruction set architecture. We\ndescribe aspects of the Armv8 ISA that need to be considered in the portable\ndesign of this benchmark. We use the benchmark to analyze the memory subsystem\nat a fine spatial granularity and to unveil microarchitectural details of three\nprocessors: Fujitsu A64FX, Ampere Altra and Cavium ThunderX2. Based on the\nresulting performance information, we show that instruction fetch and decoder\nwidths become a potential bottleneck for cache-bandwidth-sensitive workloads\ndue to the load-store concept of the Arm ISA."
                },
                "authors": [
                    {
                        "name": "Cyrill Burth"
                    },
                    {
                        "name": "Markus Velten"
                    },
                    {
                        "name": "Robert Schne"
                    }
                ],
                "author_detail": {
                    "name": "Robert Schne"
                },
                "author": "Robert Schne",
                "arxiv_doi": "10.1007/978-3-031-85697-6_7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-85697-6_7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.06813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 6 figures, published in Parallel Processing and Applied\n  Mathematics (PPAM 2024), see https://doi.org/10.1007/978-3-031-85697-6_7",
                "arxiv_journal_ref": "Parallel Processing and Applied Mathematics. PPAM 2024. Lecture\n  Notes in Computer Science, vol 15579. Springer, Cham",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05821v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05821v2",
                "updated": "2025-04-09T10:23:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    23,
                    39,
                    2,
                    99,
                    0
                ],
                "published": "2024-03-09T07:01:44Z",
                "published_parsed": [
                    2024,
                    3,
                    9,
                    7,
                    1,
                    44,
                    5,
                    69,
                    0
                ],
                "title": "Optimizing LLM Queries in Relational Data Analytics Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Queries in Relational Data Analytics Workloads"
                },
                "summary": "Batch data analytics is a growing application for Large Language Models\n(LLMs). LLMs enable users to perform a wide range of natural language tasks,\nsuch as classification, entity extraction, and translation, over large\ndatasets. However, LLM inference is highly costly and slow: for example, an\nNVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second,\ntaking about a day to handle 15 GB of data; processing a similar amount of data\ncosts around $10K on OpenAI's GPT-4o. In this paper, we propose novel\ntechniques that can significantly reduce the cost of LLM calls for relational\ndata analytics workloads. Our key contribution is developing efficient\nalgorithms for reordering the rows and the fields within each row of an input\ntable to maximize key-value (KV) cache reuse when performing LLM serving. As\nsuch, our approach can be easily applied to existing analytics systems and\nserving platforms. Our evaluation shows that our solution can yield up to 3.4x\nimprovement in job completion time on a benchmark of diverse LLM-based queries\nusing Llama 3 models. Our solution also achieves a 32% cost savings under\nOpenAI and Anthropic pricing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch data analytics is a growing application for Large Language Models\n(LLMs). LLMs enable users to perform a wide range of natural language tasks,\nsuch as classification, entity extraction, and translation, over large\ndatasets. However, LLM inference is highly costly and slow: for example, an\nNVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second,\ntaking about a day to handle 15 GB of data; processing a similar amount of data\ncosts around $10K on OpenAI's GPT-4o. In this paper, we propose novel\ntechniques that can significantly reduce the cost of LLM calls for relational\ndata analytics workloads. Our key contribution is developing efficient\nalgorithms for reordering the rows and the fields within each row of an input\ntable to maximize key-value (KV) cache reuse when performing LLM serving. As\nsuch, our approach can be easily applied to existing analytics systems and\nserving platforms. Our evaluation shows that our solution can yield up to 3.4x\nimprovement in job completion time on a benchmark of diverse LLM-based queries\nusing Llama 3 models. Our solution also achieves a 32% cost savings under\nOpenAI and Anthropic pricing models."
                },
                "authors": [
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Asim Biswal"
                    },
                    {
                        "name": "Amog Kamsetty"
                    },
                    {
                        "name": "Audrey Cheng"
                    },
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Liana Patel"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Matei Zaharia"
                    }
                ],
                "author_detail": {
                    "name": "Matei Zaharia"
                },
                "author": "Matei Zaharia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05821v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05821v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05591v3",
                "updated": "2025-04-09T09:09:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    9,
                    37,
                    2,
                    99,
                    0
                ],
                "published": "2024-09-09T13:20:31Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    13,
                    20,
                    31,
                    0,
                    253,
                    0
                ],
                "title": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation"
                },
                "summary": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Kelong Mao"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Tiejun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Tiejun Huang"
                },
                "author": "Tiejun Huang",
                "arxiv_comment": "theWebConf 2025. Codes and models are in\n  https://github.com/qhjqhj00/MemoRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v5",
                "updated": "2025-04-09T07:55:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    55,
                    43,
                    2,
                    99,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v5",
                "updated": "2025-04-09T03:49:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    3,
                    49,
                    16,
                    2,
                    99,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Feng Yao"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "This paper was accepted by VLDB2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06419v1",
                "updated": "2025-04-08T20:39:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    39,
                    20,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T20:39:20Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    39,
                    20,
                    1,
                    98,
                    0
                ],
                "title": "SPIRe: Boosting LLM Inference Throughput with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPIRe: Boosting LLM Inference Throughput with Speculative Decoding"
                },
                "summary": "Speculative decoding (SD) has been shown to reduce the latency of\nautoregressive decoding (AD) by 2-3x for small batch sizes. However, increasing\nthroughput and therefore reducing the cost per token requires decoding with\nlarge batch sizes. Recent work shows that SD can accelerate decoding with large\nbatch sizes too if the context is sufficiently long and the draft model's KV\ncache is sparse. We introduce SPIRe, a draft model that combines static sparse\nattention, pruned initialization, and feedback memory to increase the modeled\nthroughput of speculative decoding by over 100% compared to speculation with a\nmuch smaller draft model and by over 35% compared to the strong baseline of\nsparse self-speculation. Our approach is particularly effective when context\nlengths vary significantly across requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD) has been shown to reduce the latency of\nautoregressive decoding (AD) by 2-3x for small batch sizes. However, increasing\nthroughput and therefore reducing the cost per token requires decoding with\nlarge batch sizes. Recent work shows that SD can accelerate decoding with large\nbatch sizes too if the context is sufficiently long and the draft model's KV\ncache is sparse. We introduce SPIRe, a draft model that combines static sparse\nattention, pruned initialization, and feedback memory to increase the modeled\nthroughput of speculative decoding by over 100% compared to speculation with a\nmuch smaller draft model and by over 35% compared to the strong baseline of\nsparse self-speculation. Our approach is particularly effective when context\nlengths vary significantly across requests."
                },
                "authors": [
                    {
                        "name": "Sanjit Neelam"
                    },
                    {
                        "name": "Daniel Heinlein"
                    },
                    {
                        "name": "Vaclav Cvicek"
                    },
                    {
                        "name": "Akshay Mishra"
                    },
                    {
                        "name": "Reiner Pope"
                    }
                ],
                "author_detail": {
                    "name": "Reiner Pope"
                },
                "author": "Reiner Pope",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06416v1",
                "updated": "2025-04-08T20:32:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T20:32:10Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "title": "Unifying Autoregressive and Diffusion-Based Sequence Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Autoregressive and Diffusion-Based Sequence Generation"
                },
                "summary": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation."
                },
                "authors": [
                    {
                        "name": "Nima Fathi"
                    },
                    {
                        "name": "Torsten Scholak"
                    },
                    {
                        "name": "Pierre-Andr Nol"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-Andr Nol"
                },
                "author": "Pierre-Andr Nol",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.17692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.17692v2",
                "updated": "2025-04-08T19:26:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    19,
                    26,
                    41,
                    1,
                    98,
                    0
                ],
                "published": "2024-04-26T20:44:36Z",
                "published_parsed": [
                    2024,
                    4,
                    26,
                    20,
                    44,
                    36,
                    4,
                    117,
                    0
                ],
                "title": "Walking on Spheres and Talking to Neighbors: Variance Reduction for\n  Laplace's Equation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walking on Spheres and Talking to Neighbors: Variance Reduction for\n  Laplace's Equation"
                },
                "summary": "Walk on Spheres algorithms leverage properties of Brownian Motion to create\nMonte Carlo estimates of solutions to a class of elliptic partial differential\nequations. We propose a new caching strategy which leverages the continuity of\npaths of Brownian Motion. In the case of Laplace's equation with Dirichlet\nboundary conditions, our algorithm has improved asymptotic runtime compared to\nprevious approaches. Until recently, estimates were constructed pointwise and\ndid not use the relationship between solutions at nearby points within a\ndomain. Instead, our results are achieved by passing information from a cache\nof fixed size. We also provide bounds on the performance of our algorithm and\ndemonstrate its performance on example problems of increasing complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walk on Spheres algorithms leverage properties of Brownian Motion to create\nMonte Carlo estimates of solutions to a class of elliptic partial differential\nequations. We propose a new caching strategy which leverages the continuity of\npaths of Brownian Motion. In the case of Laplace's equation with Dirichlet\nboundary conditions, our algorithm has improved asymptotic runtime compared to\nprevious approaches. Until recently, estimates were constructed pointwise and\ndid not use the relationship between solutions at nearby points within a\ndomain. Instead, our results are achieved by passing information from a cache\nof fixed size. We also provide bounds on the performance of our algorithm and\ndemonstrate its performance on example problems of increasing complexity."
                },
                "authors": [
                    {
                        "name": "Michael Czekanski"
                    },
                    {
                        "name": "Benjamin Faber"
                    },
                    {
                        "name": "Margaret Fairborn"
                    },
                    {
                        "name": "Adelle Wright"
                    },
                    {
                        "name": "David Bindel"
                    }
                ],
                "author_detail": {
                    "name": "David Bindel"
                },
                "author": "David Bindel",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.17692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.17692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06067v1",
                "updated": "2025-04-08T14:09:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    9,
                    23,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T14:09:23Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    9,
                    23,
                    1,
                    98,
                    0
                ],
                "title": "GPU-accelerated Evolutionary Many-objective Optimization Using\n  Tensorized NSGA-III",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU-accelerated Evolutionary Many-objective Optimization Using\n  Tensorized NSGA-III"
                },
                "summary": "NSGA-III is one of the most widely adopted algorithms for tackling\nmany-objective optimization problems. However, its CPU-based design severely\nlimits scalability and computational efficiency. To address the limitations, we\npropose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that\nleverages GPU parallelism for large-scale many-objective optimization. Unlike\nconventional GPU-accelerated evolutionary algorithms that rely on heuristic\napproximations to improve efficiency, TensorNSGA-III maintains the exact\nselection and variation mechanisms of NSGA-III while achieving significant\nacceleration. By reformulating the selection process with tensorized data\nstructures and an optimized caching strategy, our approach effectively\neliminates computational bottlenecks inherent in traditional CPU-based and\nna\\\"ive GPU implementations. Experimental results on widely used numerical\nbenchmarks show that TensorNSGA-III achieves speedups of up to $3629\\times$\nover the CPU version of NSGA-III. Additionally, we validate its effectiveness\nin multiobjective robotic control tasks, where it discovers diverse and\nhigh-quality behavioral solutions. Furthermore, we investigate the critical\nrole of large population sizes in many-objective optimization and demonstrate\nthe scalability of TensorNSGA-III in such scenarios. The source code is\navailable at https://github.com/EMI-Group/evomo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NSGA-III is one of the most widely adopted algorithms for tackling\nmany-objective optimization problems. However, its CPU-based design severely\nlimits scalability and computational efficiency. To address the limitations, we\npropose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that\nleverages GPU parallelism for large-scale many-objective optimization. Unlike\nconventional GPU-accelerated evolutionary algorithms that rely on heuristic\napproximations to improve efficiency, TensorNSGA-III maintains the exact\nselection and variation mechanisms of NSGA-III while achieving significant\nacceleration. By reformulating the selection process with tensorized data\nstructures and an optimized caching strategy, our approach effectively\neliminates computational bottlenecks inherent in traditional CPU-based and\nna\\\"ive GPU implementations. Experimental results on widely used numerical\nbenchmarks show that TensorNSGA-III achieves speedups of up to $3629\\times$\nover the CPU version of NSGA-III. Additionally, we validate its effectiveness\nin multiobjective robotic control tasks, where it discovers diverse and\nhigh-quality behavioral solutions. Furthermore, we investigate the critical\nrole of large population sizes in many-objective optimization and demonstrate\nthe scalability of TensorNSGA-III in such scenarios. The source code is\navailable at https://github.com/EMI-Group/evomo"
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Zhenyu Liang"
                    },
                    {
                        "name": "Ran Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Ran Cheng"
                },
                "author": "Ran Cheng",
                "arxiv_comment": "Accepted by IEEE CEC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v2",
                "updated": "2025-04-08T14:05:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    5,
                    12,
                    1,
                    98,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying KV Cache Compression for Large Language Models with LeanKV"
                },
                "summary": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04760v2",
                "updated": "2025-04-08T12:46:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    12,
                    46,
                    45,
                    1,
                    98,
                    0
                ],
                "published": "2025-02-07T08:48:06Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "title": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing"
                },
                "summary": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05897v1",
                "updated": "2025-04-08T10:47:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    47,
                    37,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T10:47:37Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    47,
                    37,
                    1,
                    98,
                    0
                ],
                "title": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference"
                },
                "summary": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Yanfan Sun"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "Accepted by DAC 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06319v1",
                "updated": "2025-04-08T09:17:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    17,
                    35,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T09:17:35Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    17,
                    35,
                    1,
                    98,
                    0
                ],
                "title": "Accelerating LLM Inference Throughput via Asynchronous KV Cache\n  Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference Throughput via Asynchronous KV Cache\n  Prefetching"
                },
                "summary": "Large Language Models (LLMs) exhibit pronounced memory-bound characteristics\nduring inference due to High Bandwidth Memory (HBM) bandwidth constraints. In\nthis paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching\nmethod to break through the memory bandwidth bottleneck in LLM inference\nthrough computation-load overlap. By strategically scheduling idle memory\nbandwidth during active computation windows, our method proactively prefetches\nrequired KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for\nsubsequent accesses and effectively hiding HBM access latency within\ncomputational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that\nthe proposed method achieves 2.15x improvement in attention kernel efficiency\nand up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art\nbaseline FlashAttention-3. Notably, our solution maintains orthogonality to\nexisting optimization techniques and can be integrated with current inference\nframeworks, providing a scalable latency-hiding solution for next-generation\nLLM inference engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit pronounced memory-bound characteristics\nduring inference due to High Bandwidth Memory (HBM) bandwidth constraints. In\nthis paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching\nmethod to break through the memory bandwidth bottleneck in LLM inference\nthrough computation-load overlap. By strategically scheduling idle memory\nbandwidth during active computation windows, our method proactively prefetches\nrequired KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for\nsubsequent accesses and effectively hiding HBM access latency within\ncomputational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that\nthe proposed method achieves 2.15x improvement in attention kernel efficiency\nand up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art\nbaseline FlashAttention-3. Notably, our solution maintains orthogonality to\nexisting optimization techniques and can be integrated with current inference\nframeworks, providing a scalable latency-hiding solution for next-generation\nLLM inference engines."
                },
                "authors": [
                    {
                        "name": "Yanhao Dong"
                    },
                    {
                        "name": "Yubo Miao"
                    },
                    {
                        "name": "Weinan Li"
                    },
                    {
                        "name": "Xiao Zheng"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Feng Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Lyu"
                },
                "author": "Feng Lyu",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05807v1",
                "updated": "2025-04-08T08:40:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    40,
                    36,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T08:40:36Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    40,
                    36,
                    1,
                    98,
                    0
                ],
                "title": "Low-Complexity AoI-Optimal Status Update Control with Partial Battery\n  State Information in Energy Harvesting IoT Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Complexity AoI-Optimal Status Update Control with Partial Battery\n  State Information in Energy Harvesting IoT Networks"
                },
                "summary": "For a two-hop IoT system consisting of multiple energy harvesting sensors, a\ncache-enabled edge node, and multiple monitors, the status update control at\nthe edge node, which has partial battery state information (pBSI) of the\nsensors, is formulated as a pBSI problem. The concept of inferred pBSI is\nintroduced to reduce the noiseless single-sensor pBSI problem to a Markov\ndecision process with a moderate state-space size, enabling the optimal policy\nto be obtained through a value iteration algorithm. A lower bound on the\nexpected time-average on-demand age of information performance is established\nfor the general single-sensor status update problem. For the single-sensor pBSI\nproblem, a semi-closed-form policy called the current-next (CN) policy is\nproposed, along with an efficient post-update value iteration algorithm with a\nper-iteration time complexity proportional to the square of the battery\ncapacity. A weighted-update-gain-competition (WUGC) approach is further\nleveraged to extend the CN policy to the multi-sensor case. Numerical results\nin the single-sensor case demonstrate the near-optimal performance of the CN\npolicy across various energy arrival processes. Simulations for an IoT system\nwith $100$ sensors reveal that the WUGC-CN policy outperforms the\nmaximum-age-first policy and the random-scheduling-based CN policy under\nBernoulli energy arrival processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For a two-hop IoT system consisting of multiple energy harvesting sensors, a\ncache-enabled edge node, and multiple monitors, the status update control at\nthe edge node, which has partial battery state information (pBSI) of the\nsensors, is formulated as a pBSI problem. The concept of inferred pBSI is\nintroduced to reduce the noiseless single-sensor pBSI problem to a Markov\ndecision process with a moderate state-space size, enabling the optimal policy\nto be obtained through a value iteration algorithm. A lower bound on the\nexpected time-average on-demand age of information performance is established\nfor the general single-sensor status update problem. For the single-sensor pBSI\nproblem, a semi-closed-form policy called the current-next (CN) policy is\nproposed, along with an efficient post-update value iteration algorithm with a\nper-iteration time complexity proportional to the square of the battery\ncapacity. A weighted-update-gain-competition (WUGC) approach is further\nleveraged to extend the CN policy to the multi-sensor case. Numerical results\nin the single-sensor case demonstrate the near-optimal performance of the CN\npolicy across various energy arrival processes. Simulations for an IoT system\nwith $100$ sensors reveal that the WUGC-CN policy outperforms the\nmaximum-age-first policy and the random-scheduling-based CN policy under\nBernoulli energy arrival processes."
                },
                "authors": [
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Shengtian Yang"
                    },
                    {
                        "name": "Jun Chen"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Anding Wang"
                    }
                ],
                "author_detail": {
                    "name": "Anding Wang"
                },
                "author": "Anding Wang",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05718v1",
                "updated": "2025-04-08T06:38:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    38,
                    27,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T06:38:27Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    38,
                    27,
                    1,
                    98,
                    0
                ],
                "title": "CVA6-VMRT: A Modular Approach Towards Time-Predictable Virtual Memory in\n  a 64-bit Application Class RISC-V Processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CVA6-VMRT: A Modular Approach Towards Time-Predictable Virtual Memory in\n  a 64-bit Application Class RISC-V Processor"
                },
                "summary": "The increasing complexity of autonomous systems has driven a shift to\nintegrated heterogeneous SoCs with real-time and safety demands. Ensuring\ndeterministic WCETs and low-latency for critical tasks requires minimizing\ninterference on shared resources like virtual memory. Existing techniques, such\nas software coloring and memory replication, introduce significant area and\nperformance overhead, especially with virtualized memory where address\ntranslation adds latency uncertainty. To address these limitations, we propose\nCVA6-VMRT, an extension of the open-source RISC-V CVA6 core, adding hardware\nsupport for predictability in virtual memory access with minimal area overhead.\nCVA6-VMRT features dynamically partitioned Translation Look-aside Buffers\n(TLBs) and hybrid L1 cache/scratchpad memory (SPM) functionality. It allows\nfine-grained per-thread control of resources, enabling the operating system to\nmanage TLB replacements, including static overwrites, to ensure single-cycle\naddress translation for critical memory regions. Additionally, CVA6-VMRT\nenables runtime partitioning of data and instruction caches into cache and SPM\nsections, providing low and predictable access times for critical data without\nimpacting other accesses. In a virtualized setting, CVA6-VMRT enhances\nexecution time determinism for critical guests by 94% during interference from\nnon-critical guests, with minimal impact on their average absolute execution\ntime compared to isolated execution of the critical guests only. This\ninterference-aware behaviour is achieved with just a 4% area overhead and no\ntiming penalty compared to the baseline CVA6 core.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of autonomous systems has driven a shift to\nintegrated heterogeneous SoCs with real-time and safety demands. Ensuring\ndeterministic WCETs and low-latency for critical tasks requires minimizing\ninterference on shared resources like virtual memory. Existing techniques, such\nas software coloring and memory replication, introduce significant area and\nperformance overhead, especially with virtualized memory where address\ntranslation adds latency uncertainty. To address these limitations, we propose\nCVA6-VMRT, an extension of the open-source RISC-V CVA6 core, adding hardware\nsupport for predictability in virtual memory access with minimal area overhead.\nCVA6-VMRT features dynamically partitioned Translation Look-aside Buffers\n(TLBs) and hybrid L1 cache/scratchpad memory (SPM) functionality. It allows\nfine-grained per-thread control of resources, enabling the operating system to\nmanage TLB replacements, including static overwrites, to ensure single-cycle\naddress translation for critical memory regions. Additionally, CVA6-VMRT\nenables runtime partitioning of data and instruction caches into cache and SPM\nsections, providing low and predictable access times for critical data without\nimpacting other accesses. In a virtualized setting, CVA6-VMRT enhances\nexecution time determinism for critical guests by 94% during interference from\nnon-critical guests, with minimal impact on their average absolute execution\ntime compared to isolated execution of the critical guests only. This\ninterference-aware behaviour is achieved with just a 4% area overhead and no\ntiming penalty compared to the baseline CVA6 core."
                },
                "authors": [
                    {
                        "name": "Christopher Reinwardt"
                    },
                    {
                        "name": "Robert Balas"
                    },
                    {
                        "name": "Alessandro Ottaviano"
                    },
                    {
                        "name": "Angelo Garofalo"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "8 pages, 7 figures, accepted at the 22nd ACM International Conference\n  on Computing Frontiers 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22926v2",
                "updated": "2025-04-08T05:27:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    5,
                    27,
                    15,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-29T01:06:54Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    1,
                    6,
                    54,
                    5,
                    88,
                    0
                ],
                "title": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction"
                },
                "summary": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware."
                },
                "authors": [
                    {
                        "name": "Zikang Yuan"
                    },
                    {
                        "name": "Ruiye Ming"
                    },
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yonghao Tan"
                    },
                    {
                        "name": "Pingcheng Dong"
                    },
                    {
                        "name": "Hongcheng Luo"
                    },
                    {
                        "name": "Yuzhong Jiao"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Kwang-Ting Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Kwang-Ting Cheng"
                },
                "author": "Kwang-Ting Cheng",
                "arxiv_comment": "10 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03661v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03661v2",
                "updated": "2025-04-08T04:34:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    4,
                    34,
                    44,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-12T13:32:50Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    32,
                    50,
                    2,
                    71,
                    0
                ],
                "title": "MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV\n  Product Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV\n  Product Quantization"
                },
                "summary": "Large language models (LLMs) are increasingly utilized for complex tasks\nrequiring longer context lengths, with some models supporting up to 128K or 1M\ntokens. This trend, however, presents significant challenges in inference speed\nand memory management. Quantization emerges as a promising approach to address\nthe widening gap between LLM size and memory capacity. However, traditional\nquantization schemes often yield suboptimal compression results for KV caches\ndue to two key factors: i) On-the-fly quantization and de-quantization, causing\nsignificant performance overhead; ii) Prevalence of outliers in KV values,\nchallenging low-bitwidth uniform quantization. To this end, we propose MILLION,\na novel quantization framework achieving low-bitwidth KV cache through product\nquantization. First, we conduct a thorough analysis of KV cache distribution,\nrevealing the limitations of existing quantization schemes. Second, we\nintroduce a non-uniform quantization algorithm based on product quantization,\nwhich efficiently compresses data while preserving accuracy. Third, we develop\na high-performance GPU inference framework with efficient attention kernel and\npipeline design for MILLION that leverages sparse computation and asynchronous\nquantization, significantly enhancing inference speed. Comprehensive evaluation\nresults demonstrate that MILLION can achieve 4 bits quantization with trivial\nperplexity and accuracy loss, and achieve 2.09x end-to-end performance gains at\n32K context length. Code is released at https://github.com/ZongwuWang/MILLION.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly utilized for complex tasks\nrequiring longer context lengths, with some models supporting up to 128K or 1M\ntokens. This trend, however, presents significant challenges in inference speed\nand memory management. Quantization emerges as a promising approach to address\nthe widening gap between LLM size and memory capacity. However, traditional\nquantization schemes often yield suboptimal compression results for KV caches\ndue to two key factors: i) On-the-fly quantization and de-quantization, causing\nsignificant performance overhead; ii) Prevalence of outliers in KV values,\nchallenging low-bitwidth uniform quantization. To this end, we propose MILLION,\na novel quantization framework achieving low-bitwidth KV cache through product\nquantization. First, we conduct a thorough analysis of KV cache distribution,\nrevealing the limitations of existing quantization schemes. Second, we\nintroduce a non-uniform quantization algorithm based on product quantization,\nwhich efficiently compresses data while preserving accuracy. Third, we develop\na high-performance GPU inference framework with efficient attention kernel and\npipeline design for MILLION that leverages sparse computation and asynchronous\nquantization, significantly enhancing inference speed. Comprehensive evaluation\nresults demonstrate that MILLION can achieve 4 bits quantization with trivial\nperplexity and accuracy loss, and achieve 2.09x end-to-end performance gains at\n32K context length. Code is released at https://github.com/ZongwuWang/MILLION."
                },
                "authors": [
                    {
                        "name": "Zongwu Wang"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Fangxin Liu"
                    },
                    {
                        "name": "Yiwei Hu"
                    },
                    {
                        "name": "Qingxiao Sun"
                    },
                    {
                        "name": "Gezi Li"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Li Jiang"
                    },
                    {
                        "name": "Haibing Guan"
                    }
                ],
                "author_detail": {
                    "name": "Haibing Guan"
                },
                "author": "Haibing Guan",
                "arxiv_comment": "7 pages, 7 figures and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03661v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03661v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05646v1",
                "updated": "2025-04-08T03:48:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    48,
                    43,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T03:48:43Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    48,
                    43,
                    1,
                    98,
                    0
                ],
                "title": "Lattice: Learning to Efficiently Compress the Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lattice: Learning to Efficiently Compress the Memory"
                },
                "summary": "Attention mechanisms have revolutionized sequence learning but suffer from\nquadratic computational complexity. This paper introduces Lattice, a novel\nrecurrent neural network (RNN) mechanism that leverages the inherent low-rank\nstructure of K-V matrices to efficiently compress the cache into a fixed number\nof memory slots, achieving sub-quadratic complexity. We formulate this\ncompression as an online optimization problem and derive a dynamic memory\nupdate rule based on a single gradient descent step. The resulting recurrence\nfeatures a state- and input-dependent gating mechanism, offering an\ninterpretable memory update process. The core innovation is the orthogonal\nupdate: each memory slot is updated exclusively with information orthogonal to\nits current state hence incorporation of only novel, non-redundant data, which\nminimizes the interference with previously stored information. The experimental\nresults show that Lattice achieves the best perplexity compared to all\nbaselines across diverse context lengths, with performance improvement becoming\nmore pronounced as the context length increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms have revolutionized sequence learning but suffer from\nquadratic computational complexity. This paper introduces Lattice, a novel\nrecurrent neural network (RNN) mechanism that leverages the inherent low-rank\nstructure of K-V matrices to efficiently compress the cache into a fixed number\nof memory slots, achieving sub-quadratic complexity. We formulate this\ncompression as an online optimization problem and derive a dynamic memory\nupdate rule based on a single gradient descent step. The resulting recurrence\nfeatures a state- and input-dependent gating mechanism, offering an\ninterpretable memory update process. The core innovation is the orthogonal\nupdate: each memory slot is updated exclusively with information orthogonal to\nits current state hence incorporation of only novel, non-redundant data, which\nminimizes the interference with previously stored information. The experimental\nresults show that Lattice achieves the best perplexity compared to all\nbaselines across diverse context lengths, with performance improvement becoming\nmore pronounced as the context length increases."
                },
                "authors": [
                    {
                        "name": "Mahdi Karami"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    }
                ],
                "author_detail": {
                    "name": "Vahab Mirrokni"
                },
                "author": "Vahab Mirrokni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02533v3",
                "updated": "2025-04-07T22:48:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    22,
                    48,
                    33,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-03T12:36:01Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    12,
                    36,
                    1,
                    3,
                    93,
                    0
                ],
                "title": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions"
                },
                "summary": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based Instruction Set Architecture\nextensibility. Our implementation shows $30\\times$ to $84\\times$ performance\nimprovement when operating on 8-bit data over the same system with a\ntraditional cache when executing a worst-case 32-bit CNN workload, with only\n$41.3\\%$ area overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based Instruction Set Architecture\nextensibility. Our implementation shows $30\\times$ to $84\\times$ performance\nimprovement when operating on 8-bit data over the same system with a\ntraditional cache when executing a worst-case 32-bit CNN workload, with only\n$41.3\\%$ area overhead."
                },
                "authors": [
                    {
                        "name": "Vincenzo Petrolo"
                    },
                    {
                        "name": "Flavia Guella"
                    },
                    {
                        "name": "Michele Caon"
                    },
                    {
                        "name": "Pasquale Davide Schiavone"
                    },
                    {
                        "name": "Guido Masera"
                    },
                    {
                        "name": "Maurizio Martina"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Martina"
                },
                "author": "Maurizio Martina",
                "arxiv_comment": "6 pages, 4 figures, accepted at the Design Automation Conference\n  (DAC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07467v2",
                "updated": "2025-04-07T20:52:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    20,
                    52,
                    4,
                    0,
                    97,
                    0
                ],
                "published": "2024-06-11T17:13:18Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    13,
                    18,
                    1,
                    163,
                    0
                ],
                "title": "LLM meets ML: Data-efficient Anomaly Detection on Unseen Unstable Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM meets ML: Data-efficient Anomaly Detection on Unseen Unstable Logs"
                },
                "summary": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for ULAD, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\nin F1 score while using 62.87 percentage points less labeled data. When trained\non the same amount of data as the baselines, FlexLog achieves up to a 13\npercentage points increase in F1 score on ADFA-U across varying training\ndataset sizes. Additionally, FlexLog maintains inference time under one second\nper log sequence, making it suitable for most applications except\nlatency-sensitive systems. Further analysis reveals the positive impact of\nFlexLog's key components: cache, RAG and ensemble learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for ULAD, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\nin F1 score while using 62.87 percentage points less labeled data. When trained\non the same amount of data as the baselines, FlexLog achieves up to a 13\npercentage points increase in F1 score on ADFA-U across varying training\ndataset sizes. Additionally, FlexLog maintains inference time under one second\nper log sequence, making it suitable for most applications except\nlatency-sensitive systems. Further analysis reveals the positive impact of\nFlexLog's key components: cache, RAG and ensemble learning."
                },
                "authors": [
                    {
                        "name": "Fatemeh Hadadi"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Domenico Bianculli"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05097v1",
                "updated": "2025-04-07T14:04:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    4,
                    30,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T14:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    4,
                    30,
                    0,
                    97,
                    0
                ],
                "title": "State Tuning: State-based Test-Time Scaling on RWKV-7",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Tuning: State-based Test-Time Scaling on RWKV-7"
                },
                "summary": "Test-time scaling has emerged as a prominent research direction in machine\nlearning, enabling models to enhance their expressive capabilities during\ninference.Transformers, renowned for striking a delicate balance between\nefficiency and expressiveness, have benefited from test-time scaling techniques\nthat leverage an expanding key-value (KV) cache to significantly improve\nperformance.In this paper, we introduce a novel state-based approach to\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\nstate-of-the-art performance on the target task without altering the model's\npre-trained weights. Our approach centers on three key innovations. First, we\ndevelop an observer framework that allows a smaller model to replicate and\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\nto dynamically upscale the state size, enhancing the model's capacity to\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\nmodel can outperform larger models on the given task. This method preserves the\nefficiency of the original RWKV-7 architecture while harnessing the power of\ntest-time scaling to deliver superior results. Our findings underscore the\npotential of state tuning as an effective strategy for advancing model\nperformance in resource-constrained settings. Our code is\nhttps://github.com/TorchRWKV/flash-linear-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling has emerged as a prominent research direction in machine\nlearning, enabling models to enhance their expressive capabilities during\ninference.Transformers, renowned for striking a delicate balance between\nefficiency and expressiveness, have benefited from test-time scaling techniques\nthat leverage an expanding key-value (KV) cache to significantly improve\nperformance.In this paper, we introduce a novel state-based approach to\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\nstate-of-the-art performance on the target task without altering the model's\npre-trained weights. Our approach centers on three key innovations. First, we\ndevelop an observer framework that allows a smaller model to replicate and\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\nto dynamically upscale the state size, enhancing the model's capacity to\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\nmodel can outperform larger models on the given task. This method preserves the\nefficiency of the original RWKV-7 architecture while harnessing the power of\ntest-time scaling to deliver superior results. Our findings underscore the\npotential of state tuning as an effective strategy for advancing model\nperformance in resource-constrained settings. Our code is\nhttps://github.com/TorchRWKV/flash-linear-attention."
                },
                "authors": [
                    {
                        "name": "Liu Xiao"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Lin Yueyu"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yueyu"
                },
                "author": "Lin Yueyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04823v1",
                "updated": "2025-04-07T08:22:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T08:22:45Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models"
                },
                "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models."
                },
                "authors": [
                    {
                        "name": "Ruikang Liu"
                    },
                    {
                        "name": "Yuxuan Sun"
                    },
                    {
                        "name": "Manyi Zhang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Tiezheng Yu"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Lu Hou"
                    }
                ],
                "author_detail": {
                    "name": "Lu Hou"
                },
                "author": "Lu Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00414v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00414v3",
                "updated": "2025-04-07T06:27:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    6,
                    27,
                    48,
                    0,
                    97,
                    0
                ],
                "published": "2024-10-01T05:46:22Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    5,
                    46,
                    22,
                    1,
                    275,
                    0
                ],
                "title": "Semantic Parsing with Candidate Expressions for Knowledge Base Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Parsing with Candidate Expressions for Knowledge Base Question\n  Answering"
                },
                "summary": "Semantic parsers convert natural language to logical forms, which can be\nevaluated on knowledge bases (KBs) to produce denotations. Recent semantic\nparsers have been developed with sequence-to-sequence (seq2seq) pre-trained\nlanguage models (PLMs) or large language models, where the models treat logical\nforms as sequences of tokens. For syntactic and semantic validity, the semantic\nparsers use grammars that enable constrained decoding. However, the grammars\nlack the ability to utilize large information of KBs, although logical forms\ncontain representations of KB elements, such as entities or relations. In this\nwork, we propose a grammar augmented with candidate expressions for semantic\nparsing on a large KB with a seq2seq PLM. The grammar defines actions as\nproduction rules, and our semantic parser predicts actions during inference\nunder the constraints by types and candidate expressions. We apply the grammar\nto knowledge base question answering, where the constraints by candidate\nexpressions assist a semantic parser to generate valid KB elements. We also\nintroduce two special rules, sub-type inference and union types, and a mask\ncaching algorithm. In particular, sub-type inference and the mask caching\nalgorithm greatly increase the decoding speed of our semantic parser. We\nexperimented on two benchmarks, KQA Pro and Overnight, where the constraints by\ncandidate expressions increased the accuracy of our semantic parser, whether it\nwas trained with strong supervision or weak supervision. In addition, our\nsemantic parser had a fast decoding speed in the experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic parsers convert natural language to logical forms, which can be\nevaluated on knowledge bases (KBs) to produce denotations. Recent semantic\nparsers have been developed with sequence-to-sequence (seq2seq) pre-trained\nlanguage models (PLMs) or large language models, where the models treat logical\nforms as sequences of tokens. For syntactic and semantic validity, the semantic\nparsers use grammars that enable constrained decoding. However, the grammars\nlack the ability to utilize large information of KBs, although logical forms\ncontain representations of KB elements, such as entities or relations. In this\nwork, we propose a grammar augmented with candidate expressions for semantic\nparsing on a large KB with a seq2seq PLM. The grammar defines actions as\nproduction rules, and our semantic parser predicts actions during inference\nunder the constraints by types and candidate expressions. We apply the grammar\nto knowledge base question answering, where the constraints by candidate\nexpressions assist a semantic parser to generate valid KB elements. We also\nintroduce two special rules, sub-type inference and union types, and a mask\ncaching algorithm. In particular, sub-type inference and the mask caching\nalgorithm greatly increase the decoding speed of our semantic parser. We\nexperimented on two benchmarks, KQA Pro and Overnight, where the constraints by\ncandidate expressions increased the accuracy of our semantic parser, whether it\nwas trained with strong supervision or weak supervision. In addition, our\nsemantic parser had a fast decoding speed in the experiments."
                },
                "authors": [
                    {
                        "name": "Daehwan Nam"
                    },
                    {
                        "name": "Gary Geunbae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gary Geunbae Lee"
                },
                "author": "Gary Geunbae Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00414v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00414v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04704v1",
                "updated": "2025-04-07T03:22:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T03:22:15Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important"
                },
                "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "JiaMing Zhang"
                    },
                    {
                        "name": "Xiong Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23367v2",
                "updated": "2025-04-07T01:35:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    1,
                    35,
                    39,
                    0,
                    97,
                    0
                ],
                "published": "2025-03-30T08:51:19Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning"
                },
                "summary": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR."
                },
                "authors": [
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Jiangshan Wang"
                    },
                    {
                        "name": "Tao Dai"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10714v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10714v2",
                "updated": "2025-04-06T12:20:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    6,
                    12,
                    20,
                    25,
                    6,
                    96,
                    0
                ],
                "published": "2025-03-13T03:36:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs"
                },
                "summary": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10714v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10714v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04005v1",
                "updated": "2025-04-05T00:59:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    5,
                    0,
                    59,
                    52,
                    5,
                    95,
                    0
                ],
                "published": "2025-04-05T00:59:52Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    0,
                    59,
                    52,
                    5,
                    95,
                    0
                ],
                "title": "Learning Cache Coherence Traffic for NoC Routing Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Cache Coherence Traffic for NoC Routing Design"
                },
                "summary": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design."
                },
                "authors": [
                    {
                        "name": "Guochu Xiong"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Weichen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weichen Liu"
                },
                "author": "Weichen Liu",
                "arxiv_comment": "7 pages, 14 figures. Preprint version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03632v1",
                "updated": "2025-04-04T17:56:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    56,
                    44,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T17:56:44Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    56,
                    44,
                    4,
                    94,
                    0
                ],
                "title": "Performance Analysis of HPC applications on the Aurora Supercomputer:\n  Exploring the Impact of HBM-Enabled Intel Xeon Max CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Analysis of HPC applications on the Aurora Supercomputer:\n  Exploring the Impact of HBM-Enabled Intel Xeon Max CPUs"
                },
                "summary": "The Aurora supercomputer is an exascale-class system designed to tackle some\nof the most demanding computational workloads. Equipped with both High\nBandwidth Memory (HBM) and DDR memory, it provides unique trade-offs in\nperformance, latency, and capacity. This paper presents a comprehensive\nanalysis of the memory systems on the Aurora supercomputer, with a focus on\nevaluating the trade-offs between HBM and DDR memory systems. We explore how\ndifferent memory configurations, including memory modes (Flat and Cache) and\nclustering modes (Quad and SNC4), influence key system performance metrics such\nas memory bandwidth, latency, CPU-GPU PCIe bandwidth, and MPI communication\nbandwidth. Additionally, we examine the performance of three representative HPC\napplications -- HACC, QMCPACK, and BFS -- each illustrating the impact of\nmemory configurations on performance. By using microbenchmarks and\napplication-level analysis, we provide insights into how to select the optimal\nmemory system and configuration to maximize performance based on the\napplication characteristics. The findings presented in this paper offer\nguidance for users of the Aurora system and similar exascale systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Aurora supercomputer is an exascale-class system designed to tackle some\nof the most demanding computational workloads. Equipped with both High\nBandwidth Memory (HBM) and DDR memory, it provides unique trade-offs in\nperformance, latency, and capacity. This paper presents a comprehensive\nanalysis of the memory systems on the Aurora supercomputer, with a focus on\nevaluating the trade-offs between HBM and DDR memory systems. We explore how\ndifferent memory configurations, including memory modes (Flat and Cache) and\nclustering modes (Quad and SNC4), influence key system performance metrics such\nas memory bandwidth, latency, CPU-GPU PCIe bandwidth, and MPI communication\nbandwidth. Additionally, we examine the performance of three representative HPC\napplications -- HACC, QMCPACK, and BFS -- each illustrating the impact of\nmemory configurations on performance. By using microbenchmarks and\napplication-level analysis, we provide insights into how to select the optimal\nmemory system and configuration to maximize performance based on the\napplication characteristics. The findings presented in this paper offer\nguidance for users of the Aurora system and similar exascale systems."
                },
                "authors": [
                    {
                        "name": "Huda Ibeid"
                    },
                    {
                        "name": "Vikram Narayana"
                    },
                    {
                        "name": "Jeongnim Kim"
                    },
                    {
                        "name": "Anthony Nguyen"
                    },
                    {
                        "name": "Vitali Morozov"
                    },
                    {
                        "name": "Ye Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ye Luo"
                },
                "author": "Ye Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v2",
                "updated": "2025-04-04T16:51:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    51,
                    15,
                    4,
                    94,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "Adaptive Semantic Prompt Caching with VectorQ",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Semantic Prompt Caching with VectorQ"
                },
                "summary": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different embeddings.\nWe propose VectorQ, an online framework with a threshold convergence guarantee\nto learn embedding-specific threshold regions that adapt to the uncertainty of\nan embedding. Through evaluations on a combination of three diverse datasets,\nwe show that VectorQ consistently outperforms state-of-the-art systems across\nall static thresholds, achieving up to 26x increases in cache hit rate and\nerror rate reductions up to 74%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different embeddings.\nWe propose VectorQ, an online framework with a threshold convergence guarantee\nto learn embedding-specific threshold regions that adapt to the uncertainty of\nan embedding. Through evaluations on a combination of three diverse datasets,\nwe show that VectorQ consistently outperforms state-of-the-art systems across\nall static thresholds, achieving up to 26x increases in cache hit rate and\nerror rate reductions up to 74%."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.02073v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.02073v2",
                "updated": "2025-04-04T15:30:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    30,
                    20,
                    4,
                    94,
                    0
                ],
                "published": "2023-07-05T07:30:53Z",
                "published_parsed": [
                    2023,
                    7,
                    5,
                    7,
                    30,
                    53,
                    2,
                    186,
                    0
                ],
                "title": "Performance Modeling of Data Storage Systems using Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Modeling of Data Storage Systems using Generative Models"
                },
                "summary": "High-precision modeling of systems is one of the main areas of industrial\ndata analysis. Models of systems, their digital twins, are used to predict\ntheir behavior under various conditions. We have developed several models of a\nstorage system using machine learning-based generative models. The system\nconsists of several components: hard disk drive (HDD) and solid-state drive\n(SSD) storage pools with different RAID schemes and cache. Each storage\ncomponent is represented by a probabilistic model that describes the\nprobability distribution of the component performance in terms of IOPS and\nlatency, depending on their configuration and external data load parameters.\nThe results of the experiments demonstrate the errors of 4-10 % for IOPS and\n3-16 % for latency predictions depending on the components and models of the\nsystem. The predictions show up to 0.99 Pearson correlation with Little's law,\nwhich can be used for unsupervised reliability checks of the models. In\naddition, we present novel data sets that can be used for benchmarking\nregression algorithms, conditional generative models, and uncertainty\nestimation methods in machine learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-precision modeling of systems is one of the main areas of industrial\ndata analysis. Models of systems, their digital twins, are used to predict\ntheir behavior under various conditions. We have developed several models of a\nstorage system using machine learning-based generative models. The system\nconsists of several components: hard disk drive (HDD) and solid-state drive\n(SSD) storage pools with different RAID schemes and cache. Each storage\ncomponent is represented by a probabilistic model that describes the\nprobability distribution of the component performance in terms of IOPS and\nlatency, depending on their configuration and external data load parameters.\nThe results of the experiments demonstrate the errors of 4-10 % for IOPS and\n3-16 % for latency predictions depending on the components and models of the\nsystem. The predictions show up to 0.99 Pearson correlation with Little's law,\nwhich can be used for unsupervised reliability checks of the models. In\naddition, we present novel data sets that can be used for benchmarking\nregression algorithms, conditional generative models, and uncertainty\nestimation methods in machine learning."
                },
                "authors": [
                    {
                        "name": "Abdalaziz Rashid Al-Maeeni"
                    },
                    {
                        "name": "Aziz Temirkhanov"
                    },
                    {
                        "name": "Artem Ryzhikov"
                    },
                    {
                        "name": "Mikhail Hushchyn"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Hushchyn"
                },
                "author": "Mikhail Hushchyn",
                "arxiv_doi": "10.1109/ACCESS.2025.3552409",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2025.3552409",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.02073v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.02073v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Access 2025 ( Volume: 13) 49643 - 49658",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03499v1",
                "updated": "2025-04-04T14:55:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    55,
                    27,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T14:55:27Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    55,
                    27,
                    4,
                    94,
                    0
                ],
                "title": "Optimistic Learning for Communication Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimistic Learning for Communication Networks"
                },
                "summary": "AI/ML-based tools are at the forefront of resource management solutions for\ncommunication networks. Deep learning, in particular, is highly effective in\nfacilitating fast and high-performing decision-making whenever representative\ntraining data is available to build offline accurate models. Conversely, online\nlearning solutions do not require training and enable adaptive decisions based\non runtime observations, alas are often overly conservative. This extensive\ntutorial proposes the use of optimistic learning (OpL) as a decision engine for\nresource management frameworks in modern communication systems. When properly\ndesigned, such solutions can achieve fast and high-performing decisions --\ncomparable to offline-trained models -- while preserving the robustness and\nperformance guarantees of the respective online learning approaches. We\nintroduce the fundamental concepts, algorithms and results of OpL, discuss the\nroots of this theory and present different approaches to defining and achieving\noptimism. We proceed to showcase how OpL can enhance resource management in\ncommunication networks for several key problems such as caching, edge\ncomputing, network slicing, and workload assignment in decentralized O-RAN\nplatforms. Finally, we discuss the open challenges that must be addressed to\nunlock the full potential of this new resource management approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI/ML-based tools are at the forefront of resource management solutions for\ncommunication networks. Deep learning, in particular, is highly effective in\nfacilitating fast and high-performing decision-making whenever representative\ntraining data is available to build offline accurate models. Conversely, online\nlearning solutions do not require training and enable adaptive decisions based\non runtime observations, alas are often overly conservative. This extensive\ntutorial proposes the use of optimistic learning (OpL) as a decision engine for\nresource management frameworks in modern communication systems. When properly\ndesigned, such solutions can achieve fast and high-performing decisions --\ncomparable to offline-trained models -- while preserving the robustness and\nperformance guarantees of the respective online learning approaches. We\nintroduce the fundamental concepts, algorithms and results of OpL, discuss the\nroots of this theory and present different approaches to defining and achieving\noptimism. We proceed to showcase how OpL can enhance resource management in\ncommunication networks for several key problems such as caching, edge\ncomputing, network slicing, and workload assignment in decentralized O-RAN\nplatforms. Finally, we discuss the open challenges that must be addressed to\nunlock the full potential of this new resource management approach."
                },
                "authors": [
                    {
                        "name": "George Iosifidis"
                    },
                    {
                        "name": "Naram Mhaisen"
                    },
                    {
                        "name": "Douglas J. Leith"
                    }
                ],
                "author_detail": {
                    "name": "Douglas J. Leith"
                },
                "author": "Douglas J. Leith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10153v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10153v3",
                "updated": "2025-04-04T13:27:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    27,
                    49,
                    4,
                    94,
                    0
                ],
                "published": "2024-12-13T14:11:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector"
                },
                "summary": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Shuzhao Xie"
                    },
                    {
                        "name": "Chengwei Ren"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shijia Ge"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10153v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10153v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03140v1",
                "updated": "2025-04-04T03:30:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T03:30:15Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "title": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models"
                },
                "summary": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation."
                },
                "authors": [
                    {
                        "name": "Xuran Ma"
                    },
                    {
                        "name": "Yexin Liu"
                    },
                    {
                        "name": "Yaofu Liu"
                    },
                    {
                        "name": "Xianfeng Wu"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Ser-Nam Lim"
                    },
                    {
                        "name": "Harry Yang"
                    }
                ],
                "author_detail": {
                    "name": "Harry Yang"
                },
                "author": "Harry Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16444v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16444v3",
                "updated": "2025-04-03T22:49:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    22,
                    49,
                    22,
                    3,
                    93,
                    0
                ],
                "published": "2024-05-26T06:00:17Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    6,
                    0,
                    17,
                    6,
                    147,
                    0
                ],
                "title": "CacheBlend: Fast Large Language Model Serving for RAG with Cached\n  Knowledge Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheBlend: Fast Large Language Model Serving for RAG with Cached\n  Knowledge Fusion"
                },
                "summary": "Large language models (LLMs) often incorporate multiple text chunks in their\ninputs to provide the necessary contexts. To speed up the prefill of the long\nLLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input. However, the\nreused text chunks are not always the input prefix, which makes precomputed KV\ncaches not directly usable since they ignore the text's cross-attention with\nthe preceding texts. Thus, the benefits of reusing KV caches remain largely\nunrealized.\n  This paper tackles just one challenge: when an LLM input contains multiple\ntext chunks, how to quickly combine their precomputed KV caches in order to\nachieve the same generation quality as the expensive full prefill (i.e.,\nwithout reusing KV cache)? This challenge naturally arises in\nretrieval-augmented generation (RAG) where the input is supplemented with\nmultiple retrieved texts as the context. We present CacheBlend, a scheme that\nreuses the precomputed KV caches, regardless prefix or not, and selectively\nrecomputes the KV values of a small subset of tokens to partially update each\nreused KV cache. In the meantime, the small extra delay for recomputing some\ntokens can be pipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices with more storage\ncapacity while retrieving them without increasing the inference delay. By\ncomparing CacheBlend with the state-of-the-art KV cache reusing schemes on\nthree open-source LLMs of various sizes and four popular benchmark datasets of\ndifferent tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by\n2.2-3.3x and increases the inference throughput by 2.8-5x from full KV\nrecompute without compromising generation quality. The code is available at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often incorporate multiple text chunks in their\ninputs to provide the necessary contexts. To speed up the prefill of the long\nLLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input. However, the\nreused text chunks are not always the input prefix, which makes precomputed KV\ncaches not directly usable since they ignore the text's cross-attention with\nthe preceding texts. Thus, the benefits of reusing KV caches remain largely\nunrealized.\n  This paper tackles just one challenge: when an LLM input contains multiple\ntext chunks, how to quickly combine their precomputed KV caches in order to\nachieve the same generation quality as the expensive full prefill (i.e.,\nwithout reusing KV cache)? This challenge naturally arises in\nretrieval-augmented generation (RAG) where the input is supplemented with\nmultiple retrieved texts as the context. We present CacheBlend, a scheme that\nreuses the precomputed KV caches, regardless prefix or not, and selectively\nrecomputes the KV values of a small subset of tokens to partially update each\nreused KV cache. In the meantime, the small extra delay for recomputing some\ntokens can be pipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices with more storage\ncapacity while retrieving them without increasing the inference delay. By\ncomparing CacheBlend with the state-of-the-art KV cache reusing schemes on\nthree open-source LLMs of various sizes and four popular benchmark datasets of\ndifferent tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by\n2.2-3.3x and increases the inference throughput by 2.8-5x from full KV\nrecompute without compromising generation quality. The code is available at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16444v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16444v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03048v1",
                "updated": "2025-04-03T21:53:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    53,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T21:53:51Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    53,
                    51,
                    3,
                    93,
                    0
                ],
                "title": "LLM Library Learning Fails: A LEGO-Prover Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Library Learning Fails: A LEGO-Prover Case Study"
                },
                "summary": "Recent advancements in the coding, reasoning, and tool-using abilities of\nLLMs have spurred interest in library learning (i.e., online learning through\nthe creation, storage, and retrieval of reusable and composable functions,\nknowledge, checklists, or lemmas). Such systems often promise improved task\nperformance through the automatic creation of broadly applicable tools, as well\nas superior computational performance through the caching of reasoning (i.e.,\nthe storage of generated tools). However, we find strong reason to be\nskeptical. We perform a deep dive into one such system, LEGO-Prover, which\npurports to learn reusable lemmas for mathematical reasoning. We find no\nevidence of the direct reuse of learned lemmas, and find evidence against the\nsoft reuse of learned lemmas (i.e., reuse by modifying relevant examples).\nCrucially, we find that LEGO-Prover does not in fact improve over the simple\nbaseline of prompting the model - the improvements in task accuracy vanish once\ncomputational cost is accounted for. Our findings suggest that serious\nmisconceptions exist as to the effectiveness of these techniques, that a\nserious re-examination of the state of LLM-based library learning is required,\nand that we require much stronger standards for evaluation including\nbehavioural analysis and ensuring that an equal computational budget is used\nfor baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in the coding, reasoning, and tool-using abilities of\nLLMs have spurred interest in library learning (i.e., online learning through\nthe creation, storage, and retrieval of reusable and composable functions,\nknowledge, checklists, or lemmas). Such systems often promise improved task\nperformance through the automatic creation of broadly applicable tools, as well\nas superior computational performance through the caching of reasoning (i.e.,\nthe storage of generated tools). However, we find strong reason to be\nskeptical. We perform a deep dive into one such system, LEGO-Prover, which\npurports to learn reusable lemmas for mathematical reasoning. We find no\nevidence of the direct reuse of learned lemmas, and find evidence against the\nsoft reuse of learned lemmas (i.e., reuse by modifying relevant examples).\nCrucially, we find that LEGO-Prover does not in fact improve over the simple\nbaseline of prompting the model - the improvements in task accuracy vanish once\ncomputational cost is accounted for. Our findings suggest that serious\nmisconceptions exist as to the effectiveness of these techniques, that a\nserious re-examination of the state of LLM-based library learning is required,\nand that we require much stronger standards for evaluation including\nbehavioural analysis and ensuring that an equal computational budget is used\nfor baselines."
                },
                "authors": [
                    {
                        "name": "Ian Berlot-Attwell"
                    },
                    {
                        "name": "Frank Rudzicz"
                    },
                    {
                        "name": "Xujie Si"
                    }
                ],
                "author_detail": {
                    "name": "Xujie Si"
                },
                "author": "Xujie Si",
                "arxiv_comment": "24 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02976v1",
                "updated": "2025-04-03T18:54:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    54,
                    50,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T18:54:50Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    54,
                    50,
                    3,
                    93,
                    0
                ],
                "title": "Localized Definitions and Distributed Reasoning: A Proof-of-Concept\n  Mechanistic Interpretability Study via Activation Patching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localized Definitions and Distributed Reasoning: A Proof-of-Concept\n  Mechanistic Interpretability Study via Activation Patching"
                },
                "summary": "This study investigates the localization of knowledge representation in\nfine-tuned GPT-2 models using Causal Layer Attribution via Activation Patching\n(CLAP), a method that identifies critical neural layers responsible for correct\nanswer generation. The model was fine-tuned on 9,958 PubMed abstracts\n(epilepsy: 20,595 mentions, EEG: 11,674 mentions, seizure: 13,921 mentions)\nusing two configurations with validation loss monitoring for early stopping.\nCLAP involved (1) caching clean (correct answer) and corrupted (incorrect\nanswer) activations, (2) computing logit difference to quantify model\npreference, and (3) patching corrupted activations with clean ones to assess\nrecovery. Results revealed three findings: First, patching the first\nfeedforward layer recovered 56% of correct preference, demonstrating that\nassociative knowledge is distributed across multiple layers. Second, patching\nthe final output layer completely restored accuracy (100% recovery), indicating\nthat definitional knowledge is localised. The stronger clean logit difference\nfor definitional questions further supports this localized representation.\nThird, minimal recovery from convolutional layer patching (13.6%) suggests\nlow-level features contribute marginally to high-level reasoning. Statistical\nanalysis confirmed significant layer-specific effects (p<0.01). These findings\ndemonstrate that factual knowledge is more localized and associative knowledge\ndepends on distributed representations. We also showed that editing efficacy\ndepends on task type. Our findings not only reconcile conflicting observations\nabout localization in model editing but also emphasize on using task-adaptive\ntechniques for reliable, interpretable updates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the localization of knowledge representation in\nfine-tuned GPT-2 models using Causal Layer Attribution via Activation Patching\n(CLAP), a method that identifies critical neural layers responsible for correct\nanswer generation. The model was fine-tuned on 9,958 PubMed abstracts\n(epilepsy: 20,595 mentions, EEG: 11,674 mentions, seizure: 13,921 mentions)\nusing two configurations with validation loss monitoring for early stopping.\nCLAP involved (1) caching clean (correct answer) and corrupted (incorrect\nanswer) activations, (2) computing logit difference to quantify model\npreference, and (3) patching corrupted activations with clean ones to assess\nrecovery. Results revealed three findings: First, patching the first\nfeedforward layer recovered 56% of correct preference, demonstrating that\nassociative knowledge is distributed across multiple layers. Second, patching\nthe final output layer completely restored accuracy (100% recovery), indicating\nthat definitional knowledge is localised. The stronger clean logit difference\nfor definitional questions further supports this localized representation.\nThird, minimal recovery from convolutional layer patching (13.6%) suggests\nlow-level features contribute marginally to high-level reasoning. Statistical\nanalysis confirmed significant layer-specific effects (p<0.01). These findings\ndemonstrate that factual knowledge is more localized and associative knowledge\ndepends on distributed representations. We also showed that editing efficacy\ndepends on task type. Our findings not only reconcile conflicting observations\nabout localization in model editing but also emphasize on using task-adaptive\ntechniques for reliable, interpretable updates."
                },
                "authors": [
                    {
                        "name": "Nooshin Bahador"
                    }
                ],
                "author_detail": {
                    "name": "Nooshin Bahador"
                },
                "author": "Nooshin Bahador",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02972v1",
                "updated": "2025-04-03T18:47:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    47,
                    26,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T18:47:26Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    47,
                    26,
                    3,
                    93,
                    0
                ],
                "title": "Improved Compact Genetic Algorithms with Efficient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Compact Genetic Algorithms with Efficient Caching"
                },
                "summary": "Compact Genetic Algorithms (cGAs) are condensed variants of classical Genetic\nAlgorithms (GAs) that use a probability vector representation of the population\ninstead of the complete population. cGAs have been shown to significantly\nreduce the number of function evaluations required while producing outcomes\nsimilar to those of classical GAs. However, cGAs have a tendency to repeatedly\ngenerate the same chromosomes as they approach convergence, resulting in\nunnecessary evaluations of identical chromosomes. This article introduces the\nconcept of caching in cGAs as a means of avoiding redundant evaluations of the\nsame chromosomes. Our proposed approach operates equivalently to cGAs, but\nenhances the algorithm's time efficiency by reducing the number of function\nevaluations. We also present a data structure for efficient cache maintenance\nto ensure low overhead. The proposed caching approach has an asymptotically\nconstant time complexity on average. The proposed method further generalizes\nthe caching mechanism with higher selection pressure for elitism-based cGAs. We\nconduct a rigorous analysis based on experiments on benchmark optimization\nproblems using two well-known cache replacement strategies. The results\ndemonstrate that caching significantly reduces the number of function\nevaluations required while maintaining the same level of performance accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compact Genetic Algorithms (cGAs) are condensed variants of classical Genetic\nAlgorithms (GAs) that use a probability vector representation of the population\ninstead of the complete population. cGAs have been shown to significantly\nreduce the number of function evaluations required while producing outcomes\nsimilar to those of classical GAs. However, cGAs have a tendency to repeatedly\ngenerate the same chromosomes as they approach convergence, resulting in\nunnecessary evaluations of identical chromosomes. This article introduces the\nconcept of caching in cGAs as a means of avoiding redundant evaluations of the\nsame chromosomes. Our proposed approach operates equivalently to cGAs, but\nenhances the algorithm's time efficiency by reducing the number of function\nevaluations. We also present a data structure for efficient cache maintenance\nto ensure low overhead. The proposed caching approach has an asymptotically\nconstant time complexity on average. The proposed method further generalizes\nthe caching mechanism with higher selection pressure for elitism-based cGAs. We\nconduct a rigorous analysis based on experiments on benchmark optimization\nproblems using two well-known cache replacement strategies. The results\ndemonstrate that caching significantly reduces the number of function\nevaluations required while maintaining the same level of performance accuracy."
                },
                "authors": [
                    {
                        "name": "Prasanta Dutta"
                    },
                    {
                        "name": "Anirban Mukhopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Anirban Mukhopadhyay"
                },
                "author": "Anirban Mukhopadhyay",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02921v1",
                "updated": "2025-04-03T17:08:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    8,
                    42,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T17:08:42Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    8,
                    42,
                    3,
                    93,
                    0
                ],
                "title": "HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented\n  Generation with Reranker KV-Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented\n  Generation with Reranker KV-Cache Reuse"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing the performance of large language models (LLMs) by integrating\nexternal knowledge into the generation process. A key component of RAG\npipelines is the reranker, which selects the most relevant documents from a\npool of retrieved candidates and significantly improves the quality of the\ngenerated responses. While rerankers refine the selection of retrieved\ndocuments in RAG pipelines, they introduce computational challenges that hinder\nhigh throughput and low latency. To address this problem, we propose HyperRAG,\na system that optimizes the trade-off between quality and efficiency in RAG\npipelines by leveraging KV-cache reuse for efficient reranker inference. By\nreusing document-side KV-cache, HyperRAG achieves both high-quality generation\nand system-level efficiency. To fully realize the benefits of KV-cache reuse,\nHyperRAG incorporates a range of system-level optimizations designed to enhance\nefficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3\nthroughput improvement with decoder-only rerankers while also delivering higher\ndownstream performance compared with traditional RAG service.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing the performance of large language models (LLMs) by integrating\nexternal knowledge into the generation process. A key component of RAG\npipelines is the reranker, which selects the most relevant documents from a\npool of retrieved candidates and significantly improves the quality of the\ngenerated responses. While rerankers refine the selection of retrieved\ndocuments in RAG pipelines, they introduce computational challenges that hinder\nhigh throughput and low latency. To address this problem, we propose HyperRAG,\na system that optimizes the trade-off between quality and efficiency in RAG\npipelines by leveraging KV-cache reuse for efficient reranker inference. By\nreusing document-side KV-cache, HyperRAG achieves both high-quality generation\nand system-level efficiency. To fully realize the benefits of KV-cache reuse,\nHyperRAG incorporates a range of system-level optimizations designed to enhance\nefficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3\nthroughput improvement with decoder-only rerankers while also delivering higher\ndownstream performance compared with traditional RAG service."
                },
                "authors": [
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Seo Jin Park"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01380v2",
                "updated": "2025-04-03T13:28:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    28,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2024-12-02T11:07:51Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "title": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking"
                },
                "summary": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which results in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46\\% reduction in memory and 40\\% increase in throughput with $<$\n0.1 loss in perplexity when compared to streaming the dense model from Flash.\nThe open source code for HW simulator, methods, and experiments in this paper\nis available at https://github.com/Qualcomm-AI-research/dynamic-sparsity .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which results in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46\\% reduction in memory and 40\\% increase in throughput with $<$\n0.1 loss in perplexity when compared to streaming the dense model from Flash.\nThe open source code for HW simulator, methods, and experiments in this paper\nis available at https://github.com/Qualcomm-AI-research/dynamic-sparsity ."
                },
                "authors": [
                    {
                        "name": "Marco Federici"
                    },
                    {
                        "name": "Davide Belli"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Amir Jalalirad"
                    },
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Bence Major"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    }
                ],
                "author_detail": {
                    "name": "Paul Whatmough"
                },
                "author": "Paul Whatmough",
                "arxiv_comment": "Main Text: 10 pages, 11 figures. Appendix: 6 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02441v1",
                "updated": "2025-04-03T09:58:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Cognitive Memory in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Memory in Large Language Models"
                },
                "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."
                },
                "authors": [
                    {
                        "name": "Lianlei Shan"
                    },
                    {
                        "name": "Shixian Luo"
                    },
                    {
                        "name": "Zezhou Zhu"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Yong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wu"
                },
                "author": "Yong Wu",
                "arxiv_comment": "37 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03775v1",
                "updated": "2025-04-03T08:58:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    58,
                    5,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T08:58:05Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    58,
                    5,
                    3,
                    93,
                    0
                ],
                "title": "FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache\n  Transfer and Load-Aware Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache\n  Transfer and Load-Aware Scheduling"
                },
                "summary": "Disaggregated inference has become an essential framework that separates the\nprefill (P) and decode (D) stages in large language model inference to improve\nthroughput. However, the KV cache transfer faces significant delays between\nprefill and decode nodes. The block-wise calling method and discontinuous KV\ncache memory allocation increase the number of calls to the transmission\nkernel. Additionally, existing frameworks often fix the roles of P and D nodes,\nleading to computational imbalances. In this paper, we propose FlowKV, a novel\ndisaggregated inference framework, which reduces the average transmission\nlatency of KV cache by 96%, from 0.944s to 0.053s, almost eliminating the\ntransfer time relative to the total request latency by optimizing the KV cache\ntransfer. FlowKV introduces the Load-Aware Scheduler for balanced request\nscheduling and flexible PD node allocation. This design maximizes hardware\nresource utilization, achieving peak system throughput across various\nscenarios, including normal, computational imbalance, and extreme overload\nconditions. Experimental results demonstrate that FlowKV significantly\naccelerates inference by 15.2%-48.9% on LongBench dataset compared to the\nbaseline and supports applications with heterogeneous GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated inference has become an essential framework that separates the\nprefill (P) and decode (D) stages in large language model inference to improve\nthroughput. However, the KV cache transfer faces significant delays between\nprefill and decode nodes. The block-wise calling method and discontinuous KV\ncache memory allocation increase the number of calls to the transmission\nkernel. Additionally, existing frameworks often fix the roles of P and D nodes,\nleading to computational imbalances. In this paper, we propose FlowKV, a novel\ndisaggregated inference framework, which reduces the average transmission\nlatency of KV cache by 96%, from 0.944s to 0.053s, almost eliminating the\ntransfer time relative to the total request latency by optimizing the KV cache\ntransfer. FlowKV introduces the Load-Aware Scheduler for balanced request\nscheduling and flexible PD node allocation. This design maximizes hardware\nresource utilization, achieving peak system throughput across various\nscenarios, including normal, computational imbalance, and extreme overload\nconditions. Experimental results demonstrate that FlowKV significantly\naccelerates inference by 15.2%-48.9% on LongBench dataset compared to the\nbaseline and supports applications with heterogeneous GPUs."
                },
                "authors": [
                    {
                        "name": "Weiqing Li"
                    },
                    {
                        "name": "Guochao Jiang"
                    },
                    {
                        "name": "Xiangyong Ding"
                    },
                    {
                        "name": "Zhangcheng Tao"
                    },
                    {
                        "name": "Chuzhan Hao"
                    },
                    {
                        "name": "Chenfeng Xu"
                    },
                    {
                        "name": "Yuewei Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02268v1",
                "updated": "2025-04-03T04:27:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    27,
                    2,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T04:27:02Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    27,
                    2,
                    3,
                    93,
                    0
                ],
                "title": "Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and\n  Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and\n  Synthetic Data"
                },
                "summary": "This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Justin Cechmanek"
                    },
                    {
                        "name": "Tyler Hutcherson"
                    },
                    {
                        "name": "Srijith Rajamohan"
                    },
                    {
                        "name": "Jen Agarwal"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    },
                    {
                        "name": "Manvinder Singh"
                    },
                    {
                        "name": "Benoit Dion"
                    }
                ],
                "author_detail": {
                    "name": "Benoit Dion"
                },
                "arxiv_affiliation": "Redis",
                "author": "Benoit Dion",
                "arxiv_comment": "Initial study on embedding fine tuning for semantic cache. It also\n  explores synthetic data. Total pages are 12, including refrences",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02220v1",
                "updated": "2025-04-03T02:24:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    2,
                    24,
                    21,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T02:24:21Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    2,
                    24,
                    21,
                    3,
                    93,
                    0
                ],
                "title": "Comparative Analysis of Distributed Caching Algorithms: Performance\n  Metrics and Implementation Considerations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of Distributed Caching Algorithms: Performance\n  Metrics and Implementation Considerations"
                },
                "summary": "This paper presents a comprehensive comparison of distributed caching\nalgorithms employed in modern distributed systems. We evaluate various caching\nstrategies including Least Recently Used (LRU), Least Frequently Used (LFU),\nAdaptive Replacement Cache (ARC), and Time-Aware Least Recently Used (TLRU)\nagainst metrics such as hit ratio, latency reduction, memory overhead, and\nscalability. Our analysis reveals that while traditional algorithms like LRU\nremain prevalent, hybrid approaches incorporating machine learning techniques\ndemonstrate superior performance in dynamic environments. Additionally, we\nanalyze implementation patterns across different distributed architectures and\nprovide recommendations for algorithm selection based on specific workload\ncharacteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive comparison of distributed caching\nalgorithms employed in modern distributed systems. We evaluate various caching\nstrategies including Least Recently Used (LRU), Least Frequently Used (LFU),\nAdaptive Replacement Cache (ARC), and Time-Aware Least Recently Used (TLRU)\nagainst metrics such as hit ratio, latency reduction, memory overhead, and\nscalability. Our analysis reveals that while traditional algorithms like LRU\nremain prevalent, hybrid approaches incorporating machine learning techniques\ndemonstrate superior performance in dynamic environments. Additionally, we\nanalyze implementation patterns across different distributed architectures and\nprovide recommendations for algorithm selection based on specific workload\ncharacteristics."
                },
                "authors": [
                    {
                        "name": "Helen Mayer"
                    },
                    {
                        "name": "James Richards"
                    }
                ],
                "author_detail": {
                    "name": "James Richards"
                },
                "author": "James Richards",
                "arxiv_comment": "International Conference on Computing Technologies and Artificial\n  Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01281v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01281v2",
                "updated": "2025-04-03T01:23:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    1,
                    23,
                    22,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-02T01:16:10Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    16,
                    10,
                    2,
                    92,
                    0
                ],
                "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding"
                },
                "summary": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01281v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01281v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22875v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22875v2",
                "updated": "2025-04-02T18:51:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    18,
                    51,
                    53,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-28T21:02:32Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    21,
                    2,
                    32,
                    4,
                    87,
                    0
                ],
                "title": "A Pilot Study on Tunable Precision Emulation via Automatic BLAS\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Pilot Study on Tunable Precision Emulation via Automatic BLAS\n  Offloading"
                },
                "summary": "This study explores the use of automatic BLAS offloading and INT8-based\nemulation for accelerating traditional HPC workloads on modern GPU\narchitectures. Through the use of low-bitwidth integer units and cache-coherent\nUnified Memory Architecture, we emulate double-precision matrix multiplications\nin the MuST application without code changes. We find that accuracy depends on\nboth arithmetic precision and the properties of the operator, which can be\ndealt with through tunable precision emulation. Unlike traditional\nmixed-precision approaches, this method preserves original algorithms while\noptimizing hardware utilization. We showcases the potential of improving\naccuracy and performance at the same time. This work highlights the potential\nof AI-driven hardware to transform HPC, advocating for adaptive precision\nstrategies in future scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the use of automatic BLAS offloading and INT8-based\nemulation for accelerating traditional HPC workloads on modern GPU\narchitectures. Through the use of low-bitwidth integer units and cache-coherent\nUnified Memory Architecture, we emulate double-precision matrix multiplications\nin the MuST application without code changes. We find that accuracy depends on\nboth arithmetic precision and the properties of the operator, which can be\ndealt with through tunable precision emulation. Unlike traditional\nmixed-precision approaches, this method preserves original algorithms while\noptimizing hardware utilization. We showcases the potential of improving\naccuracy and performance at the same time. This work highlights the potential\nof AI-driven hardware to transform HPC, advocating for adaptive precision\nstrategies in future scientific computing."
                },
                "authors": [
                    {
                        "name": "Hang Liu"
                    },
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Yinzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yinzhi Wang"
                },
                "author": "Yinzhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22875v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22875v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01582v1",
                "updated": "2025-04-02T10:38:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    38,
                    25,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T10:38:25Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    38,
                    25,
                    2,
                    92,
                    0
                ],
                "title": "MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in\n  Embedded Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in\n  Embedded Processors"
                },
                "summary": "Runahead execution is a technique to mask memory latency caused by irregular\nmemory accesses. By pre-executing the application code during occurrences of\nlong-latency operations and prefetching anticipated cache-missed data into the\ncache hierarchy, runahead effectively masks memory latency for subsequent cache\nmisses and achieves high prefetching accuracy; however, this technique has been\nlimited to superscalar out-of-order and superscalar in-order cores. For\nimplementation in scalar in-order cores, the challenges of\narea-/energy-constraint and severe cache contention remain.\n  Here, we build the first full-stack system featuring runahead, MERE, from SoC\nand a dedicated ISA to the OS and programming model. Through this deployment,\nwe show that enabling runahead in scalar in-order cores is possible, with\nminimal area and power overheads, while still achieving high performance. By\nre-constructing the sequential runahead employing a hardware/software co-design\napproach, the system can be implemented on a mature processor and SoC. Building\non this, an adaptive runahead mechanism is proposed to mitigate the severe\ncache contention in scalar in-order cores. Combining this, we provide a\ncomprehensive solution for embedded processors managing irregular workloads.\nOur evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide\nout-of-order core's performance while constraining area and power overheads\nbelow 5%, with the adaptive runahead mechanism delivering an additional 20.1%\nperformance gain through mitigating the severe cache contention issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Runahead execution is a technique to mask memory latency caused by irregular\nmemory accesses. By pre-executing the application code during occurrences of\nlong-latency operations and prefetching anticipated cache-missed data into the\ncache hierarchy, runahead effectively masks memory latency for subsequent cache\nmisses and achieves high prefetching accuracy; however, this technique has been\nlimited to superscalar out-of-order and superscalar in-order cores. For\nimplementation in scalar in-order cores, the challenges of\narea-/energy-constraint and severe cache contention remain.\n  Here, we build the first full-stack system featuring runahead, MERE, from SoC\nand a dedicated ISA to the OS and programming model. Through this deployment,\nwe show that enabling runahead in scalar in-order cores is possible, with\nminimal area and power overheads, while still achieving high performance. By\nre-constructing the sequential runahead employing a hardware/software co-design\napproach, the system can be implemented on a mature processor and SoC. Building\non this, an adaptive runahead mechanism is proposed to mitigate the severe\ncache contention in scalar in-order cores. Combining this, we provide a\ncomprehensive solution for embedded processors managing irregular workloads.\nOur evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide\nout-of-order core's performance while constraining area and power overheads\nbelow 5%, with the adaptive runahead mechanism delivering an additional 20.1%\nperformance gain through mitigating the severe cache contention issues."
                },
                "authors": [
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Xiaoxuan Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Zhihang Tan"
                    },
                    {
                        "name": "Wenbo Xu"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Zhenyuan Wang"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v4",
                "updated": "2025-04-02T04:57:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    4,
                    57,
                    15,
                    2,
                    92,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "Hexa-MoE: Efficient and Heterogeneous-aware Training for\n  Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hexa-MoE: Efficient and Heterogeneous-aware Training for\n  Mixture-of-Experts"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Hanrui Wang"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v5",
                "updated": "2025-04-02T01:58:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    58,
                    38,
                    2,
                    92,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks."
                },
                "authors": [
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01291v1",
                "updated": "2025-04-02T01:49:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T01:49:58Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "title": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures"
                },
                "summary": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs."
                },
                "authors": [
                    {
                        "name": "Seungheon Shin"
                    },
                    {
                        "name": "Kyle Liddy"
                    },
                    {
                        "name": "Yinxuan Zhu"
                    },
                    {
                        "name": "Chandan Joishi"
                    },
                    {
                        "name": "Brianna A. Klein"
                    },
                    {
                        "name": "Andrew Armstrong"
                    },
                    {
                        "name": "Andrew A. Allerman"
                    },
                    {
                        "name": "Siddharth Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Rajan"
                },
                "author": "Siddharth Rajan",
                "arxiv_comment": "11 pages, 6 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01157v1",
                "updated": "2025-04-01T19:48:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    48,
                    17,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T19:48:17Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    48,
                    17,
                    1,
                    91,
                    0
                ],
                "title": "Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB"
                },
                "summary": "Knowledge-intensive analytical applications retrieve context from both\nstructured tabular data and unstructured, text-free documents for effective\ndecision-making. Large language models (LLMs) have made it significantly easier\nto prototype such retrieval and reasoning data pipelines. However, implementing\nthese pipelines efficiently still demands significant effort and has several\nchallenges. This often involves orchestrating heterogeneous data systems,\nmanaging data movement, and handling low-level implementation details, e.g.,\nLLM context management.\n  To address these challenges, we introduce FlockMTL: an extension for DBMSs\nthat deeply integrates LLM capabilities and retrieval-augmented generation\n(RAG). FlockMTL includes model-driven scalar and aggregate functions, enabling\nchained predictions through tuple-level mappings and reductions. Drawing\ninspiration from the relational model, FlockMTL incorporates: (i) cost-based\noptimizations, which seamlessly apply techniques such as batching and caching;\nand (ii) resource independence, enabled through novel SQL DDL abstractions:\nPROMPT and MODEL, introduced as first-class schema objects alongside TABLE.\nFlockMTL streamlines the development of knowledge-intensive analytical\napplications, and its optimizations ease the implementation burden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-intensive analytical applications retrieve context from both\nstructured tabular data and unstructured, text-free documents for effective\ndecision-making. Large language models (LLMs) have made it significantly easier\nto prototype such retrieval and reasoning data pipelines. However, implementing\nthese pipelines efficiently still demands significant effort and has several\nchallenges. This often involves orchestrating heterogeneous data systems,\nmanaging data movement, and handling low-level implementation details, e.g.,\nLLM context management.\n  To address these challenges, we introduce FlockMTL: an extension for DBMSs\nthat deeply integrates LLM capabilities and retrieval-augmented generation\n(RAG). FlockMTL includes model-driven scalar and aggregate functions, enabling\nchained predictions through tuple-level mappings and reductions. Drawing\ninspiration from the relational model, FlockMTL incorporates: (i) cost-based\noptimizations, which seamlessly apply techniques such as batching and caching;\nand (ii) resource independence, enabled through novel SQL DDL abstractions:\nPROMPT and MODEL, introduced as first-class schema objects alongside TABLE.\nFlockMTL streamlines the development of knowledge-intensive analytical\napplications, and its optimizations ease the implementation burden."
                },
                "authors": [
                    {
                        "name": "Anas Dorbani"
                    },
                    {
                        "name": "Sunny Yasser"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Amine Mhedhbi"
                    }
                ],
                "author_detail": {
                    "name": "Amine Mhedhbi"
                },
                "author": "Amine Mhedhbi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01104v1",
                "updated": "2025-04-01T18:21:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    21,
                    43,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T18:21:43Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    21,
                    43,
                    1,
                    91,
                    0
                ],
                "title": "Fundamentals of Caching Layered Data objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamentals of Caching Layered Data objects"
                },
                "summary": "The effective management of large amounts of data processed or required by\ntoday's cloud or edge computing systems remains a fundamental challenge. This\npaper focuses on cache management for applications where data objects can be\nstored in layered representations. In such representations, each additional\ndata layer enhances the \"quality\" of the object's version but comes with an\nincremental cost of memory space. This layered approach proves beneficial in\nvarious scenarios, including the delivery of zoomable maps, video coding,\nfuture Virtual Reality gaming, and layered neural network models where\nadditional data layers improve inference accuracy. In systems where users or\ndevices demand different versions of a data object, layered representations\noffer flexibility for caching policies to achieve improved hit rates.\n  In this paper, we explore the performance of various traditionally studied\ncaching policies, such as Belady, LRU, and LFU, both with and without layering.\nTo this end, we develop an asymptotically accurate analytical model for Layered\nLRU (LLRU). We study how the performance of LLRU is impacted by factors such as\nthe number of layers, the popularity of different objects and layers, and\noverheads associated with storing layered representations. For instance, we\nshow that, for LLRU, more layers are not always beneficial and indeed\nperformance depends in subtle ways on the popularity and size profiles of\nlayers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effective management of large amounts of data processed or required by\ntoday's cloud or edge computing systems remains a fundamental challenge. This\npaper focuses on cache management for applications where data objects can be\nstored in layered representations. In such representations, each additional\ndata layer enhances the \"quality\" of the object's version but comes with an\nincremental cost of memory space. This layered approach proves beneficial in\nvarious scenarios, including the delivery of zoomable maps, video coding,\nfuture Virtual Reality gaming, and layered neural network models where\nadditional data layers improve inference accuracy. In systems where users or\ndevices demand different versions of a data object, layered representations\noffer flexibility for caching policies to achieve improved hit rates.\n  In this paper, we explore the performance of various traditionally studied\ncaching policies, such as Belady, LRU, and LFU, both with and without layering.\nTo this end, we develop an asymptotically accurate analytical model for Layered\nLRU (LLRU). We study how the performance of LLRU is impacted by factors such as\nthe number of layers, the popularity of different objects and layers, and\noverheads associated with storing layered representations. For instance, we\nshow that, for LLRU, more layers are not always beneficial and indeed\nperformance depends in subtle ways on the popularity and size profiles of\nlayers."
                },
                "authors": [
                    {
                        "name": "Agrim Bari"
                    },
                    {
                        "name": "Gustavo de Veciana"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "arxiv_comment": "An abridged version of this paper has been accepted at the 45th IEEE\n  International Conference on Distributed Computing Systems (ICDCS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01084v1",
                "updated": "2025-04-01T18:00:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    0,
                    48,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T18:00:48Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    0,
                    48,
                    1,
                    91,
                    0
                ],
                "title": "Surfactants Screen Slide Electrification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surfactants Screen Slide Electrification"
                },
                "summary": "Water drops spontaneously accumulate charges when they move on hydrophobic\ndielectric surfaces by slide electrification. On the one hand, slide\nelectrification generates electricity with possible applications on tiny\ndevices. On the other hand, the potential of up to 1 KV generated by slide\nelectrification alters wetting and drop motion. Therefore, it is important to\nknow the factors that affect slide electrification. To find out how surfactants\naffect slide electrification, we measured drop charges of aqueous drops\ncontaining cationic CTAB, anionic SDS and neutral C8E3 sliding on different\nhydrophobic surfaces. The result is: addition of surfactant significantly\nreduces the spontaneous charging of moving water drops. Based on zeta potential\nmeasurements, confocal microscopy of deposited surface-active dyes and drop\nimpact studies, we propose that several factors contribute to this suppression\nof charge separation: (1) Surfactants tend to lower the contact angles, which\nreduces charge separation. (2) Surfactant adsorption at the solid-liquid\ninterface can reduce the density of primary ions, particularly for anionic\nsurfactants. (3) Anionic and neutral surfactants are mostly transferred to the\nliquid-air interface at the rear of the sliding drop, retaining primary ions\nwithin the drop. (4) Deposited cationic surfactant directly reduces the charge\nof the drop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Water drops spontaneously accumulate charges when they move on hydrophobic\ndielectric surfaces by slide electrification. On the one hand, slide\nelectrification generates electricity with possible applications on tiny\ndevices. On the other hand, the potential of up to 1 KV generated by slide\nelectrification alters wetting and drop motion. Therefore, it is important to\nknow the factors that affect slide electrification. To find out how surfactants\naffect slide electrification, we measured drop charges of aqueous drops\ncontaining cationic CTAB, anionic SDS and neutral C8E3 sliding on different\nhydrophobic surfaces. The result is: addition of surfactant significantly\nreduces the spontaneous charging of moving water drops. Based on zeta potential\nmeasurements, confocal microscopy of deposited surface-active dyes and drop\nimpact studies, we propose that several factors contribute to this suppression\nof charge separation: (1) Surfactants tend to lower the contact angles, which\nreduces charge separation. (2) Surfactant adsorption at the solid-liquid\ninterface can reduce the density of primary ions, particularly for anionic\nsurfactants. (3) Anionic and neutral surfactants are mostly transferred to the\nliquid-air interface at the rear of the sliding drop, retaining primary ions\nwithin the drop. (4) Deposited cationic surfactant directly reduces the charge\nof the drop."
                },
                "authors": [
                    {
                        "name": "Xiaomei Li"
                    },
                    {
                        "name": "Zhongyuan Ni"
                    },
                    {
                        "name": "Xiaoteng Zhou"
                    },
                    {
                        "name": "Lisa S. Bauer"
                    },
                    {
                        "name": "Diego Diaz"
                    },
                    {
                        "name": "Gabriele Schfer"
                    },
                    {
                        "name": "Hans-Jrgen Butt"
                    }
                ],
                "author_detail": {
                    "name": "Hans-Jrgen Butt"
                },
                "author": "Hans-Jrgen Butt",
                "arxiv_comment": "13 pages, 4 figures, 50 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00999v1",
                "updated": "2025-04-01T17:39:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    39,
                    19,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T17:39:19Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    39,
                    19,
                    1,
                    91,
                    0
                ],
                "title": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization"
                },
                "summary": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ."
                },
                "authors": [
                    {
                        "name": "Siyuan Li"
                    },
                    {
                        "name": "Luyuan Zhang"
                    },
                    {
                        "name": "Zedong Wang"
                    },
                    {
                        "name": "Juanxi Tian"
                    },
                    {
                        "name": "Cheng Tan"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Chang Yu"
                    },
                    {
                        "name": "Qingsong Xie"
                    },
                    {
                        "name": "Haonan Lu"
                    },
                    {
                        "name": "Haoqian Wang"
                    },
                    {
                        "name": "Zhen Lei"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Lei"
                },
                "author": "Zhen Lei",
                "arxiv_comment": "CVPR2025 (in process for more analysis and extension)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00970v1",
                "updated": "2025-04-01T17:08:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T17:08:57Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "title": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching"
                },
                "summary": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Ali Falahati"
                    },
                    {
                        "name": "David H. Yang"
                    },
                    {
                        "name": "Mohammad Mohammadi Amiri"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Mohammadi Amiri"
                },
                "author": "Mohammad Mohammadi Amiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13275v2",
                "updated": "2025-04-01T14:21:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    21,
                    15,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-17T15:27:02Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "title": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems"
                },
                "summary": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability."
                },
                "authors": [
                    {
                        "name": "Seyoung Song"
                    }
                ],
                "author_detail": {
                    "name": "Seyoung Song"
                },
                "author": "Seyoung Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; I.2.11; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00726v1",
                "updated": "2025-04-01T12:34:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    34,
                    58,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T12:34:58Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    34,
                    58,
                    1,
                    91,
                    0
                ],
                "title": "EMO: Edge Model Overlays to Scale Model Size in Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMO: Edge Model Overlays to Scale Model Size in Federated Learning"
                },
                "summary": "Federated Learning (FL) trains machine learning models on edge devices with\ndistributed data. However, the computational and memory limitations of these\ndevices restrict the training of large models using FL. Split Federated\nLearning (SFL) addresses this challenge by distributing the model across the\ndevice and server, but it introduces a tightly coupled data flow, leading to\ncomputational bottlenecks and high communication costs. We propose EMO as a\nsolution to enable the training of large models in FL while mitigating the\nchallenges of SFL. EMO introduces Edge Model Overlay(s) between the device and\nserver, enabling the creation of a larger ensemble model without modifying the\nFL workflow. The key innovation in EMO is Augmented Federated Learning (AFL),\nwhich builds an ensemble model by connecting the original (smaller) FL model\nwith model(s) trained in the overlay(s) to facilitate horizontal or vertical\nscaling. This is accomplished through three key modules: a hierarchical\nactivation replay cache to decouple AFL from FL, a convergence-aware\ncommunication controller to optimize communication overhead, and an ensemble\ninference module. Evaluations on a real-world prototype show that EMO improves\naccuracy by up to 17.77% compared to FL, and reduces communication costs by up\nto 7.17x and decreases training time by up to 6.9x compared to SFL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) trains machine learning models on edge devices with\ndistributed data. However, the computational and memory limitations of these\ndevices restrict the training of large models using FL. Split Federated\nLearning (SFL) addresses this challenge by distributing the model across the\ndevice and server, but it introduces a tightly coupled data flow, leading to\ncomputational bottlenecks and high communication costs. We propose EMO as a\nsolution to enable the training of large models in FL while mitigating the\nchallenges of SFL. EMO introduces Edge Model Overlay(s) between the device and\nserver, enabling the creation of a larger ensemble model without modifying the\nFL workflow. The key innovation in EMO is Augmented Federated Learning (AFL),\nwhich builds an ensemble model by connecting the original (smaller) FL model\nwith model(s) trained in the overlay(s) to facilitate horizontal or vertical\nscaling. This is accomplished through three key modules: a hierarchical\nactivation replay cache to decouple AFL from FL, a convergence-aware\ncommunication controller to optimize communication overhead, and an ensemble\ninference module. Evaluations on a real-world prototype show that EMO improves\naccuracy by up to 17.77% compared to FL, and reduces communication costs by up\nto 7.17x and decreases training time by up to 6.9x compared to SFL."
                },
                "authors": [
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Weibo He"
                    },
                    {
                        "name": "Wanglei Feng"
                    },
                    {
                        "name": "Zhenyu Wen"
                    },
                    {
                        "name": "Bin Qian"
                    },
                    {
                        "name": "Blesson Varghese"
                    }
                ],
                "author_detail": {
                    "name": "Blesson Varghese"
                },
                "author": "Blesson Varghese",
                "arxiv_comment": "Poster accepted at IEEE ICDCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00557v1",
                "updated": "2025-04-01T09:10:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    10,
                    32,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T09:10:32Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    10,
                    32,
                    1,
                    91,
                    0
                ],
                "title": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features"
                },
                "summary": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity."
                },
                "authors": [
                    {
                        "name": "Jewon Lee"
                    },
                    {
                        "name": "Ki-Ung Song"
                    },
                    {
                        "name": "Seungmin Yang"
                    },
                    {
                        "name": "Donguk Lim"
                    },
                    {
                        "name": "Jaeyeon Kim"
                    },
                    {
                        "name": "Wooksu Shin"
                    },
                    {
                        "name": "Bo-Kyeong Kim"
                    },
                    {
                        "name": "Yong Jae Lee"
                    },
                    {
                        "name": "Tae-Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Tae-Ho Kim"
                },
                "author": "Tae-Ho Kim",
                "arxiv_comment": "accepted at CVPR 2025 Workshop on ELVM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00474v1",
                "updated": "2025-04-01T07:04:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    7,
                    4,
                    30,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T07:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    7,
                    4,
                    30,
                    1,
                    91,
                    0
                ],
                "title": "High specific impulse electrospray propulsion with small capillary\n  emitters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High specific impulse electrospray propulsion with small capillary\n  emitters"
                },
                "summary": "This study demonstrates the feasibility of using smaller capillary emitters\nto achieve higher specific impulse ($I_\\text{sp}$) in electrospray propulsion.\nFour ionic liquids were characterized using capillary emitters with tip\ndiameters from 15 to 50 $\\mu$m. Smaller diameter capillaries produced smaller\nand more stable Taylor cones. This stabilization enabled steady cone-jet\noperation at significantly lower flow rates compared to larger emitters. This\nwas unexpected because when the jet diameter is much smaller than far-field\ngeometric features, the minimum flow rate is thought to be solely determined by\nthe physical properties of the propellant. Using the smaller emitters and\nacceleration voltages of 10 kV, specific impulses up to 3000 s could be\nachieved with efficiencies above 50%, approximately doubling the $I_\\text{sp}$\nobserved with larger emitters. For one of the liquids and the smallest\nemitters, the beam consisted solely of ions at the lowest flow rates, similarly\nto studies using externally wetted and porous emitters. Another important\nfinding was that at sufficiently low flow rates, a significant fraction of the\npropellant fed to the emitter is not accelerated by the electrostatic field.\nThese propellant losses make the time-of-flight technique unreliable for\ndetermining the $I_\\text{sp}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study demonstrates the feasibility of using smaller capillary emitters\nto achieve higher specific impulse ($I_\\text{sp}$) in electrospray propulsion.\nFour ionic liquids were characterized using capillary emitters with tip\ndiameters from 15 to 50 $\\mu$m. Smaller diameter capillaries produced smaller\nand more stable Taylor cones. This stabilization enabled steady cone-jet\noperation at significantly lower flow rates compared to larger emitters. This\nwas unexpected because when the jet diameter is much smaller than far-field\ngeometric features, the minimum flow rate is thought to be solely determined by\nthe physical properties of the propellant. Using the smaller emitters and\nacceleration voltages of 10 kV, specific impulses up to 3000 s could be\nachieved with efficiencies above 50%, approximately doubling the $I_\\text{sp}$\nobserved with larger emitters. For one of the liquids and the smallest\nemitters, the beam consisted solely of ions at the lowest flow rates, similarly\nto studies using externally wetted and porous emitters. Another important\nfinding was that at sufficiently low flow rates, a significant fraction of the\npropellant fed to the emitter is not accelerated by the electrostatic field.\nThese propellant losses make the time-of-flight technique unreliable for\ndetermining the $I_\\text{sp}$."
                },
                "authors": [
                    {
                        "name": "Manel Caballero-Prez"
                    },
                    {
                        "name": "Marc Galobardes-Esteban"
                    },
                    {
                        "name": "Manuel Gamero-Castao"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Gamero-Castao"
                },
                "author": "Manuel Gamero-Castao",
                "arxiv_comment": "29 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24358v1",
                "updated": "2025-03-31T17:37:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:37:32Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "title": "SQuat: Subspace-orthogonal KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQuat: Subspace-orthogonal KV Cache Quantization"
                },
                "summary": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24007v1",
                "updated": "2025-03-31T12:32:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:32:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting"
                },
                "summary": "Covariates play an indispensable role in practical time series forecasting,\noffering rich context from the past and sometimes extending into the future.\nHowever, their availability varies depending on the scenario, and situations\noften involve multiple target variables simultaneously. Moreover, the\ncross-variate dependencies between them are multi-granular, with some\ncovariates having a short-term impact on target variables and others showing\nlong-term correlations. This heterogeneity and the intricate dependencies\narising in covariate-informed forecasting present significant challenges to\nexisting deep models. To address these issues, we propose CITRAS, a patch-based\nTransformer that flexibly leverages multiple targets and covariates covering\nboth the past and the future forecasting horizon. While preserving the strong\nautoregressive capabilities of the canonical Transformer, CITRAS introduces two\nnovel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift\nand Attention Score Smoothing. KV Shift seamlessly incorporates future known\ncovariates into the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing transforms locally\naccurate patch-wise cross-variate dependencies into global variate-level\ndependencies by smoothing the past series of attention scores. Experimentally,\nCITRAS achieves state-of-the-art performance in both covariate-informed and\nmultivariate forecasting, demonstrating its versatile ability to leverage\ncross-variate dependency for improved forecasting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Covariates play an indispensable role in practical time series forecasting,\noffering rich context from the past and sometimes extending into the future.\nHowever, their availability varies depending on the scenario, and situations\noften involve multiple target variables simultaneously. Moreover, the\ncross-variate dependencies between them are multi-granular, with some\ncovariates having a short-term impact on target variables and others showing\nlong-term correlations. This heterogeneity and the intricate dependencies\narising in covariate-informed forecasting present significant challenges to\nexisting deep models. To address these issues, we propose CITRAS, a patch-based\nTransformer that flexibly leverages multiple targets and covariates covering\nboth the past and the future forecasting horizon. While preserving the strong\nautoregressive capabilities of the canonical Transformer, CITRAS introduces two\nnovel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift\nand Attention Score Smoothing. KV Shift seamlessly incorporates future known\ncovariates into the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing transforms locally\naccurate patch-wise cross-variate dependencies into global variate-level\ndependencies by smoothing the past series of attention scores. Experimentally,\nCITRAS achieves state-of-the-art performance in both covariate-informed and\nmultivariate forecasting, demonstrating its versatile ability to leverage\ncross-variate dependency for improved forecasting accuracy."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamaguchi"
                    },
                    {
                        "name": "Issei Suemitsu"
                    },
                    {
                        "name": "Wenpeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Wei"
                },
                "author": "Wenpeng Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24000v1",
                "updated": "2025-03-31T12:23:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    23,
                    31,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:23:31Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    23,
                    31,
                    0,
                    90,
                    0
                ],
                "title": "Rethinking Key-Value Cache Compression Techniques for Large Language\n  Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Key-Value Cache Compression Techniques for Large Language\n  Model Serving"
                },
                "summary": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}."
                },
                "authors": [
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Yonggang Wen"
                    }
                ],
                "author_detail": {
                    "name": "Yonggang Wen"
                },
                "author": "Yonggang Wen",
                "arxiv_comment": "21 pages, 18 figures, published to MLSys2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23988v1",
                "updated": "2025-03-31T11:58:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    58,
                    37,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:58:37Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    58,
                    37,
                    0,
                    90,
                    0
                ],
                "title": "Deep Learning Model Deployment in Multiple Cloud Providers: an\n  Exploratory Study Using Low Computing Power Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Model Deployment in Multiple Cloud Providers: an\n  Exploratory Study Using Low Computing Power Environments"
                },
                "summary": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups."
                },
                "authors": [
                    {
                        "name": "Elayne Lemos"
                    },
                    {
                        "name": "Rodrigo Oliveira"
                    },
                    {
                        "name": "Jairson Rodrigues"
                    },
                    {
                        "name": "Rosalvo F. Oliveira Neto"
                    }
                ],
                "author_detail": {
                    "name": "Rosalvo F. Oliveira Neto"
                },
                "author": "Rosalvo F. Oliveira Neto",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68U01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; I.2.0; B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2308.03351v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.03351v5",
                "updated": "2025-04-16T17:57:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    17,
                    57,
                    59,
                    2,
                    106,
                    0
                ],
                "published": "2023-08-07T07:06:47Z",
                "published_parsed": [
                    2023,
                    8,
                    7,
                    7,
                    6,
                    47,
                    0,
                    219,
                    0
                ],
                "title": "Unsupervised Super-Resolution Data Assimilation Using Conditional\n  Variational Autoencoders with Estimating Background Covariances via\n  Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised Super-Resolution Data Assimilation Using Conditional\n  Variational Autoencoders with Estimating Background Covariances via\n  Super-Resolution"
                },
                "summary": "This study proposes a theory of unsupervised super-resolution data\nassimilation (SRDA) using conditional variational autoencoders (CVAEs). We\nderive an evidence lower bound for unsupervised learning, showing that our\ntheory is an extension of a traditional data assimilation (DA) method, namely\nthe three-dimensional variational (3D-Var) formalism. In contrast to 3D-Var,\nour theory exploits the non-locality of super-resolution (SR) to learn\nbackground covariances without explicitly imposing them for assimilating\ndistant observations. For linear SR, SR operators serve as background error\ncovariance matrices,whereas for nonlinear SR, error backpropagation through SR\nneural networks induces covariance structures in inference. SRDA can naturally\nbe realized with CVAEs because the loss function for CVAEs is generally an\nevidence lower bound. By incorporating the SR neural network into the CVAE, the\nencoder estimates the high-resolution (HR) analysis from HR observations and\nlow-resolution forecasts. The decoder acts as the observation operator by\nreconstructing the HR observations from the estimated HR analysis. The\neffectiveness of SRDA was evaluated through numerical experiments using an\nidealized barotropic ocean jet system. Compared to inference with an ensemble\nKalman filter, SRDA demonstrated superior accuracy in HR inference. SRDA was\nalso computationally efficient because it does not require HR numerical\nintegration or ensemble calculations. The findings of this study provide a\ntheoretical basis for integrating SR and DA, which will stimulate further\nresearch in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study proposes a theory of unsupervised super-resolution data\nassimilation (SRDA) using conditional variational autoencoders (CVAEs). We\nderive an evidence lower bound for unsupervised learning, showing that our\ntheory is an extension of a traditional data assimilation (DA) method, namely\nthe three-dimensional variational (3D-Var) formalism. In contrast to 3D-Var,\nour theory exploits the non-locality of super-resolution (SR) to learn\nbackground covariances without explicitly imposing them for assimilating\ndistant observations. For linear SR, SR operators serve as background error\ncovariance matrices,whereas for nonlinear SR, error backpropagation through SR\nneural networks induces covariance structures in inference. SRDA can naturally\nbe realized with CVAEs because the loss function for CVAEs is generally an\nevidence lower bound. By incorporating the SR neural network into the CVAE, the\nencoder estimates the high-resolution (HR) analysis from HR observations and\nlow-resolution forecasts. The decoder acts as the observation operator by\nreconstructing the HR observations from the estimated HR analysis. The\neffectiveness of SRDA was evaluated through numerical experiments using an\nidealized barotropic ocean jet system. Compared to inference with an ensemble\nKalman filter, SRDA demonstrated superior accuracy in HR inference. SRDA was\nalso computationally efficient because it does not require HR numerical\nintegration or ensemble calculations. The findings of this study provide a\ntheoretical basis for integrating SR and DA, which will stimulate further\nresearch in this direction."
                },
                "authors": [
                    {
                        "name": "Yuki Yasuda"
                    },
                    {
                        "name": "Ryo Onishi"
                    }
                ],
                "author_detail": {
                    "name": "Ryo Onishi"
                },
                "author": "Ryo Onishi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.03351v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.03351v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12285v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12285v1",
                "updated": "2025-04-16T17:51:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    17,
                    51,
                    43,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T17:51:43Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    17,
                    51,
                    43,
                    2,
                    106,
                    0
                ],
                "title": "BitNet b1.58 2B4T Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitNet b1.58 2B4T Technical Report"
                },
                "summary": "We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large\nLanguage Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4\ntrillion tokens, the model has been rigorously evaluated across benchmarks\ncovering language understanding, mathematical reasoning, coding proficiency,\nand conversational ability. Our results demonstrate that BitNet b1.58 2B4T\nachieves performance on par with leading open-weight, full-precision LLMs of\nsimilar size, while offering significant advantages in computational\nefficiency, including substantially reduced memory footprint, energy\nconsumption, and decoding latency. To facilitate further research and adoption,\nthe model weights are released via Hugging Face along with open-source\ninference implementations for both GPU and CPU architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large\nLanguage Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4\ntrillion tokens, the model has been rigorously evaluated across benchmarks\ncovering language understanding, mathematical reasoning, coding proficiency,\nand conversational ability. Our results demonstrate that BitNet b1.58 2B4T\nachieves performance on par with leading open-weight, full-precision LLMs of\nsimilar size, while offering significant advantages in computational\nefficiency, including substantially reduced memory footprint, energy\nconsumption, and decoding latency. To facilitate further research and adoption,\nthe model weights are released via Hugging Face along with open-source\ninference implementations for both GPU and CPU architectures."
                },
                "authors": [
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Hongyu Wang"
                    },
                    {
                        "name": "Shaohan Huang"
                    },
                    {
                        "name": "Xingxing Zhang"
                    },
                    {
                        "name": "Ying Hu"
                    },
                    {
                        "name": "Ting Song"
                    },
                    {
                        "name": "Yan Xia"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12285v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12285v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02108v2",
                "updated": "2025-04-16T17:50:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    17,
                    50,
                    12,
                    2,
                    106,
                    0
                ],
                "published": "2024-10-03T00:09:15Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    0,
                    9,
                    15,
                    3,
                    277,
                    0
                ],
                "title": "ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement"
                },
                "summary": "Post-training Large Language Models (LLMs) with explicit reasoning\ntrajectories can enhance their reasoning abilities. However, acquiring such\nhigh-quality trajectory data typically demands meticulous supervision from\nhumans or superior models, which can be either expensive or\nlicense-constrained. In this paper, we explore how far an LLM can improve its\nreasoning by self-synthesizing reasoning paths as training data without any\nadditional supervision. Existing self-synthesizing methods, such as STaR,\nsuffer from poor generalization to out-of-domain (OOD) reasoning tasks. We\nhypothesize it is due to that their self-synthesized reasoning paths are too\ntask-specific, lacking general task-agnostic reasoning guidance. To address\nthis, we propose Reasoning Generalist via Self-Improvement (ReGenesis), a\nmethod to self-synthesize reasoning paths as post-training data by progressing\nfrom abstract to concrete. More specifically, ReGenesis self-synthesizes\nreasoning paths by converting general reasoning guidelines into task-specific\nones, generating reasoning structures, and subsequently transforming these\nstructures into reasoning paths, without the need for human-designed\ntask-specific examples used in existing methods. We show that ReGenesis\nachieves superior performance on all in-domain and OOD settings tested compared\nto existing methods. For six OOD tasks specifically, while previous methods\nexhibited an average performance decrease of approximately 4.6% after post\ntraining, ReGenesis delivers around 6.1% performance improvement. We also\nconduct in-depth analysis of our framework and show ReGenesis is effective\nacross various LLMs and design choices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training Large Language Models (LLMs) with explicit reasoning\ntrajectories can enhance their reasoning abilities. However, acquiring such\nhigh-quality trajectory data typically demands meticulous supervision from\nhumans or superior models, which can be either expensive or\nlicense-constrained. In this paper, we explore how far an LLM can improve its\nreasoning by self-synthesizing reasoning paths as training data without any\nadditional supervision. Existing self-synthesizing methods, such as STaR,\nsuffer from poor generalization to out-of-domain (OOD) reasoning tasks. We\nhypothesize it is due to that their self-synthesized reasoning paths are too\ntask-specific, lacking general task-agnostic reasoning guidance. To address\nthis, we propose Reasoning Generalist via Self-Improvement (ReGenesis), a\nmethod to self-synthesize reasoning paths as post-training data by progressing\nfrom abstract to concrete. More specifically, ReGenesis self-synthesizes\nreasoning paths by converting general reasoning guidelines into task-specific\nones, generating reasoning structures, and subsequently transforming these\nstructures into reasoning paths, without the need for human-designed\ntask-specific examples used in existing methods. We show that ReGenesis\nachieves superior performance on all in-domain and OOD settings tested compared\nto existing methods. For six OOD tasks specifically, while previous methods\nexhibited an average performance decrease of approximately 4.6% after post\ntraining, ReGenesis delivers around 6.1% performance improvement. We also\nconduct in-depth analysis of our framework and show ReGenesis is effective\nacross various LLMs and design choices."
                },
                "authors": [
                    {
                        "name": "Xiangyu Peng"
                    },
                    {
                        "name": "Congying Xia"
                    },
                    {
                        "name": "Xinyi Yang"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Chien-Sheng Wu"
                    },
                    {
                        "name": "Chen Xing"
                    }
                ],
                "author_detail": {
                    "name": "Chen Xing"
                },
                "author": "Chen Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12279v1",
                "updated": "2025-04-16T17:41:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    17,
                    41,
                    19,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T17:41:19Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    17,
                    41,
                    19,
                    2,
                    106,
                    0
                ],
                "title": "Dysarthria Normalization via Local Lie Group Transformations for Robust\n  ASR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dysarthria Normalization via Local Lie Group Transformations for Robust\n  ASR"
                },
                "summary": "We present a geometry-driven method for normalizing dysarthric speech using\nlocal Lie group transformations of spectrograms. Time, frequency, and amplitude\ndistortions are modeled as smooth, invertible deformations, parameterized by\nscalar fields and applied via exponential maps. A neural network is trained to\ninfer these fields from synthetic distortions of typical speech-without using\nany pathological data. At test time, the model applies an approximate inverse\nto real dysarthric inputs. Despite zero-shot generalization, we observe\nsubstantial ASR gains, including up to 16 percentage points WER reduction on\nchallenging TORGO samples, with no degradation on clean speech. This work\nintroduces a principled, interpretable approach for robust speech recognition\nunder motor speech disorders",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a geometry-driven method for normalizing dysarthric speech using\nlocal Lie group transformations of spectrograms. Time, frequency, and amplitude\ndistortions are modeled as smooth, invertible deformations, parameterized by\nscalar fields and applied via exponential maps. A neural network is trained to\ninfer these fields from synthetic distortions of typical speech-without using\nany pathological data. At test time, the model applies an approximate inverse\nto real dysarthric inputs. Despite zero-shot generalization, we observe\nsubstantial ASR gains, including up to 16 percentage points WER reduction on\nchallenging TORGO samples, with no degradation on clean speech. This work\nintroduces a principled, interpretable approach for robust speech recognition\nunder motor speech disorders"
                },
                "authors": [
                    {
                        "name": "Mikhail Osipov"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Osipov"
                },
                "author": "Mikhail Osipov",
                "arxiv_comment": "Preprint. 11 pages, 3 figures, 2 tables, 8 appendices. Code and data\n  available upon request",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.08759v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.08759v2",
                "updated": "2025-04-16T17:35:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    17,
                    35,
                    41,
                    2,
                    106,
                    0
                ],
                "published": "2024-05-14T16:48:46Z",
                "published_parsed": [
                    2024,
                    5,
                    14,
                    16,
                    48,
                    46,
                    1,
                    135,
                    0
                ],
                "title": "Optimal Sequential Procedure for Early Detection of Multiple Side\n  Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Sequential Procedure for Early Detection of Multiple Side\n  Effects"
                },
                "summary": "In this paper, we propose an optimal sequential procedure for the early\ndetection of potential side effects resulting from the administration of some\ntreatment (e.g. a vaccine, say). The results presented here extend previous\nresults obtained in Wang and Boukai (2024) who study the single side effect\ncase to the case of two (or more) side effects. While the sequential procedure\nwe employ, simultaneously monitors several of the treatment's side effects, the\n$(\\alpha, \\beta)$-optimal test we propose does not require any information\nabout the inter-correlation between these potential side effects. However, in\nall of the subsequent analyses, including the derivations of the exact\nexpressions of the Average Sample Number (ASN), the Power function, and the\nproperties of the post-test (or post-detection) estimators, we accounted\nspecifically, for the correlation between the potential side effects. In the\nreal-life application (such as post-marketing surveillance), the number of\navailable observations is large enough to justify asymptotic analyses of the\nsequential procedure (testing and post-detection estimation) properties.\nAccordingly, we also derive the consistency and asymptotic normality of our\npost-test estimators; results which enable us to also provide (asymptotic,\npost-detection) confidence intervals for the probabilities of various\nside-effects. Moreover, to compare two specific side effects, their relative\nrisk plays an important role. We derive the distribution of the estimated\nrelative risk in the asymptotic framework to provide appropriate inference. To\nillustrate the theoretical results presented, we provide two detailed examples\nbased on the data of side effects on COVID-19 vaccine collected in Nigeria (see\nNigeria (see Ilori et al. (2022)).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose an optimal sequential procedure for the early\ndetection of potential side effects resulting from the administration of some\ntreatment (e.g. a vaccine, say). The results presented here extend previous\nresults obtained in Wang and Boukai (2024) who study the single side effect\ncase to the case of two (or more) side effects. While the sequential procedure\nwe employ, simultaneously monitors several of the treatment's side effects, the\n$(\\alpha, \\beta)$-optimal test we propose does not require any information\nabout the inter-correlation between these potential side effects. However, in\nall of the subsequent analyses, including the derivations of the exact\nexpressions of the Average Sample Number (ASN), the Power function, and the\nproperties of the post-test (or post-detection) estimators, we accounted\nspecifically, for the correlation between the potential side effects. In the\nreal-life application (such as post-marketing surveillance), the number of\navailable observations is large enough to justify asymptotic analyses of the\nsequential procedure (testing and post-detection estimation) properties.\nAccordingly, we also derive the consistency and asymptotic normality of our\npost-test estimators; results which enable us to also provide (asymptotic,\npost-detection) confidence intervals for the probabilities of various\nside-effects. Moreover, to compare two specific side effects, their relative\nrisk plays an important role. We derive the distribution of the estimated\nrelative risk in the asymptotic framework to provide appropriate inference. To\nillustrate the theoretical results presented, we provide two detailed examples\nbased on the data of side effects on COVID-19 vaccine collected in Nigeria (see\nNigeria (see Ilori et al. (2022))."
                },
                "authors": [
                    {
                        "name": "Jiayue Wang"
                    },
                    {
                        "name": "Ben Boukai"
                    }
                ],
                "author_detail": {
                    "name": "Ben Boukai"
                },
                "author": "Ben Boukai",
                "arxiv_comment": "A total of 30 pages with 6 Tables and 8 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.08759v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.08759v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62L10, 62L12",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09775v2",
                "updated": "2025-04-16T17:34:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    17,
                    34,
                    4,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-14T00:29:49Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    0,
                    29,
                    49,
                    0,
                    104,
                    0
                ],
                "title": "Understanding and Optimizing Multi-Stage AI Inference Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Optimizing Multi-Stage AI Inference Pipelines"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads."
                },
                "authors": [
                    {
                        "name": "Abhimanyu Rajeshkumar Bambhaniya"
                    },
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "name": "Sudarshan Srinivasan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Amir Yazdanbakhsh"
                    },
                    {
                        "name": "Midhilesh Elavazhagan"
                    },
                    {
                        "name": "Madhu Kumar"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "arxiv_comment": "Inference System Design for Multi-Stage AI Inference Pipelines. 13\n  Pages, 15 Figues, 3 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12268v1",
                "updated": "2025-04-16T17:30:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    17,
                    30,
                    36,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T17:30:36Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    17,
                    30,
                    36,
                    2,
                    106,
                    0
                ],
                "title": "HLS-Eval: A Benchmark and Framework for Evaluating LLMs on High-Level\n  Synthesis Design Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HLS-Eval: A Benchmark and Framework for Evaluating LLMs on High-Level\n  Synthesis Design Tasks"
                },
                "summary": "The rapid scaling of large language model (LLM) training and inference has\ndriven their adoption in semiconductor design across academia and industry.\nWhile most prior work evaluates LLMs on hardware description language (HDL)\ntasks, particularly Verilog, designers are increasingly using high-level\nsynthesis (HLS) to build domain-specific accelerators and complex hardware\nsystems. However, benchmarks and tooling to comprehensively evaluate LLMs for\nHLS design tasks remain scarce.\n  To address this, we introduce HLS-Eval, the first complete benchmark and\nevaluation framework for LLM-driven HLS design. HLS-Eval targets two core\ntasks: (1) generating HLS code from natural language descriptions, and (2)\nperforming HLS-specific code edits to optimize performance and hardware\nefficiency. The benchmark includes 94 unique designs drawn from standard HLS\nbenchmarks and novel sources. Each case is prepared via a semi-automated flow\nthat produces a natural language description and a paired testbench for\nC-simulation and synthesis validation, ensuring each task is \"LLM-ready.\"\n  Beyond the benchmark, HLS-Eval offers a modular Python framework for\nautomated, parallel evaluation of both local and hosted LLMs. It includes a\nparallel evaluation engine, direct HLS tool integration, and abstractions for\nto support different LLM interaction paradigms, enabling rapid prototyping of\nnew benchmarks, tasks, and LLM methods.\n  We demonstrate HLS-Eval through baseline evaluations of open-source LLMs on\nVitis HLS, measuring outputs across four key metrics - parseability,\ncompilability, runnability, and synthesizability - reflecting the iterative HLS\ndesign cycle. We also report pass@k metrics, establishing clear baselines and\nreusable infrastructure for the broader LLM-for-hardware community.\n  All benchmarks, framework code, and results are open-sourced at\nhttps://github.com/stefanpie/hls-eval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid scaling of large language model (LLM) training and inference has\ndriven their adoption in semiconductor design across academia and industry.\nWhile most prior work evaluates LLMs on hardware description language (HDL)\ntasks, particularly Verilog, designers are increasingly using high-level\nsynthesis (HLS) to build domain-specific accelerators and complex hardware\nsystems. However, benchmarks and tooling to comprehensively evaluate LLMs for\nHLS design tasks remain scarce.\n  To address this, we introduce HLS-Eval, the first complete benchmark and\nevaluation framework for LLM-driven HLS design. HLS-Eval targets two core\ntasks: (1) generating HLS code from natural language descriptions, and (2)\nperforming HLS-specific code edits to optimize performance and hardware\nefficiency. The benchmark includes 94 unique designs drawn from standard HLS\nbenchmarks and novel sources. Each case is prepared via a semi-automated flow\nthat produces a natural language description and a paired testbench for\nC-simulation and synthesis validation, ensuring each task is \"LLM-ready.\"\n  Beyond the benchmark, HLS-Eval offers a modular Python framework for\nautomated, parallel evaluation of both local and hosted LLMs. It includes a\nparallel evaluation engine, direct HLS tool integration, and abstractions for\nto support different LLM interaction paradigms, enabling rapid prototyping of\nnew benchmarks, tasks, and LLM methods.\n  We demonstrate HLS-Eval through baseline evaluations of open-source LLMs on\nVitis HLS, measuring outputs across four key metrics - parseability,\ncompilability, runnability, and synthesizability - reflecting the iterative HLS\ndesign cycle. We also report pass@k metrics, establishing clear baselines and\nreusable infrastructure for the broader LLM-for-hardware community.\n  All benchmarks, framework code, and results are open-sourced at\nhttps://github.com/stefanpie/hls-eval."
                },
                "authors": [
                    {
                        "name": "Stefan Abi-Karam"
                    },
                    {
                        "name": "Cong Hao"
                    }
                ],
                "author_detail": {
                    "name": "Cong Hao"
                },
                "author": "Cong Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12267v1",
                "updated": "2025-04-16T17:28:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    17,
                    28,
                    53,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T17:28:53Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    17,
                    28,
                    53,
                    2,
                    106,
                    0
                ],
                "title": "New Constraints on DMS and DMDS in the Atmosphere of K2-18 b from JWST\n  MIRI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New Constraints on DMS and DMDS in the Atmosphere of K2-18 b from JWST\n  MIRI"
                },
                "summary": "The sub-Neptune frontier has opened a new window into the rich diversity of\nplanetary environments beyond the solar system. The possibility of hycean\nworlds, with planet-wide oceans and H$_2$-rich atmospheres, significantly\nexpands and accelerates the search for habitable environments elsewhere. Recent\nJWST transmission spectroscopy of the candidate hycean world K2-18 b in the\nnear-infrared led to the first detections of carbon-bearing molecules CH$_4$\nand CO$_2$ in its atmosphere, with a composition consistent with predictions\nfor hycean conditions. The observations also provided a tentative hint of\ndimethyl sulfide (DMS), a possible biosignature gas, but the inference was of\nlow statistical significance. We report a mid-infrared transmission spectrum of\nK2-18 b obtained using the JWST MIRI LRS instrument in the ~6-12 $\\mu$m range.\nThe spectrum shows distinct features and is inconsistent with a featureless\nspectrum at 3.4-$\\sigma$ significance compared to our canonical model. We find\nthat the spectrum cannot be explained by most molecules predicted for K2-18 b\nwith the exception of DMS and dimethyl disulfide (DMDS), also a potential\nbiosignature gas. We report new independent evidence for DMS and/or DMDS in the\natmosphere at 3-$\\sigma$ significance, with high abundance ($\\gtrsim$10 ppmv)\nof at least one of the two molecules. More observations are needed to increase\nthe robustness of the findings and resolve the degeneracy between DMS and DMDS.\nThe results also highlight the need for additional experimental and theoretical\nwork to determine accurate cross sections of important biosignature gases and\nidentify potential abiotic sources. We discuss the implications of the present\nfindings for the possibility of biological activity on K2-18 b.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The sub-Neptune frontier has opened a new window into the rich diversity of\nplanetary environments beyond the solar system. The possibility of hycean\nworlds, with planet-wide oceans and H$_2$-rich atmospheres, significantly\nexpands and accelerates the search for habitable environments elsewhere. Recent\nJWST transmission spectroscopy of the candidate hycean world K2-18 b in the\nnear-infrared led to the first detections of carbon-bearing molecules CH$_4$\nand CO$_2$ in its atmosphere, with a composition consistent with predictions\nfor hycean conditions. The observations also provided a tentative hint of\ndimethyl sulfide (DMS), a possible biosignature gas, but the inference was of\nlow statistical significance. We report a mid-infrared transmission spectrum of\nK2-18 b obtained using the JWST MIRI LRS instrument in the ~6-12 $\\mu$m range.\nThe spectrum shows distinct features and is inconsistent with a featureless\nspectrum at 3.4-$\\sigma$ significance compared to our canonical model. We find\nthat the spectrum cannot be explained by most molecules predicted for K2-18 b\nwith the exception of DMS and dimethyl disulfide (DMDS), also a potential\nbiosignature gas. We report new independent evidence for DMS and/or DMDS in the\natmosphere at 3-$\\sigma$ significance, with high abundance ($\\gtrsim$10 ppmv)\nof at least one of the two molecules. More observations are needed to increase\nthe robustness of the findings and resolve the degeneracy between DMS and DMDS.\nThe results also highlight the need for additional experimental and theoretical\nwork to determine accurate cross sections of important biosignature gases and\nidentify potential abiotic sources. We discuss the implications of the present\nfindings for the possibility of biological activity on K2-18 b."
                },
                "authors": [
                    {
                        "name": "Nikku Madhusudhan"
                    },
                    {
                        "name": "Savvas Constantinou"
                    },
                    {
                        "name": "Mns Holmberg"
                    },
                    {
                        "name": "Subhajit Sarkar"
                    },
                    {
                        "name": "Anjali A. A. Piette"
                    },
                    {
                        "name": "Julianne I. Moses"
                    }
                ],
                "author_detail": {
                    "name": "Julianne I. Moses"
                },
                "author": "Julianne I. Moses",
                "arxiv_comment": "Accepted for publication in ApJL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12266v1",
                "updated": "2025-04-16T17:27:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    17,
                    27,
                    52,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T17:27:52Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    17,
                    27,
                    52,
                    2,
                    106,
                    0
                ],
                "title": "Semiparametric Dynamic Copula Models for Portfolio Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semiparametric Dynamic Copula Models for Portfolio Optimization"
                },
                "summary": "The mean-variance portfolio model, based on the risk-return trade-off for\noptimal asset allocation, remains foundational in portfolio optimization.\nHowever, its reliance on restrictive assumptions about asset return\ndistributions limits its applicability to real-world data. Parametric copula\nstructures provide a novel way to overcome these limitations by accounting for\nasymmetry, heavy tails, and time-varying dependencies. Existing methods have\nbeen shown to rely on fixed or static dependence structures, thus overlooking\nthe dynamic nature of the financial market. In this study, a semiparametric\nmodel is proposed that combines non-parametrically estimated copulas with\nparametrically estimated marginals to allow all parameters to dynamically\nevolve over time. A novel framework was developed that integrates time-varying\ndependence modeling with flexible empirical beta copula structures. Marginal\ndistributions were modeled using the Skewed Generalized T family. This\neffectively captures asymmetry and heavy tails and makes the model suitable for\npredictive inferences in real world scenarios. Furthermore, the model was\napplied to rolling windows of financial returns from the USA, India and Hong\nKong economies to understand the influence of dynamic market conditions. The\napproach addresses the limitations of models that rely on parametric\nassumptions. By accounting for asymmetry, heavy tails, and cross-correlated\nasset prices, the proposed method offers a robust solution for optimizing\ndiverse portfolios in an interconnected financial market. Through adaptive\nmodeling, it allows for better management of risk and return across varying\neconomic conditions, leading to more efficient asset allocation and improved\nportfolio performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The mean-variance portfolio model, based on the risk-return trade-off for\noptimal asset allocation, remains foundational in portfolio optimization.\nHowever, its reliance on restrictive assumptions about asset return\ndistributions limits its applicability to real-world data. Parametric copula\nstructures provide a novel way to overcome these limitations by accounting for\nasymmetry, heavy tails, and time-varying dependencies. Existing methods have\nbeen shown to rely on fixed or static dependence structures, thus overlooking\nthe dynamic nature of the financial market. In this study, a semiparametric\nmodel is proposed that combines non-parametrically estimated copulas with\nparametrically estimated marginals to allow all parameters to dynamically\nevolve over time. A novel framework was developed that integrates time-varying\ndependence modeling with flexible empirical beta copula structures. Marginal\ndistributions were modeled using the Skewed Generalized T family. This\neffectively captures asymmetry and heavy tails and makes the model suitable for\npredictive inferences in real world scenarios. Furthermore, the model was\napplied to rolling windows of financial returns from the USA, India and Hong\nKong economies to understand the influence of dynamic market conditions. The\napproach addresses the limitations of models that rely on parametric\nassumptions. By accounting for asymmetry, heavy tails, and cross-correlated\nasset prices, the proposed method offers a robust solution for optimizing\ndiverse portfolios in an interconnected financial market. Through adaptive\nmodeling, it allows for better management of risk and return across varying\neconomic conditions, leading to more efficient asset allocation and improved\nportfolio performance."
                },
                "authors": [
                    {
                        "name": "Savita Pareek"
                    },
                    {
                        "name": "Sujit K. Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Sujit K. Ghosh"
                },
                "author": "Sujit K. Ghosh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.PM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12264v1",
                "updated": "2025-04-16T17:21:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    17,
                    21,
                    55,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T17:21:55Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    17,
                    21,
                    55,
                    2,
                    106,
                    0
                ],
                "title": "Towards Learning to Complete Anything in Lidar",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Learning to Complete Anything in Lidar"
                },
                "summary": "We propose CAL (Complete Anything in Lidar) for Lidar-based shape-completion\nin-the-wild. This is closely related to Lidar-based semantic/panoptic scene\ncompletion. However, contemporary methods can only complete and recognize\nobjects from a closed vocabulary labeled in existing Lidar datasets. Different\nto that, our zero-shot approach leverages the temporal context from multi-modal\nsensor sequences to mine object shapes and semantic features of observed\nobjects. These are then distilled into a Lidar-only instance-level completion\nand recognition model. Although we only mine partial shape completions, we find\nthat our distilled model learns to infer full object shapes from multiple such\npartial observations across the dataset. We show that our model can be prompted\non standard benchmarks for Semantic and Panoptic Scene Completion, localize\nobjects as (amodal) 3D bounding boxes, and recognize objects beyond fixed class\nvocabularies. Our project page is\nhttps://research.nvidia.com/labs/dvl/projects/complete-anything-lidar",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose CAL (Complete Anything in Lidar) for Lidar-based shape-completion\nin-the-wild. This is closely related to Lidar-based semantic/panoptic scene\ncompletion. However, contemporary methods can only complete and recognize\nobjects from a closed vocabulary labeled in existing Lidar datasets. Different\nto that, our zero-shot approach leverages the temporal context from multi-modal\nsensor sequences to mine object shapes and semantic features of observed\nobjects. These are then distilled into a Lidar-only instance-level completion\nand recognition model. Although we only mine partial shape completions, we find\nthat our distilled model learns to infer full object shapes from multiple such\npartial observations across the dataset. We show that our model can be prompted\non standard benchmarks for Semantic and Panoptic Scene Completion, localize\nobjects as (amodal) 3D bounding boxes, and recognize objects beyond fixed class\nvocabularies. Our project page is\nhttps://research.nvidia.com/labs/dvl/projects/complete-anything-lidar"
                },
                "authors": [
                    {
                        "name": "Ayca Takmaz"
                    },
                    {
                        "name": "Cristiano Saltori"
                    },
                    {
                        "name": "Neehar Peri"
                    },
                    {
                        "name": "Tim Meinhardt"
                    },
                    {
                        "name": "Riccardo de Lutio"
                    },
                    {
                        "name": "Laura Leal-Taix"
                    },
                    {
                        "name": "Aljoa Oep"
                    }
                ],
                "author_detail": {
                    "name": "Aljoa Oep"
                },
                "author": "Aljoa Oep",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12256v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12256v1",
                "updated": "2025-04-16T17:07:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    17,
                    7,
                    16,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T17:07:16Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    17,
                    7,
                    16,
                    2,
                    106,
                    0
                ],
                "title": "FLIP Reasoning Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLIP Reasoning Challenge"
                },
                "summary": "Over the past years, advances in artificial intelligence (AI) have\ndemonstrated how AI can solve many perception and generation tasks, such as\nimage classification and text writing, yet reasoning remains a challenge. This\npaper introduces the FLIP dataset, a benchmark for evaluating AI reasoning\ncapabilities based on human verification tasks on the Idena blockchain. FLIP\nchallenges present users with two orderings of 4 images, requiring them to\nidentify the logically coherent one. By emphasizing sequential reasoning,\nvisual storytelling, and common sense, FLIP provides a unique testbed for\nmultimodal AI systems. Our experiments evaluate state-of-the-art models,\nleveraging both vision-language models (VLMs) and large language models (LLMs).\nResults reveal that even the best open-sourced and closed-sourced models\nachieve maximum accuracies of 75.5% and 77.9%, respectively, in zero-shot\nsettings, compared to human performance of 95.3%. Captioning models aid\nreasoning models by providing text descriptions of images, yielding better\nresults than when using the raw images directly, 69.6% vs. 75.2% for Gemini 1.5\nPro. Combining the predictions from 15 models in an ensemble increases the\naccuracy to 85.2%. These findings highlight the limitations of existing\nreasoning models and the need for robust multimodal benchmarks like FLIP. The\nfull codebase and dataset will be available at\nhttps://github.com/aplesner/FLIP-Reasoning-Challenge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past years, advances in artificial intelligence (AI) have\ndemonstrated how AI can solve many perception and generation tasks, such as\nimage classification and text writing, yet reasoning remains a challenge. This\npaper introduces the FLIP dataset, a benchmark for evaluating AI reasoning\ncapabilities based on human verification tasks on the Idena blockchain. FLIP\nchallenges present users with two orderings of 4 images, requiring them to\nidentify the logically coherent one. By emphasizing sequential reasoning,\nvisual storytelling, and common sense, FLIP provides a unique testbed for\nmultimodal AI systems. Our experiments evaluate state-of-the-art models,\nleveraging both vision-language models (VLMs) and large language models (LLMs).\nResults reveal that even the best open-sourced and closed-sourced models\nachieve maximum accuracies of 75.5% and 77.9%, respectively, in zero-shot\nsettings, compared to human performance of 95.3%. Captioning models aid\nreasoning models by providing text descriptions of images, yielding better\nresults than when using the raw images directly, 69.6% vs. 75.2% for Gemini 1.5\nPro. Combining the predictions from 15 models in an ensemble increases the\naccuracy to 85.2%. These findings highlight the limitations of existing\nreasoning models and the need for robust multimodal benchmarks like FLIP. The\nfull codebase and dataset will be available at\nhttps://github.com/aplesner/FLIP-Reasoning-Challenge."
                },
                "authors": [
                    {
                        "name": "Andreas Plesner"
                    },
                    {
                        "name": "Turlan Kuzhagaliyev"
                    },
                    {
                        "name": "Roger Wattenhofer"
                    }
                ],
                "author_detail": {
                    "name": "Roger Wattenhofer"
                },
                "author": "Roger Wattenhofer",
                "arxiv_comment": "Published at First Workshop on Open Science for Foundation Models at\n  ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12256v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12256v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07923v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07923v3",
                "updated": "2025-04-16T16:59:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    59,
                    7,
                    2,
                    106,
                    0
                ],
                "published": "2024-04-11T17:10:08Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    17,
                    10,
                    8,
                    3,
                    102,
                    0
                ],
                "title": "A Bayesian Estimator of Sample Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bayesian Estimator of Sample Size"
                },
                "summary": "We consider a Bayesian estimator of sample size (BESS) and an application to\noncology dose optimization clinical trials. BESS is built upon three pillars,\nSample size, Evidence from observed data, and Confidence in posterior\ninference. It uses a simple logic of \"given the evidence from data, a specific\nsample size can achieve a degree of confidence in the posterior inference.\" The\nkey distinction between BESS and standard sample size estimation (SSE) is that\nSSE, typically based on Frequentist inference, specifies the true parameters\nvalues in its calculation while BESS assumes possible outcome from the observed\ndata. As a result, the calibration of the sample size is not based on type I or\ntype II error rates, but on posterior probabilities. We demonstrate that BESS\nleads to a more interpretable statement for investigators, and can easily\naccommodates prior information as well as sample size re-estimation. We explore\nits performance in comparison to the standard SSE and demonstrate its usage\nthrough a case study of oncology optimization trial. BESS can be applied to\ngeneral hypothesis tests. An R tool is available at\nhttps://ccte.uchicago.edu/BESS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a Bayesian estimator of sample size (BESS) and an application to\noncology dose optimization clinical trials. BESS is built upon three pillars,\nSample size, Evidence from observed data, and Confidence in posterior\ninference. It uses a simple logic of \"given the evidence from data, a specific\nsample size can achieve a degree of confidence in the posterior inference.\" The\nkey distinction between BESS and standard sample size estimation (SSE) is that\nSSE, typically based on Frequentist inference, specifies the true parameters\nvalues in its calculation while BESS assumes possible outcome from the observed\ndata. As a result, the calibration of the sample size is not based on type I or\ntype II error rates, but on posterior probabilities. We demonstrate that BESS\nleads to a more interpretable statement for investigators, and can easily\naccommodates prior information as well as sample size re-estimation. We explore\nits performance in comparison to the standard SSE and demonstrate its usage\nthrough a case study of oncology optimization trial. BESS can be applied to\ngeneral hypothesis tests. An R tool is available at\nhttps://ccte.uchicago.edu/BESS."
                },
                "authors": [
                    {
                        "name": "Dehua Bi"
                    },
                    {
                        "name": "Yuan Ji"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Ji"
                },
                "author": "Yuan Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07923v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07923v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12250v1",
                "updated": "2025-04-16T16:54:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    54,
                    38,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T16:54:38Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    54,
                    38,
                    2,
                    106,
                    0
                ],
                "title": "AnomalyGen: An Automated Semantic Log Sequence Generation Framework with\n  LLM for Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnomalyGen: An Automated Semantic Log Sequence Generation Framework with\n  LLM for Anomaly Detection"
                },
                "summary": "The scarcity of high-quality public log datasets has become a critical\nbottleneck in advancing log-based anomaly detection techniques. Current\ndatasets exhibit three fundamental limitations: (1) incomplete event coverage,\n(2) artificial patterns introduced by static analysis-based generation\nframeworks, and (3) insufficient semantic awareness. To address these\nchallenges, we present AnomalyGen, the first automated log synthesis framework\nspecifically designed for anomaly detection. Our framework introduces a novel\nfour-phase architecture that integrates enhanced program analysis with\nChain-of-Thought reasoning (CoT reasoning), enabling iterative log generation\nand anomaly annotation without requiring physical system execution. Evaluations\non Hadoop and HDFS distributed systems demonstrate that AnomalyGen achieves\nsubstantially broader log event coverage (38-95 times improvement over existing\ndatasets) while producing more operationally realistic log sequences compared\nto static analysis-based approaches. When augmenting benchmark datasets with\nsynthesized logs, we observe maximum F1-score improvements of 3.7% (average\n1.8% improvement across three state-of-the-art anomaly detection models). This\nwork not only establishes a high-quality benchmarking resource for automated\nlog analysis but also pioneers a new paradigm for applying large language\nmodels (LLMs) in software engineering workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of high-quality public log datasets has become a critical\nbottleneck in advancing log-based anomaly detection techniques. Current\ndatasets exhibit three fundamental limitations: (1) incomplete event coverage,\n(2) artificial patterns introduced by static analysis-based generation\nframeworks, and (3) insufficient semantic awareness. To address these\nchallenges, we present AnomalyGen, the first automated log synthesis framework\nspecifically designed for anomaly detection. Our framework introduces a novel\nfour-phase architecture that integrates enhanced program analysis with\nChain-of-Thought reasoning (CoT reasoning), enabling iterative log generation\nand anomaly annotation without requiring physical system execution. Evaluations\non Hadoop and HDFS distributed systems demonstrate that AnomalyGen achieves\nsubstantially broader log event coverage (38-95 times improvement over existing\ndatasets) while producing more operationally realistic log sequences compared\nto static analysis-based approaches. When augmenting benchmark datasets with\nsynthesized logs, we observe maximum F1-score improvements of 3.7% (average\n1.8% improvement across three state-of-the-art anomaly detection models). This\nwork not only establishes a high-quality benchmarking resource for automated\nlog analysis but also pioneers a new paradigm for applying large language\nmodels (LLMs) in software engineering workflows."
                },
                "authors": [
                    {
                        "name": "Xinyu Li"
                    },
                    {
                        "name": "Yingtong Huo"
                    },
                    {
                        "name": "Chenxi Mao"
                    },
                    {
                        "name": "Shiwen Shan"
                    },
                    {
                        "name": "Yuxin Su"
                    },
                    {
                        "name": "Dan Li"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16660v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16660v4",
                "updated": "2025-04-16T16:49:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    49,
                    34,
                    2,
                    106,
                    0
                ],
                "published": "2025-02-23T17:38:10Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    17,
                    38,
                    10,
                    6,
                    54,
                    0
                ],
                "title": "BioMaze: Benchmarking and Enhancing Large Language Models for Biological\n  Pathway Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BioMaze: Benchmarking and Enhancing Large Language Models for Biological\n  Pathway Reasoning"
                },
                "summary": "The applications of large language models (LLMs) in various biological\ndomains have been explored recently, but their reasoning ability in complex\nbiological systems, such as pathways, remains underexplored, which is crucial\nfor predicting biological phenomena, formulating hypotheses, and designing\nexperiments. This work explores the potential of LLMs in pathway reasoning. We\nintroduce BioMaze, a dataset with 5.1K complex pathway problems derived from\nreal research, covering various biological contexts including natural dynamic\nchanges, disturbances, additional intervention conditions, and multi-scale\nresearch targets. Our evaluation of methods such as CoT and graph-augmented\nreasoning, shows that LLMs struggle with pathway reasoning, especially in\nperturbed systems. To address this, we propose PathSeeker, an LLM agent that\nenhances reasoning through interactive subgraph-based navigation, enabling a\nmore effective approach to handling the complexities of biological systems in a\nscientifically aligned manner. The dataset and code are available at\nhttps://github.com/zhao-ht/BioMaze.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The applications of large language models (LLMs) in various biological\ndomains have been explored recently, but their reasoning ability in complex\nbiological systems, such as pathways, remains underexplored, which is crucial\nfor predicting biological phenomena, formulating hypotheses, and designing\nexperiments. This work explores the potential of LLMs in pathway reasoning. We\nintroduce BioMaze, a dataset with 5.1K complex pathway problems derived from\nreal research, covering various biological contexts including natural dynamic\nchanges, disturbances, additional intervention conditions, and multi-scale\nresearch targets. Our evaluation of methods such as CoT and graph-augmented\nreasoning, shows that LLMs struggle with pathway reasoning, especially in\nperturbed systems. To address this, we propose PathSeeker, an LLM agent that\nenhances reasoning through interactive subgraph-based navigation, enabling a\nmore effective approach to handling the complexities of biological systems in a\nscientifically aligned manner. The dataset and code are available at\nhttps://github.com/zhao-ht/BioMaze."
                },
                "authors": [
                    {
                        "name": "Haiteng Zhao"
                    },
                    {
                        "name": "Chang Ma"
                    },
                    {
                        "name": "Fangzhi Xu"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Zhi-Hong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhi-Hong Deng"
                },
                "author": "Zhi-Hong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16660v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16660v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12240v1",
                "updated": "2025-04-16T16:45:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T16:45:19Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cobra: Efficient Line Art COlorization with BRoAder References"
                },
                "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/."
                },
                "authors": [
                    {
                        "name": "Junhao Zhuang"
                    },
                    {
                        "name": "Lingen Li"
                    },
                    {
                        "name": "Xuan Ju"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project page with code: https://zhuang2002.github.io/Cobra/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12234v1",
                "updated": "2025-04-16T16:33:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    33,
                    53,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T16:33:53Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    33,
                    53,
                    2,
                    106,
                    0
                ],
                "title": "MOS: Towards Effective Smart Contract Vulnerability Detection through\n  Mixture-of-Experts Tuning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOS: Towards Effective Smart Contract Vulnerability Detection through\n  Mixture-of-Experts Tuning of Large Language Models"
                },
                "summary": "Smart contract vulnerabilities pose significant security risks to blockchain\nsystems, potentially leading to severe financial losses. Existing methods face\nseveral limitations: (1) Program analysis-based approaches rely on predefined\npatterns, lacking flexibility for new vulnerability types; (2) Deep\nlearning-based methods lack explanations; (3) Large language model-based\napproaches suffer from high false positives. We propose MOS, a smart contract\nvulnerability detection framework based on mixture-of-experts tuning\n(MOE-Tuning) of large language models. First, we conduct continual pre-training\non a large-scale smart contract dataset to provide domain-enhanced\ninitialization. Second, we construct a high-quality MOE-Tuning dataset through\na multi-stage pipeline combining LLM generation and expert verification for\nreliable explanations. Third, we design a vulnerability-aware routing mechanism\nthat activates the most relevant expert networks by analyzing code features and\ntheir matching degree with experts. Finally, we extend the feed-forward layers\ninto multiple parallel expert networks, each specializing in specific\nvulnerability patterns. We employ a dual-objective loss function: one for\noptimizing detection and explanation performance, and another for ensuring\nreasonable distribution of vulnerability types to experts through entropy\ncalculation. Experiments show that MOS significantly outperforms existing\nmethods with average improvements of 6.32% in F1 score and 4.80% in accuracy.\nThe vulnerability explanations achieve positive ratings (scores of 3-4 on a\n4-point scale) of 82.96%, 85.21% and 94.58% for correctness, completeness, and\nconciseness through human and LLM evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart contract vulnerabilities pose significant security risks to blockchain\nsystems, potentially leading to severe financial losses. Existing methods face\nseveral limitations: (1) Program analysis-based approaches rely on predefined\npatterns, lacking flexibility for new vulnerability types; (2) Deep\nlearning-based methods lack explanations; (3) Large language model-based\napproaches suffer from high false positives. We propose MOS, a smart contract\nvulnerability detection framework based on mixture-of-experts tuning\n(MOE-Tuning) of large language models. First, we conduct continual pre-training\non a large-scale smart contract dataset to provide domain-enhanced\ninitialization. Second, we construct a high-quality MOE-Tuning dataset through\na multi-stage pipeline combining LLM generation and expert verification for\nreliable explanations. Third, we design a vulnerability-aware routing mechanism\nthat activates the most relevant expert networks by analyzing code features and\ntheir matching degree with experts. Finally, we extend the feed-forward layers\ninto multiple parallel expert networks, each specializing in specific\nvulnerability patterns. We employ a dual-objective loss function: one for\noptimizing detection and explanation performance, and another for ensuring\nreasonable distribution of vulnerability types to experts through entropy\ncalculation. Experiments show that MOS significantly outperforms existing\nmethods with average improvements of 6.32% in F1 score and 4.80% in accuracy.\nThe vulnerability explanations achieve positive ratings (scores of 3-4 on a\n4-point scale) of 82.96%, 85.21% and 94.58% for correctness, completeness, and\nconciseness through human and LLM evaluation."
                },
                "authors": [
                    {
                        "name": "Hang Yuan"
                    },
                    {
                        "name": "Lei Yu"
                    },
                    {
                        "name": "Zhirong Huang"
                    },
                    {
                        "name": "Jingyuan Zhang"
                    },
                    {
                        "name": "Junyi Lu"
                    },
                    {
                        "name": "Shiqi Cheng"
                    },
                    {
                        "name": "Li Yang"
                    },
                    {
                        "name": "Fengjun Zhang"
                    },
                    {
                        "name": "Jiajia Ma"
                    },
                    {
                        "name": "Chun Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Chun Zuo"
                },
                "author": "Chun Zuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12229v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12229v1",
                "updated": "2025-04-16T16:25:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    25,
                    26,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T16:25:26Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    25,
                    26,
                    2,
                    106,
                    0
                ],
                "title": "Watermarking Needs Input Repetition Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking Needs Input Repetition Masking"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) raised concerns over\npotential misuse, such as for spreading misinformation. In response two counter\nmeasures emerged: machine learning-based detectors that predict if text is\nsynthetic, and LLM watermarking, which subtly marks generated text for\nidentification and attribution. Meanwhile, humans are known to adjust language\nto their conversational partners both syntactically and lexically. By\nimplication, it is possible that humans or unwatermarked LLMs could\nunintentionally mimic properties of LLM generated text, making counter measures\nunreliable. In this work we investigate the extent to which such conversational\nadaptation happens. We call the concept $\\textit{mimicry}$ and demonstrate that\nboth humans and LLMs end up mimicking, including the watermarking signal even\nin seemingly improbable settings. This challenges current academic assumptions\nand suggests that for long-term watermarking to be reliable, the likelihood of\nfalse positives needs to be significantly lower, while longer word sequences\nshould be used for seeding watermarking mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) raised concerns over\npotential misuse, such as for spreading misinformation. In response two counter\nmeasures emerged: machine learning-based detectors that predict if text is\nsynthetic, and LLM watermarking, which subtly marks generated text for\nidentification and attribution. Meanwhile, humans are known to adjust language\nto their conversational partners both syntactically and lexically. By\nimplication, it is possible that humans or unwatermarked LLMs could\nunintentionally mimic properties of LLM generated text, making counter measures\nunreliable. In this work we investigate the extent to which such conversational\nadaptation happens. We call the concept $\\textit{mimicry}$ and demonstrate that\nboth humans and LLMs end up mimicking, including the watermarking signal even\nin seemingly improbable settings. This challenges current academic assumptions\nand suggests that for long-term watermarking to be reliable, the likelihood of\nfalse positives needs to be significantly lower, while longer word sequences\nshould be used for seeding watermarking mechanisms."
                },
                "authors": [
                    {
                        "name": "David Khachaturov"
                    },
                    {
                        "name": "Robert Mullins"
                    },
                    {
                        "name": "Ilia Shumailov"
                    },
                    {
                        "name": "Sumanth Dathathri"
                    }
                ],
                "author_detail": {
                    "name": "Sumanth Dathathri"
                },
                "author": "Sumanth Dathathri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12229v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12229v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17404v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17404v3",
                "updated": "2025-04-16T16:21:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    21,
                    29,
                    2,
                    106,
                    0
                ],
                "published": "2024-11-26T13:05:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    5,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical\n  Modeling Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical\n  Modeling Problem Solving"
                },
                "summary": "LLMs exhibit advanced reasoning capabilities, offering the potential to\ntransform natural language questions into mathematical models. However,\nexisting open-source datasets in operations research domain lack detailed\nannotations of the modeling process, such as variable definitions, focusing\nsolely on objective values, which hinders reinforcement learning applications.\nTo address this, we release the StructuredOR dataset, annotated with\ncomprehensive labels that capture the complete mathematical modeling process.\nWe further propose BPP-Search, an algorithm that integrates reinforcement\nlearning into a tree-of-thought structure using Beam search, a Process reward\nmodel, and a pairwise Preference algorithm. This approach enables efficient\nexploration of tree structures, avoiding exhaustive search while improving\naccuracy. Extensive experiments on StructuredOR, NL4OPT, and MAMO-ComplexLP\ndatasets show that BPP-Search significantly outperforms state-of-the-art\nmethods. In tree-based reasoning, BPP-Search excels in accuracy and efficiency,\nenabling faster retrieval of correct solutions. The StructuredOR dataset is\navailable at https://github.com/tengwang0318/StructuredOR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs exhibit advanced reasoning capabilities, offering the potential to\ntransform natural language questions into mathematical models. However,\nexisting open-source datasets in operations research domain lack detailed\nannotations of the modeling process, such as variable definitions, focusing\nsolely on objective values, which hinders reinforcement learning applications.\nTo address this, we release the StructuredOR dataset, annotated with\ncomprehensive labels that capture the complete mathematical modeling process.\nWe further propose BPP-Search, an algorithm that integrates reinforcement\nlearning into a tree-of-thought structure using Beam search, a Process reward\nmodel, and a pairwise Preference algorithm. This approach enables efficient\nexploration of tree structures, avoiding exhaustive search while improving\naccuracy. Extensive experiments on StructuredOR, NL4OPT, and MAMO-ComplexLP\ndatasets show that BPP-Search significantly outperforms state-of-the-art\nmethods. In tree-based reasoning, BPP-Search excels in accuracy and efficiency,\nenabling faster retrieval of correct solutions. The StructuredOR dataset is\navailable at https://github.com/tengwang0318/StructuredOR."
                },
                "authors": [
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Wing-Yin Yu"
                    },
                    {
                        "name": "Zhenqi He"
                    },
                    {
                        "name": "Zehua Liu"
                    },
                    {
                        "name": "Hailei Gong"
                    },
                    {
                        "name": "Han Wu"
                    },
                    {
                        "name": "Xiongwei Han"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Ruifeng She"
                    },
                    {
                        "name": "Fangzhou Zhu"
                    },
                    {
                        "name": "Tao Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Tao Zhong"
                },
                "author": "Tao Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17404v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17404v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12216v1",
                "updated": "2025-04-16T16:08:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    8,
                    45,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T16:08:45Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    8,
                    45,
                    2,
                    106,
                    0
                ],
                "title": "d1: Scaling Reasoning in Diffusion Large Language Models via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "d1: Scaling Reasoning in Diffusion Large Language Models via\n  Reinforcement Learning"
                },
                "summary": "Recent large language models (LLMs) have demonstrated strong reasoning\ncapabilities that benefits from online reinforcement learning (RL). These\ncapabilities have primarily been demonstrated within the left-to-right\nautoregressive (AR) generation paradigm. In contrast, non-autoregressive\nparadigms based on diffusion generate text in a coarse-to-fine manner. Although\nrecent diffusion-based large language models (dLLMs) have achieved competitive\nlanguage modeling performance compared to their AR counterparts, it remains\nunclear if dLLMs can also leverage recent advances in LLM reasoning. To this\nend, we propose d1, a framework to adapt pre-trained masked dLLMs into\nreasoning models via a combination of supervised finetuning (SFT) and RL.\nSpecifically, we develop and extend techniques to improve reasoning in\npretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge\nand instill self-improvement behavior directly from existing datasets, and (b)\nwe introduce a novel critic-free, policy-gradient based RL algorithm called\ndiffu-GRPO. Through empirical studies, we investigate the performance of\ndifferent post-training recipes on multiple mathematical and logical reasoning\nbenchmarks. We find that d1 yields the best performance and significantly\nimproves performance of a state-of-the-art dLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) have demonstrated strong reasoning\ncapabilities that benefits from online reinforcement learning (RL). These\ncapabilities have primarily been demonstrated within the left-to-right\nautoregressive (AR) generation paradigm. In contrast, non-autoregressive\nparadigms based on diffusion generate text in a coarse-to-fine manner. Although\nrecent diffusion-based large language models (dLLMs) have achieved competitive\nlanguage modeling performance compared to their AR counterparts, it remains\nunclear if dLLMs can also leverage recent advances in LLM reasoning. To this\nend, we propose d1, a framework to adapt pre-trained masked dLLMs into\nreasoning models via a combination of supervised finetuning (SFT) and RL.\nSpecifically, we develop and extend techniques to improve reasoning in\npretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge\nand instill self-improvement behavior directly from existing datasets, and (b)\nwe introduce a novel critic-free, policy-gradient based RL algorithm called\ndiffu-GRPO. Through empirical studies, we investigate the performance of\ndifferent post-training recipes on multiple mathematical and logical reasoning\nbenchmarks. We find that d1 yields the best performance and significantly\nimproves performance of a state-of-the-art dLLM."
                },
                "authors": [
                    {
                        "name": "Siyan Zhao"
                    },
                    {
                        "name": "Devaansh Gupta"
                    },
                    {
                        "name": "Qinqing Zheng"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "25 pages, project page at https://dllm-reasoning.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05813v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05813v2",
                "updated": "2025-04-16T16:02:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    2,
                    35,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-08T08:53:38Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    53,
                    38,
                    1,
                    98,
                    0
                ],
                "title": "A new approach for simulating PBH formation from generic curvature\n  fluctuations with the Misner-Sharp formalism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new approach for simulating PBH formation from generic curvature\n  fluctuations with the Misner-Sharp formalism"
                },
                "summary": "Primordial Black Holes (PBHs) may have formed in the early Universe due to\nthe collapse of super-horizon curvature fluctuations. Simulations of PBH\nformation have been essential for inferring the initial conditions that lead to\nblack hole formation and for studying their properties and impact on our\nUniverse. The Misner-Sharp formalism is commonly used as a standard approach\nfor these simulations. Recently, type-II fluctuations, characterized by a\nnon-monotonic areal radius, have gained interest. In the standard Misner-Sharp\napproach for simulating PBH formation with these fluctuations, the evolution\nequations exhibit divergent terms ($0/0$), which complicate and prevent the\nsimulations. We formulate a new approach to overcome this issue in a simple\nmanner by using the trace of the extrinsic curvature as an auxiliary variable,\nallowing simulations of type-II fluctuations within the Misner-Sharp formalism.\nUsing a set of standard exponential-shaped curvature profiles, we apply our new\napproach and numerical code based on pseudospectral methods to study the time\nevolution of the gravitational collapse, threshold values of type A/B PBHs and\nPBH mass. Interestingly, we identify cases of type-II fluctuations that do not\nnecessarily result in PBH formation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Primordial Black Holes (PBHs) may have formed in the early Universe due to\nthe collapse of super-horizon curvature fluctuations. Simulations of PBH\nformation have been essential for inferring the initial conditions that lead to\nblack hole formation and for studying their properties and impact on our\nUniverse. The Misner-Sharp formalism is commonly used as a standard approach\nfor these simulations. Recently, type-II fluctuations, characterized by a\nnon-monotonic areal radius, have gained interest. In the standard Misner-Sharp\napproach for simulating PBH formation with these fluctuations, the evolution\nequations exhibit divergent terms ($0/0$), which complicate and prevent the\nsimulations. We formulate a new approach to overcome this issue in a simple\nmanner by using the trace of the extrinsic curvature as an auxiliary variable,\nallowing simulations of type-II fluctuations within the Misner-Sharp formalism.\nUsing a set of standard exponential-shaped curvature profiles, we apply our new\napproach and numerical code based on pseudospectral methods to study the time\nevolution of the gravitational collapse, threshold values of type A/B PBHs and\nPBH mass. Interestingly, we identify cases of type-II fluctuations that do not\nnecessarily result in PBH formation."
                },
                "authors": [
                    {
                        "name": "Albert Escriv"
                    }
                ],
                "author_detail": {
                    "name": "Albert Escriv"
                },
                "author": "Albert Escriv",
                "arxiv_comment": "27 pages and 7 figures. v2: small typos corrected and reference\n  added. Basic version of the numerical code publicly available here:\n  https://github.com/albert-escriva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05813v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05813v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12144v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12144v3",
                "updated": "2025-04-16T15:53:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    15,
                    53,
                    3,
                    2,
                    106,
                    0
                ],
                "published": "2024-12-10T09:13:32Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    13,
                    32,
                    1,
                    345,
                    0
                ],
                "title": "Automatic Item Generation for Personality Situational Judgment Tests\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Item Generation for Personality Situational Judgment Tests\n  with Large Language Models"
                },
                "summary": "Personality assessment, particularly through situational judgment tests\n(SJTs), is a vital tool for psychological research, talent selection, and\neducational evaluation. This study explores the potential of GPT-4, a\nstate-of-the-art large language model (LLM), to automate the generation of\npersonality situational judgment tests (PSJTs) in Chinese. Traditional SJT\ndevelopment is labor-intensive and prone to biases, while GPT-4 offers a\nscalable, efficient alternative. Two studies were conducted: Study 1 evaluated\nthe impact of prompt design and temperature settings on content validity,\nfinding that optimized prompts with a temperature of 1.0 produced creative and\naccurate items. Study 2 assessed the psychometric properties of GPT-4-generated\nPSJTs, revealing that they demonstrated satisfactory reliability and validity,\nsurpassing the performance of manually developed tests in measuring the Big\nFive personality traits. This research highlights GPT-4's effectiveness in\ndeveloping high-quality PSJTs, providing a scalable and innovative method for\npsychometric test development. These findings expand the possibilities of\nautomatic item generation and the application of LLMs in psychology, and offer\npractical implications for streamlining test development processes in\nresource-limited settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personality assessment, particularly through situational judgment tests\n(SJTs), is a vital tool for psychological research, talent selection, and\neducational evaluation. This study explores the potential of GPT-4, a\nstate-of-the-art large language model (LLM), to automate the generation of\npersonality situational judgment tests (PSJTs) in Chinese. Traditional SJT\ndevelopment is labor-intensive and prone to biases, while GPT-4 offers a\nscalable, efficient alternative. Two studies were conducted: Study 1 evaluated\nthe impact of prompt design and temperature settings on content validity,\nfinding that optimized prompts with a temperature of 1.0 produced creative and\naccurate items. Study 2 assessed the psychometric properties of GPT-4-generated\nPSJTs, revealing that they demonstrated satisfactory reliability and validity,\nsurpassing the performance of manually developed tests in measuring the Big\nFive personality traits. This research highlights GPT-4's effectiveness in\ndeveloping high-quality PSJTs, providing a scalable and innovative method for\npsychometric test development. These findings expand the possibilities of\nautomatic item generation and the application of LLMs in psychology, and offer\npractical implications for streamlining test development processes in\nresource-limited settings."
                },
                "authors": [
                    {
                        "name": "Chang-Jin Li"
                    },
                    {
                        "name": "Jiyuan Zhang"
                    },
                    {
                        "name": "Yun Tang"
                    },
                    {
                        "name": "Jian Li"
                    }
                ],
                "author_detail": {
                    "name": "Jian Li"
                },
                "author": "Jian Li",
                "arxiv_comment": "Submitted to Psychological Methods. 56 pages (main text), 12 pages\n  (appendix), and 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12144v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12144v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12192v1",
                "updated": "2025-04-16T15:46:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    15,
                    46,
                    56,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T15:46:56Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    15,
                    46,
                    56,
                    2,
                    106,
                    0
                ],
                "title": "From Requirements to Architecture: Semi-Automatically Generating\n  Software Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Requirements to Architecture: Semi-Automatically Generating\n  Software Architectures"
                },
                "summary": "To support junior and senior architects, I propose developing a new\narchitecture creation method that leverages LLMs' evolving capabilities to\nsupport the architect. This method involves the architect's close collaboration\nwith LLM-fueled tooling over the whole process. The architect is guided through\nDomain Model creation, Use Case specification, architectural decisions, and\narchitecture evaluation. While the architect can take complete control of the\nprocess and the results, and use the tooling as a building set, they can follow\nthe intended process for maximum tooling support. The preliminary results\nsuggest the feasibility of this process and indicate major time savings for the\narchitect.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To support junior and senior architects, I propose developing a new\narchitecture creation method that leverages LLMs' evolving capabilities to\nsupport the architect. This method involves the architect's close collaboration\nwith LLM-fueled tooling over the whole process. The architect is guided through\nDomain Model creation, Use Case specification, architectural decisions, and\narchitecture evaluation. While the architect can take complete control of the\nprocess and the results, and use the tooling as a building set, they can follow\nthe intended process for maximum tooling support. The preliminary results\nsuggest the feasibility of this process and indicate major time savings for the\narchitect."
                },
                "authors": [
                    {
                        "name": "Tobias Eisenreich"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Eisenreich"
                },
                "author": "Tobias Eisenreich",
                "arxiv_comment": "to be published in EMISA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12187v1",
                "updated": "2025-04-16T15:42:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    15,
                    42,
                    33,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T15:42:33Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    15,
                    42,
                    33,
                    2,
                    106,
                    0
                ],
                "title": "What Do Large Language Models Know? Tacit Knowledge as a Potential\n  Causal-Explanatory Structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Do Large Language Models Know? Tacit Knowledge as a Potential\n  Causal-Explanatory Structure"
                },
                "summary": "It is sometimes assumed that Large Language Models (LLMs) know language, or\nfor example that they know that Paris is the capital of France. But what -- if\nanything -- do LLMs actually know? In this paper, I argue that LLMs can acquire\ntacit knowledge as defined by Martin Davies (1990). Whereas Davies himself\ndenies that neural networks can acquire tacit knowledge, I demonstrate that\ncertain architectural features of LLMs satisfy the constraints of semantic\ndescription, syntactic structure, and causal systematicity. Thus, tacit\nknowledge may serve as a conceptual framework for describing, explaining, and\nintervening on LLMs and their behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is sometimes assumed that Large Language Models (LLMs) know language, or\nfor example that they know that Paris is the capital of France. But what -- if\nanything -- do LLMs actually know? In this paper, I argue that LLMs can acquire\ntacit knowledge as defined by Martin Davies (1990). Whereas Davies himself\ndenies that neural networks can acquire tacit knowledge, I demonstrate that\ncertain architectural features of LLMs satisfy the constraints of semantic\ndescription, syntactic structure, and causal systematicity. Thus, tacit\nknowledge may serve as a conceptual framework for describing, explaining, and\nintervening on LLMs and their behavior."
                },
                "authors": [
                    {
                        "name": "Cline Budding"
                    }
                ],
                "author_detail": {
                    "name": "Cline Budding"
                },
                "author": "Cline Budding",
                "arxiv_doi": "10.1017/psa.2025.19",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1017/psa.2025.19",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.12187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication in Philosophy of Science",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12185v1",
                "updated": "2025-04-16T15:40:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    15,
                    40,
                    10,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T15:40:10Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    15,
                    40,
                    10,
                    2,
                    106,
                    0
                ],
                "title": "SALAD: Improving Robustness and Generalization through Contrastive\n  Learning with Structure-Aware and LLM-Driven Augmented Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALAD: Improving Robustness and Generalization through Contrastive\n  Learning with Structure-Aware and LLM-Driven Augmented Data"
                },
                "summary": "In various natural language processing (NLP) tasks, fine-tuning Pre-trained\nLanguage Models (PLMs) often leads to the issue of spurious correlations, which\nnegatively impacts performance, particularly when dealing with\nout-of-distribution data. To address this problem, we propose SALAD}(Structure\nAware and LLM-driven Augmented Data), a novel approach designed to enhance\nmodel robustness and generalization by generating structure-aware and\ncounterfactually augmented data for contrastive learning. Our method leverages\na tagging-based approach to generate structure-aware positive samples and\nutilizes large language models (LLMs) to generate counterfactual negative\nsamples with diverse sentence patterns. By applying contrastive learning, SALAD\nenables the model to focus on learning the structural relationships between key\nsentence components while minimizing reliance on spurious correlations. We\nvalidate our approach through experiments on three tasks: Sentiment\nClassification, Sexism Detection, and Natural Language Inference. The results\ndemonstrate that SALAD not only improves model robustness and performance\nacross different environments but also enhances generalization to\nout-of-distribution datasets and cross-domain scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In various natural language processing (NLP) tasks, fine-tuning Pre-trained\nLanguage Models (PLMs) often leads to the issue of spurious correlations, which\nnegatively impacts performance, particularly when dealing with\nout-of-distribution data. To address this problem, we propose SALAD}(Structure\nAware and LLM-driven Augmented Data), a novel approach designed to enhance\nmodel robustness and generalization by generating structure-aware and\ncounterfactually augmented data for contrastive learning. Our method leverages\na tagging-based approach to generate structure-aware positive samples and\nutilizes large language models (LLMs) to generate counterfactual negative\nsamples with diverse sentence patterns. By applying contrastive learning, SALAD\nenables the model to focus on learning the structural relationships between key\nsentence components while minimizing reliance on spurious correlations. We\nvalidate our approach through experiments on three tasks: Sentiment\nClassification, Sexism Detection, and Natural Language Inference. The results\ndemonstrate that SALAD not only improves model robustness and performance\nacross different environments but also enhances generalization to\nout-of-distribution datasets and cross-domain scenarios."
                },
                "authors": [
                    {
                        "name": "Suyoung Bae"
                    },
                    {
                        "name": "Hyojun Kim"
                    },
                    {
                        "name": "YunSeok Choi"
                    },
                    {
                        "name": "Jee-Hyong Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jee-Hyong Lee"
                },
                "author": "Jee-Hyong Lee",
                "arxiv_comment": "Accepted to NAACL 2025 main. 15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11168v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11168v2",
                "updated": "2025-04-16T15:33:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    15,
                    33,
                    6,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-15T13:16:02Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    16,
                    2,
                    1,
                    105,
                    0
                ],
                "title": "Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails"
                },
                "summary": "Large Language Models (LLMs) guardrail systems are designed to protect\nagainst prompt injection and jailbreak attacks. However, they remain vulnerable\nto evasion techniques. We demonstrate two approaches for bypassing LLM prompt\ninjection and jailbreak detection systems via traditional character injection\nmethods and algorithmic Adversarial Machine Learning (AML) evasion techniques.\nThrough testing against six prominent protection systems, including Microsoft's\nAzure Prompt Shield and Meta's Prompt Guard, we show that both methods can be\nused to evade detection while maintaining adversarial utility achieving in some\ninstances up to 100% evasion success. Furthermore, we demonstrate that\nadversaries can enhance Attack Success Rates (ASR) against black-box targets by\nleveraging word importance ranking computed by offline white-box models. Our\nfindings reveal vulnerabilities within current LLM protection mechanisms and\nhighlight the need for more robust guardrail systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) guardrail systems are designed to protect\nagainst prompt injection and jailbreak attacks. However, they remain vulnerable\nto evasion techniques. We demonstrate two approaches for bypassing LLM prompt\ninjection and jailbreak detection systems via traditional character injection\nmethods and algorithmic Adversarial Machine Learning (AML) evasion techniques.\nThrough testing against six prominent protection systems, including Microsoft's\nAzure Prompt Shield and Meta's Prompt Guard, we show that both methods can be\nused to evade detection while maintaining adversarial utility achieving in some\ninstances up to 100% evasion success. Furthermore, we demonstrate that\nadversaries can enhance Attack Success Rates (ASR) against black-box targets by\nleveraging word importance ranking computed by offline white-box models. Our\nfindings reveal vulnerabilities within current LLM protection mechanisms and\nhighlight the need for more robust guardrail systems."
                },
                "authors": [
                    {
                        "name": "William Hackett"
                    },
                    {
                        "name": "Lewis Birch"
                    },
                    {
                        "name": "Stefan Trawicki"
                    },
                    {
                        "name": "Neeraj Suri"
                    },
                    {
                        "name": "Peter Garraghan"
                    }
                ],
                "author_detail": {
                    "name": "Peter Garraghan"
                },
                "author": "Peter Garraghan",
                "arxiv_comment": "12 pages, 5 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11168v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11168v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09752v2",
                "updated": "2025-04-16T15:30:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    15,
                    30,
                    14,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-13T23:07:20Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    23,
                    7,
                    20,
                    6,
                    103,
                    0
                ],
                "title": "Deciphering Sub-Neptune Atmospheres: New Insights from Geochemical\n  Models of TOI-270 d",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deciphering Sub-Neptune Atmospheres: New Insights from Geochemical\n  Models of TOI-270 d"
                },
                "summary": "The nature of sub-Neptunes is one of the hottest topics in exoplanetary\nscience. Temperate sub-Neptunes are of special interest because some could be\nhabitable. Here, we consider whether these planets might instead be rocky\nworlds with thick, hot atmospheres. Can recent JWST observations of TOI-270 d\nbe understood in terms of such a model? We perform thermochemical equilibrium\ncalculations to infer conditions of quenching of C-H-O-N species. Our results\nindicate apparent CO$_2$-CH$_4$ equilibrium between ~900 and ~1100 K. The CO\nabundance should be quenched higher in the atmosphere where the equilibrium\nCO/CO$_2$ ratio is lower, potentially explaining a lack of CO. N$_2$ is\npredicted to dominate the nitrogen budget. We confirm that the atmosphere of\nTOI-270 d is strongly enriched in both C and O$_g$$_a$$_s$ relative to\nprotosolar H, whereas N is likely to be less enriched or even depleted. We\nattempt to reproduce these enrichments by modeling the atmosphere as nebular\ngas that extracted heavy elements from accreted solids. This type of model can\nexplain the C/H and O$_g$$_a$$_s$/H ratios, but despite supersolar C/N ratios\nprovided by solids, the NH$_3$ abundance will probably be too high unless there\nis a nitrogen sink in addition to N$_2$. A magma ocean may be implied, and\nindeed the oxygen fugacity of the deep atmosphere seems sufficiently low to\nsupport the sequestration of reduced N in silicate melt. The evaluation\npresented here demonstrates that exoplanetary geochemistry now approaches a\nlevel of sophistication comparable to that achieved within our own solar\nsystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nature of sub-Neptunes is one of the hottest topics in exoplanetary\nscience. Temperate sub-Neptunes are of special interest because some could be\nhabitable. Here, we consider whether these planets might instead be rocky\nworlds with thick, hot atmospheres. Can recent JWST observations of TOI-270 d\nbe understood in terms of such a model? We perform thermochemical equilibrium\ncalculations to infer conditions of quenching of C-H-O-N species. Our results\nindicate apparent CO$_2$-CH$_4$ equilibrium between ~900 and ~1100 K. The CO\nabundance should be quenched higher in the atmosphere where the equilibrium\nCO/CO$_2$ ratio is lower, potentially explaining a lack of CO. N$_2$ is\npredicted to dominate the nitrogen budget. We confirm that the atmosphere of\nTOI-270 d is strongly enriched in both C and O$_g$$_a$$_s$ relative to\nprotosolar H, whereas N is likely to be less enriched or even depleted. We\nattempt to reproduce these enrichments by modeling the atmosphere as nebular\ngas that extracted heavy elements from accreted solids. This type of model can\nexplain the C/H and O$_g$$_a$$_s$/H ratios, but despite supersolar C/N ratios\nprovided by solids, the NH$_3$ abundance will probably be too high unless there\nis a nitrogen sink in addition to N$_2$. A magma ocean may be implied, and\nindeed the oxygen fugacity of the deep atmosphere seems sufficiently low to\nsupport the sequestration of reduced N in silicate melt. The evaluation\npresented here demonstrates that exoplanetary geochemistry now approaches a\nlevel of sophistication comparable to that achieved within our own solar\nsystem."
                },
                "authors": [
                    {
                        "name": "Christopher R. Glein"
                    },
                    {
                        "name": "Xinting Yu"
                    },
                    {
                        "name": "Cindy N. Luu"
                    }
                ],
                "author_detail": {
                    "name": "Cindy N. Luu"
                },
                "author": "Cindy N. Luu",
                "arxiv_comment": "Accepted by ApJ, 49 pages, 11 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04285v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04285v3",
                "updated": "2025-04-16T15:24:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    15,
                    24,
                    31,
                    2,
                    106,
                    0
                ],
                "published": "2025-01-08T05:17:09Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    5,
                    17,
                    9,
                    2,
                    8,
                    0
                ],
                "title": "Separate Source Channel Coding Is Still What You Need: An LLM-based\n  Rethinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Separate Source Channel Coding Is Still What You Need: An LLM-based\n  Rethinking"
                },
                "summary": "Along with the proliferating research interest in Semantic Communication\n(SemCom), Joint Source Channel Coding (JSCC) has dominated the attention due to\nthe widely assumed existence in efficiently delivering information semantics.\n%has emerged as a pivotal area of research, aiming to enhance the efficiency\nand reliability of information transmission through deep learning-based\nmethods. Nevertheless, this paper challenges the conventional JSCC paradigm,\nand advocates for adoption of Separate Source Channel Coding (SSCC) to enjoy\nthe underlying more degree of freedom for optimization. We demonstrate that\nSSCC, after leveraging the strengths of Large Language Model (LLM) for source\ncoding and Error Correction Code Transformer (ECCT) complemented for channel\ndecoding, offers superior performance over JSCC. Our proposed framework also\neffectively highlights the compatibility challenges between SemCom approaches\nand digital communication systems, particularly concerning the resource costs\nassociated with the transmission of high precision floating point numbers.\nThrough comprehensive evaluations, we establish that empowered by LLM-based\ncompression and ECCT-enhanced error correction, SSCC remains a viable and\neffective solution for modern communication systems. In other words, separate\nsource and channel coding is still what we need!",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Along with the proliferating research interest in Semantic Communication\n(SemCom), Joint Source Channel Coding (JSCC) has dominated the attention due to\nthe widely assumed existence in efficiently delivering information semantics.\n%has emerged as a pivotal area of research, aiming to enhance the efficiency\nand reliability of information transmission through deep learning-based\nmethods. Nevertheless, this paper challenges the conventional JSCC paradigm,\nand advocates for adoption of Separate Source Channel Coding (SSCC) to enjoy\nthe underlying more degree of freedom for optimization. We demonstrate that\nSSCC, after leveraging the strengths of Large Language Model (LLM) for source\ncoding and Error Correction Code Transformer (ECCT) complemented for channel\ndecoding, offers superior performance over JSCC. Our proposed framework also\neffectively highlights the compatibility challenges between SemCom approaches\nand digital communication systems, particularly concerning the resource costs\nassociated with the transmission of high precision floating point numbers.\nThrough comprehensive evaluations, we establish that empowered by LLM-based\ncompression and ECCT-enhanced error correction, SSCC remains a viable and\neffective solution for modern communication systems. In other words, separate\nsource and channel coding is still what we need!"
                },
                "authors": [
                    {
                        "name": "Tianqi Ren"
                    },
                    {
                        "name": "Rongpeng Li"
                    },
                    {
                        "name": "Ming-min Zhao"
                    },
                    {
                        "name": "Xianfu Chen"
                    },
                    {
                        "name": "Guangyi Liu"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Zhifeng Zhao"
                    },
                    {
                        "name": "Honggang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Honggang Zhang"
                },
                "author": "Honggang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04285v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04285v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09063v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09063v2",
                "updated": "2025-04-16T15:15:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    15,
                    15,
                    56,
                    2,
                    106,
                    0
                ],
                "published": "2024-02-14T10:20:03Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    10,
                    20,
                    3,
                    2,
                    45,
                    0
                ],
                "title": "Soft Prompt Threats: Attacking Safety Alignment and Unlearning in\n  Open-Source LLMs through the Embedding Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft Prompt Threats: Attacking Safety Alignment and Unlearning in\n  Open-Source LLMs through the Embedding Space"
                },
                "summary": "Current research in adversarial robustness of LLMs focuses on discrete input\nmanipulations in the natural language space, which can be directly transferred\nto closed-source models. However, this approach neglects the steady progression\nof open-source models. As open-source models advance in capability, ensuring\ntheir safety also becomes increasingly imperative. Yet, attacks tailored to\nopen-source LLMs that exploit full model access remain largely unexplored. We\naddress this research gap and propose the embedding space attack, which\ndirectly attacks the continuous embedding representation of input tokens. We\nfind that embedding space attacks circumvent model alignments and trigger\nharmful behaviors more efficiently than discrete attacks or model fine-tuning.\nFurthermore, we present a novel threat model in the context of unlearning and\nshow that embedding space attacks can extract supposedly deleted information\nfrom unlearned LLMs across multiple datasets and models. Our findings highlight\nembedding space attacks as an important threat model in open-source LLMs.\nTrigger Warning: the appendix contains LLM-generated text with violence and\nharassment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current research in adversarial robustness of LLMs focuses on discrete input\nmanipulations in the natural language space, which can be directly transferred\nto closed-source models. However, this approach neglects the steady progression\nof open-source models. As open-source models advance in capability, ensuring\ntheir safety also becomes increasingly imperative. Yet, attacks tailored to\nopen-source LLMs that exploit full model access remain largely unexplored. We\naddress this research gap and propose the embedding space attack, which\ndirectly attacks the continuous embedding representation of input tokens. We\nfind that embedding space attacks circumvent model alignments and trigger\nharmful behaviors more efficiently than discrete attacks or model fine-tuning.\nFurthermore, we present a novel threat model in the context of unlearning and\nshow that embedding space attacks can extract supposedly deleted information\nfrom unlearned LLMs across multiple datasets and models. Our findings highlight\nembedding space attacks as an important threat model in open-source LLMs.\nTrigger Warning: the appendix contains LLM-generated text with violence and\nharassment."
                },
                "authors": [
                    {
                        "name": "Leo Schwinn"
                    },
                    {
                        "name": "David Dobre"
                    },
                    {
                        "name": "Sophie Xhonneux"
                    },
                    {
                        "name": "Gauthier Gidel"
                    },
                    {
                        "name": "Stephan Gunnemann"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Gunnemann"
                },
                "author": "Stephan Gunnemann",
                "arxiv_comment": "Trigger Warning: the appendix contains LLM-generated text with\n  violence and harassment",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09063v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09063v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01533v2",
                "updated": "2025-04-16T15:12:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    15,
                    12,
                    21,
                    2,
                    106,
                    0
                ],
                "published": "2024-05-02T17:59:24Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    17,
                    59,
                    24,
                    3,
                    123,
                    0
                ],
                "title": "OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving\n  with Counterfactual Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving\n  with Counterfactual Reasoning"
                },
                "summary": "The advances in vision-language models (VLMs) have led to a growing interest\nin autonomous driving to leverage their strong reasoning capabilities. However,\nextending these capabilities from 2D to full 3D understanding is crucial for\nreal-world applications. To address this challenge, we propose OmniDrive, a\nholistic vision-language dataset that aligns agent models with 3D driving tasks\nthrough counterfactual reasoning. This approach enhances decision-making by\nevaluating potential scenarios and their outcomes, similar to human drivers\nconsidering alternative actions. Our counterfactual-based synthetic data\nannotation process generates large-scale, high-quality datasets, providing\ndenser supervision signals that bridge planning trajectories and language-based\nreasoning. Futher, we explore two advanced OmniDrive-Agent frameworks, namely\nOmni-L and Omni-Q, to assess the importance of vision-language alignment versus\n3D perception, revealing critical insights into designing effective LLM-agents.\nSignificant improvements on the DriveLM Q\\&A benchmark and nuScenes open-loop\nplanning demonstrate the effectiveness of our dataset and methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advances in vision-language models (VLMs) have led to a growing interest\nin autonomous driving to leverage their strong reasoning capabilities. However,\nextending these capabilities from 2D to full 3D understanding is crucial for\nreal-world applications. To address this challenge, we propose OmniDrive, a\nholistic vision-language dataset that aligns agent models with 3D driving tasks\nthrough counterfactual reasoning. This approach enhances decision-making by\nevaluating potential scenarios and their outcomes, similar to human drivers\nconsidering alternative actions. Our counterfactual-based synthetic data\nannotation process generates large-scale, high-quality datasets, providing\ndenser supervision signals that bridge planning trajectories and language-based\nreasoning. Futher, we explore two advanced OmniDrive-Agent frameworks, namely\nOmni-L and Omni-Q, to assess the importance of vision-language alignment versus\n3D perception, revealing critical insights into designing effective LLM-agents.\nSignificant improvements on the DriveLM Q\\&A benchmark and nuScenes open-loop\nplanning demonstrate the effectiveness of our dataset and methods."
                },
                "authors": [
                    {
                        "name": "Shihao Wang"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Xiaohui Jiang"
                    },
                    {
                        "name": "Shiyi Lan"
                    },
                    {
                        "name": "Min Shi"
                    },
                    {
                        "name": "Nadine Chang"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Jose M. Alvarez"
                    }
                ],
                "author_detail": {
                    "name": "Jose M. Alvarez"
                },
                "author": "Jose M. Alvarez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12154v1",
                "updated": "2025-04-16T15:03:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    15,
                    3,
                    1,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T15:03:01Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    15,
                    3,
                    1,
                    2,
                    106,
                    0
                ],
                "title": "Deep Generative Models for Bayesian Inference on High-Rate Sensor Data:\n  Applications in Automotive Radar and Medical Imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Generative Models for Bayesian Inference on High-Rate Sensor Data:\n  Applications in Automotive Radar and Medical Imaging"
                },
                "summary": "Deep generative models have been studied and developed primarily in the\ncontext of natural images and computer vision. This has spurred the development\nof (Bayesian) methods that use these generative models for inverse problems in\nimage restoration, such as denoising, inpainting, and super-resolution. In\nrecent years, generative modeling for Bayesian inference on sensory data has\nalso gained traction. Nevertheless, the direct application of generative\nmodeling techniques initially designed for natural images on raw sensory data\nis not straightforward, requiring solutions that deal with high dynamic range\nsignals acquired from multiple sensors or arrays of sensors that interfere with\neach other, and that typically acquire data at a very high rate. Moreover, the\nexact physical data-generating process is often complex or unknown. As a\nconsequence, approximate models are used, resulting in discrepancies between\nmodel predictions and the observations that are non-Gaussian, in turn\ncomplicating the Bayesian inverse problem. Finally, sensor data is often used\nin real-time processing or decision-making systems, imposing stringent\nrequirements on, e.g., latency and throughput. In this paper, we will discuss\nsome of these challenges and offer approaches to address them, all in the\ncontext of high-rate real-time sensing applications in automotive radar and\nmedical imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep generative models have been studied and developed primarily in the\ncontext of natural images and computer vision. This has spurred the development\nof (Bayesian) methods that use these generative models for inverse problems in\nimage restoration, such as denoising, inpainting, and super-resolution. In\nrecent years, generative modeling for Bayesian inference on sensory data has\nalso gained traction. Nevertheless, the direct application of generative\nmodeling techniques initially designed for natural images on raw sensory data\nis not straightforward, requiring solutions that deal with high dynamic range\nsignals acquired from multiple sensors or arrays of sensors that interfere with\neach other, and that typically acquire data at a very high rate. Moreover, the\nexact physical data-generating process is often complex or unknown. As a\nconsequence, approximate models are used, resulting in discrepancies between\nmodel predictions and the observations that are non-Gaussian, in turn\ncomplicating the Bayesian inverse problem. Finally, sensor data is often used\nin real-time processing or decision-making systems, imposing stringent\nrequirements on, e.g., latency and throughput. In this paper, we will discuss\nsome of these challenges and offer approaches to address them, all in the\ncontext of high-rate real-time sensing applications in automotive radar and\nmedical imaging."
                },
                "authors": [
                    {
                        "name": "Tristan S. W. Stevens"
                    },
                    {
                        "name": "Jeroen Overdevest"
                    },
                    {
                        "name": "Oisn Nolan"
                    },
                    {
                        "name": "Wessel L. van Nierop"
                    },
                    {
                        "name": "Ruud J. G. van Sloun"
                    },
                    {
                        "name": "Yonina C. Eldar"
                    }
                ],
                "author_detail": {
                    "name": "Yonina C. Eldar"
                },
                "author": "Yonina C. Eldar",
                "arxiv_doi": "10.1098/rsta.2024-0327",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1098/rsta.2024-0327",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.12154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "18 pages, 5 figures, accepted author manuscript, Philosophical\n  Transactions of the Royal Society A",
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04348v2",
                "updated": "2025-04-16T15:00:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    15,
                    0,
                    11,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-06T03:54:21Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    3,
                    54,
                    21,
                    6,
                    96,
                    0
                ],
                "title": "OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving\n  with Counterfactual Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving\n  with Counterfactual Reasoning"
                },
                "summary": "The advances in vision-language models (VLMs) have led to a growing interest\nin autonomous driving to leverage their strong reasoning capabilities. However,\nextending these capabilities from 2D to full 3D understanding is crucial for\nreal-world applications. To address this challenge, we propose OmniDrive, a\nholistic vision-language dataset that aligns agent models with 3D driving tasks\nthrough counterfactual reasoning. This approach enhances decision-making by\nevaluating potential scenarios and their outcomes, similar to human drivers\nconsidering alternative actions. Our counterfactual-based synthetic data\nannotation process generates large-scale, high-quality datasets, providing\ndenser supervision signals that bridge planning trajectories and language-based\nreasoning. Futher, we explore two advanced OmniDrive-Agent frameworks, namely\nOmni-L and Omni-Q, to assess the importance of vision-language alignment versus\n3D perception, revealing critical insights into designing effective LLM-agents.\nSignificant improvements on the DriveLM Q\\&A benchmark and nuScenes open-loop\nplanning demonstrate the effectiveness of our dataset and methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advances in vision-language models (VLMs) have led to a growing interest\nin autonomous driving to leverage their strong reasoning capabilities. However,\nextending these capabilities from 2D to full 3D understanding is crucial for\nreal-world applications. To address this challenge, we propose OmniDrive, a\nholistic vision-language dataset that aligns agent models with 3D driving tasks\nthrough counterfactual reasoning. This approach enhances decision-making by\nevaluating potential scenarios and their outcomes, similar to human drivers\nconsidering alternative actions. Our counterfactual-based synthetic data\nannotation process generates large-scale, high-quality datasets, providing\ndenser supervision signals that bridge planning trajectories and language-based\nreasoning. Futher, we explore two advanced OmniDrive-Agent frameworks, namely\nOmni-L and Omni-Q, to assess the importance of vision-language alignment versus\n3D perception, revealing critical insights into designing effective LLM-agents.\nSignificant improvements on the DriveLM Q\\&A benchmark and nuScenes open-loop\nplanning demonstrate the effectiveness of our dataset and methods."
                },
                "authors": [
                    {
                        "name": "Shihao Wang"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Xiaohui Jiang"
                    },
                    {
                        "name": "Shiyi Lan"
                    },
                    {
                        "name": "Min Shi"
                    },
                    {
                        "name": "Nadine Chang"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Jose M. Alvarez"
                    }
                ],
                "author_detail": {
                    "name": "Jose M. Alvarez"
                },
                "author": "Jose M. Alvarez",
                "arxiv_comment": "Mistaken resubmission. The original version is at arXiv:2405.01533",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16514v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16514v3",
                "updated": "2025-04-16T14:58:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    58,
                    48,
                    2,
                    106,
                    0
                ],
                "published": "2025-03-15T23:43:06Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    23,
                    43,
                    6,
                    5,
                    74,
                    0
                ],
                "title": "VeriMind: Agentic LLM for Automated Verilog Generation with a Novel\n  Evaluation Metric",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriMind: Agentic LLM for Automated Verilog Generation with a Novel\n  Evaluation Metric"
                },
                "summary": "Designing Verilog modules requires meticulous attention to correctness,\nefficiency, and adherence to design specifications. However, manually writing\nVerilog code remains a complex and time-consuming task that demands both expert\nknowledge and iterative refinement. Leveraging recent advancements in large\nlanguage models (LLMs) and their structured text generation capabilities, we\npropose VeriMind, an agentic LLM framework for Verilog code generation that\nsignificantly automates and optimizes the synthesis process. Unlike traditional\nLLM-based code generators, VeriMind employs a structured reasoning approach:\ngiven a user-provided prompt describing design requirements, the system first\nformulates a detailed train of thought before the final Verilog code is\ngenerated. This multi-step methodology enhances interpretability, accuracy, and\nadaptability in hardware design. In addition, we introduce a novel evaluation\nmetric-pass@ARC-which combines the conventional pass@k measure with Average\nRefinement Cycles (ARC) to capture both success rate and the efficiency of\niterative refinement. Experimental results on diverse hardware design tasks\ndemonstrated that our approach achieved up to $8.3\\%$ improvement on pass@k\nmetric and $8.1\\%$ on pass@ARC metric. These findings underscore the\ntransformative potential of agentic LLMs in automated hardware design, RTL\ndevelopment, and digital system synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing Verilog modules requires meticulous attention to correctness,\nefficiency, and adherence to design specifications. However, manually writing\nVerilog code remains a complex and time-consuming task that demands both expert\nknowledge and iterative refinement. Leveraging recent advancements in large\nlanguage models (LLMs) and their structured text generation capabilities, we\npropose VeriMind, an agentic LLM framework for Verilog code generation that\nsignificantly automates and optimizes the synthesis process. Unlike traditional\nLLM-based code generators, VeriMind employs a structured reasoning approach:\ngiven a user-provided prompt describing design requirements, the system first\nformulates a detailed train of thought before the final Verilog code is\ngenerated. This multi-step methodology enhances interpretability, accuracy, and\nadaptability in hardware design. In addition, we introduce a novel evaluation\nmetric-pass@ARC-which combines the conventional pass@k measure with Average\nRefinement Cycles (ARC) to capture both success rate and the efficiency of\niterative refinement. Experimental results on diverse hardware design tasks\ndemonstrated that our approach achieved up to $8.3\\%$ improvement on pass@k\nmetric and $8.1\\%$ on pass@ARC metric. These findings underscore the\ntransformative potential of agentic LLMs in automated hardware design, RTL\ndevelopment, and digital system synthesis."
                },
                "authors": [
                    {
                        "name": "Bardia Nadimi"
                    },
                    {
                        "name": "Ghali Omar Boutaib"
                    },
                    {
                        "name": "Hao Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zheng"
                },
                "author": "Hao Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16514v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16514v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10337v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10337v2",
                "updated": "2025-04-16T14:58:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    58,
                    26,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-14T15:46:33Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    46,
                    33,
                    0,
                    104,
                    0
                ],
                "title": "Heimdall: test-time scaling on the generative verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heimdall: test-time scaling on the generative verification"
                },
                "summary": "An AI system can create and maintain knowledge only to the extent that it can\nverify that knowledge itself. Recent work on long Chain-of-Thought reasoning\nhas demonstrated great potential of LLMs on solving competitive problems, but\ntheir verification ability remains to be weak and not sufficiently\ninvestigated. In this paper, we propose Heimdall, the long CoT verification LLM\nthat can accurately judge the correctness of solutions. With pure reinforcement\nlearning, we boost the verification accuracy from 62.5% to 94.5% on competitive\nmath problems. By scaling with repeated sampling, the accuracy further\nincreases to 97.5%. Through human evaluation, Heimdall demonstrates impressive\ngeneralization capabilities, successfully detecting most issues in challenging\nmath proofs, the type of which is not included during training. Furthermore, we\npropose Pessimistic Verification to extend the functionality of Heimdall to\nscaling up the problem solving. It calls Heimdall to judge the solutions from a\nsolver model and based on the pessimistic principle, selects the most likely\ncorrect solution with the least uncertainty. Taking\nDeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification\nimproves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute\nbudget and to 83.3% with more compute budget. With the stronger solver Gemini\n2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge\ndiscovery system, a ternary system where one poses questions, another provides\nsolutions, and the third verifies the solutions. Using the data synthesis work\nNuminaMath for the first two components, Heimdall effectively identifies\nproblematic records within the dataset and reveals that nearly half of the data\nis flawed, which interestingly aligns with the recent ablation studies from\nNuminaMath.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An AI system can create and maintain knowledge only to the extent that it can\nverify that knowledge itself. Recent work on long Chain-of-Thought reasoning\nhas demonstrated great potential of LLMs on solving competitive problems, but\ntheir verification ability remains to be weak and not sufficiently\ninvestigated. In this paper, we propose Heimdall, the long CoT verification LLM\nthat can accurately judge the correctness of solutions. With pure reinforcement\nlearning, we boost the verification accuracy from 62.5% to 94.5% on competitive\nmath problems. By scaling with repeated sampling, the accuracy further\nincreases to 97.5%. Through human evaluation, Heimdall demonstrates impressive\ngeneralization capabilities, successfully detecting most issues in challenging\nmath proofs, the type of which is not included during training. Furthermore, we\npropose Pessimistic Verification to extend the functionality of Heimdall to\nscaling up the problem solving. It calls Heimdall to judge the solutions from a\nsolver model and based on the pessimistic principle, selects the most likely\ncorrect solution with the least uncertainty. Taking\nDeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification\nimproves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute\nbudget and to 83.3% with more compute budget. With the stronger solver Gemini\n2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge\ndiscovery system, a ternary system where one poses questions, another provides\nsolutions, and the third verifies the solutions. Using the data synthesis work\nNuminaMath for the first two components, Heimdall effectively identifies\nproblematic records within the dataset and reveals that nearly half of the data\nis flawed, which interestingly aligns with the recent ablation studies from\nNuminaMath."
                },
                "authors": [
                    {
                        "name": "Wenlei Shi"
                    },
                    {
                        "name": "Xing Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xing Jin"
                },
                "author": "Xing Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10337v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10337v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04070v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04070v2",
                "updated": "2025-04-16T14:54:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    54,
                    0,
                    2,
                    106,
                    0
                ],
                "published": "2024-12-05T11:06:12Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    11,
                    6,
                    12,
                    3,
                    340,
                    0
                ],
                "title": "Radio pulsar population synthesis with consistent flux measurements\n  using simulation-based inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radio pulsar population synthesis with consistent flux measurements\n  using simulation-based inference"
                },
                "summary": "The properties of the entire neutron star population can be inferred by\nmodeling their evolution, from birth to the present, through pulsar population\nsynthesis. This involves simulating a mock population, applying observational\nfilters, and comparing the resulting sources to the limited subset of detected\npulsars. We specifically focus on the magneto-rotational properties of Galactic\nisolated neutron stars and provide new insights into the intrinsic radio\nluminosity law by combining pulsar population synthesis with a simulation-based\ninference (SBI) technique called truncated sequential neural posterior\nestimation (TSNPE). We employ TSNPE to train a neural density estimator on\nsimulated pulsar populations to approximate the posterior distribution of the\nunderlying parameters. This technique efficiently explores the parameter space\nby concentrating on regions that are most likely to match the observed data\nthus allowing a significant reduction in training dataset size. We demonstrate\nthe efficiency of TSNPE over standard neural posterior estimation (NPE),\nachieving robust inferences of magneto-rotational parameters consistent with\nprevious studies using only around 4% of the simulations required by NPE\napproaches. Moreover, for the first time, we incorporate data from the Thousand\nPulsar Array (TPA) program on MeerKAT, the largest unified sample of neutron\nstars with consistent fluxes measurement to date, to help constrain the stars'\nintrinsic radio luminosity. We find that adding flux information as an input to\nthe neural network largely improves the constraints on the pulsars' radio\nluminosity, as well as improving the estimates on other input parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The properties of the entire neutron star population can be inferred by\nmodeling their evolution, from birth to the present, through pulsar population\nsynthesis. This involves simulating a mock population, applying observational\nfilters, and comparing the resulting sources to the limited subset of detected\npulsars. We specifically focus on the magneto-rotational properties of Galactic\nisolated neutron stars and provide new insights into the intrinsic radio\nluminosity law by combining pulsar population synthesis with a simulation-based\ninference (SBI) technique called truncated sequential neural posterior\nestimation (TSNPE). We employ TSNPE to train a neural density estimator on\nsimulated pulsar populations to approximate the posterior distribution of the\nunderlying parameters. This technique efficiently explores the parameter space\nby concentrating on regions that are most likely to match the observed data\nthus allowing a significant reduction in training dataset size. We demonstrate\nthe efficiency of TSNPE over standard neural posterior estimation (NPE),\nachieving robust inferences of magneto-rotational parameters consistent with\nprevious studies using only around 4% of the simulations required by NPE\napproaches. Moreover, for the first time, we incorporate data from the Thousand\nPulsar Array (TPA) program on MeerKAT, the largest unified sample of neutron\nstars with consistent fluxes measurement to date, to help constrain the stars'\nintrinsic radio luminosity. We find that adding flux information as an input to\nthe neural network largely improves the constraints on the pulsars' radio\nluminosity, as well as improving the estimates on other input parameters."
                },
                "authors": [
                    {
                        "name": "Celsa Pardo Araujo"
                    },
                    {
                        "name": "Michele Ronchi"
                    },
                    {
                        "name": "Vanessa Graber"
                    },
                    {
                        "name": "Nanda Rea"
                    }
                ],
                "author_detail": {
                    "name": "Nanda Rea"
                },
                "author": "Nanda Rea",
                "arxiv_doi": "10.1051/0004-6361/202453314",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202453314",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.04070v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04070v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "18 pages, 9 figures",
                "arxiv_journal_ref": "A&A 696, A114 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12143v1",
                "updated": "2025-04-16T14:53:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    53,
                    28,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T14:53:28Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    53,
                    28,
                    2,
                    106,
                    0
                ],
                "title": "ARCeR: an Agentic RAG for the Automated Definition of Cyber Ranges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCeR: an Agentic RAG for the Automated Definition of Cyber Ranges"
                },
                "summary": "The growing and evolving landscape of cybersecurity threats necessitates the\ndevelopment of supporting tools and platforms that allow for the creation of\nrealistic IT environments operating within virtual, controlled settings as\nCyber Ranges (CRs). CRs can be exploited for analyzing vulnerabilities and\nexperimenting with the effectiveness of devised countermeasures, as well as\nserving as training environments for building cyber security skills and\nabilities for IT operators. This paper proposes ARCeR as an innovative solution\nfor the automatic generation and deployment of CRs, starting from user-provided\ndescriptions in a natural language. ARCeR relies on the Agentic RAG paradigm,\nwhich allows it to fully exploit state-of-art AI technologies. Experimental\nresults show that ARCeR is able to successfully process prompts even in cases\nthat LLMs or basic RAG systems are not able to cope with. Furthermore, ARCeR is\nable to target any CR framework provided that specific knowledge is made\navailable to it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing and evolving landscape of cybersecurity threats necessitates the\ndevelopment of supporting tools and platforms that allow for the creation of\nrealistic IT environments operating within virtual, controlled settings as\nCyber Ranges (CRs). CRs can be exploited for analyzing vulnerabilities and\nexperimenting with the effectiveness of devised countermeasures, as well as\nserving as training environments for building cyber security skills and\nabilities for IT operators. This paper proposes ARCeR as an innovative solution\nfor the automatic generation and deployment of CRs, starting from user-provided\ndescriptions in a natural language. ARCeR relies on the Agentic RAG paradigm,\nwhich allows it to fully exploit state-of-art AI technologies. Experimental\nresults show that ARCeR is able to successfully process prompts even in cases\nthat LLMs or basic RAG systems are not able to cope with. Furthermore, ARCeR is\nable to target any CR framework provided that specific knowledge is made\navailable to it."
                },
                "authors": [
                    {
                        "name": "Matteo Lupinacci"
                    },
                    {
                        "name": "Francesco Blefari"
                    },
                    {
                        "name": "Francesco Romeo"
                    },
                    {
                        "name": "Francesco Aurelio Pironti"
                    },
                    {
                        "name": "Angelo Furfaro"
                    }
                ],
                "author_detail": {
                    "name": "Angelo Furfaro"
                },
                "author": "Angelo Furfaro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12142v1",
                "updated": "2025-04-16T14:53:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    53,
                    12,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T14:53:12Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    53,
                    12,
                    2,
                    106,
                    0
                ],
                "title": "Overlapping Error Correction Codes on Two-Dimensional Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overlapping Error Correction Codes on Two-Dimensional Structures"
                },
                "summary": "The growing demand for highly reliable communication systems drives the\nresearch and development of algorithms that identify and correct errors during\ndata transmission and storage. This need becomes even more critical in\nhard-to-access or sensitive systems, such as those used in space applications,\npassenger transportation, and the financial sector. In this context, Error\nCorrection Codes (ECCs) are essential tools for ensuring a certain level of\nreliability. This work proposes a technique to enhance ECC error correction\ncapability by overlapping data regions. The approach consists of protecting the\nsame data area with multiple ECCs organized in a two-dimensional structure,\nenabling logical inferences that correlate the codes and improve their error\ndetection and correction capabilities. More specifically, the overlapping is\ncharacterized by the organization of multiple ECCs, whose intersection\nexclusively covers the entire data region. Different configurations of\noverlapping ECCs were analyzed to evaluate the proposal regarding error\ndetection and correction capability, scalability, and reliability. Experimental\nresults confirm the technique's effectiveness and demonstrate its high\nscalability potential, reducing the need for redundancy bits relative to the\nnumber of data bits. Furthermore, comparisons with state-of-the-art ECC\napproaches indicate the technique's applicability in critical systems that\nrequire high reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for highly reliable communication systems drives the\nresearch and development of algorithms that identify and correct errors during\ndata transmission and storage. This need becomes even more critical in\nhard-to-access or sensitive systems, such as those used in space applications,\npassenger transportation, and the financial sector. In this context, Error\nCorrection Codes (ECCs) are essential tools for ensuring a certain level of\nreliability. This work proposes a technique to enhance ECC error correction\ncapability by overlapping data regions. The approach consists of protecting the\nsame data area with multiple ECCs organized in a two-dimensional structure,\nenabling logical inferences that correlate the codes and improve their error\ndetection and correction capabilities. More specifically, the overlapping is\ncharacterized by the organization of multiple ECCs, whose intersection\nexclusively covers the entire data region. Different configurations of\noverlapping ECCs were analyzed to evaluate the proposal regarding error\ndetection and correction capability, scalability, and reliability. Experimental\nresults confirm the technique's effectiveness and demonstrate its high\nscalability potential, reducing the need for redundancy bits relative to the\nnumber of data bits. Furthermore, comparisons with state-of-the-art ECC\napproaches indicate the technique's applicability in critical systems that\nrequire high reliability."
                },
                "authors": [
                    {
                        "name": "Andrew Rafael Fritsch"
                    },
                    {
                        "name": "Csar Augusto Missio Marcon"
                    }
                ],
                "author_detail": {
                    "name": "Csar Augusto Missio Marcon"
                },
                "author": "Csar Augusto Missio Marcon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12140v1",
                "updated": "2025-04-16T14:52:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    52,
                    22,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T14:52:22Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    52,
                    22,
                    2,
                    106,
                    0
                ],
                "title": "Multilingual Contextualization of Large Language Models for\n  Document-Level Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Contextualization of Large Language Models for\n  Document-Level Machine Translation"
                },
                "summary": "Large language models (LLMs) have demonstrated strong performance in\nsentence-level machine translation, but scaling to document-level translation\nremains challenging, particularly in modeling long-range dependencies and\ndiscourse phenomena across sentences and paragraphs. In this work, we propose a\nmethod to improve LLM-based long-document translation through targeted\nfine-tuning on high-quality document-level data, which we curate and introduce\nas DocBlocks. Our approach supports multiple translation paradigms, including\ndirect document-to-document and chunk-level translation, by integrating\ninstructions both with and without surrounding context. This enables models to\nbetter capture cross-sentence dependencies while maintaining strong\nsentence-level translation performance. Experimental results show that\nincorporating multiple translation paradigms improves document-level\ntranslation quality and inference speed compared to prompting and agent-based\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong performance in\nsentence-level machine translation, but scaling to document-level translation\nremains challenging, particularly in modeling long-range dependencies and\ndiscourse phenomena across sentences and paragraphs. In this work, we propose a\nmethod to improve LLM-based long-document translation through targeted\nfine-tuning on high-quality document-level data, which we curate and introduce\nas DocBlocks. Our approach supports multiple translation paradigms, including\ndirect document-to-document and chunk-level translation, by integrating\ninstructions both with and without surrounding context. This enables models to\nbetter capture cross-sentence dependencies while maintaining strong\nsentence-level translation performance. Experimental results show that\nincorporating multiple translation paradigms improves document-level\ntranslation quality and inference speed compared to prompting and agent-based\nmethods."
                },
                "authors": [
                    {
                        "name": "Miguel Moura Ramos"
                    },
                    {
                        "name": "Patrick Fernandes"
                    },
                    {
                        "name": "Sweta Agrawal"
                    },
                    {
                        "name": "Andr F. T. Martins"
                    }
                ],
                "author_detail": {
                    "name": "Andr F. T. Martins"
                },
                "author": "Andr F. T. Martins",
                "arxiv_comment": "9 pages, work-in-progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12137v1",
                "updated": "2025-04-16T14:50:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    50,
                    25,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T14:50:25Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    50,
                    25,
                    2,
                    106,
                    0
                ],
                "title": "Efficient Contrastive Decoding with Probabilistic Hallucination\n  Detection - Mitigating Hallucinations in Large Vision Language Models -",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Contrastive Decoding with Probabilistic Hallucination\n  Detection - Mitigating Hallucinations in Large Vision Language Models -"
                },
                "summary": "Despite recent advances in Large Vision Language Models (LVLMs), these models\nstill suffer from generating hallucinatory responses that do not align with the\nvisual input provided. To mitigate such hallucinations, we introduce Efficient\nContrastive Decoding (ECD), a simple method that leverages probabilistic\nhallucination detection to shift the output distribution towards contextually\naccurate answers at inference time. By contrasting token probabilities and\nhallucination scores, ECD subtracts hallucinated concepts from the original\ndistribution, effectively suppressing hallucinations. Notably, our proposed\nmethod can be applied to any open-source LVLM and does not require additional\nLVLM training. We evaluate our method on several benchmark datasets and across\ndifferent LVLMs. Our experiments show that ECD effectively mitigates\nhallucinations, outperforming state-of-the-art methods with respect to\nperformance on LVLM benchmarks and computation time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advances in Large Vision Language Models (LVLMs), these models\nstill suffer from generating hallucinatory responses that do not align with the\nvisual input provided. To mitigate such hallucinations, we introduce Efficient\nContrastive Decoding (ECD), a simple method that leverages probabilistic\nhallucination detection to shift the output distribution towards contextually\naccurate answers at inference time. By contrasting token probabilities and\nhallucination scores, ECD subtracts hallucinated concepts from the original\ndistribution, effectively suppressing hallucinations. Notably, our proposed\nmethod can be applied to any open-source LVLM and does not require additional\nLVLM training. We evaluate our method on several benchmark datasets and across\ndifferent LVLMs. Our experiments show that ECD effectively mitigates\nhallucinations, outperforming state-of-the-art methods with respect to\nperformance on LVLM benchmarks and computation time."
                },
                "authors": [
                    {
                        "name": "Laura Fieback"
                    },
                    {
                        "name": "Nishilkumar Balar"
                    },
                    {
                        "name": "Jakob Spiegelberg"
                    },
                    {
                        "name": "Hanno Gottschalk"
                    }
                ],
                "author_detail": {
                    "name": "Hanno Gottschalk"
                },
                "author": "Hanno Gottschalk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08525v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08525v3",
                "updated": "2025-04-16T14:48:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    48,
                    13,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-11T13:38:36Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    13,
                    38,
                    36,
                    4,
                    101,
                    0
                ],
                "title": "Task Memory Engine (TME): A Structured Memory Framework with Graph-Aware\n  Extensions for Multi-Step LLM Agent Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task Memory Engine (TME): A Structured Memory Framework with Graph-Aware\n  Extensions for Multi-Step LLM Agent Tasks"
                },
                "summary": "Large Language Models (LLMs) are increasingly used as autonomous agents for\nmulti-step tasks. However, most existing frameworks fail to maintain a\nstructured understanding of the task state, often relying on linear prompt\nconcatenation or shallow memory buffers. This leads to brittle performance,\nfrequent hallucinations, and poor long-range coherence. In this work, we\npropose the Task Memory Engine (TME), a lightweight and structured memory\nmodule that tracks task execution using a hierarchical Task Memory Tree (TMT).\nEach node in the tree corresponds to a task step, storing relevant input,\noutput, status, and sub-task relationships. We introduce a prompt synthesis\nmethod that dynamically generates LLM prompts based on the active node path,\nsignificantly improving execution consistency and contextual grounding. Through\ncase studies and comparative experiments on multi-step agent tasks, we\ndemonstrate that TME leads to better task completion accuracy and more\ninterpretable behavior with minimal implementation overhead. A reference\nimplementation of the core TME components is available at\nhttps://github.com/biubiutomato/TME-Agent, including basic examples and\nstructured memory integration. While the current implementation uses a\ntree-based structure, TME is designed to be graph-aware, supporting reusable\nsubsteps, converging task paths, and shared dependencies. This lays the\ngroundwork for future DAG-based memory architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used as autonomous agents for\nmulti-step tasks. However, most existing frameworks fail to maintain a\nstructured understanding of the task state, often relying on linear prompt\nconcatenation or shallow memory buffers. This leads to brittle performance,\nfrequent hallucinations, and poor long-range coherence. In this work, we\npropose the Task Memory Engine (TME), a lightweight and structured memory\nmodule that tracks task execution using a hierarchical Task Memory Tree (TMT).\nEach node in the tree corresponds to a task step, storing relevant input,\noutput, status, and sub-task relationships. We introduce a prompt synthesis\nmethod that dynamically generates LLM prompts based on the active node path,\nsignificantly improving execution consistency and contextual grounding. Through\ncase studies and comparative experiments on multi-step agent tasks, we\ndemonstrate that TME leads to better task completion accuracy and more\ninterpretable behavior with minimal implementation overhead. A reference\nimplementation of the core TME components is available at\nhttps://github.com/biubiutomato/TME-Agent, including basic examples and\nstructured memory integration. While the current implementation uses a\ntree-based structure, TME is designed to be graph-aware, supporting reusable\nsubsteps, converging task paths, and shared dependencies. This lays the\ngroundwork for future DAG-based memory architectures."
                },
                "authors": [
                    {
                        "name": "Ye Ye"
                    }
                ],
                "author_detail": {
                    "name": "Ye Ye"
                },
                "author": "Ye Ye",
                "arxiv_comment": "14 pages, 5 figures. Preprint prepared for future submission.\n  Includes implementation and token-efficiency analysis. Code at\n  https://github.com/biubiutomato/TME-Agent",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08525v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08525v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.8; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10185v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10185v2",
                "updated": "2025-04-16T14:45:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    45,
                    55,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-14T12:38:37Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    38,
                    37,
                    0,
                    104,
                    0
                ],
                "title": "LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in\n  Current Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in\n  Current Benchmarks"
                },
                "summary": "Large language model unlearning has become a critical challenge in ensuring\nsafety and controlled model behavior by removing undesired data-model\ninfluences from the pretrained model while preserving general utility.\nSignificant recent efforts have been dedicated to developing LLM unlearning\nbenchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine\nUnlearning Six-way Evaluation), facilitating standardized unlearning\nperformance assessment and method comparison. Despite their usefulness, we\nuncover for the first time a novel coreset effect within these benchmarks.\nSpecifically, we find that LLM unlearning achieved with the original (full)\nforget set can be effectively maintained using a significantly smaller subset\n(functioning as a \"coreset\"), e.g., as little as 5% of the forget set, even\nwhen selected at random. This suggests that LLM unlearning in these benchmarks\ncan be performed surprisingly easily, even in an extremely low-data regime. We\ndemonstrate that this coreset effect remains strong, regardless of the LLM\nunlearning method used, such as NPO (Negative Preference Optimization) and RMU\n(Representation Misdirection Unlearning), the popular ones in these benchmarks.\nThe surprisingly strong coreset effect is also robust across various data\nselection methods, ranging from random selection to more sophisticated\nheuristic approaches. We explain the coreset effect in LLM unlearning through a\nkeyword-based perspective, showing that keywords extracted from the forget set\nalone contribute significantly to unlearning effectiveness and indicating that\ncurrent unlearning is driven by a compact set of high-impact tokens rather than\nthe entire dataset. We further justify the faithfulness of coreset-unlearned\nmodels along additional dimensions, such as mode connectivity and robustness to\njailbreaking attacks. Codes are available at\nhttps://github.com/OPTML-Group/MU-Coreset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model unlearning has become a critical challenge in ensuring\nsafety and controlled model behavior by removing undesired data-model\ninfluences from the pretrained model while preserving general utility.\nSignificant recent efforts have been dedicated to developing LLM unlearning\nbenchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine\nUnlearning Six-way Evaluation), facilitating standardized unlearning\nperformance assessment and method comparison. Despite their usefulness, we\nuncover for the first time a novel coreset effect within these benchmarks.\nSpecifically, we find that LLM unlearning achieved with the original (full)\nforget set can be effectively maintained using a significantly smaller subset\n(functioning as a \"coreset\"), e.g., as little as 5% of the forget set, even\nwhen selected at random. This suggests that LLM unlearning in these benchmarks\ncan be performed surprisingly easily, even in an extremely low-data regime. We\ndemonstrate that this coreset effect remains strong, regardless of the LLM\nunlearning method used, such as NPO (Negative Preference Optimization) and RMU\n(Representation Misdirection Unlearning), the popular ones in these benchmarks.\nThe surprisingly strong coreset effect is also robust across various data\nselection methods, ranging from random selection to more sophisticated\nheuristic approaches. We explain the coreset effect in LLM unlearning through a\nkeyword-based perspective, showing that keywords extracted from the forget set\nalone contribute significantly to unlearning effectiveness and indicating that\ncurrent unlearning is driven by a compact set of high-impact tokens rather than\nthe entire dataset. We further justify the faithfulness of coreset-unlearned\nmodels along additional dimensions, such as mode connectivity and robustness to\njailbreaking attacks. Codes are available at\nhttps://github.com/OPTML-Group/MU-Coreset."
                },
                "authors": [
                    {
                        "name": "Soumyadeep Pal"
                    },
                    {
                        "name": "Changsheng Wang"
                    },
                    {
                        "name": "James Diffenderfer"
                    },
                    {
                        "name": "Bhavya Kailkhura"
                    },
                    {
                        "name": "Sijia Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sijia Liu"
                },
                "author": "Sijia Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10185v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10185v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04196v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04196v2",
                "updated": "2025-04-16T14:37:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    37,
                    31,
                    2,
                    106,
                    0
                ],
                "published": "2024-09-06T11:34:24Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    11,
                    34,
                    24,
                    4,
                    250,
                    0
                ],
                "title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers"
                },
                "summary": "Reconstructing posed 3D human models from monocular images has important\napplications in the sports industry, including performance tracking, injury\nprevention and virtual training. In this work, we combine 3D human pose and\nshape estimation with 3D Gaussian Splatting (3DGS), a representation of the\nscene composed of a mixture of Gaussians. This allows training or fine-tuning a\nhuman model predictor on multi-view images alone, without 3D ground truth.\nPredicting such mixtures for a human from a single input image is challenging\ndue to self-occlusions and dependence on articulations, while also needing to\nretain enough flexibility to accommodate a variety of clothes and poses. Our\nkey observation is that the vertices of standardized human meshes (such as\nSMPL) can provide an adequate spatial density and approximate initial position\nfor the Gaussians. We can then train a transformer model to jointly predict\ncomparatively small adjustments to these positions, as well as the other 3DGS\nattributes and the SMPL parameters. We show empirically that this combination\n(using only multi-view supervision) can achieve near real-time inference of 3D\nhuman models from a single image without expensive diffusion models or 3D\npoints supervision, thus making it ideal for the sport industry at any level.\nMore importantly, rendering is an effective auxiliary objective to refine 3D\npose estimation by accounting for clothes and other geometric variations. The\ncode is available at https://github.com/prosperolo/GST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing posed 3D human models from monocular images has important\napplications in the sports industry, including performance tracking, injury\nprevention and virtual training. In this work, we combine 3D human pose and\nshape estimation with 3D Gaussian Splatting (3DGS), a representation of the\nscene composed of a mixture of Gaussians. This allows training or fine-tuning a\nhuman model predictor on multi-view images alone, without 3D ground truth.\nPredicting such mixtures for a human from a single input image is challenging\ndue to self-occlusions and dependence on articulations, while also needing to\nretain enough flexibility to accommodate a variety of clothes and poses. Our\nkey observation is that the vertices of standardized human meshes (such as\nSMPL) can provide an adequate spatial density and approximate initial position\nfor the Gaussians. We can then train a transformer model to jointly predict\ncomparatively small adjustments to these positions, as well as the other 3DGS\nattributes and the SMPL parameters. We show empirically that this combination\n(using only multi-view supervision) can achieve near real-time inference of 3D\nhuman models from a single image without expensive diffusion models or 3D\npoints supervision, thus making it ideal for the sport industry at any level.\nMore importantly, rendering is an effective auxiliary objective to refine 3D\npose estimation by accounting for clothes and other geometric variations. The\ncode is available at https://github.com/prosperolo/GST."
                },
                "authors": [
                    {
                        "name": "Lorenza Prospero"
                    },
                    {
                        "name": "Abdullah Hamdi"
                    },
                    {
                        "name": "Joao F. Henriques"
                    },
                    {
                        "name": "Christian Rupprecht"
                    }
                ],
                "author_detail": {
                    "name": "Christian Rupprecht"
                },
                "author": "Christian Rupprecht",
                "arxiv_comment": "Camera ready for CVSports workshop at CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04196v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04196v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12113v1",
                "updated": "2025-04-16T14:21:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    21,
                    2,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T14:21:02Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    21,
                    2,
                    2,
                    106,
                    0
                ],
                "title": "Clarifying Ambiguities: on the Role of Ambiguity Types in Prompting\n  Methods for Clarification Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clarifying Ambiguities: on the Role of Ambiguity Types in Prompting\n  Methods for Clarification Generation"
                },
                "summary": "In information retrieval (IR), providing appropriate clarifications to better\nunderstand users' information needs is crucial for building a proactive\nsearch-oriented dialogue system. Due to the strong in-context learning ability\nof large language models (LLMs), recent studies investigate prompting methods\nto generate clarifications using few-shot or Chain of Thought (CoT) prompts.\nHowever, vanilla CoT prompting does not distinguish the characteristics of\ndifferent information needs, making it difficult to understand how LLMs resolve\nambiguities in user queries. In this work, we focus on the concept of ambiguity\nfor clarification, seeking to model and integrate ambiguities in the\nclarification process. To this end, we comprehensively study the impact of\nprompting schemes based on reasoning and ambiguity for clarification. The idea\nis to enhance the reasoning abilities of LLMs by limiting CoT to predict first\nambiguity types that can be interpreted as instructions to clarify, then\ncorrespondingly generate clarifications. We name this new prompting scheme\nAmbiguity Type-Chain of Thought (AT-CoT). Experiments are conducted on various\ndatasets containing human-annotated clarifying questions to compare AT-CoT with\nmultiple baselines. We also perform user simulations to implicitly measure the\nquality of generated clarifications under various IR scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In information retrieval (IR), providing appropriate clarifications to better\nunderstand users' information needs is crucial for building a proactive\nsearch-oriented dialogue system. Due to the strong in-context learning ability\nof large language models (LLMs), recent studies investigate prompting methods\nto generate clarifications using few-shot or Chain of Thought (CoT) prompts.\nHowever, vanilla CoT prompting does not distinguish the characteristics of\ndifferent information needs, making it difficult to understand how LLMs resolve\nambiguities in user queries. In this work, we focus on the concept of ambiguity\nfor clarification, seeking to model and integrate ambiguities in the\nclarification process. To this end, we comprehensively study the impact of\nprompting schemes based on reasoning and ambiguity for clarification. The idea\nis to enhance the reasoning abilities of LLMs by limiting CoT to predict first\nambiguity types that can be interpreted as instructions to clarify, then\ncorrespondingly generate clarifications. We name this new prompting scheme\nAmbiguity Type-Chain of Thought (AT-CoT). Experiments are conducted on various\ndatasets containing human-annotated clarifying questions to compare AT-CoT with\nmultiple baselines. We also perform user simulations to implicitly measure the\nquality of generated clarifications under various IR scenarios."
                },
                "authors": [
                    {
                        "name": "Anfu Tang"
                    },
                    {
                        "name": "Laure Soulier"
                    },
                    {
                        "name": "Vincent Guigue"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Guigue"
                },
                "author": "Vincent Guigue",
                "arxiv_comment": "11 pages, 3 figures. Accepted at SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12110v1",
                "updated": "2025-04-16T14:19:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    19,
                    25,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T14:19:25Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    19,
                    25,
                    2,
                    106,
                    0
                ],
                "title": "Towards LLM Agents for Earth Observation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards LLM Agents for Earth Observation"
                },
                "summary": "Earth Observation (EO) provides critical planetary data for environmental\nmonitoring, disaster management, climate science, and other scientific domains.\nHere we ask: Are AI systems ready for reliable Earth Observation? We introduce\n\\datasetnamenospace, a benchmark of 140 yes/no questions from NASA Earth\nObservatory articles across 13 topics and 17 satellite sensors. Using Google\nEarth Engine API as a tool, LLM agents can only achieve an accuracy of 33%\nbecause the code fails to run over 58% of the time. We improve the failure rate\nfor open models by fine-tuning synthetic data, allowing much smaller models\n(Llama-3.1-8B) to achieve comparable accuracy to much larger ones (e.g.,\nDeepSeek-R1). Taken together, our findings identify significant challenges to\nbe solved before AI agents can automate earth observation, and suggest paths\nforward. The project page is available at\nhttps://iandrover.github.io/UnivEarth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Earth Observation (EO) provides critical planetary data for environmental\nmonitoring, disaster management, climate science, and other scientific domains.\nHere we ask: Are AI systems ready for reliable Earth Observation? We introduce\n\\datasetnamenospace, a benchmark of 140 yes/no questions from NASA Earth\nObservatory articles across 13 topics and 17 satellite sensors. Using Google\nEarth Engine API as a tool, LLM agents can only achieve an accuracy of 33%\nbecause the code fails to run over 58% of the time. We improve the failure rate\nfor open models by fine-tuning synthetic data, allowing much smaller models\n(Llama-3.1-8B) to achieve comparable accuracy to much larger ones (e.g.,\nDeepSeek-R1). Taken together, our findings identify significant challenges to\nbe solved before AI agents can automate earth observation, and suggest paths\nforward. The project page is available at\nhttps://iandrover.github.io/UnivEarth."
                },
                "authors": [
                    {
                        "name": "Chia Hsiang Kao"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Shreelekha Revankar"
                    },
                    {
                        "name": "Samuel Speas"
                    },
                    {
                        "name": "Snehal Bhagat"
                    },
                    {
                        "name": "Rajeev Datta"
                    },
                    {
                        "name": "Cheng Perng Phoo"
                    },
                    {
                        "name": "Utkarsh Mall"
                    },
                    {
                        "name": "Carl Vondrick"
                    },
                    {
                        "name": "Kavita Bala"
                    },
                    {
                        "name": "Bharath Hariharan"
                    }
                ],
                "author_detail": {
                    "name": "Bharath Hariharan"
                },
                "author": "Bharath Hariharan",
                "arxiv_comment": "36 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12108v1",
                "updated": "2025-04-16T14:16:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    16,
                    38,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T14:16:38Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    16,
                    38,
                    2,
                    106,
                    0
                ],
                "title": "Entropy-Guided Watermarking for LLMs: A Test-Time Framework for Robust\n  and Traceable Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entropy-Guided Watermarking for LLMs: A Test-Time Framework for Robust\n  and Traceable Text Generation"
                },
                "summary": "The rapid development of Large Language Models (LLMs) has intensified\nconcerns about content traceability and potential misuse. Existing watermarking\nschemes for sampled text often face trade-offs between maintaining text quality\nand ensuring robust detection against various attacks. To address these issues,\nwe propose a novel watermarking scheme that improves both detectability and\ntext quality by introducing a cumulative watermark entropy threshold. Our\napproach is compatible with and generalizes existing sampling functions,\nenhancing adaptability. Experimental results across multiple LLMs show that our\nscheme significantly outperforms existing methods, achieving over 80\\%\nimprovements on widely-used datasets, e.g., MATH and GSM8K, while maintaining\nhigh detection accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Large Language Models (LLMs) has intensified\nconcerns about content traceability and potential misuse. Existing watermarking\nschemes for sampled text often face trade-offs between maintaining text quality\nand ensuring robust detection against various attacks. To address these issues,\nwe propose a novel watermarking scheme that improves both detectability and\ntext quality by introducing a cumulative watermark entropy threshold. Our\napproach is compatible with and generalizes existing sampling functions,\nenhancing adaptability. Experimental results across multiple LLMs show that our\nscheme significantly outperforms existing methods, achieving over 80\\%\nimprovements on widely-used datasets, e.g., MATH and GSM8K, while maintaining\nhigh detection accuracy."
                },
                "authors": [
                    {
                        "name": "Shizhan Cai"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12098v1",
                "updated": "2025-04-16T14:02:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    2,
                    21,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T14:02:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    2,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Gauging Overprecision in LLMs: An Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gauging Overprecision in LLMs: An Empirical Study"
                },
                "summary": "Recently, overconfidence in large language models (LLMs) has garnered\nconsiderable attention due to its fundamental importance in quantifying the\ntrustworthiness of LLM generation. However, existing approaches prompt the\n\\textit{black box LLMs} to produce their confidence (\\textit{verbalized\nconfidence}), which can be subject to many biases and hallucinations. Inspired\nby a different aspect of overconfidence in cognitive science called\n\\textit{overprecision}, we designed a framework for its study in black box\nLLMs. This framework contains three main phases: 1) generation, 2) refinement\nand 3) evaluation. In the generation phase we prompt the LLM to generate\nanswers to numerical questions in the form of intervals with a certain level of\nconfidence. This confidence level is imposed in the prompt and not required for\nthe LLM to generate as in previous approaches. We use various prompting\ntechniques and use the same prompt multiple times to gauge the effects of\nrandomness in the generation process. In the refinement phase, answers from the\nprevious phase are refined to generate better answers. The LLM answers are\nevaluated and studied in the evaluation phase to understand its internal\nworkings. This study allowed us to gain various insights into LLM\noverprecision: 1) LLMs are highly uncalibrated for numerical tasks 2)\n{\\color{blue}there is no correlation between the length of the interval and the\nimposed confidence level, which can be symptomatic of a a) lack of\nunderstanding of the concept of confidence or b) inability to adjust\nself-confidence by following instructions}, {\\color{blue}3)} LLM numerical\nprecision differs depending on the task, scale of answer and prompting\ntechnique {\\color{blue}4) Refinement of answers doesn't improve precision in\nmost cases}. We believe this study offers new perspectives on LLM\noverconfidence and serves as a strong baseline for overprecision in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, overconfidence in large language models (LLMs) has garnered\nconsiderable attention due to its fundamental importance in quantifying the\ntrustworthiness of LLM generation. However, existing approaches prompt the\n\\textit{black box LLMs} to produce their confidence (\\textit{verbalized\nconfidence}), which can be subject to many biases and hallucinations. Inspired\nby a different aspect of overconfidence in cognitive science called\n\\textit{overprecision}, we designed a framework for its study in black box\nLLMs. This framework contains three main phases: 1) generation, 2) refinement\nand 3) evaluation. In the generation phase we prompt the LLM to generate\nanswers to numerical questions in the form of intervals with a certain level of\nconfidence. This confidence level is imposed in the prompt and not required for\nthe LLM to generate as in previous approaches. We use various prompting\ntechniques and use the same prompt multiple times to gauge the effects of\nrandomness in the generation process. In the refinement phase, answers from the\nprevious phase are refined to generate better answers. The LLM answers are\nevaluated and studied in the evaluation phase to understand its internal\nworkings. This study allowed us to gain various insights into LLM\noverprecision: 1) LLMs are highly uncalibrated for numerical tasks 2)\n{\\color{blue}there is no correlation between the length of the interval and the\nimposed confidence level, which can be symptomatic of a a) lack of\nunderstanding of the concept of confidence or b) inability to adjust\nself-confidence by following instructions}, {\\color{blue}3)} LLM numerical\nprecision differs depending on the task, scale of answer and prompting\ntechnique {\\color{blue}4) Refinement of answers doesn't improve precision in\nmost cases}. We believe this study offers new perspectives on LLM\noverconfidence and serves as a strong baseline for overprecision in LLMs."
                },
                "authors": [
                    {
                        "name": "Adil Bahaj"
                    },
                    {
                        "name": "Hamed Rahimi"
                    },
                    {
                        "name": "Mohamed Chetouani"
                    },
                    {
                        "name": "Mounir Ghogho"
                    }
                ],
                "author_detail": {
                    "name": "Mounir Ghogho"
                },
                "author": "Mounir Ghogho",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12090v1",
                "updated": "2025-04-16T13:53:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    13,
                    53,
                    42,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T13:53:42Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    13,
                    53,
                    42,
                    2,
                    106,
                    0
                ],
                "title": "Reasoning-Based AI for Startup Evaluation (R.A.I.S.E.): A\n  Memory-Augmented, Multi-Step Decision Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-Based AI for Startup Evaluation (R.A.I.S.E.): A\n  Memory-Augmented, Multi-Step Decision Framework"
                },
                "summary": "We present a novel framework that bridges the gap between the\ninterpretability of decision trees and the advanced reasoning capabilities of\nlarge language models (LLMs) to predict startup success. Our approach leverages\nchain-of-thought prompting to generate detailed reasoning logs, which are\nsubsequently distilled into structured, human-understandable logical rules. The\npipeline integrates multiple enhancements - efficient data ingestion, a\ntwo-step refinement process, ensemble candidate sampling, simulated\nreinforcement learning scoring, and persistent memory - to ensure both stable\ndecision-making and transparent output. Experimental evaluations on curated\nstartup datasets demonstrate that our combined pipeline improves precision by\n54% from 0.225 to 0.346 and accuracy by 50% from 0.46 to 0.70 compared to a\nstandalone OpenAI o3 model. Notably, our model achieves over 2x the precision\nof a random classifier (16%). By combining state-of-the-art AI reasoning with\nexplicit rule-based explanations, our method not only augments traditional\ndecision-making processes but also facilitates expert intervention and\ncontinuous policy refinement. This work lays the foundation for the\nimplementation of interpretable LLM-powered decision frameworks in high-stakes\ninvestment environments and other domains that require transparent and\ndata-driven insights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel framework that bridges the gap between the\ninterpretability of decision trees and the advanced reasoning capabilities of\nlarge language models (LLMs) to predict startup success. Our approach leverages\nchain-of-thought prompting to generate detailed reasoning logs, which are\nsubsequently distilled into structured, human-understandable logical rules. The\npipeline integrates multiple enhancements - efficient data ingestion, a\ntwo-step refinement process, ensemble candidate sampling, simulated\nreinforcement learning scoring, and persistent memory - to ensure both stable\ndecision-making and transparent output. Experimental evaluations on curated\nstartup datasets demonstrate that our combined pipeline improves precision by\n54% from 0.225 to 0.346 and accuracy by 50% from 0.46 to 0.70 compared to a\nstandalone OpenAI o3 model. Notably, our model achieves over 2x the precision\nof a random classifier (16%). By combining state-of-the-art AI reasoning with\nexplicit rule-based explanations, our method not only augments traditional\ndecision-making processes but also facilitates expert intervention and\ncontinuous policy refinement. This work lays the foundation for the\nimplementation of interpretable LLM-powered decision frameworks in high-stakes\ninvestment environments and other domains that require transparent and\ndata-driven insights."
                },
                "authors": [
                    {
                        "name": "Jack Preuveneers"
                    },
                    {
                        "name": "Joseph Ternasky"
                    },
                    {
                        "name": "Fuat Alican"
                    },
                    {
                        "name": "Yigit Ihlamur"
                    }
                ],
                "author_detail": {
                    "name": "Yigit Ihlamur"
                },
                "author": "Yigit Ihlamur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12085v1",
                "updated": "2025-04-16T13:46:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    13,
                    46,
                    45,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T13:46:45Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    13,
                    46,
                    45,
                    2,
                    106,
                    0
                ],
                "title": "Semiparametric Causal Discovery and Inference with Invalid Instruments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semiparametric Causal Discovery and Inference with Invalid Instruments"
                },
                "summary": "Learning causal relationships among a set of variables, as encoded by a\ndirected acyclic graph, from observational data is complicated by the presence\nof unobserved confounders. Instrumental variables (IVs) are a popular remedy\nfor this issue, but most existing methods either assume the validity of all IVs\nor postulate a specific form of relationship, such as a linear model, between\nthe primary variables and the IVs. To overcome these limitations, we introduce\na partially linear structural equation model for causal discovery and inference\nthat accommodates potentially invalid IVs and allows for general dependence of\nthe primary variables on the IVs. We establish identification under this\nsemiparametric model by constructing surrogate valid IVs, and develop a\nfinite-sample procedure for estimating the causal structures and effects.\nTheoretically, we show that our procedure consistently learns the causal\nstructures, yields asymptotically normal estimates, and effectively controls\nthe false discovery rate in edge recovery. Simulation studies demonstrate the\nsuperiority of our method over existing competitors, and an application to\ninferring gene regulatory networks in Alzheimer's disease illustrates its\nusefulness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning causal relationships among a set of variables, as encoded by a\ndirected acyclic graph, from observational data is complicated by the presence\nof unobserved confounders. Instrumental variables (IVs) are a popular remedy\nfor this issue, but most existing methods either assume the validity of all IVs\nor postulate a specific form of relationship, such as a linear model, between\nthe primary variables and the IVs. To overcome these limitations, we introduce\na partially linear structural equation model for causal discovery and inference\nthat accommodates potentially invalid IVs and allows for general dependence of\nthe primary variables on the IVs. We establish identification under this\nsemiparametric model by constructing surrogate valid IVs, and develop a\nfinite-sample procedure for estimating the causal structures and effects.\nTheoretically, we show that our procedure consistently learns the causal\nstructures, yields asymptotically normal estimates, and effectively controls\nthe false discovery rate in edge recovery. Simulation studies demonstrate the\nsuperiority of our method over existing competitors, and an application to\ninferring gene regulatory networks in Alzheimer's disease illustrates its\nusefulness."
                },
                "authors": [
                    {
                        "name": "Jing Zou"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12076v1",
                "updated": "2025-04-16T13:35:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    13,
                    35,
                    41,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T13:35:41Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    13,
                    35,
                    41,
                    2,
                    106,
                    0
                ],
                "title": "Subitizing-Inspired_Large_Language_Models_for_Floorplanning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Subitizing-Inspired_Large_Language_Models_for_Floorplanning"
                },
                "summary": "We present a novel approach to solving the floorplanning problem by\nleveraging fine-tuned Large Language Models (LLMs). Inspired by subitizing--the\nhuman ability to instantly and accurately count small numbers of items at a\nglance--we hypothesize that LLMs can similarly address floorplanning challenges\nswiftly and accurately. We propose an efficient representation of the\nfloorplanning problem and introduce a method for generating high-quality\ndatasets tailored for model fine-tuning. We fine-tune LLMs on datasets with a\nspecified number of modules to test whether LLMs can emulate the human ability\nto quickly count and arrange items. Our experimental results demonstrate that\nfine-tuned LLMs, particularly GPT4o-mini, achieve high success and optimal\nrates while attaining relatively low average dead space. These findings\nunderscore the potential of LLMs as promising solutions for complex\noptimization tasks in VLSI design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach to solving the floorplanning problem by\nleveraging fine-tuned Large Language Models (LLMs). Inspired by subitizing--the\nhuman ability to instantly and accurately count small numbers of items at a\nglance--we hypothesize that LLMs can similarly address floorplanning challenges\nswiftly and accurately. We propose an efficient representation of the\nfloorplanning problem and introduce a method for generating high-quality\ndatasets tailored for model fine-tuning. We fine-tune LLMs on datasets with a\nspecified number of modules to test whether LLMs can emulate the human ability\nto quickly count and arrange items. Our experimental results demonstrate that\nfine-tuned LLMs, particularly GPT4o-mini, achieve high success and optimal\nrates while attaining relatively low average dead space. These findings\nunderscore the potential of LLMs as promising solutions for complex\noptimization tasks in VLSI design."
                },
                "authors": [
                    {
                        "name": "Shao-Chien Lu"
                    },
                    {
                        "name": "Chen-Chen Yeh"
                    },
                    {
                        "name": "Hui-Lin Cho"
                    },
                    {
                        "name": "Yu-Cheng Lin"
                    },
                    {
                        "name": "Rung-Bin Lin"
                    }
                ],
                "author_detail": {
                    "name": "Rung-Bin Lin"
                },
                "author": "Rung-Bin Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12063v1",
                "updated": "2025-04-16T13:18:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    13,
                    18,
                    16,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T13:18:16Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    13,
                    18,
                    16,
                    2,
                    106,
                    0
                ],
                "title": "Optimizing Compound Retrieval Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Compound Retrieval Systems"
                },
                "summary": "Modern retrieval systems do not rely on a single ranking model to construct\ntheir rankings. Instead, they generally take a cascading approach where a\nsequence of ranking models are applied in multiple re-ranking stages. Thereby,\nthey balance the quality of the top-K ranking with computational costs by\nlimiting the number of documents each model re-ranks. However, the cascading\napproach is not the only way models can interact to form a retrieval system.\n  We propose the concept of compound retrieval systems as a broader class of\nretrieval systems that apply multiple prediction models. This encapsulates\ncascading models but also allows other types of interactions than top-K\nre-ranking. In particular, we enable interactions with large language models\n(LLMs) which can provide relative relevance comparisons. We focus on the\noptimization of compound retrieval system design which uniquely involves\nlearning where to apply the component models and how to aggregate their\npredictions into a final ranking. This work shows how our compound approach can\ncombine the classic BM25 retrieval model with state-of-the-art (pairwise) LLM\nrelevance predictions, while optimizing a given ranking metric and efficiency\ntarget. Our experimental results show optimized compound retrieval systems\nprovide better trade-offs between effectiveness and efficiency than cascading\napproaches, even when applied in a self-supervised manner.\n  With the introduction of compound retrieval systems, we hope to inspire the\ninformation retrieval field to more out-of-the-box thinking on how prediction\nmodels can interact to form rankings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern retrieval systems do not rely on a single ranking model to construct\ntheir rankings. Instead, they generally take a cascading approach where a\nsequence of ranking models are applied in multiple re-ranking stages. Thereby,\nthey balance the quality of the top-K ranking with computational costs by\nlimiting the number of documents each model re-ranks. However, the cascading\napproach is not the only way models can interact to form a retrieval system.\n  We propose the concept of compound retrieval systems as a broader class of\nretrieval systems that apply multiple prediction models. This encapsulates\ncascading models but also allows other types of interactions than top-K\nre-ranking. In particular, we enable interactions with large language models\n(LLMs) which can provide relative relevance comparisons. We focus on the\noptimization of compound retrieval system design which uniquely involves\nlearning where to apply the component models and how to aggregate their\npredictions into a final ranking. This work shows how our compound approach can\ncombine the classic BM25 retrieval model with state-of-the-art (pairwise) LLM\nrelevance predictions, while optimizing a given ranking metric and efficiency\ntarget. Our experimental results show optimized compound retrieval systems\nprovide better trade-offs between effectiveness and efficiency than cascading\napproaches, even when applied in a self-supervised manner.\n  With the introduction of compound retrieval systems, we hope to inspire the\ninformation retrieval field to more out-of-the-box thinking on how prediction\nmodels can interact to form rankings."
                },
                "authors": [
                    {
                        "name": "Harrie Oosterhuis"
                    },
                    {
                        "name": "Rolf Jagerman"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Xuanhui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanhui Wang"
                },
                "author": "Xuanhui Wang",
                "arxiv_doi": "10.1145/3726302.3730051",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730051",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.12063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SIGIR 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10149v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10149v2",
                "updated": "2025-04-16T13:16:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    13,
                    16,
                    19,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-14T12:00:00Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    0,
                    0,
                    0,
                    104,
                    0
                ],
                "title": "BoTTA: Benchmarking on-device Test Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BoTTA: Benchmarking on-device Test Time Adaptation"
                },
                "summary": "The performance of deep learning models depends heavily on test samples at\nruntime, and shifts from the training data distribution can significantly\nreduce accuracy. Test-time adaptation (TTA) addresses this by adapting models\nduring inference without requiring labeled test data or access to the original\ntraining set. While research has explored TTA from various perspectives like\nalgorithmic complexity, data and class distribution shifts, model\narchitectures, and offline versus continuous learning, constraints specific to\nmobile and edge devices remain underexplored. We propose BoTTA, a benchmark\ndesigned to evaluate TTA methods under practical constraints on mobile and edge\ndevices. Our evaluation targets four key challenges caused by limited resources\nand usage conditions: (i) limited test samples, (ii) limited exposure to\ncategories, (iii) diverse distribution shifts, and (iv) overlapping shifts\nwithin a sample. We assess state-of-the-art TTA methods under these scenarios\nusing benchmark datasets and report system-level metrics on a real testbed.\nFurthermore, unlike prior work, we align with on-device requirements by\nadvocating periodic adaptation instead of continuous inference-time adaptation.\nExperiments reveal key insights: many recent TTA algorithms struggle with small\ndatasets, fail to generalize to unseen categories, and depend on the diversity\nand complexity of distribution shifts. BoTTA also reports device-specific\nresource use. For example, while SHOT improves accuracy by $2.25\\times$ with\n$512$ adaptation samples, it uses $1.08\\times$ peak memory on Raspberry Pi\nversus the base model. BoTTA offers actionable guidance for TTA in real-world,\nresource-constrained deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of deep learning models depends heavily on test samples at\nruntime, and shifts from the training data distribution can significantly\nreduce accuracy. Test-time adaptation (TTA) addresses this by adapting models\nduring inference without requiring labeled test data or access to the original\ntraining set. While research has explored TTA from various perspectives like\nalgorithmic complexity, data and class distribution shifts, model\narchitectures, and offline versus continuous learning, constraints specific to\nmobile and edge devices remain underexplored. We propose BoTTA, a benchmark\ndesigned to evaluate TTA methods under practical constraints on mobile and edge\ndevices. Our evaluation targets four key challenges caused by limited resources\nand usage conditions: (i) limited test samples, (ii) limited exposure to\ncategories, (iii) diverse distribution shifts, and (iv) overlapping shifts\nwithin a sample. We assess state-of-the-art TTA methods under these scenarios\nusing benchmark datasets and report system-level metrics on a real testbed.\nFurthermore, unlike prior work, we align with on-device requirements by\nadvocating periodic adaptation instead of continuous inference-time adaptation.\nExperiments reveal key insights: many recent TTA algorithms struggle with small\ndatasets, fail to generalize to unseen categories, and depend on the diversity\nand complexity of distribution shifts. BoTTA also reports device-specific\nresource use. For example, while SHOT improves accuracy by $2.25\\times$ with\n$512$ adaptation samples, it uses $1.08\\times$ peak memory on Raspberry Pi\nversus the base model. BoTTA offers actionable guidance for TTA in real-world,\nresource-constrained deployments."
                },
                "authors": [
                    {
                        "name": "Michal Danilowski"
                    },
                    {
                        "name": "Soumyajit Chatterjee"
                    },
                    {
                        "name": "Abhirup Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Abhirup Ghosh"
                },
                "author": "Abhirup Ghosh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10149v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10149v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12052v1",
                "updated": "2025-04-16T13:06:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    13,
                    6,
                    24,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T13:06:24Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    13,
                    6,
                    24,
                    2,
                    106,
                    0
                ],
                "title": "Bayesian dynamic borrowing considering semantic similarity between\n  outcomes for disproportionality analysis in FAERS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian dynamic borrowing considering semantic similarity between\n  outcomes for disproportionality analysis in FAERS"
                },
                "summary": "We present a Bayesian dynamic borrowing (BDB) approach to enhance the\nquantitative identification of adverse events (AEs) in spontaneous reporting\nsystems (SRSs). The method embeds a robust meta-analytic predictive (MAP) prior\nwithin a Bayesian hierarchical model and incorporates semantic similarity\nmeasures (SSMs) to enable weighted information sharing from MedDRA Preferred\nTerms (PTs) that are clinical similar to the target PT. This continuous\nsimilarity-based borrowing addresses limitation of rigid hierarchical grouping\nin current disproportionality analysis (DPA).\n  Using data from the FDA Adverse Event Reporting System (FAERS) between 2015\nand 2019, we evalute this approach - termed IC SSM - against standard\nInformation Component (IC) analysis and IC with borrowing at the MedDRA\nhigh-level group term (HLGT) level. A novel references set (PVLens), derived\nfrom FDA product label updates, enabled prospective evaluation of method\nperformance in identifying AEs prior to official labeling.\n  The IC SSM approach demonstrated improved sensitivity compared to both\ntraditional IC and HLGT-based borrowing, with minor trade-offs in F1 scores and\nYouden's index. IC SSM consistently identified more true positives and detected\nsignals over 5 months sooner than traditional IC. Despite a marginally lower\naggregate Youden's index, IC SSM showed higher performance in the early\npost-marketing period, providing more stable and relevant estimates than\nHLGT-based borrowing and traditional IC.\n  These findings support the use of SSM-informed Bayesian borrowing as a\nscalable and context-aware enhancement to traditional DPA methods. Future\nresearch should validate this approach across other datasets and explore\nadditional similarity metrics and Bayesian inference strategies using\ncase-level data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a Bayesian dynamic borrowing (BDB) approach to enhance the\nquantitative identification of adverse events (AEs) in spontaneous reporting\nsystems (SRSs). The method embeds a robust meta-analytic predictive (MAP) prior\nwithin a Bayesian hierarchical model and incorporates semantic similarity\nmeasures (SSMs) to enable weighted information sharing from MedDRA Preferred\nTerms (PTs) that are clinical similar to the target PT. This continuous\nsimilarity-based borrowing addresses limitation of rigid hierarchical grouping\nin current disproportionality analysis (DPA).\n  Using data from the FDA Adverse Event Reporting System (FAERS) between 2015\nand 2019, we evalute this approach - termed IC SSM - against standard\nInformation Component (IC) analysis and IC with borrowing at the MedDRA\nhigh-level group term (HLGT) level. A novel references set (PVLens), derived\nfrom FDA product label updates, enabled prospective evaluation of method\nperformance in identifying AEs prior to official labeling.\n  The IC SSM approach demonstrated improved sensitivity compared to both\ntraditional IC and HLGT-based borrowing, with minor trade-offs in F1 scores and\nYouden's index. IC SSM consistently identified more true positives and detected\nsignals over 5 months sooner than traditional IC. Despite a marginally lower\naggregate Youden's index, IC SSM showed higher performance in the early\npost-marketing period, providing more stable and relevant estimates than\nHLGT-based borrowing and traditional IC.\n  These findings support the use of SSM-informed Bayesian borrowing as a\nscalable and context-aware enhancement to traditional DPA methods. Future\nresearch should validate this approach across other datasets and explore\nadditional similarity metrics and Bayesian inference strategies using\ncase-level data."
                },
                "authors": [
                    {
                        "name": "Franois Haguinet"
                    },
                    {
                        "name": "Jeffery L Painter"
                    },
                    {
                        "name": "Gregory E Powell"
                    },
                    {
                        "name": "Andrea Callegaro"
                    },
                    {
                        "name": "Andrew Bate"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Bate"
                },
                "author": "Andrew Bate",
                "arxiv_comment": "30 pages, 7 figures, 5 supplementary figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.4; G.3; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12048v1",
                "updated": "2025-04-16T13:04:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    13,
                    4,
                    1,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T13:04:01Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    13,
                    4,
                    1,
                    2,
                    106,
                    0
                ],
                "title": "Modular-Cam: Modular Dynamic Camera-view Video Generation with LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modular-Cam: Modular Dynamic Camera-view Video Generation with LLM"
                },
                "summary": "Text-to-Video generation, which utilizes the provided text prompt to generate\nhigh-quality videos, has drawn increasing attention and achieved great success\ndue to the development of diffusion models recently. Existing methods mainly\nrely on a pre-trained text encoder to capture the semantic information and\nperform cross attention with the encoded text prompt to guide the generation of\nvideo. However, when it comes to complex prompts that contain dynamic scenes\nand multiple camera-view transformations, these methods can not decompose the\noverall information into separate scenes, as well as fail to smoothly change\nscenes based on the corresponding camera-views. To solve these problems, we\npropose a novel method, i.e., Modular-Cam. Specifically, to better understand a\ngiven complex prompt, we utilize a large language model to analyze user\ninstructions and decouple them into multiple scenes together with transition\nactions. To generate a video containing dynamic scenes that match the given\ncamera-views, we incorporate the widely-used temporal transformer into the\ndiffusion model to ensure continuity within a single scene and propose\nCamOperator, a modular network based module that well controls the camera\nmovements. Moreover, we propose AdaControlNet, which utilizes ControlNet to\nensure consistency across scenes and adaptively adjusts the color tone of the\ngenerated video. Extensive qualitative and quantitative experiments prove our\nproposed Modular-Cam's strong capability of generating multi-scene videos\ntogether with its ability to achieve fine-grained control of camera movements.\nGenerated results are available at https://modular-cam.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-Video generation, which utilizes the provided text prompt to generate\nhigh-quality videos, has drawn increasing attention and achieved great success\ndue to the development of diffusion models recently. Existing methods mainly\nrely on a pre-trained text encoder to capture the semantic information and\nperform cross attention with the encoded text prompt to guide the generation of\nvideo. However, when it comes to complex prompts that contain dynamic scenes\nand multiple camera-view transformations, these methods can not decompose the\noverall information into separate scenes, as well as fail to smoothly change\nscenes based on the corresponding camera-views. To solve these problems, we\npropose a novel method, i.e., Modular-Cam. Specifically, to better understand a\ngiven complex prompt, we utilize a large language model to analyze user\ninstructions and decouple them into multiple scenes together with transition\nactions. To generate a video containing dynamic scenes that match the given\ncamera-views, we incorporate the widely-used temporal transformer into the\ndiffusion model to ensure continuity within a single scene and propose\nCamOperator, a modular network based module that well controls the camera\nmovements. Moreover, we propose AdaControlNet, which utilizes ControlNet to\nensure consistency across scenes and adaptively adjusts the color tone of the\ngenerated video. Extensive qualitative and quantitative experiments prove our\nproposed Modular-Cam's strong capability of generating multi-scene videos\ntogether with its ability to achieve fine-grained control of camera movements.\nGenerated results are available at https://modular-cam.github.io."
                },
                "authors": [
                    {
                        "name": "Zirui Pan"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Yipeng Zhang"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Kwan Man Cheng"
                    },
                    {
                        "name": "Yaofei Wu"
                    },
                    {
                        "name": "Wenwu Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Wenwu Zhu"
                },
                "author": "Wenwu Zhu",
                "arxiv_comment": "AAAI 2025 Poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14744v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14744v3",
                "updated": "2025-04-16T13:02:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    13,
                    2,
                    25,
                    2,
                    106,
                    0
                ],
                "published": "2024-08-27T02:45:26Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    45,
                    26,
                    1,
                    240,
                    0
                ],
                "title": "RSTeller: Scaling Up Visual Language Modeling in Remote Sensing with\n  Rich Linguistic Semantics from Openly Available Data and Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RSTeller: Scaling Up Visual Language Modeling in Remote Sensing with\n  Rich Linguistic Semantics from Openly Available Data and Large Language\n  Models"
                },
                "summary": "Abundant, well-annotated multimodal data in remote sensing are pivotal for\naligning complex visual remote sensing (RS) scenes with human language,\nenabling the development of specialized vision language models across diverse\nRS interpretation tasks. However, annotating RS images with rich linguistic\nsemantics at scale demands expertise in RS and substantial human labor, making\nit costly and often impractical. In this study, we propose a workflow that\nleverages large language models (LLMs) to generate multimodal datasets with\nsemantically rich captions at scale from plain OpenStreetMap (OSM) data for\nimages sourced from the Google Earth Engine (GEE) platform. This approach\nfacilitates the generation of paired remote sensing data and can be readily\nscaled up using openly available data. Within this framework, we present\nRSTeller, a multimodal dataset comprising over 1.3 million RS images, each\naccompanied by two descriptive captions. Extensive experiments demonstrate that\nRSTeller enhances the performance of multiple existing vision language models\nfor RS scene understanding through continual pre-training. Our methodology\nsignificantly reduces the manual effort and expertise needed for annotating\nremote sensing imagery while democratizing access to high-quality annotated\ndata. This advancement fosters progress in visual language modeling and\nencourages broader participation in remote sensing research and applications.\nThe RSTeller dataset is available at https://github.com/SlytherinGe/RSTeller.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abundant, well-annotated multimodal data in remote sensing are pivotal for\naligning complex visual remote sensing (RS) scenes with human language,\nenabling the development of specialized vision language models across diverse\nRS interpretation tasks. However, annotating RS images with rich linguistic\nsemantics at scale demands expertise in RS and substantial human labor, making\nit costly and often impractical. In this study, we propose a workflow that\nleverages large language models (LLMs) to generate multimodal datasets with\nsemantically rich captions at scale from plain OpenStreetMap (OSM) data for\nimages sourced from the Google Earth Engine (GEE) platform. This approach\nfacilitates the generation of paired remote sensing data and can be readily\nscaled up using openly available data. Within this framework, we present\nRSTeller, a multimodal dataset comprising over 1.3 million RS images, each\naccompanied by two descriptive captions. Extensive experiments demonstrate that\nRSTeller enhances the performance of multiple existing vision language models\nfor RS scene understanding through continual pre-training. Our methodology\nsignificantly reduces the manual effort and expertise needed for annotating\nremote sensing imagery while democratizing access to high-quality annotated\ndata. This advancement fosters progress in visual language modeling and\nencourages broader participation in remote sensing research and applications.\nThe RSTeller dataset is available at https://github.com/SlytherinGe/RSTeller."
                },
                "authors": [
                    {
                        "name": "Junyao Ge"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Yang Zheng"
                    },
                    {
                        "name": "Kaitai Guo"
                    },
                    {
                        "name": "Jimin Liang"
                    }
                ],
                "author_detail": {
                    "name": "Jimin Liang"
                },
                "author": "Jimin Liang",
                "arxiv_comment": "Submitted to ISPRS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14744v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14744v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.8; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07722v2",
                "updated": "2025-04-16T12:57:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    12,
                    57,
                    23,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-10T13:15:52Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    15,
                    52,
                    3,
                    100,
                    0
                ],
                "title": "Relaxing the Markov Requirements on Reinforcement Learning Under Weak\n  Partial Ignorability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relaxing the Markov Requirements on Reinforcement Learning Under Weak\n  Partial Ignorability"
                },
                "summary": "Incomplete data, confounding effects, and violations of the Markov property\nare interrelated problems which are ubiquitous in Reinforcement Learning\napplications. We introduce the concept of ``partial ignorabilty\" and leverage\nit to establish a novel convergence theorem for adaptive Reinforcement\nLearning. This theoretical result relaxes the Markov assumption on the\nstochastic process underlying conventional $Q$-learning, deploying a\ngeneralized form of the Robbins-Monro stochastic approximation theorem to\nestablish optimality. This result has clear downstream implications for most\nactive subfields of Reinforcement Learning, with clear paths for extension to\nthe field of Causal Inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incomplete data, confounding effects, and violations of the Markov property\nare interrelated problems which are ubiquitous in Reinforcement Learning\napplications. We introduce the concept of ``partial ignorabilty\" and leverage\nit to establish a novel convergence theorem for adaptive Reinforcement\nLearning. This theoretical result relaxes the Markov assumption on the\nstochastic process underlying conventional $Q$-learning, deploying a\ngeneralized form of the Robbins-Monro stochastic approximation theorem to\nestablish optimality. This result has clear downstream implications for most\nactive subfields of Reinforcement Learning, with clear paths for extension to\nthe field of Causal Inference."
                },
                "authors": [
                    {
                        "name": "MaryLena Bleile"
                    }
                ],
                "author_detail": {
                    "name": "MaryLena Bleile"
                },
                "author": "MaryLena Bleile",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60G",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17461v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17461v3",
                "updated": "2025-04-16T12:51:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    12,
                    51,
                    11,
                    2,
                    106,
                    0
                ],
                "published": "2024-11-26T14:28:25Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    28,
                    25,
                    1,
                    331,
                    0
                ],
                "title": "SoK: Decentralized AI (DeAI)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: Decentralized AI (DeAI)"
                },
                "summary": "Centralization enhances the efficiency of Artificial Intelligence (AI), but\nit also brings critical challenges, such as single points of failure, inherent\nbiases, data privacy concerns, and scalability issues, for AI systems. These\nproblems are especially common in closed-source large language models (LLMs),\nwhere user data is collected and used with full transparency. To address these\nissues, blockchain-based decentralized AI (DeAI) has been introduced. DeAI\nleverages the strengths of blockchain technologies to enhance the transparency,\nsecurity, decentralization, as well as trustworthiness of AI systems. Although\nDeAI has been widely developed in industry, a comprehensive understanding of\nstate-of-the-art practical DeAI solutions is still lacking. In this work, we\npresent a Systematization of Knowledge (SoK) for blockchain-based DeAI\nsolutions. We propose a taxonomy to classify existing DeAI protocols based on\nthe model lifecycle. Based on this taxonomy, we provide a structured way to\nclarify the landscape of DeAI protocols and identify their similarities and\ndifferences. Specifically, we analyze the functionalities of blockchain in\nDeAI, investigate how blockchain features contribute to enhancing the security,\ntransparency, and trustworthiness of AI processes, and also ensure fair\nincentives for AI data and model contributors. In addition, we provide key\ninsights and research gaps in developing DeAI protocols for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Centralization enhances the efficiency of Artificial Intelligence (AI), but\nit also brings critical challenges, such as single points of failure, inherent\nbiases, data privacy concerns, and scalability issues, for AI systems. These\nproblems are especially common in closed-source large language models (LLMs),\nwhere user data is collected and used with full transparency. To address these\nissues, blockchain-based decentralized AI (DeAI) has been introduced. DeAI\nleverages the strengths of blockchain technologies to enhance the transparency,\nsecurity, decentralization, as well as trustworthiness of AI systems. Although\nDeAI has been widely developed in industry, a comprehensive understanding of\nstate-of-the-art practical DeAI solutions is still lacking. In this work, we\npresent a Systematization of Knowledge (SoK) for blockchain-based DeAI\nsolutions. We propose a taxonomy to classify existing DeAI protocols based on\nthe model lifecycle. Based on this taxonomy, we provide a structured way to\nclarify the landscape of DeAI protocols and identify their similarities and\ndifferences. Specifically, we analyze the functionalities of blockchain in\nDeAI, investigate how blockchain features contribute to enhancing the security,\ntransparency, and trustworthiness of AI processes, and also ensure fair\nincentives for AI data and model contributors. In addition, we provide key\ninsights and research gaps in developing DeAI protocols for future research."
                },
                "authors": [
                    {
                        "name": "Zhipeng Wang"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Elizabeth Lui"
                    },
                    {
                        "name": "Vatsal Shah"
                    },
                    {
                        "name": "Xihan Xiong"
                    },
                    {
                        "name": "Jiahao Sun"
                    },
                    {
                        "name": "Davide Crapis"
                    },
                    {
                        "name": "William Knottenbelt"
                    }
                ],
                "author_detail": {
                    "name": "William Knottenbelt"
                },
                "author": "William Knottenbelt",
                "arxiv_comment": "This is a Systematization of Knowledge (SoK) for the rapidly evolving\n  field of Decentralized AI (DeAI). We welcome valuable comments, suggestions,\n  and collaboration to further refine and enhance this work. We hope our\n  contribution will help accelerate the advancement of DeAI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17461v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17461v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12034v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12034v1",
                "updated": "2025-04-16T12:48:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    12,
                    48,
                    0,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T12:48:00Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    12,
                    48,
                    0,
                    2,
                    106,
                    0
                ],
                "title": "OpDiffer: LLM-Assisted Opcode-Level Differential Testing of Ethereum\n  Virtual Machine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpDiffer: LLM-Assisted Opcode-Level Differential Testing of Ethereum\n  Virtual Machine"
                },
                "summary": "As Ethereum continues to thrive, the Ethereum Virtual Machine (EVM) has\nbecome the cornerstone powering tens of millions of active smart contracts.\nIntuitively, security issues in EVMs could lead to inconsistent behaviors among\nsmart contracts or even denial-of-service of the entire blockchain network.\nHowever, to the best of our knowledge, only a limited number of studies focus\non the security of EVMs. Moreover, they suffer from 1) insufficient test input\ndiversity and invalid semantics; and 2) the inability to automatically identify\nbugs and locate root causes. To bridge this gap, we propose OpDiffer, a\ndifferential testing framework for EVM, which takes advantage of LLMs and\nstatic analysis methods to address the above two limitations. We conducted the\nlargest-scale evaluation, covering nine EVMs and uncovering 26 previously\nunknown bugs, 22 of which have been confirmed by developers and three have been\nassigned CNVD IDs. Compared to state-of-the-art baselines, OpDiffer can improve\ncode coverage by at most 71.06%, 148.40% and 655.56%, respectively. Through an\nanalysis of real-world deployed Ethereum contracts, we estimate that 7.21% of\nthe contracts could trigger our identified EVM bugs under certain environmental\nsettings, potentially resulting in severe negative impact on the Ethereum\necosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Ethereum continues to thrive, the Ethereum Virtual Machine (EVM) has\nbecome the cornerstone powering tens of millions of active smart contracts.\nIntuitively, security issues in EVMs could lead to inconsistent behaviors among\nsmart contracts or even denial-of-service of the entire blockchain network.\nHowever, to the best of our knowledge, only a limited number of studies focus\non the security of EVMs. Moreover, they suffer from 1) insufficient test input\ndiversity and invalid semantics; and 2) the inability to automatically identify\nbugs and locate root causes. To bridge this gap, we propose OpDiffer, a\ndifferential testing framework for EVM, which takes advantage of LLMs and\nstatic analysis methods to address the above two limitations. We conducted the\nlargest-scale evaluation, covering nine EVMs and uncovering 26 previously\nunknown bugs, 22 of which have been confirmed by developers and three have been\nassigned CNVD IDs. Compared to state-of-the-art baselines, OpDiffer can improve\ncode coverage by at most 71.06%, 148.40% and 655.56%, respectively. Through an\nanalysis of real-world deployed Ethereum contracts, we estimate that 7.21% of\nthe contracts could trigger our identified EVM bugs under certain environmental\nsettings, potentially resulting in severe negative impact on the Ethereum\necosystem."
                },
                "authors": [
                    {
                        "name": "Jie Ma"
                    },
                    {
                        "name": "Ningyu He"
                    },
                    {
                        "name": "Jinwen Xi"
                    },
                    {
                        "name": "Mingzhe Xing"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Ying Gao"
                    },
                    {
                        "name": "Yinliang Yue"
                    }
                ],
                "author_detail": {
                    "name": "Yinliang Yue"
                },
                "author": "Yinliang Yue",
                "arxiv_doi": "10.1145/3728946",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3728946",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.12034v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12034v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To appear in ISSTA'25",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12697v2",
                "updated": "2025-04-16T12:29:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    12,
                    29,
                    45,
                    2,
                    106,
                    0
                ],
                "published": "2024-11-19T18:06:06Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    6,
                    6,
                    1,
                    324,
                    0
                ],
                "title": "Attribute Inference Attacks for Federated Regression Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attribute Inference Attacks for Federated Regression Tasks"
                },
                "summary": "Federated Learning (FL) enables multiple clients, such as mobile phones and\nIoT devices, to collaboratively train a global machine learning model while\nkeeping their data localized. However, recent studies have revealed that the\ntraining phase of FL is vulnerable to reconstruction attacks, such as attribute\ninference attacks (AIA), where adversaries exploit exchanged messages and\nauxiliary public information to uncover sensitive attributes of targeted\nclients. While these attacks have been extensively studied in the context of\nclassification tasks, their impact on regression tasks remains largely\nunexplored. In this paper, we address this gap by proposing novel model-based\nAIAs specifically designed for regression tasks in FL environments. Our\napproach considers scenarios where adversaries can either eavesdrop on\nexchanged messages or directly interfere with the training process. We\nbenchmark our proposed attacks against state-of-the-art methods using\nreal-world datasets. The results demonstrate a significant increase in\nreconstruction accuracy, particularly in heterogeneous client datasets, a\ncommon scenario in FL. The efficacy of our model-based AIAs makes them better\ncandidates for empirically quantifying privacy leakage for federated regression\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) enables multiple clients, such as mobile phones and\nIoT devices, to collaboratively train a global machine learning model while\nkeeping their data localized. However, recent studies have revealed that the\ntraining phase of FL is vulnerable to reconstruction attacks, such as attribute\ninference attacks (AIA), where adversaries exploit exchanged messages and\nauxiliary public information to uncover sensitive attributes of targeted\nclients. While these attacks have been extensively studied in the context of\nclassification tasks, their impact on regression tasks remains largely\nunexplored. In this paper, we address this gap by proposing novel model-based\nAIAs specifically designed for regression tasks in FL environments. Our\napproach considers scenarios where adversaries can either eavesdrop on\nexchanged messages or directly interfere with the training process. We\nbenchmark our proposed attacks against state-of-the-art methods using\nreal-world datasets. The results demonstrate a significant increase in\nreconstruction accuracy, particularly in heterogeneous client datasets, a\ncommon scenario in FL. The efficacy of our model-based AIAs makes them better\ncandidates for empirically quantifying privacy leakage for federated regression\ntasks."
                },
                "authors": [
                    {
                        "name": "Francesco Diana"
                    },
                    {
                        "name": "Othmane Marfoq"
                    },
                    {
                        "name": "Chuan Xu"
                    },
                    {
                        "name": "Giovanni Neglia"
                    },
                    {
                        "name": "Frdric Giroire"
                    },
                    {
                        "name": "Eoin Thomas"
                    }
                ],
                "author_detail": {
                    "name": "Eoin Thomas"
                },
                "author": "Eoin Thomas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00578v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00578v2",
                "updated": "2025-04-16T12:28:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    12,
                    28,
                    23,
                    2,
                    106,
                    0
                ],
                "published": "2024-09-01T01:59:21Z",
                "published_parsed": [
                    2024,
                    9,
                    1,
                    1,
                    59,
                    21,
                    6,
                    245,
                    0
                ],
                "title": "Exact moments for a run and tumble particle in a harmonic trap with a\n  finite tumble time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exact moments for a run and tumble particle in a harmonic trap with a\n  finite tumble time"
                },
                "summary": "We study the problem of a run and tumble particle in a harmonic trap, with a\nfinite run and tumble time, by a direct integration of the equation of motion.\nAn exact 1D steady state distribution, diagram laws and a programmable Volterra\ndifference equation are derived to calculate any order of moments in any other\ndimension, both for steady state as well as the Laplace transform in time for\nthe intermediate states. We also use the moments to infer the distribution by\nconsidering a Gaussian quadrature for the corresponding measure, and from the\nscaling law of high order moments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of a run and tumble particle in a harmonic trap, with a\nfinite run and tumble time, by a direct integration of the equation of motion.\nAn exact 1D steady state distribution, diagram laws and a programmable Volterra\ndifference equation are derived to calculate any order of moments in any other\ndimension, both for steady state as well as the Laplace transform in time for\nthe intermediate states. We also use the moments to infer the distribution by\nconsidering a Gaussian quadrature for the corresponding measure, and from the\nscaling law of high order moments."
                },
                "authors": [
                    {
                        "name": "Aoran Sun"
                    },
                    {
                        "name": "Fangfu Ye"
                    },
                    {
                        "name": "Rudolf Podgornik"
                    }
                ],
                "author_detail": {
                    "name": "Rudolf Podgornik"
                },
                "author": "Rudolf Podgornik",
                "arxiv_comment": "13 pages 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00578v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00578v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12016v1",
                "updated": "2025-04-16T12:16:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    12,
                    16,
                    10,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T12:16:10Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    12,
                    16,
                    10,
                    2,
                    106,
                    0
                ],
                "title": "Active Human Feedback Collection via Neural Contextual Dueling Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Human Feedback Collection via Neural Contextual Dueling Bandits"
                },
                "summary": "Collecting human preference feedback is often expensive, leading recent works\nto develop principled algorithms to select them more efficiently. However,\nthese works assume that the underlying reward function is linear, an assumption\nthat does not hold in many real-life applications, such as online\nrecommendation and LLM alignment. To address this limitation, we propose\nNeural-ADB, an algorithm based on the neural contextual dueling bandit\nframework that provides a principled and practical method for collecting human\npreference feedback when the underlying latent reward function is non-linear.\nWe theoretically show that when preference feedback follows the\nBradley-Terry-Luce model, the worst sub-optimality gap of the policy learned by\nNeural-ADB decreases at a sub-linear rate as the preference dataset increases.\nOur experimental results on problem instances derived from synthetic preference\ndatasets further validate the effectiveness of Neural-ADB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collecting human preference feedback is often expensive, leading recent works\nto develop principled algorithms to select them more efficiently. However,\nthese works assume that the underlying reward function is linear, an assumption\nthat does not hold in many real-life applications, such as online\nrecommendation and LLM alignment. To address this limitation, we propose\nNeural-ADB, an algorithm based on the neural contextual dueling bandit\nframework that provides a principled and practical method for collecting human\npreference feedback when the underlying latent reward function is non-linear.\nWe theoretically show that when preference feedback follows the\nBradley-Terry-Luce model, the worst sub-optimality gap of the policy learned by\nNeural-ADB decreases at a sub-linear rate as the preference dataset increases.\nOur experimental results on problem instances derived from synthetic preference\ndatasets further validate the effectiveness of Neural-ADB."
                },
                "authors": [
                    {
                        "name": "Arun Verma"
                    },
                    {
                        "name": "Xiaoqiang Lin"
                    },
                    {
                        "name": "Zhongxiang Dai"
                    },
                    {
                        "name": "Daniela Rus"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Kian Hsiang Low"
                },
                "author": "Bryan Kian Hsiang Low",
                "arxiv_comment": "Accepted at ICLR 2025 Workshop on Bidirectional Human-AI Alignment\n  (BiAlign)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12012v1",
                "updated": "2025-04-16T12:13:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    12,
                    13,
                    2,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T12:13:02Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    12,
                    13,
                    2,
                    2,
                    106,
                    0
                ],
                "title": "Purposefully Induced Psychosis (PIP): Embracing Hallucination as\n  Imagination in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purposefully Induced Psychosis (PIP): Embracing Hallucination as\n  Imagination in Large Language Models"
                },
                "summary": "Hallucinations in Large Language Models (LLMs) are widely regarded as errors\n- outputs that deviate from factual accuracy. However, in creative or\nexploratory contexts, these \"mistakes\" may represent unexpected avenues for\ninnovation. We introduce Purposefully Induced Psychosis (PIP), a novel approach\nthat amplifies LLM hallucinations for imaginative tasks such as speculative\nfiction, interactive storytelling, and mixed-reality simulations. Drawing on\nHerman Melville's Moby-Dick, where Pip's \"madness\" reveals profound insight, we\nreframe hallucinations as a source of computational imagination rather than a\nflaw. Our method fine-tunes LLMs to encourage speculative, metaphorical, and\nsurreal outputs - hallucinations that are useful when factual accuracy is not\nthe chief objective. Inspired by the consensual illusions of theater and stage\nmagic, PIP situates these creative missteps in contexts where users willingly\nsuspend disbelief, thereby transforming \"errors\" into catalysts for new ways of\nthinking. We discuss potential applications, design principles for ensuring\nuser consent, preliminary observations, and implications for broader AI ethics\nand human-AI collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations in Large Language Models (LLMs) are widely regarded as errors\n- outputs that deviate from factual accuracy. However, in creative or\nexploratory contexts, these \"mistakes\" may represent unexpected avenues for\ninnovation. We introduce Purposefully Induced Psychosis (PIP), a novel approach\nthat amplifies LLM hallucinations for imaginative tasks such as speculative\nfiction, interactive storytelling, and mixed-reality simulations. Drawing on\nHerman Melville's Moby-Dick, where Pip's \"madness\" reveals profound insight, we\nreframe hallucinations as a source of computational imagination rather than a\nflaw. Our method fine-tunes LLMs to encourage speculative, metaphorical, and\nsurreal outputs - hallucinations that are useful when factual accuracy is not\nthe chief objective. Inspired by the consensual illusions of theater and stage\nmagic, PIP situates these creative missteps in contexts where users willingly\nsuspend disbelief, thereby transforming \"errors\" into catalysts for new ways of\nthinking. We discuss potential applications, design principles for ensuring\nuser consent, preliminary observations, and implications for broader AI ethics\nand human-AI collaboration."
                },
                "authors": [
                    {
                        "name": "Kris Pilcher"
                    },
                    {
                        "name": "Esen K. Ttnc"
                    }
                ],
                "author_detail": {
                    "name": "Esen K. Ttnc"
                },
                "author": "Esen K. Ttnc",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12007v1",
                "updated": "2025-04-16T12:01:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    12,
                    1,
                    3,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T12:01:03Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    12,
                    1,
                    3,
                    2,
                    106,
                    0
                ],
                "title": "Generative Recommendation with Continuous-Token Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Recommendation with Continuous-Token Diffusion"
                },
                "summary": "In recent years, there has been a significant trend toward using large\nlanguage model (LLM)-based recommender systems (RecSys). Current research\nprimarily focuses on representing complex user-item interactions within a\ndiscrete space to align with the inherent discrete nature of language models.\nHowever, this approach faces limitations due to its discrete nature: (i)\ninformation is often compressed during discretization; (ii) the tokenization\nand generation for the vast number of users and items in real-world scenarios\nare constrained by a limited vocabulary. Embracing continuous data presents a\npromising alternative to enhance expressive capabilities, though this approach\nis still in its early stages. To address this gap, we propose a novel\nframework, DeftRec, which incorporates \\textbf{de}noising di\\textbf{f}fusion\nmodels to enable LLM-based RecSys to seamlessly support continuous\n\\textbf{t}oken as input and target. First, we introduce a robust tokenizer with\na masking operation and an additive K-way architecture to index users and\nitems, capturing their complex collaborative relationships into continuous\ntokens. Crucially, we develop a denoising diffusion model to process user\npreferences within continuous domains by conditioning on reasoning content from\npre-trained large language model. During the denoising process, we reformulate\nthe objective to include negative interactions, building a comprehensive\nunderstanding of user preferences for effective and accurate recommendation\ngeneration. Finally, given a continuous token as output, recommendations can be\neasily generated through score-based retrieval. Extensive experiments\ndemonstrate the effectiveness of the proposed methods, showing that DeftRec\nsurpasses competitive benchmarks, including both traditional and emerging\nLLM-based RecSys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, there has been a significant trend toward using large\nlanguage model (LLM)-based recommender systems (RecSys). Current research\nprimarily focuses on representing complex user-item interactions within a\ndiscrete space to align with the inherent discrete nature of language models.\nHowever, this approach faces limitations due to its discrete nature: (i)\ninformation is often compressed during discretization; (ii) the tokenization\nand generation for the vast number of users and items in real-world scenarios\nare constrained by a limited vocabulary. Embracing continuous data presents a\npromising alternative to enhance expressive capabilities, though this approach\nis still in its early stages. To address this gap, we propose a novel\nframework, DeftRec, which incorporates \\textbf{de}noising di\\textbf{f}fusion\nmodels to enable LLM-based RecSys to seamlessly support continuous\n\\textbf{t}oken as input and target. First, we introduce a robust tokenizer with\na masking operation and an additive K-way architecture to index users and\nitems, capturing their complex collaborative relationships into continuous\ntokens. Crucially, we develop a denoising diffusion model to process user\npreferences within continuous domains by conditioning on reasoning content from\npre-trained large language model. During the denoising process, we reformulate\nthe objective to include negative interactions, building a comprehensive\nunderstanding of user preferences for effective and accurate recommendation\ngeneration. Finally, given a continuous token as output, recommendations can be\neasily generated through score-based retrieval. Extensive experiments\ndemonstrate the effectiveness of the proposed methods, showing that DeftRec\nsurpasses competitive benchmarks, including both traditional and emerging\nLLM-based RecSys."
                },
                "authors": [
                    {
                        "name": "Haohao Qu"
                    },
                    {
                        "name": "Wenqi Fan"
                    },
                    {
                        "name": "Shanru Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shanru Lin"
                },
                "author": "Shanru Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08989v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08989v2",
                "updated": "2025-04-16T11:52:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    11,
                    52,
                    45,
                    2,
                    106,
                    0
                ],
                "published": "2025-02-13T06:01:09Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    1,
                    9,
                    3,
                    44,
                    0
                ],
                "title": "RLSA-PFL: Robust Lightweight Secure Aggregation with Model Inconsistency\n  Detection in Privacy-Preserving Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RLSA-PFL: Robust Lightweight Secure Aggregation with Model Inconsistency\n  Detection in Privacy-Preserving Federated Learning"
                },
                "summary": "Federated Learning (FL) allows users to collaboratively train a global\nmachine learning model by sharing local model only, without exposing their\nprivate data to a central server. This distributed learning is particularly\nappealing in scenarios where data privacy is crucial, and it has garnered\nsubstantial attention from both industry and academia. However, studies have\nrevealed privacy vulnerabilities in FL, where adversaries can potentially infer\nsensitive information from the shared model parameters. In this paper, we\npresent an efficient masking-based secure aggregation scheme utilizing\nlightweight cryptographic primitives to mitigate privacy risks. Our scheme\noffers several advantages over existing methods. First, it requires only a\nsingle setup phase for the entire FL training session, significantly reducing\ncommunication overhead. Second, it minimizes user-side overhead by eliminating\nthe need for user-to-user interactions, utilizing an intermediate server layer\nand a lightweight key negotiation method. Third, the scheme is highly resilient\nto user dropouts, and the users can join at any FL round. Fourth, it can detect\nand defend against malicious server activities, including recently discovered\nmodel inconsistency attacks. Finally, our scheme ensures security in both\nsemi-honest and malicious settings. We provide security analysis to formally\nprove the robustness of our approach. Furthermore, we implemented an end-to-end\nprototype of our scheme. We conducted comprehensive experiments and\ncomparisons, which show that it outperforms existing solutions in terms of\ncommunication and computation overhead, functionality, and security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) allows users to collaboratively train a global\nmachine learning model by sharing local model only, without exposing their\nprivate data to a central server. This distributed learning is particularly\nappealing in scenarios where data privacy is crucial, and it has garnered\nsubstantial attention from both industry and academia. However, studies have\nrevealed privacy vulnerabilities in FL, where adversaries can potentially infer\nsensitive information from the shared model parameters. In this paper, we\npresent an efficient masking-based secure aggregation scheme utilizing\nlightweight cryptographic primitives to mitigate privacy risks. Our scheme\noffers several advantages over existing methods. First, it requires only a\nsingle setup phase for the entire FL training session, significantly reducing\ncommunication overhead. Second, it minimizes user-side overhead by eliminating\nthe need for user-to-user interactions, utilizing an intermediate server layer\nand a lightweight key negotiation method. Third, the scheme is highly resilient\nto user dropouts, and the users can join at any FL round. Fourth, it can detect\nand defend against malicious server activities, including recently discovered\nmodel inconsistency attacks. Finally, our scheme ensures security in both\nsemi-honest and malicious settings. We provide security analysis to formally\nprove the robustness of our approach. Furthermore, we implemented an end-to-end\nprototype of our scheme. We conducted comprehensive experiments and\ncomparisons, which show that it outperforms existing solutions in terms of\ncommunication and computation overhead, functionality, and security."
                },
                "authors": [
                    {
                        "name": "Nazatul H. Sultan"
                    },
                    {
                        "name": "Yan Bo"
                    },
                    {
                        "name": "Yansong Gao"
                    },
                    {
                        "name": "Seyit Camtepe"
                    },
                    {
                        "name": "Arash Mahboubi"
                    },
                    {
                        "name": "Hang Thanh Bui"
                    },
                    {
                        "name": "Aufeef Chauhan"
                    },
                    {
                        "name": "Hamed Aboutorab"
                    },
                    {
                        "name": "Michael Bewong"
                    },
                    {
                        "name": "Dineshkumar Singh"
                    },
                    {
                        "name": "Praveen Gauravaram"
                    },
                    {
                        "name": "Rafiqul Islam"
                    },
                    {
                        "name": "Sharif Abuadbba"
                    }
                ],
                "author_detail": {
                    "name": "Sharif Abuadbba"
                },
                "author": "Sharif Abuadbba",
                "arxiv_comment": "16 pages, 10 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08989v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08989v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68P27",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "E.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11995v1",
                "updated": "2025-04-16T11:40:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    11,
                    40,
                    55,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T11:40:55Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    11,
                    40,
                    55,
                    2,
                    106,
                    0
                ],
                "title": "A Review of YOLOv12: Attention-Based Enhancements vs. Previous Versions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Review of YOLOv12: Attention-Based Enhancements vs. Previous Versions"
                },
                "summary": "The YOLO (You Only Look Once) series has been a leading framework in\nreal-time object detection, consistently improving the balance between speed\nand accuracy. However, integrating attention mechanisms into YOLO has been\nchallenging due to their high computational overhead. YOLOv12 introduces a\nnovel approach that successfully incorporates attention-based enhancements\nwhile preserving real-time performance. This paper provides a comprehensive\nreview of YOLOv12's architectural innovations, including Area Attention for\ncomputationally efficient self-attention, Residual Efficient Layer Aggregation\nNetworks for improved feature aggregation, and FlashAttention for optimized\nmemory access. Additionally, we benchmark YOLOv12 against prior YOLO versions\nand competing object detectors, analyzing its improvements in accuracy,\ninference speed, and computational efficiency. Through this analysis, we\ndemonstrate how YOLOv12 advances real-time object detection by refining the\nlatency-accuracy trade-off and optimizing computational resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The YOLO (You Only Look Once) series has been a leading framework in\nreal-time object detection, consistently improving the balance between speed\nand accuracy. However, integrating attention mechanisms into YOLO has been\nchallenging due to their high computational overhead. YOLOv12 introduces a\nnovel approach that successfully incorporates attention-based enhancements\nwhile preserving real-time performance. This paper provides a comprehensive\nreview of YOLOv12's architectural innovations, including Area Attention for\ncomputationally efficient self-attention, Residual Efficient Layer Aggregation\nNetworks for improved feature aggregation, and FlashAttention for optimized\nmemory access. Additionally, we benchmark YOLOv12 against prior YOLO versions\nand competing object detectors, analyzing its improvements in accuracy,\ninference speed, and computational efficiency. Through this analysis, we\ndemonstrate how YOLOv12 advances real-time object detection by refining the\nlatency-accuracy trade-off and optimizing computational resources."
                },
                "authors": [
                    {
                        "name": "Rahima Khanam"
                    },
                    {
                        "name": "Muhammad Hussain"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Hussain"
                },
                "author": "Muhammad Hussain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11986v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11986v1",
                "updated": "2025-04-16T11:27:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    11,
                    27,
                    47,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T11:27:47Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    11,
                    27,
                    47,
                    2,
                    106,
                    0
                ],
                "title": "Language Models as Quasi-Crystalline Thought: Structure, Constraint, and\n  Emergence in Generative Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models as Quasi-Crystalline Thought: Structure, Constraint, and\n  Emergence in Generative Systems"
                },
                "summary": "This essay proposes an analogy between large language models (LLMs) and\nquasicrystals: systems that exhibit global coherence without periodic\nrepetition and that are generated through local constraints. While LLMs are\noften evaluated in terms of predictive accuracy, factuality, or alignment, this\nstructural perspective suggests that their most characteristic behavior is the\nproduction of internally resonant linguistic patterns. Just as quasicrystals\nforced a redefinition of order in physical systems, viewing LLMs as generators\nof quasi-structured language opens new paths for evaluation and design:\nprivileging propagation of constraint over token-level accuracy, and coherence\nof form over fixed meaning. LLM outputs should be read not only for what they\nsay, but for the patterns of constraint and coherence that organize them. This\nshift reframes generative language as a space of emergent patterning: LLMs are\nneither fully random nor strictly rule-based, but defined by a logic of\nconstraint, resonance, and structural depth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This essay proposes an analogy between large language models (LLMs) and\nquasicrystals: systems that exhibit global coherence without periodic\nrepetition and that are generated through local constraints. While LLMs are\noften evaluated in terms of predictive accuracy, factuality, or alignment, this\nstructural perspective suggests that their most characteristic behavior is the\nproduction of internally resonant linguistic patterns. Just as quasicrystals\nforced a redefinition of order in physical systems, viewing LLMs as generators\nof quasi-structured language opens new paths for evaluation and design:\nprivileging propagation of constraint over token-level accuracy, and coherence\nof form over fixed meaning. LLM outputs should be read not only for what they\nsay, but for the patterns of constraint and coherence that organize them. This\nshift reframes generative language as a space of emergent patterning: LLMs are\nneither fully random nor strictly rule-based, but defined by a logic of\nconstraint, resonance, and structural depth."
                },
                "authors": [
                    {
                        "name": "Jose Manuel Guevara-Vela"
                    }
                ],
                "author_detail": {
                    "name": "Jose Manuel Guevara-Vela"
                },
                "author": "Jose Manuel Guevara-Vela",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11986v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11985v1",
                "updated": "2025-04-16T11:27:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    11,
                    27,
                    23,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T11:27:23Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    11,
                    27,
                    23,
                    2,
                    106,
                    0
                ],
                "title": "A Heavy-Metal Scenario of Ultra-High-Energy Cosmic Rays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Heavy-Metal Scenario of Ultra-High-Energy Cosmic Rays"
                },
                "summary": "The mass composition of ultra-high-energy cosmic rays is an open problem in\nastroparticle physics. It is usually inferred from the depth of the shower\nmaximum (Xmax) of cosmic-ray showers, which is only ambiguously determined by\nmodern hadronic interaction models. We examine a data-driven scenario, in which\nwe consider the expectation value of Xmax as a free parameter. We test the\nnovel hypothesis whether the cosmic-ray data from the Pierre Auger Observatory\ncan be interpreted in a consistent picture, under the assumption that the mass\ncomposition of cosmic rays at the highest energies is dominated by high\nmetallicity, resulting in pure iron nuclei at energies above ~40 EeV. We\ninvestigate the implications on astrophysical observations and hadronic\ninteractions, and we discuss the global consistency of the data assuming this\nheavy-metal scenario. We conclude that the data from the Pierre Auger\nObservatory can be interpreted consistently if the expectation values for Xmax\nfrom modern hadronic interaction models are shifted to larger values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The mass composition of ultra-high-energy cosmic rays is an open problem in\nastroparticle physics. It is usually inferred from the depth of the shower\nmaximum (Xmax) of cosmic-ray showers, which is only ambiguously determined by\nmodern hadronic interaction models. We examine a data-driven scenario, in which\nwe consider the expectation value of Xmax as a free parameter. We test the\nnovel hypothesis whether the cosmic-ray data from the Pierre Auger Observatory\ncan be interpreted in a consistent picture, under the assumption that the mass\ncomposition of cosmic rays at the highest energies is dominated by high\nmetallicity, resulting in pure iron nuclei at energies above ~40 EeV. We\ninvestigate the implications on astrophysical observations and hadronic\ninteractions, and we discuss the global consistency of the data assuming this\nheavy-metal scenario. We conclude that the data from the Pierre Auger\nObservatory can be interpreted consistently if the expectation values for Xmax\nfrom modern hadronic interaction models are shifted to larger values."
                },
                "authors": [
                    {
                        "name": "Jakub Vcha"
                    },
                    {
                        "name": "Alena Bakalov"
                    },
                    {
                        "name": "Ana L. Mller"
                    },
                    {
                        "name": "Olena Tkachenko"
                    },
                    {
                        "name": "Maximilian K. Stadelmaier"
                    }
                ],
                "author_detail": {
                    "name": "Maximilian K. Stadelmaier"
                },
                "author": "Maximilian K. Stadelmaier",
                "arxiv_comment": "Submitted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20791v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20791v2",
                "updated": "2025-04-16T11:18:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    11,
                    18,
                    52,
                    2,
                    106,
                    0
                ],
                "published": "2025-02-28T07:16:09Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    7,
                    16,
                    9,
                    4,
                    59,
                    0
                ],
                "title": "Cyber Defense Reinvented: Large Language Models as Threat Intelligence\n  Copilots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyber Defense Reinvented: Large Language Models as Threat Intelligence\n  Copilots"
                },
                "summary": "The exponential growth of cyber threat knowledge, exemplified by the\nexpansion of databases such as MITRE-CVE and NVD, poses significant challenges\nfor cyber threat analysis. Security professionals are increasingly burdened by\nthe sheer volume and complexity of information, creating an urgent need for\neffective tools to navigate, synthesize, and act on large-scale data to counter\nevolving threats proactively. However, conventional threat intelligence tools\noften fail to scale with the dynamic nature of this data and lack the\nadaptability to support diverse threat intelligence tasks.\n  In this work, we introduce CYLENS, a cyber threat intelligence copilot\npowered by large language models (LLMs). CYLENS is designed to assist security\nprofessionals throughout the entire threat management lifecycle, supporting\nthreat attribution, contextualization, detection, correlation, prioritization,\nand remediation. To ensure domain expertise, CYLENS integrates knowledge from\n271,570 threat reports into its model parameters and incorporates six\nspecialized NLP modules to enhance reasoning capabilities. Furthermore, CYLENS\ncan be customized to meet the unique needs of different or ganizations,\nunderscoring its adaptability. Through extensive evaluations, we demonstrate\nthat CYLENS consistently outperforms industry-leading LLMs and state-of-the-art\ncybersecurity agents. By detailing its design, development, and evaluation,\nthis work provides a blueprint for leveraging LLMs to address complex,\ndata-intensive cybersecurity challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of cyber threat knowledge, exemplified by the\nexpansion of databases such as MITRE-CVE and NVD, poses significant challenges\nfor cyber threat analysis. Security professionals are increasingly burdened by\nthe sheer volume and complexity of information, creating an urgent need for\neffective tools to navigate, synthesize, and act on large-scale data to counter\nevolving threats proactively. However, conventional threat intelligence tools\noften fail to scale with the dynamic nature of this data and lack the\nadaptability to support diverse threat intelligence tasks.\n  In this work, we introduce CYLENS, a cyber threat intelligence copilot\npowered by large language models (LLMs). CYLENS is designed to assist security\nprofessionals throughout the entire threat management lifecycle, supporting\nthreat attribution, contextualization, detection, correlation, prioritization,\nand remediation. To ensure domain expertise, CYLENS integrates knowledge from\n271,570 threat reports into its model parameters and incorporates six\nspecialized NLP modules to enhance reasoning capabilities. Furthermore, CYLENS\ncan be customized to meet the unique needs of different or ganizations,\nunderscoring its adaptability. Through extensive evaluations, we demonstrate\nthat CYLENS consistently outperforms industry-leading LLMs and state-of-the-art\ncybersecurity agents. By detailing its design, development, and evaluation,\nthis work provides a blueprint for leveraging LLMs to address complex,\ndata-intensive cybersecurity challenges."
                },
                "authors": [
                    {
                        "name": "Xiaoqun Liu"
                    },
                    {
                        "name": "Jiacheng Liang"
                    },
                    {
                        "name": "Qiben Yan"
                    },
                    {
                        "name": "Jiyong Jang"
                    },
                    {
                        "name": "Sicheng Mao"
                    },
                    {
                        "name": "Muchao Ye"
                    },
                    {
                        "name": "Jinyuan Jia"
                    },
                    {
                        "name": "Zhaohan Xi"
                    }
                ],
                "author_detail": {
                    "name": "Zhaohan Xi"
                },
                "author": "Zhaohan Xi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20791v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20791v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11975v1",
                "updated": "2025-04-16T11:15:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    11,
                    15,
                    26,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T11:15:26Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    11,
                    15,
                    26,
                    2,
                    106,
                    0
                ],
                "title": "SemEval-2025 Task 3: Mu-SHROOM, the Multilingual Shared Task on\n  Hallucinations and Related Observable Overgeneration Mistakes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemEval-2025 Task 3: Mu-SHROOM, the Multilingual Shared Task on\n  Hallucinations and Related Observable Overgeneration Mistakes"
                },
                "summary": "We present the Mu-SHROOM shared task which is focused on detecting\nhallucinations and other overgeneration mistakes in the output of\ninstruction-tuned large language models (LLMs). Mu-SHROOM addresses\ngeneral-purpose LLMs in 14 languages, and frames the hallucination detection\nproblem as a span-labeling task. We received 2,618 submissions from 43\nparticipating teams employing diverse methodologies. The large number of\nsubmissions underscores the interest of the community in hallucination\ndetection. We present the results of the participating systems and conduct an\nempirical analysis to identify key factors contributing to strong performance\nin this task. We also emphasize relevant current challenges, notably the\nvarying degree of hallucinations across languages and the high annotator\ndisagreement when labeling hallucination spans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the Mu-SHROOM shared task which is focused on detecting\nhallucinations and other overgeneration mistakes in the output of\ninstruction-tuned large language models (LLMs). Mu-SHROOM addresses\ngeneral-purpose LLMs in 14 languages, and frames the hallucination detection\nproblem as a span-labeling task. We received 2,618 submissions from 43\nparticipating teams employing diverse methodologies. The large number of\nsubmissions underscores the interest of the community in hallucination\ndetection. We present the results of the participating systems and conduct an\nempirical analysis to identify key factors contributing to strong performance\nin this task. We also emphasize relevant current challenges, notably the\nvarying degree of hallucinations across languages and the high annotator\ndisagreement when labeling hallucination spans."
                },
                "authors": [
                    {
                        "name": "Ral Vzquez"
                    },
                    {
                        "name": "Timothee Mickus"
                    },
                    {
                        "name": "Elaine Zosa"
                    },
                    {
                        "name": "Teemu Vahtola"
                    },
                    {
                        "name": "Jrg Tiedemann"
                    },
                    {
                        "name": "Aman Sinha"
                    },
                    {
                        "name": "Vincent Segonne"
                    },
                    {
                        "name": "Fernando Snchez-Vega"
                    },
                    {
                        "name": "Alessandro Raganato"
                    },
                    {
                        "name": "Jindich Libovick"
                    },
                    {
                        "name": "Jussi Karlgren"
                    },
                    {
                        "name": "Shaoxiong Ji"
                    },
                    {
                        "name": "Jindich Helcl"
                    },
                    {
                        "name": "Liane Guillou"
                    },
                    {
                        "name": "Ona de Gibert"
                    },
                    {
                        "name": "Jaione Bengoetxea"
                    },
                    {
                        "name": "Joseph Attieh"
                    },
                    {
                        "name": "Marianna Apidianaki"
                    }
                ],
                "author_detail": {
                    "name": "Marianna Apidianaki"
                },
                "author": "Marianna Apidianaki",
                "arxiv_comment": "Mu-SHROOM is part of SemEval-2025 (Task 3). TBP: Proceedings of the\n  19th International Workshop on Semantic Evaluation (SemEval-2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11972v1",
                "updated": "2025-04-16T11:08:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    11,
                    8,
                    46,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T11:08:46Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    11,
                    8,
                    46,
                    2,
                    106,
                    0
                ],
                "title": "LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA"
                },
                "summary": "Extractive reading comprehension question answering (QA) datasets are\ntypically evaluated using Exact Match (EM) and F1-score, but these metrics\noften fail to fully capture model performance. With the success of large\nlanguage models (LLMs), they have been employed in various tasks, including\nserving as judges (LLM-as-a-judge). In this paper, we reassess the performance\nof QA models using LLM-as-a-judge across four reading comprehension QA\ndatasets. We examine different families of LLMs and various answer types to\nevaluate the effectiveness of LLM-as-a-judge in these tasks. Our results show\nthat LLM-as-a-judge is highly correlated with human judgments and can replace\ntraditional EM/F1 metrics. By using LLM-as-a-judge, the correlation with human\njudgments improves significantly, from 0.17 (EM) and 0.36 (F1-score) to 0.85.\nThese findings confirm that EM and F1 metrics underestimate the true\nperformance of the QA models. While LLM-as-a-judge is not perfect for more\ndifficult answer types (e.g., job), it still outperforms EM/F1, and we observe\nno bias issues, such as self-preference, when the same model is used for both\nthe QA and judgment tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extractive reading comprehension question answering (QA) datasets are\ntypically evaluated using Exact Match (EM) and F1-score, but these metrics\noften fail to fully capture model performance. With the success of large\nlanguage models (LLMs), they have been employed in various tasks, including\nserving as judges (LLM-as-a-judge). In this paper, we reassess the performance\nof QA models using LLM-as-a-judge across four reading comprehension QA\ndatasets. We examine different families of LLMs and various answer types to\nevaluate the effectiveness of LLM-as-a-judge in these tasks. Our results show\nthat LLM-as-a-judge is highly correlated with human judgments and can replace\ntraditional EM/F1 metrics. By using LLM-as-a-judge, the correlation with human\njudgments improves significantly, from 0.17 (EM) and 0.36 (F1-score) to 0.85.\nThese findings confirm that EM and F1 metrics underestimate the true\nperformance of the QA models. While LLM-as-a-judge is not perfect for more\ndifficult answer types (e.g., job), it still outperforms EM/F1, and we observe\nno bias issues, such as self-preference, when the same model is used for both\nthe QA and judgment tasks."
                },
                "authors": [
                    {
                        "name": "Xanh Ho"
                    },
                    {
                        "name": "Jiahao Huang"
                    },
                    {
                        "name": "Florian Boudin"
                    },
                    {
                        "name": "Akiko Aizawa"
                    }
                ],
                "author_detail": {
                    "name": "Akiko Aizawa"
                },
                "author": "Akiko Aizawa",
                "arxiv_comment": "17 pages; code and data are available at\n  https://github.com/Alab-NII/llm-judge-extract-qa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11970v1",
                "updated": "2025-04-16T11:03:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    11,
                    3,
                    56,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T11:03:56Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    11,
                    3,
                    56,
                    2,
                    106,
                    0
                ],
                "title": "Online Training and Inference System on Edge FPGA Using Delayed Feedback\n  Reservoir",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Training and Inference System on Edge FPGA Using Delayed Feedback\n  Reservoir"
                },
                "summary": "A delayed feedback reservoir (DFR) is a hardwarefriendly reservoir computing\nsystem. Implementing DFRs in embedded hardware requires efficient online\ntraining. However, two main challenges prevent this: hyperparameter selection,\nwhich is typically done by offline grid search, and training of the output\nlinear layer, which is memory-intensive. This paper introduces a fast and\naccurate parameter optimization method for the reservoir layer utilizing\nbackpropagation and gradient descent by adopting a modular DFR model. A\ntruncated backpropagation strategy is proposed to reduce memory consumption\nassociated with the expansion of the recursive structure while maintaining\naccuracy. The computation time is significantly reduced compared to grid\nsearch. Additionally, an in-place Ridge regression for the output layer via 1-D\nCholesky decomposition is presented, reducing memory usage to be 1/4. These\nmethods enable the realization of an online edge training and inference system\nof DFR on an FPGA, reducing computation time by about 1/13 and power\nconsumption by about 1/27 compared to software implementation on the same\nboard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A delayed feedback reservoir (DFR) is a hardwarefriendly reservoir computing\nsystem. Implementing DFRs in embedded hardware requires efficient online\ntraining. However, two main challenges prevent this: hyperparameter selection,\nwhich is typically done by offline grid search, and training of the output\nlinear layer, which is memory-intensive. This paper introduces a fast and\naccurate parameter optimization method for the reservoir layer utilizing\nbackpropagation and gradient descent by adopting a modular DFR model. A\ntruncated backpropagation strategy is proposed to reduce memory consumption\nassociated with the expansion of the recursive structure while maintaining\naccuracy. The computation time is significantly reduced compared to grid\nsearch. Additionally, an in-place Ridge regression for the output layer via 1-D\nCholesky decomposition is presented, reducing memory usage to be 1/4. These\nmethods enable the realization of an online edge training and inference system\nof DFR on an FPGA, reducing computation time by about 1/13 and power\nconsumption by about 1/27 compared to software implementation on the same\nboard."
                },
                "authors": [
                    {
                        "name": "Sosei Ikeda"
                    },
                    {
                        "name": "Hiromitsu Awano"
                    },
                    {
                        "name": "Takashi Sato"
                    }
                ],
                "author_detail": {
                    "name": "Takashi Sato"
                },
                "author": "Takashi Sato",
                "arxiv_doi": "10.1109/TCAD.2025.3541565",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TCAD.2025.3541565",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.11970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05858v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05858v2",
                "updated": "2025-04-16T11:02:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    11,
                    2,
                    32,
                    2,
                    106,
                    0
                ],
                "published": "2024-10-08T09:50:43Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    50,
                    43,
                    1,
                    282,
                    0
                ],
                "title": "Detecting dependence structure: visualization and inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting dependence structure: visualization and inference"
                },
                "summary": "Identifying dependency between two random variables is a fundamental problem.\nThe clear interpretability and ability of a procedure to provide information on\nthe form of possible dependence is particularly important when exploring\ndependencies. In this paper, we introduce a novel method that employs a new\nestimator of the quantile dependence function and pertinent local acceptance\nregions. This leads to an insightful visualisation and a rigorous evaluation of\nthe underlying dependence structure. We also propose a test of independence of\ntwo random variables, pertinent to this new estimator. Our procedures are based\non ranks, and we derive a finite-sample theory that guarantees the inferential\nvalidity of our solutions at any given sample size. The procedures are simple\nto implement and computationally efficient. The large sample consistency of the\nproposed test is also proved. We show that, in terms of power, the new test is\none of the best statistics for independence testing when considering a wide\nrange of alternative models. Finally, we demonstrate the use of our approach to\nvisualise dependence structure and to detect local departures from independence\nthrough analysing some real-world datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying dependency between two random variables is a fundamental problem.\nThe clear interpretability and ability of a procedure to provide information on\nthe form of possible dependence is particularly important when exploring\ndependencies. In this paper, we introduce a novel method that employs a new\nestimator of the quantile dependence function and pertinent local acceptance\nregions. This leads to an insightful visualisation and a rigorous evaluation of\nthe underlying dependence structure. We also propose a test of independence of\ntwo random variables, pertinent to this new estimator. Our procedures are based\non ranks, and we derive a finite-sample theory that guarantees the inferential\nvalidity of our solutions at any given sample size. The procedures are simple\nto implement and computationally efficient. The large sample consistency of the\nproposed test is also proved. We show that, in terms of power, the new test is\none of the best statistics for independence testing when considering a wide\nrange of alternative models. Finally, we demonstrate the use of our approach to\nvisualise dependence structure and to detect local departures from independence\nthrough analysing some real-world datasets."
                },
                "authors": [
                    {
                        "name": "Bogdan miel"
                    },
                    {
                        "name": "Teresa Ledwina"
                    }
                ],
                "author_detail": {
                    "name": "Teresa Ledwina"
                },
                "author": "Teresa Ledwina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05858v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05858v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14073v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14073v3",
                "updated": "2025-04-16T10:42:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    10,
                    42,
                    17,
                    2,
                    106,
                    0
                ],
                "published": "2023-09-25T12:07:00Z",
                "published_parsed": [
                    2023,
                    9,
                    25,
                    12,
                    7,
                    0,
                    0,
                    268,
                    0
                ],
                "title": "Neural Network Parameter-optimization of Gaussian pmDAGs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Network Parameter-optimization of Gaussian pmDAGs"
                },
                "summary": "Finding the parameters of a latent variable causal model is central to causal\ninference and causal identification. In this article, we show that existing\ngraphical structures that are used in causal inference are not stable under\nmarginalization of Gaussian Bayesian networks, and present a graphical\nstructure that faithfully represent margins of Gaussian Bayesian networks. We\npresent the first duality between parameter optimization of a latent variable\nmodel and training a feed-forward neural network in the parameter space of the\nassumed family of distributions. Based on this observation, we develop an\nalgorithm for parameter optimization of these graphical structures based on a\ngiven observational distribution. Then, we provide conditions for causal effect\nidentifiability in the Gaussian setting. We propose an meta-algorithm that\nchecks whether a causal effect is identifiable or not. Moreover, we lay a\ngrounding for generalizing the duality between a neural network and a causal\nmodel from the Gaussian to other distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding the parameters of a latent variable causal model is central to causal\ninference and causal identification. In this article, we show that existing\ngraphical structures that are used in causal inference are not stable under\nmarginalization of Gaussian Bayesian networks, and present a graphical\nstructure that faithfully represent margins of Gaussian Bayesian networks. We\npresent the first duality between parameter optimization of a latent variable\nmodel and training a feed-forward neural network in the parameter space of the\nassumed family of distributions. Based on this observation, we develop an\nalgorithm for parameter optimization of these graphical structures based on a\ngiven observational distribution. Then, we provide conditions for causal effect\nidentifiability in the Gaussian setting. We propose an meta-algorithm that\nchecks whether a causal effect is identifiable or not. Moreover, we lay a\ngrounding for generalizing the duality between a neural network and a causal\nmodel from the Gaussian to other distributions."
                },
                "authors": [
                    {
                        "name": "Mehrzad Saremi"
                    }
                ],
                "author_detail": {
                    "name": "Mehrzad Saremi"
                },
                "author": "Mehrzad Saremi",
                "arxiv_comment": "48 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.14073v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14073v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11952v1",
                "updated": "2025-04-16T10:29:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    10,
                    29,
                    30,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T10:29:30Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    10,
                    29,
                    30,
                    2,
                    106,
                    0
                ],
                "title": "Robust and Fine-Grained Detection of AI Generated Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust and Fine-Grained Detection of AI Generated Texts"
                },
                "summary": "An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts."
                },
                "authors": [
                    {
                        "name": "Ram Mohan Rao Kadiyala"
                    },
                    {
                        "name": "Siddartha Pullakhandam"
                    },
                    {
                        "name": "Kanwal Mehreen"
                    },
                    {
                        "name": "Drishti Sharma"
                    },
                    {
                        "name": "Siddhant Gupta"
                    },
                    {
                        "name": "Jebish Purbey"
                    },
                    {
                        "name": "Ashay Srivastava"
                    },
                    {
                        "name": "Subhasya TippaReddy"
                    },
                    {
                        "name": "Arvind Reddy Bobbili"
                    },
                    {
                        "name": "Suraj Telugara Chandrashekhar"
                    },
                    {
                        "name": "Modabbir Adeeb"
                    },
                    {
                        "name": "Srinadh Vura"
                    },
                    {
                        "name": "Hamza Farooq"
                    }
                ],
                "author_detail": {
                    "name": "Hamza Farooq"
                },
                "author": "Hamza Farooq",
                "arxiv_comment": "ACL 2025 Feb ARR Submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22675v2",
                "updated": "2025-04-16T10:20:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    10,
                    20,
                    11,
                    2,
                    106,
                    0
                ],
                "published": "2025-03-28T17:59:03Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    17,
                    59,
                    3,
                    4,
                    87,
                    0
                ],
                "title": "Think Before Recommend: Unleashing the Latent Reasoning Power for\n  Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Before Recommend: Unleashing the Latent Reasoning Power for\n  Sequential Recommendation"
                },
                "summary": "Sequential Recommendation (SeqRec) aims to predict the next item by capturing\nsequential patterns from users' historical interactions, playing a crucial role\nin many real-world recommender systems. However, existing approaches\npredominantly adopt a direct forward computation paradigm, where the final\nhidden state of the sequence encoder serves as the user representation. We\nargue that this inference paradigm, due to its limited computational depth,\nstruggles to model the complex evolving nature of user preferences and lacks a\nnuanced understanding of long-tail items, leading to suboptimal performance. To\naddress this issue, we propose \\textbf{ReaRec}, the first inference-time\ncomputing framework for recommender systems, which enhances user\nrepresentations through implicit multi-step reasoning. Specifically, ReaRec\nautoregressively feeds the sequence's last hidden state into the sequential\nrecommender while incorporating special reasoning position embeddings to\ndecouple the original item encoding space from the multi-step reasoning space.\nMoreover, we introduce two lightweight reasoning-based learning methods,\nEnsemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to\nfurther effectively exploit ReaRec's reasoning potential. Extensive experiments\non five public real-world datasets and different SeqRec architectures\ndemonstrate the generality and effectiveness of our proposed ReaRec.\nRemarkably, post-hoc analyses reveal that ReaRec significantly elevates the\nperformance ceiling of multiple sequential recommendation backbones by\napproximately 30\\%-50\\%. Thus, we believe this work can open a new and\npromising avenue for future research in inference-time computing for sequential\nrecommendation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Recommendation (SeqRec) aims to predict the next item by capturing\nsequential patterns from users' historical interactions, playing a crucial role\nin many real-world recommender systems. However, existing approaches\npredominantly adopt a direct forward computation paradigm, where the final\nhidden state of the sequence encoder serves as the user representation. We\nargue that this inference paradigm, due to its limited computational depth,\nstruggles to model the complex evolving nature of user preferences and lacks a\nnuanced understanding of long-tail items, leading to suboptimal performance. To\naddress this issue, we propose \\textbf{ReaRec}, the first inference-time\ncomputing framework for recommender systems, which enhances user\nrepresentations through implicit multi-step reasoning. Specifically, ReaRec\nautoregressively feeds the sequence's last hidden state into the sequential\nrecommender while incorporating special reasoning position embeddings to\ndecouple the original item encoding space from the multi-step reasoning space.\nMoreover, we introduce two lightweight reasoning-based learning methods,\nEnsemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to\nfurther effectively exploit ReaRec's reasoning potential. Extensive experiments\non five public real-world datasets and different SeqRec architectures\ndemonstrate the generality and effectiveness of our proposed ReaRec.\nRemarkably, post-hoc analyses reveal that ReaRec significantly elevates the\nperformance ceiling of multiple sequential recommendation backbones by\napproximately 30\\%-50\\%. Thus, we believe this work can open a new and\npromising avenue for future research in inference-time computing for sequential\nrecommendation."
                },
                "authors": [
                    {
                        "name": "Jiakai Tang"
                    },
                    {
                        "name": "Sunhao Dai"
                    },
                    {
                        "name": "Teng Shi"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Wu Jian"
                    },
                    {
                        "name": "Yuning Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yuning Jiang"
                },
                "author": "Yuning Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03104v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03104v3",
                "updated": "2025-04-16T10:18:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    10,
                    18,
                    1,
                    2,
                    106,
                    0
                ],
                "published": "2024-12-04T08:06:15Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    6,
                    15,
                    2,
                    339,
                    0
                ],
                "title": "ChatTS: Aligning Time Series with LLMs via Synthetic Data for Enhanced\n  Understanding and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatTS: Aligning Time Series with LLMs via Synthetic Data for Enhanced\n  Understanding and Reasoning"
                },
                "summary": "Understanding time series is crucial for its application in real-world\nscenarios. Recently, large language models (LLMs) have been increasingly\napplied to time series tasks, leveraging their strong language capabilities to\nenhance various applications. However, research on multimodal LLMs (MLLMs) for\ntime series understanding and reasoning remains limited, primarily due to the\nscarcity of high-quality datasets that align time series with textual\ninformation. This paper introduces ChatTS, a novel MLLM designed for time\nseries analysis. ChatTS treats time series as a modality, similar to how vision\nMLLMs process images, enabling it to perform both understanding and reasoning\nwith time series. To address the scarcity of training data, we propose an\nattribute-based method for generating synthetic time series with detailed\nattribute descriptions. We further introduce Time Series Evol-Instruct, a novel\napproach that generates diverse time series Q&As, enhancing the model's\nreasoning capabilities. To the best of our knowledge, ChatTS is the first\nTS-MLLM that takes multivariate time series as input for understanding and\nreasoning, which is fine-tuned exclusively on synthetic datasets. We evaluate\nits performance using benchmark datasets with real-world data, including six\nalignment tasks and four reasoning tasks. Our results show that ChatTS\nsignificantly outperforms existing vision-based MLLMs (e.g., GPT-4o) and\ntext/agent-based LLMs, achieving a 46.0% improvement in alignment tasks and a\n25.8% improvement in reasoning tasks. We have open-sourced the source code,\nmodel checkpoint and datasets at https://github.com/NetManAIOps/ChatTS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding time series is crucial for its application in real-world\nscenarios. Recently, large language models (LLMs) have been increasingly\napplied to time series tasks, leveraging their strong language capabilities to\nenhance various applications. However, research on multimodal LLMs (MLLMs) for\ntime series understanding and reasoning remains limited, primarily due to the\nscarcity of high-quality datasets that align time series with textual\ninformation. This paper introduces ChatTS, a novel MLLM designed for time\nseries analysis. ChatTS treats time series as a modality, similar to how vision\nMLLMs process images, enabling it to perform both understanding and reasoning\nwith time series. To address the scarcity of training data, we propose an\nattribute-based method for generating synthetic time series with detailed\nattribute descriptions. We further introduce Time Series Evol-Instruct, a novel\napproach that generates diverse time series Q&As, enhancing the model's\nreasoning capabilities. To the best of our knowledge, ChatTS is the first\nTS-MLLM that takes multivariate time series as input for understanding and\nreasoning, which is fine-tuned exclusively on synthetic datasets. We evaluate\nits performance using benchmark datasets with real-world data, including six\nalignment tasks and four reasoning tasks. Our results show that ChatTS\nsignificantly outperforms existing vision-based MLLMs (e.g., GPT-4o) and\ntext/agent-based LLMs, achieving a 46.0% improvement in alignment tasks and a\n25.8% improvement in reasoning tasks. We have open-sourced the source code,\nmodel checkpoint and datasets at https://github.com/NetManAIOps/ChatTS."
                },
                "authors": [
                    {
                        "name": "Zhe Xie"
                    },
                    {
                        "name": "Zeyan Li"
                    },
                    {
                        "name": "Xiao He"
                    },
                    {
                        "name": "Longlong Xu"
                    },
                    {
                        "name": "Xidao Wen"
                    },
                    {
                        "name": "Tieying Zhang"
                    },
                    {
                        "name": "Jianjun Chen"
                    },
                    {
                        "name": "Rui Shi"
                    },
                    {
                        "name": "Dan Pei"
                    }
                ],
                "author_detail": {
                    "name": "Dan Pei"
                },
                "author": "Dan Pei",
                "arxiv_comment": "accepted by VLDB' 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03104v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03104v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11934v1",
                "updated": "2025-04-16T10:14:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    10,
                    14,
                    27,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T10:14:27Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    10,
                    14,
                    27,
                    2,
                    106,
                    0
                ],
                "title": "An LLM-as-a-judge Approach for Scalable Gender-Neutral Translation\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM-as-a-judge Approach for Scalable Gender-Neutral Translation\n  Evaluation"
                },
                "summary": "Gender-neutral translation (GNT) aims to avoid expressing the gender of human\nreferents when the source text lacks explicit cues about the gender of those\nreferents. Evaluating GNT automatically is particularly challenging, with\ncurrent solutions being limited to monolingual classifiers. Such solutions are\nnot ideal because they do not factor in the source sentence and require\ndedicated data and fine-tuning to scale to new languages. In this work, we\naddress such limitations by investigating the use of large language models\n(LLMs) as evaluators of GNT. Specifically, we explore two prompting approaches:\none in which LLMs generate sentence-level assessments only, and another, akin\nto a chain-of-thought approach, where they first produce detailed phrase-level\nannotations before a sentence-level judgment. Through extensive experiments on\nmultiple languages with five models, both open and proprietary, we show that\nLLMs can serve as evaluators of GNT. Moreover, we find that prompting for\nphrase-level annotations before sentence-level assessments consistently\nimproves the accuracy of all models, providing a better and more scalable\nalternative to current solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gender-neutral translation (GNT) aims to avoid expressing the gender of human\nreferents when the source text lacks explicit cues about the gender of those\nreferents. Evaluating GNT automatically is particularly challenging, with\ncurrent solutions being limited to monolingual classifiers. Such solutions are\nnot ideal because they do not factor in the source sentence and require\ndedicated data and fine-tuning to scale to new languages. In this work, we\naddress such limitations by investigating the use of large language models\n(LLMs) as evaluators of GNT. Specifically, we explore two prompting approaches:\none in which LLMs generate sentence-level assessments only, and another, akin\nto a chain-of-thought approach, where they first produce detailed phrase-level\nannotations before a sentence-level judgment. Through extensive experiments on\nmultiple languages with five models, both open and proprietary, we show that\nLLMs can serve as evaluators of GNT. Moreover, we find that prompting for\nphrase-level annotations before sentence-level assessments consistently\nimproves the accuracy of all models, providing a better and more scalable\nalternative to current solutions."
                },
                "authors": [
                    {
                        "name": "Andrea Piergentili"
                    },
                    {
                        "name": "Beatrice Savoldi"
                    },
                    {
                        "name": "Matteo Negri"
                    },
                    {
                        "name": "Luisa Bentivogli"
                    }
                ],
                "author_detail": {
                    "name": "Luisa Bentivogli"
                },
                "author": "Luisa Bentivogli",
                "arxiv_comment": "Accepted at GITT 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18134v2",
                "updated": "2025-04-16T10:13:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    10,
                    13,
                    48,
                    2,
                    106,
                    0
                ],
                "published": "2025-01-30T04:54:59Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    4,
                    54,
                    59,
                    3,
                    30,
                    0
                ],
                "title": "Nonlocal prior mixture-based Bayesian wavelet regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlocal prior mixture-based Bayesian wavelet regression"
                },
                "summary": "We propose a novel Bayesian wavelet regression approach using a\nthree-component spike-and-slab prior for wavelet coefficients, combining a\npoint mass at zero, a moment (MOM) prior, and an inverse moment (IMOM) prior.\nThis flexible prior supports small and large coefficients differently, offering\nadvantages for highly dispersed data where wavelet coefficients span multiple\nscales. The IMOM prior's heavy tails capture large coefficients, while the MOM\nprior is better suited for smaller non-zero coefficients. Further, our method\nintroduces innovative hyperparameter specifications for mixture probabilities\nand scale parameters, including generalized logit, hyperbolic secant, and\ngeneralized normal decay for probabilities, and double exponential decay for\nscaling. Hyperparameters are estimated via an empirical Bayes approach,\nenabling posterior inference tailored to the data. Extensive simulations\ndemonstrate significant performance gains over two-component wavelet methods.\nApplications to electroencephalography and noisy audio data illustrate the\nmethod's utility in capturing complex signal characteristics. We implement our\nmethod in an R package NLPwavelet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel Bayesian wavelet regression approach using a\nthree-component spike-and-slab prior for wavelet coefficients, combining a\npoint mass at zero, a moment (MOM) prior, and an inverse moment (IMOM) prior.\nThis flexible prior supports small and large coefficients differently, offering\nadvantages for highly dispersed data where wavelet coefficients span multiple\nscales. The IMOM prior's heavy tails capture large coefficients, while the MOM\nprior is better suited for smaller non-zero coefficients. Further, our method\nintroduces innovative hyperparameter specifications for mixture probabilities\nand scale parameters, including generalized logit, hyperbolic secant, and\ngeneralized normal decay for probabilities, and double exponential decay for\nscaling. Hyperparameters are estimated via an empirical Bayes approach,\nenabling posterior inference tailored to the data. Extensive simulations\ndemonstrate significant performance gains over two-component wavelet methods.\nApplications to electroencephalography and noisy audio data illustrate the\nmethod's utility in capturing complex signal characteristics. We implement our\nmethod in an R package NLPwavelet."
                },
                "authors": [
                    {
                        "name": "Nilotpal Sanyal"
                    }
                ],
                "author_detail": {
                    "name": "Nilotpal Sanyal"
                },
                "author": "Nilotpal Sanyal",
                "arxiv_comment": "25 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11925v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11925v1",
                "updated": "2025-04-16T09:59:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    59,
                    31,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T09:59:31Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    59,
                    31,
                    2,
                    106,
                    0
                ],
                "title": "Reducing Calls to the Simulator in Simulation Based Inference (SBI)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing Calls to the Simulator in Simulation Based Inference (SBI)"
                },
                "summary": "Simulation-Based Inference (SBI) deals with statistical inference in problems\nwhere the data are generated from a system that is described by a complex\nstochastic simulator. The challenge for inference in these problems is that the\nlikelihood is intractable; SBI proceeds by using the simulator to sample from\nthe likelihood. In many real world applications, simulator calls are expensive,\nlimiting the associated sample size. Our goal in this work is to extend SBI to\nexploit two proposals for reducing simulator calls: to draw likelihood samples\nfrom a Neural Density Estimator (NDE) surrogate rather than from the stochastic\nsimulator; and use of Support Points rather than simple random sampling to\ngenerate evaluation sites. We embed these methods in the Sequential Neural\nPosterior Estimator (SNPE) algorithm. Across a suite of test cases, we find\nthat the NDE surrogate improves the quality of the inference; support points\nworked well in some examples, but not in others.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-Based Inference (SBI) deals with statistical inference in problems\nwhere the data are generated from a system that is described by a complex\nstochastic simulator. The challenge for inference in these problems is that the\nlikelihood is intractable; SBI proceeds by using the simulator to sample from\nthe likelihood. In many real world applications, simulator calls are expensive,\nlimiting the associated sample size. Our goal in this work is to extend SBI to\nexploit two proposals for reducing simulator calls: to draw likelihood samples\nfrom a Neural Density Estimator (NDE) surrogate rather than from the stochastic\nsimulator; and use of Support Points rather than simple random sampling to\ngenerate evaluation sites. We embed these methods in the Sequential Neural\nPosterior Estimator (SNPE) algorithm. Across a suite of test cases, we find\nthat the NDE surrogate improves the quality of the inference; support points\nworked well in some examples, but not in others."
                },
                "authors": [
                    {
                        "name": "David Refaeli"
                    },
                    {
                        "name": "Mira Marcus-Kalish"
                    },
                    {
                        "name": "David M. Steinberg"
                    }
                ],
                "author_detail": {
                    "name": "David M. Steinberg"
                },
                "author": "David M. Steinberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11925v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62-08",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11919v1",
                "updated": "2025-04-16T09:55:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    55,
                    34,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T09:55:34Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    55,
                    34,
                    2,
                    106,
                    0
                ],
                "title": "Rethinking the Generation of High-Quality CoT Data from the Perspective\n  of LLM-Adaptive Question Difficulty Grading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking the Generation of High-Quality CoT Data from the Perspective\n  of LLM-Adaptive Question Difficulty Grading"
                },
                "summary": "Recently, DeepSeek-R1 (671B) (DeepSeek-AIet al., 2025) has demonstrated its\nexcellent reasoning ability in complex tasks and has publiclyshared its\nmethodology. This provides potentially high-quality chain-of-thought (CoT) data\nfor stimulating the reasoning abilities of small-sized large language models\n(LLMs). To generate high-quality CoT data for different LLMs, we seek an\nefficient method for generating high-quality CoT data with LLM-Adaptive\nquestiondifficulty levels. First, we grade the difficulty of the questions\naccording to the reasoning ability of the LLMs themselves and construct a\nLLM-Adaptive question database. Second, we sample the problem database based on\na distribution of difficulty levels of the questions and then use DeepSeek-R1\n(671B) (DeepSeek-AI et al., 2025) to generate the corresponding high-quality\nCoT data with correct answers. Thanks to the construction of CoT data with\nLLM-Adaptive difficulty levels, we have significantly reduced the cost of data\ngeneration and enhanced the efficiency of model supervised fine-tuning (SFT).\nFinally, we have validated the effectiveness and generalizability of the\nproposed method in the fields of complex mathematical competitions and code\ngeneration tasks. Notably, with only 2k high-quality mathematical CoT data, our\nZMath-32B surpasses DeepSeek-Distill-32B in math reasoning task. Similarly,\nwith only 2k high-quality code CoT data, our ZCode-32B surpasses\nDeepSeek-Distill-32B in code reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, DeepSeek-R1 (671B) (DeepSeek-AIet al., 2025) has demonstrated its\nexcellent reasoning ability in complex tasks and has publiclyshared its\nmethodology. This provides potentially high-quality chain-of-thought (CoT) data\nfor stimulating the reasoning abilities of small-sized large language models\n(LLMs). To generate high-quality CoT data for different LLMs, we seek an\nefficient method for generating high-quality CoT data with LLM-Adaptive\nquestiondifficulty levels. First, we grade the difficulty of the questions\naccording to the reasoning ability of the LLMs themselves and construct a\nLLM-Adaptive question database. Second, we sample the problem database based on\na distribution of difficulty levels of the questions and then use DeepSeek-R1\n(671B) (DeepSeek-AI et al., 2025) to generate the corresponding high-quality\nCoT data with correct answers. Thanks to the construction of CoT data with\nLLM-Adaptive difficulty levels, we have significantly reduced the cost of data\ngeneration and enhanced the efficiency of model supervised fine-tuning (SFT).\nFinally, we have validated the effectiveness and generalizability of the\nproposed method in the fields of complex mathematical competitions and code\ngeneration tasks. Notably, with only 2k high-quality mathematical CoT data, our\nZMath-32B surpasses DeepSeek-Distill-32B in math reasoning task. Similarly,\nwith only 2k high-quality code CoT data, our ZCode-32B surpasses\nDeepSeek-Distill-32B in code reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Qianjin Yu"
                    },
                    {
                        "name": "Keyu Wu"
                    },
                    {
                        "name": "Zihan Chen"
                    },
                    {
                        "name": "Chushu Zhang"
                    },
                    {
                        "name": "Manlin Mei"
                    },
                    {
                        "name": "Lingjun Huang"
                    },
                    {
                        "name": "Fang Tan"
                    },
                    {
                        "name": "Yongsheng Du"
                    },
                    {
                        "name": "Kunlin Liu"
                    },
                    {
                        "name": "Yurui Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yurui Zhu"
                },
                "author": "Yurui Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01329v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01329v2",
                "updated": "2025-04-16T09:54:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    54,
                    20,
                    2,
                    106,
                    0
                ],
                "published": "2025-03-03T09:12:14Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    12,
                    14,
                    0,
                    62,
                    0
                ],
                "title": "Neural ODE Transformers: Analyzing Internal Dynamics and Adaptive\n  Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural ODE Transformers: Analyzing Internal Dynamics and Adaptive\n  Fine-tuning"
                },
                "summary": "Recent advancements in large language models (LLMs) based on transformer\narchitectures have sparked significant interest in understanding their inner\nworkings. In this paper, we introduce a novel approach to modeling transformer\narchitectures using highly flexible non-autonomous neural ordinary differential\nequations (ODEs). Our proposed model parameterizes all weights of attention and\nfeed-forward blocks through neural networks, expressing these weights as\nfunctions of a continuous layer index. Through spectral analysis of the model's\ndynamics, we uncover an increase in eigenvalue magnitude that challenges the\nweight-sharing assumption prevalent in existing theoretical studies. We also\nleverage the Lyapunov exponent to examine token-level sensitivity, enhancing\nmodel interpretability. Our neural ODE transformer demonstrates performance\ncomparable to or better than vanilla transformers across various configurations\nand datasets, while offering flexible fine-tuning capabilities that can adapt\nto different architectural constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) based on transformer\narchitectures have sparked significant interest in understanding their inner\nworkings. In this paper, we introduce a novel approach to modeling transformer\narchitectures using highly flexible non-autonomous neural ordinary differential\nequations (ODEs). Our proposed model parameterizes all weights of attention and\nfeed-forward blocks through neural networks, expressing these weights as\nfunctions of a continuous layer index. Through spectral analysis of the model's\ndynamics, we uncover an increase in eigenvalue magnitude that challenges the\nweight-sharing assumption prevalent in existing theoretical studies. We also\nleverage the Lyapunov exponent to examine token-level sensitivity, enhancing\nmodel interpretability. Our neural ODE transformer demonstrates performance\ncomparable to or better than vanilla transformers across various configurations\nand datasets, while offering flexible fine-tuning capabilities that can adapt\nto different architectural constraints."
                },
                "authors": [
                    {
                        "name": "Anh Tong"
                    },
                    {
                        "name": "Thanh Nguyen-Tang"
                    },
                    {
                        "name": "Dongeun Lee"
                    },
                    {
                        "name": "Duc Nguyen"
                    },
                    {
                        "name": "Toan Tran"
                    },
                    {
                        "name": "David Hall"
                    },
                    {
                        "name": "Cheongwoong Kang"
                    },
                    {
                        "name": "Jaesik Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jaesik Choi"
                },
                "author": "Jaesik Choi",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01329v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01329v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07826v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07826v2",
                "updated": "2025-04-16T09:51:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    51,
                    33,
                    2,
                    106,
                    0
                ],
                "published": "2024-11-12T14:22:16Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    14,
                    22,
                    16,
                    1,
                    317,
                    0
                ],
                "title": "Efficient Federated Finetuning of Tiny Transformers with\n  Resource-Constrained Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Federated Finetuning of Tiny Transformers with\n  Resource-Constrained Devices"
                },
                "summary": "In recent years, Large Language Models (LLMs) through Transformer structures\nhave dominated many machine learning tasks, especially text processing.\nHowever, these models require massive amounts of data for training and induce\nhigh resource requirements, particularly in terms of the large number of\nFloating Point Operations (FLOPs) and the high amounts of memory needed. To\nfine-tune such a model in a parameter-efficient way, techniques like Adapter or\nLoRA have been developed. However, we observe that the application of LoRA,\nwhen used in federated learning (FL), while still being parameter-efficient, is\nmemory and FLOP inefficient. Based on that observation, we develop a novel\nlayer finetuning scheme that allows devices in cross-device FL to make use of\npretrained neural networks (NNs) while adhering to given resource constraints.\nWe show that our presented scheme outperforms the current state of the art when\ndealing with homogeneous or heterogeneous computation and memory constraints\nand is on par with LoRA regarding limited communication, thereby achieving\nsignificantly higher accuracies in FL training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) through Transformer structures\nhave dominated many machine learning tasks, especially text processing.\nHowever, these models require massive amounts of data for training and induce\nhigh resource requirements, particularly in terms of the large number of\nFloating Point Operations (FLOPs) and the high amounts of memory needed. To\nfine-tune such a model in a parameter-efficient way, techniques like Adapter or\nLoRA have been developed. However, we observe that the application of LoRA,\nwhen used in federated learning (FL), while still being parameter-efficient, is\nmemory and FLOP inefficient. Based on that observation, we develop a novel\nlayer finetuning scheme that allows devices in cross-device FL to make use of\npretrained neural networks (NNs) while adhering to given resource constraints.\nWe show that our presented scheme outperforms the current state of the art when\ndealing with homogeneous or heterogeneous computation and memory constraints\nand is on par with LoRA regarding limited communication, thereby achieving\nsignificantly higher accuracies in FL training."
                },
                "authors": [
                    {
                        "name": "Kilian Pfeiffer"
                    },
                    {
                        "name": "Mohamed Aboelenien Ahmed"
                    },
                    {
                        "name": "Ramin Khalili"
                    },
                    {
                        "name": "Jrg Henkel"
                    }
                ],
                "author_detail": {
                    "name": "Jrg Henkel"
                },
                "author": "Jrg Henkel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07826v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07826v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07157v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07157v3",
                "updated": "2025-04-16T09:41:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    41,
                    16,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-09T11:19:42Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    11,
                    19,
                    42,
                    2,
                    99,
                    0
                ],
                "title": "GAAPO: Genetic Algorithmic Applied to Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GAAPO: Genetic Algorithmic Applied to Prompt Optimization"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, with their performance heavily dependent on the quality of input\nprompts. While prompt engineering has proven effective, it typically relies on\nmanual adjustments, making it time-consuming and potentially suboptimal. This\npaper introduces GAAPO (Genetic Algorithm Applied to Prompt Optimization), a\nnovel hybrid optimization framework that leverages genetic algorithm principles\nto evolve prompts through successive generations. Unlike traditional genetic\napproaches that rely solely on mutation and crossover operations, GAAPO\nintegrates multiple specialized prompt generation strategies within its\nevolutionary framework. Through extensive experimentation on diverse datasets\nincluding ETHOS, MMLU-Pro, and GPQA, our analysis reveals several important\npoint for the future development of automatic prompt optimization methods:\nimportance of the tradeoff between the population size and the number of\ngenerations, effect of selection methods on stability results, capacity of\ndifferent LLMs and especially reasoning models to be able to automatically\ngenerate prompts from similar queries... Furthermore, we provide insights into\nthe relative effectiveness of different prompt generation strategies and their\nevolution across optimization phases. These findings contribute to both the\ntheoretical understanding of prompt optimization and practical applications in\nimproving LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, with their performance heavily dependent on the quality of input\nprompts. While prompt engineering has proven effective, it typically relies on\nmanual adjustments, making it time-consuming and potentially suboptimal. This\npaper introduces GAAPO (Genetic Algorithm Applied to Prompt Optimization), a\nnovel hybrid optimization framework that leverages genetic algorithm principles\nto evolve prompts through successive generations. Unlike traditional genetic\napproaches that rely solely on mutation and crossover operations, GAAPO\nintegrates multiple specialized prompt generation strategies within its\nevolutionary framework. Through extensive experimentation on diverse datasets\nincluding ETHOS, MMLU-Pro, and GPQA, our analysis reveals several important\npoint for the future development of automatic prompt optimization methods:\nimportance of the tradeoff between the population size and the number of\ngenerations, effect of selection methods on stability results, capacity of\ndifferent LLMs and especially reasoning models to be able to automatically\ngenerate prompts from similar queries... Furthermore, we provide insights into\nthe relative effectiveness of different prompt generation strategies and their\nevolution across optimization phases. These findings contribute to both the\ntheoretical understanding of prompt optimization and practical applications in\nimproving LLM performance."
                },
                "authors": [
                    {
                        "name": "Xavier Scheresse"
                    },
                    {
                        "name": "Jacques-Yves Guilbert--Ly"
                    },
                    {
                        "name": "Antoine Villedieu de Torcy"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Villedieu de Torcy"
                },
                "author": "Antoine Villedieu de Torcy",
                "arxiv_comment": "26 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07157v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07157v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09373v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09373v3",
                "updated": "2025-04-16T09:26:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    26,
                    10,
                    2,
                    106,
                    0
                ],
                "published": "2025-02-13T14:43:03Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    43,
                    3,
                    3,
                    44,
                    0
                ],
                "title": "Low-Acceleration Gravitational Anomaly from Bayesian 3D Modeling of Wide\n  Binary Orbits: Methodology and Results with Gaia DR3",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Acceleration Gravitational Anomaly from Bayesian 3D Modeling of Wide\n  Binary Orbits: Methodology and Results with Gaia DR3"
                },
                "summary": "Isolated wide binary stars provide natural laboratories to directly probe\ngravity for Newtonian acceleration $g_{\\rm{N}}\\lesssim 10^{-9}$ m s$^{-2}$.\nRecent statistical analyses of wide binaries have been performed only with\nsky-projected relative velocities $v_p$ in the pairs. A new method of Bayesian\norbit modeling exploiting three relative velocity components including the\nradial (line-of-sight) component $v_r$ is developed to measure a gravitational\nanomaly parameter\n$\\Gamma\\equiv\\log_{10}\\sqrt{\\gamma_g}\\equiv\\log_{10}\\sqrt{G_{\\rm{eff}}/G_{\\rm{N}}}$\nwhere $G_{\\rm{eff}}$ is the effective gravitational constant for\npseudo-Newtonian elliptical orbits, while $G_{\\rm{N}}$ is Newton's constant.\nThe method infers individual probability distributions of $\\Gamma$ and then\ncombines the independent distributions to obtain a consolidated distribution in\na specific range of $g_{\\rm{N}}$. Here the method is described and applied to a\nsample of 312 wide binaries in a broad dynamic range $10^{-11.0}\\lesssim\ng_{\\rm{N}}\\lesssim 10^{-6.7}$ m s$^{-2}$ with $v_r$ uncertainties in the range\n$168<\\sigma_{v_r}<380$ m s$^{-1}$ selected from the Gaia DR3 database. The\nfollowing results are obtained: $\\Gamma = 0.000\\pm 0.011$\n($N_{\\rm{binary}}=125$) for a high acceleration regime ($10^{-7.9} \\lesssim\ng_{\\rm{N}} \\lesssim 10^{-6.7}$ m s$^{-2}$) agreeing well with Newton, but\n$\\Gamma = 0.085\\pm 0.040$ or $\\gamma_g=1.48_{-0.23}^{+0.33}$ (35) for a MOND\nregime ($10^{-11.0}\\lesssim g_{\\rm{N}}\\lesssim 10^{-9.5}$ m s$^{-2}$) and\n$\\Gamma = 0.063\\pm 0.015$ or $\\gamma_g=1.34_{-0.08}^{+0.10}$ (111) for a\nMOND+transition regime ($10^{-11.0}\\lesssim g_{\\rm{N}}\\lesssim 10^{-8.5}$ m\ns$^{-2}$). These results show that gravitational anomaly is evident for\n$g_{\\rm{N}}\\lesssim 10^{-9}$ m s$^{-2}$ and $\\Gamma$ in the MOND regime\n($\\lesssim 10^{-9.5}$ m s$^{-2}$) agrees with the prediction ($\\approx 0.07$)\nof MOND gravity theories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Isolated wide binary stars provide natural laboratories to directly probe\ngravity for Newtonian acceleration $g_{\\rm{N}}\\lesssim 10^{-9}$ m s$^{-2}$.\nRecent statistical analyses of wide binaries have been performed only with\nsky-projected relative velocities $v_p$ in the pairs. A new method of Bayesian\norbit modeling exploiting three relative velocity components including the\nradial (line-of-sight) component $v_r$ is developed to measure a gravitational\nanomaly parameter\n$\\Gamma\\equiv\\log_{10}\\sqrt{\\gamma_g}\\equiv\\log_{10}\\sqrt{G_{\\rm{eff}}/G_{\\rm{N}}}$\nwhere $G_{\\rm{eff}}$ is the effective gravitational constant for\npseudo-Newtonian elliptical orbits, while $G_{\\rm{N}}$ is Newton's constant.\nThe method infers individual probability distributions of $\\Gamma$ and then\ncombines the independent distributions to obtain a consolidated distribution in\na specific range of $g_{\\rm{N}}$. Here the method is described and applied to a\nsample of 312 wide binaries in a broad dynamic range $10^{-11.0}\\lesssim\ng_{\\rm{N}}\\lesssim 10^{-6.7}$ m s$^{-2}$ with $v_r$ uncertainties in the range\n$168<\\sigma_{v_r}<380$ m s$^{-1}$ selected from the Gaia DR3 database. The\nfollowing results are obtained: $\\Gamma = 0.000\\pm 0.011$\n($N_{\\rm{binary}}=125$) for a high acceleration regime ($10^{-7.9} \\lesssim\ng_{\\rm{N}} \\lesssim 10^{-6.7}$ m s$^{-2}$) agreeing well with Newton, but\n$\\Gamma = 0.085\\pm 0.040$ or $\\gamma_g=1.48_{-0.23}^{+0.33}$ (35) for a MOND\nregime ($10^{-11.0}\\lesssim g_{\\rm{N}}\\lesssim 10^{-9.5}$ m s$^{-2}$) and\n$\\Gamma = 0.063\\pm 0.015$ or $\\gamma_g=1.34_{-0.08}^{+0.10}$ (111) for a\nMOND+transition regime ($10^{-11.0}\\lesssim g_{\\rm{N}}\\lesssim 10^{-8.5}$ m\ns$^{-2}$). These results show that gravitational anomaly is evident for\n$g_{\\rm{N}}\\lesssim 10^{-9}$ m s$^{-2}$ and $\\Gamma$ in the MOND regime\n($\\lesssim 10^{-9.5}$ m s$^{-2}$) agrees with the prediction ($\\approx 0.07$)\nof MOND gravity theories."
                },
                "authors": [
                    {
                        "name": "Kyu-Hyun Chae"
                    }
                ],
                "author_detail": {
                    "name": "Kyu-Hyun Chae"
                },
                "author": "Kyu-Hyun Chae",
                "arxiv_comment": "39 pages, 28 figures, 3 tables, accepted for publication in the\n  Astrophysical Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09373v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09373v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11901v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11901v2",
                "updated": "2025-04-17T08:41:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    41,
                    44,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-16T09:26:04Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    26,
                    4,
                    2,
                    106,
                    0
                ],
                "title": "Causality-enhanced Decision-Making for Autonomous Mobile Robots in\n  Dynamic Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causality-enhanced Decision-Making for Autonomous Mobile Robots in\n  Dynamic Environments"
                },
                "summary": "The growing integration of robots in shared environments -- such as\nwarehouses, shopping centres, and hospitals -- demands a deep understanding of\nthe underlying dynamics and human behaviours, including how, when, and where\nindividuals engage in various activities and interactions. This knowledge goes\nbeyond simple correlation studies and requires a more comprehensive causal\nanalysis. By leveraging causal inference to model cause-and-effect\nrelationships, we can better anticipate critical environmental factors and\nenable autonomous robots to plan and execute tasks more effectively. To this\nend, we propose a novel causality-based decision-making framework that reasons\nover a learned causal model to predict battery usage and human obstructions,\nunderstanding how these factors could influence robot task execution. Such\nreasoning framework assists the robot in deciding when and how to complete a\ngiven task. To achieve this, we developed also PeopleFlow, a new Gazebo-based\nsimulator designed to model context-sensitive human-robot spatial interactions\nin shared workspaces. PeopleFlow features realistic human and robot\ntrajectories influenced by contextual factors such as time, environment layout,\nand robot state, and can simulate a large number of agents. While the simulator\nis general-purpose, in this paper we focus on a warehouse-like environment as a\ncase study, where we conduct an extensive evaluation benchmarking our causal\napproach against a non-causal baseline. Our findings demonstrate the efficacy\nof the proposed solutions, highlighting how causal reasoning enables autonomous\nrobots to operate more efficiently and safely in dynamic environments shared\nwith humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing integration of robots in shared environments -- such as\nwarehouses, shopping centres, and hospitals -- demands a deep understanding of\nthe underlying dynamics and human behaviours, including how, when, and where\nindividuals engage in various activities and interactions. This knowledge goes\nbeyond simple correlation studies and requires a more comprehensive causal\nanalysis. By leveraging causal inference to model cause-and-effect\nrelationships, we can better anticipate critical environmental factors and\nenable autonomous robots to plan and execute tasks more effectively. To this\nend, we propose a novel causality-based decision-making framework that reasons\nover a learned causal model to predict battery usage and human obstructions,\nunderstanding how these factors could influence robot task execution. Such\nreasoning framework assists the robot in deciding when and how to complete a\ngiven task. To achieve this, we developed also PeopleFlow, a new Gazebo-based\nsimulator designed to model context-sensitive human-robot spatial interactions\nin shared workspaces. PeopleFlow features realistic human and robot\ntrajectories influenced by contextual factors such as time, environment layout,\nand robot state, and can simulate a large number of agents. While the simulator\nis general-purpose, in this paper we focus on a warehouse-like environment as a\ncase study, where we conduct an extensive evaluation benchmarking our causal\napproach against a non-causal baseline. Our findings demonstrate the efficacy\nof the proposed solutions, highlighting how causal reasoning enables autonomous\nrobots to operate more efficiently and safely in dynamic environments shared\nwith humans."
                },
                "authors": [
                    {
                        "name": "Luca Castri"
                    },
                    {
                        "name": "Gloria Beraldo"
                    },
                    {
                        "name": "Nicola Bellotto"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Bellotto"
                },
                "author": "Nicola Bellotto",
                "arxiv_comment": "Causal Discovery and Inference - Robot Autonomy - Human-Robot Spatial\n  Interaction - Decision-Making",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11901v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11901v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11900v1",
                "updated": "2025-04-16T09:25:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    25,
                    54,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T09:25:54Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    25,
                    54,
                    2,
                    106,
                    0
                ],
                "title": "Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models\n  via Plot Hole Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models\n  via Plot Hole Detection"
                },
                "summary": "Stories are a fundamental aspect of human experience. Engaging deeply with\nstories and spotting plot holes -- inconsistencies in a storyline that break\nthe internal logic or rules of a story's world -- requires nuanced reasoning\nskills, including tracking entities and events and their interplay, abstract\nthinking, pragmatic narrative understanding, commonsense and social reasoning,\nand theory of mind. As Large Language Models (LLMs) increasingly generate,\ninterpret, and modify text, rigorously assessing their narrative consistency\nand deeper language understanding becomes critical. However, existing\nbenchmarks focus mainly on surface-level comprehension. In this work, we\npropose plot hole detection in stories as a proxy to evaluate language\nunderstanding and reasoning in LLMs. We introduce FlawedFictionsMaker, a novel\nalgorithm to controllably and carefully synthesize plot holes in human-written\nstories. Using this algorithm, we construct a benchmark to evaluate LLMs' plot\nhole detection abilities in stories -- FlawedFictions -- , which is robust to\ncontamination, with human filtering ensuring high quality. We find that\nstate-of-the-art LLMs struggle in accurately solving FlawedFictions regardless\nof the reasoning effort allowed, with performance significantly degrading as\nstory length increases. Finally, we show that LLM-based story summarization and\nstory generation are prone to introducing plot holes, with more than 50% and\n100% increases in plot hole detection rates with respect to human-written\noriginals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stories are a fundamental aspect of human experience. Engaging deeply with\nstories and spotting plot holes -- inconsistencies in a storyline that break\nthe internal logic or rules of a story's world -- requires nuanced reasoning\nskills, including tracking entities and events and their interplay, abstract\nthinking, pragmatic narrative understanding, commonsense and social reasoning,\nand theory of mind. As Large Language Models (LLMs) increasingly generate,\ninterpret, and modify text, rigorously assessing their narrative consistency\nand deeper language understanding becomes critical. However, existing\nbenchmarks focus mainly on surface-level comprehension. In this work, we\npropose plot hole detection in stories as a proxy to evaluate language\nunderstanding and reasoning in LLMs. We introduce FlawedFictionsMaker, a novel\nalgorithm to controllably and carefully synthesize plot holes in human-written\nstories. Using this algorithm, we construct a benchmark to evaluate LLMs' plot\nhole detection abilities in stories -- FlawedFictions -- , which is robust to\ncontamination, with human filtering ensuring high quality. We find that\nstate-of-the-art LLMs struggle in accurately solving FlawedFictions regardless\nof the reasoning effort allowed, with performance significantly degrading as\nstory length increases. Finally, we show that LLM-based story summarization and\nstory generation are prone to introducing plot holes, with more than 50% and\n100% increases in plot hole detection rates with respect to human-written\noriginals."
                },
                "authors": [
                    {
                        "name": "Kabir Ahuja"
                    },
                    {
                        "name": "Melanie Sclar"
                    },
                    {
                        "name": "Yulia Tsvetkov"
                    }
                ],
                "author_detail": {
                    "name": "Yulia Tsvetkov"
                },
                "author": "Yulia Tsvetkov",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11889v1",
                "updated": "2025-04-16T09:17:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    17,
                    45,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T09:17:45Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    17,
                    45,
                    2,
                    106,
                    0
                ],
                "title": "Rethinking LLM-Based Recommendations: A Query Generation-Based,\n  Training-Free Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking LLM-Based Recommendations: A Query Generation-Based,\n  Training-Free Approach"
                },
                "summary": "Existing large language model LLM-based recommendation methods face several\nchallenges, including inefficiency in handling large candidate pools,\nsensitivity to item order within prompts (\"lost in the middle\" phenomenon) poor\nscalability, and unrealistic evaluation due to random negative sampling. To\naddress these issues, we propose a Query-to-Recommendation approach that\nleverages LLMs to generate personalized queries for retrieving relevant items\nfrom the entire candidate pool, eliminating the need for candidate\npre-selection. This method can be integrated into an ID-based recommendation\nsystem without additional training, enhances recommendation performance and\ndiversity through LLMs' world knowledge, and performs well even for less\npopular item groups. Experiments on three datasets show up to 57 percent\nimprovement, with an average gain of 31 percent, demonstrating strong zero-shot\nperformance and further gains when ensembled with existing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large language model LLM-based recommendation methods face several\nchallenges, including inefficiency in handling large candidate pools,\nsensitivity to item order within prompts (\"lost in the middle\" phenomenon) poor\nscalability, and unrealistic evaluation due to random negative sampling. To\naddress these issues, we propose a Query-to-Recommendation approach that\nleverages LLMs to generate personalized queries for retrieving relevant items\nfrom the entire candidate pool, eliminating the need for candidate\npre-selection. This method can be integrated into an ID-based recommendation\nsystem without additional training, enhances recommendation performance and\ndiversity through LLMs' world knowledge, and performs well even for less\npopular item groups. Experiments on three datasets show up to 57 percent\nimprovement, with an average gain of 31 percent, demonstrating strong zero-shot\nperformance and further gains when ensembled with existing models."
                },
                "authors": [
                    {
                        "name": "Donghee Han"
                    },
                    {
                        "name": "Hwanjun Song"
                    },
                    {
                        "name": "Mun Yong Yi"
                    }
                ],
                "author_detail": {
                    "name": "Mun Yong Yi"
                },
                "author": "Mun Yong Yi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11886v1",
                "updated": "2025-04-16T09:14:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    14,
                    14,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T09:14:14Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    14,
                    14,
                    2,
                    106,
                    0
                ],
                "title": "Detection of wave activity within a realistic 3D MHD quiet sun\n  simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detection of wave activity within a realistic 3D MHD quiet sun\n  simulation"
                },
                "summary": "Context. Tracing wave activity from the photosphere to the corona has\nimportant implications for coronal heating and prediction of the solar wind.\nDespite extensive theory and simulations, the detection of waves in realistic\nMHD simulations still presents a large challenge due to wave interaction, mode\nconversion, and damping mechanisms. Aims. We conducted this study to detect\nlocalised wave activity within a realistic MHD simulation of the solar\natmosphere by the Bifrost code. Methods. We present a new method of detecting\nthe most significant contributions of wave activity within localised areas of\nthe domain, aided by Discrete Fourier Transforms and frequency filtering. We\ncorrelate oscillations in the vertical & horizontal magnetic field, velocities\nparallel & perpendicular to the magnetic field, and pressure to infer the\nnature of the dominant wave modes. Results. Our method captures the most\npowerful frequencies and wavenumbers, as well as providing a new diagnostic for\ndamping processes. We infer the presence of magnetoacoustic waves in the\nboundaries of prominent chromospheric/coronal swirling features. We find these\nwaves are likely damped by viscous heating in the swirl boundaries,\ncontributing to heating in the upper atmosphere. Conclusions. Using the most\nsignificant frequencies decomposition, we highlight that energy can be\ntransported from the lower atmosphere to the upper atmosphere through waves and\nfluctuations along the swirl boundaries. Although further analysis is needed to\nconfirm these findings, our new method provides a path forward to investigate\nwave activity in the solar atmosphere",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context. Tracing wave activity from the photosphere to the corona has\nimportant implications for coronal heating and prediction of the solar wind.\nDespite extensive theory and simulations, the detection of waves in realistic\nMHD simulations still presents a large challenge due to wave interaction, mode\nconversion, and damping mechanisms. Aims. We conducted this study to detect\nlocalised wave activity within a realistic MHD simulation of the solar\natmosphere by the Bifrost code. Methods. We present a new method of detecting\nthe most significant contributions of wave activity within localised areas of\nthe domain, aided by Discrete Fourier Transforms and frequency filtering. We\ncorrelate oscillations in the vertical & horizontal magnetic field, velocities\nparallel & perpendicular to the magnetic field, and pressure to infer the\nnature of the dominant wave modes. Results. Our method captures the most\npowerful frequencies and wavenumbers, as well as providing a new diagnostic for\ndamping processes. We infer the presence of magnetoacoustic waves in the\nboundaries of prominent chromospheric/coronal swirling features. We find these\nwaves are likely damped by viscous heating in the swirl boundaries,\ncontributing to heating in the upper atmosphere. Conclusions. Using the most\nsignificant frequencies decomposition, we highlight that energy can be\ntransported from the lower atmosphere to the upper atmosphere through waves and\nfluctuations along the swirl boundaries. Although further analysis is needed to\nconfirm these findings, our new method provides a path forward to investigate\nwave activity in the solar atmosphere"
                },
                "authors": [
                    {
                        "name": "George Cherry"
                    },
                    {
                        "name": "Boris Gudiksen"
                    },
                    {
                        "name": "Adam J. Finley"
                    }
                ],
                "author_detail": {
                    "name": "Adam J. Finley"
                },
                "author": "Adam J. Finley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02644v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02644v3",
                "updated": "2025-04-16T09:10:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    10,
                    17,
                    2,
                    106,
                    0
                ],
                "published": "2024-10-03T16:30:47Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    30,
                    47,
                    3,
                    277,
                    0
                ],
                "title": "Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and\n  Defenses in LLM-based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and\n  Defenses in LLM-based Agents"
                },
                "summary": "Although LLM-based agents, powered by Large Language Models (LLMs), can use\nexternal tools and memory mechanisms to solve complex real-world tasks, they\nmay also introduce critical security vulnerabilities. However, the existing\nliterature does not comprehensively evaluate attacks and defenses against\nLLM-based agents. To address this, we introduce Agent Security Bench (ASB), a\ncomprehensive framework designed to formalize, benchmark, and evaluate the\nattacks and defenses of LLM-based agents, including 10 scenarios (e.g.,\ne-commerce, autonomous driving, finance), 10 agents targeting the scenarios,\nover 400 tools, 27 different types of attack/defense methods, and 7 evaluation\nmetrics. Based on ASB, we benchmark 10 prompt injection attacks, a memory\npoisoning attack, a novel Plan-of-Thought backdoor attack, 4 mixed attacks, and\n11 corresponding defenses across 13 LLM backbones. Our benchmark results reveal\ncritical vulnerabilities in different stages of agent operation, including\nsystem prompt, user prompt handling, tool usage, and memory retrieval, with the\nhighest average attack success rate of 84.30\\%, but limited effectiveness shown\nin current defenses, unveiling important works to be done in terms of agent\nsecurity for the community. We also introduce a new metric to evaluate the\nagents' capability to balance utility and security. Our code can be found at\nhttps://github.com/agiresearch/ASB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although LLM-based agents, powered by Large Language Models (LLMs), can use\nexternal tools and memory mechanisms to solve complex real-world tasks, they\nmay also introduce critical security vulnerabilities. However, the existing\nliterature does not comprehensively evaluate attacks and defenses against\nLLM-based agents. To address this, we introduce Agent Security Bench (ASB), a\ncomprehensive framework designed to formalize, benchmark, and evaluate the\nattacks and defenses of LLM-based agents, including 10 scenarios (e.g.,\ne-commerce, autonomous driving, finance), 10 agents targeting the scenarios,\nover 400 tools, 27 different types of attack/defense methods, and 7 evaluation\nmetrics. Based on ASB, we benchmark 10 prompt injection attacks, a memory\npoisoning attack, a novel Plan-of-Thought backdoor attack, 4 mixed attacks, and\n11 corresponding defenses across 13 LLM backbones. Our benchmark results reveal\ncritical vulnerabilities in different stages of agent operation, including\nsystem prompt, user prompt handling, tool usage, and memory retrieval, with the\nhighest average attack success rate of 84.30\\%, but limited effectiveness shown\nin current defenses, unveiling important works to be done in terms of agent\nsecurity for the community. We also introduce a new metric to evaluate the\nagents' capability to balance utility and security. Our code can be found at\nhttps://github.com/agiresearch/ASB."
                },
                "authors": [
                    {
                        "name": "Hanrong Zhang"
                    },
                    {
                        "name": "Jingyuan Huang"
                    },
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Yifei Yao"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Chenlu Zhan"
                    },
                    {
                        "name": "Hongwei Wang"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02644v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02644v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11170v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11170v2",
                "updated": "2025-04-16T08:50:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    8,
                    50,
                    55,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-15T13:17:14Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    17,
                    14,
                    1,
                    105,
                    0
                ],
                "title": "A Real-time Anomaly Detection Method for Robots based on a Flexible and\n  Sparse Latent Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Real-time Anomaly Detection Method for Robots based on a Flexible and\n  Sparse Latent Space"
                },
                "summary": "The growing demand for robots to operate effectively in diverse environments\nnecessitates the need for robust real-time anomaly detection techniques during\nrobotic operations. However, deep learning-based models in robotics face\nsignificant challenges due to limited training data and highly noisy signal\nfeatures. In this paper, we present Sparse Masked Autoregressive Flow-based\nAdversarial AutoEncoders model to address these problems. This approach\nintegrates Masked Autoregressive Flow model into Adversarial AutoEncoders to\nconstruct a flexible latent space and utilize Sparse autoencoder to efficiently\nfocus on important features, even in scenarios with limited feature space. Our\nexperiments demonstrate that the proposed model achieves a 4.96% to 9.75%\nhigher area under the receiver operating characteristic curve for\npick-and-place robotic operations with randomly placed cans, compared to\nexisting state-of-the-art methods. Notably, it showed up to 19.67% better\nperformance in scenarios involving collisions with lightweight objects.\nAdditionally, unlike the existing state-of-the-art model, our model performs\ninferences within 1 millisecond, ensuring real-time anomaly detection. These\ncapabilities make our model highly applicable to machine learning-based robotic\nsafety systems in dynamic environments. The code will be made publicly\navailable after acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for robots to operate effectively in diverse environments\nnecessitates the need for robust real-time anomaly detection techniques during\nrobotic operations. However, deep learning-based models in robotics face\nsignificant challenges due to limited training data and highly noisy signal\nfeatures. In this paper, we present Sparse Masked Autoregressive Flow-based\nAdversarial AutoEncoders model to address these problems. This approach\nintegrates Masked Autoregressive Flow model into Adversarial AutoEncoders to\nconstruct a flexible latent space and utilize Sparse autoencoder to efficiently\nfocus on important features, even in scenarios with limited feature space. Our\nexperiments demonstrate that the proposed model achieves a 4.96% to 9.75%\nhigher area under the receiver operating characteristic curve for\npick-and-place robotic operations with randomly placed cans, compared to\nexisting state-of-the-art methods. Notably, it showed up to 19.67% better\nperformance in scenarios involving collisions with lightweight objects.\nAdditionally, unlike the existing state-of-the-art model, our model performs\ninferences within 1 millisecond, ensuring real-time anomaly detection. These\ncapabilities make our model highly applicable to machine learning-based robotic\nsafety systems in dynamic environments. The code will be made publicly\navailable after acceptance."
                },
                "authors": [
                    {
                        "name": "Taewook Kang"
                    },
                    {
                        "name": "Bum-Jae You"
                    },
                    {
                        "name": "Juyoun Park"
                    },
                    {
                        "name": "Yisoo Lee"
                    }
                ],
                "author_detail": {
                    "name": "Yisoo Lee"
                },
                "author": "Yisoo Lee",
                "arxiv_comment": "20 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11170v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11170v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11873v1",
                "updated": "2025-04-16T08:50:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    8,
                    50,
                    51,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T08:50:51Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    8,
                    50,
                    51,
                    2,
                    106,
                    0
                ],
                "title": "Transferable Deployment of Semantic Edge Inference Systems via\n  Unsupervised Domain Adaption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transferable Deployment of Semantic Edge Inference Systems via\n  Unsupervised Domain Adaption"
                },
                "summary": "This paper investigates deploying semantic edge inference systems for\nperforming a common image clarification task. In particular, each system\nconsists of multiple Internet of Things (IoT) devices that first locally encode\nthe sensing data into semantic features and then transmit them to an edge\nserver for subsequent data fusion and task inference. The inference accuracy is\ndetermined by efficient training of the feature encoder/decoder using labeled\ndata samples. Due to the difference in sensing data and communication channel\ndistributions, deploying the system in a new environment may induce high costs\nin annotating data labels and re-training the encoder/decoder models. To\nachieve cost-effective transferable system deployment, we propose an efficient\nDomain Adaptation method for Semantic Edge INference systems (DASEIN) that can\nmaintain high inference accuracy in a new environment without the need for\nlabeled samples. Specifically, DASEIN exploits the task-relevant data\ncorrelation between different deployment scenarios by leveraging the techniques\nof unsupervised domain adaptation and knowledge distillation. It devises an\nefficient two-step adaptation procedure that sequentially aligns the data\ndistributions and adapts to the channel variations. Numerical results show\nthat, under a substantial change in sensing data distributions, the proposed\nDASEIN outperforms the best-performing benchmark method by 7.09% and 21.33% in\ninference accuracy when the new environment has similar or 25 dB lower channel\nsignal to noise power ratios (SNRs), respectively. This verifies the\neffectiveness of the proposed method in adapting both data and channel\ndistributions in practical transfer deployment applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates deploying semantic edge inference systems for\nperforming a common image clarification task. In particular, each system\nconsists of multiple Internet of Things (IoT) devices that first locally encode\nthe sensing data into semantic features and then transmit them to an edge\nserver for subsequent data fusion and task inference. The inference accuracy is\ndetermined by efficient training of the feature encoder/decoder using labeled\ndata samples. Due to the difference in sensing data and communication channel\ndistributions, deploying the system in a new environment may induce high costs\nin annotating data labels and re-training the encoder/decoder models. To\nachieve cost-effective transferable system deployment, we propose an efficient\nDomain Adaptation method for Semantic Edge INference systems (DASEIN) that can\nmaintain high inference accuracy in a new environment without the need for\nlabeled samples. Specifically, DASEIN exploits the task-relevant data\ncorrelation between different deployment scenarios by leveraging the techniques\nof unsupervised domain adaptation and knowledge distillation. It devises an\nefficient two-step adaptation procedure that sequentially aligns the data\ndistributions and adapts to the channel variations. Numerical results show\nthat, under a substantial change in sensing data distributions, the proposed\nDASEIN outperforms the best-performing benchmark method by 7.09% and 21.33% in\ninference accuracy when the new environment has similar or 25 dB lower channel\nsignal to noise power ratios (SNRs), respectively. This verifies the\neffectiveness of the proposed method in adapting both data and channel\ndistributions in practical transfer deployment applications."
                },
                "authors": [
                    {
                        "name": "Weiqiang Jiao"
                    },
                    {
                        "name": "Suzhi Bi"
                    },
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Cheng Guo"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Zhi Quan"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Quan"
                },
                "author": "Zhi Quan",
                "arxiv_comment": "14 pages, 14 figures, the paper is submitted for potential journal\n  publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11848v1",
                "updated": "2025-04-16T08:14:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    8,
                    14,
                    55,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T08:14:55Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    8,
                    14,
                    55,
                    2,
                    106,
                    0
                ],
                "title": "Proximal Inference on Population Intervention Indirect Effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proximal Inference on Population Intervention Indirect Effect"
                },
                "summary": "The population intervention indirect effect (PIIE) is a novel mediation\neffect representing the indirect component of the population intervention\neffect. Unlike traditional mediation measures, such as the natural indirect\neffect, the PIIE holds particular relevance in observational studies involving\nunethical exposures, when hypothetical interventions that impose harmful\nexposures are inappropriate. Although prior research has identified PIIE under\nunmeasured confounders between exposure and outcome, it has not fully addressed\nthe confounding that affects the mediator. This study extends the PIIE\nidentification to settings where unmeasured confounders influence\nexposure-outcome, exposure-mediator, and mediator-outcome relationships.\nSpecifically, we leverage observed covariates as proxy variables for unmeasured\nconfounders, constructing three proximal identification frameworks.\nAdditionally, we characterize the semiparametric efficiency bound and develop\nmultiply robust and locally efficient estimators. To handle high-dimensional\nnuisance parameters, we propose a debiased machine learning approach that\nachieves $\\sqrt{n}$-consistency and asymptotic normality to estimate the true\nPIIE values, even when the machine learning estimators for the nuisance\nfunctions do not converge at $\\sqrt{n}$-rate. In simulations, our estimators\ndemonstrate higher confidence interval coverage rates than conventional methods\nacross various model misspecifications. In a real data application, our\napproaches reveal an indirect effect of alcohol consumption on depression risk\nmediated by depersonalization symptoms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The population intervention indirect effect (PIIE) is a novel mediation\neffect representing the indirect component of the population intervention\neffect. Unlike traditional mediation measures, such as the natural indirect\neffect, the PIIE holds particular relevance in observational studies involving\nunethical exposures, when hypothetical interventions that impose harmful\nexposures are inappropriate. Although prior research has identified PIIE under\nunmeasured confounders between exposure and outcome, it has not fully addressed\nthe confounding that affects the mediator. This study extends the PIIE\nidentification to settings where unmeasured confounders influence\nexposure-outcome, exposure-mediator, and mediator-outcome relationships.\nSpecifically, we leverage observed covariates as proxy variables for unmeasured\nconfounders, constructing three proximal identification frameworks.\nAdditionally, we characterize the semiparametric efficiency bound and develop\nmultiply robust and locally efficient estimators. To handle high-dimensional\nnuisance parameters, we propose a debiased machine learning approach that\nachieves $\\sqrt{n}$-consistency and asymptotic normality to estimate the true\nPIIE values, even when the machine learning estimators for the nuisance\nfunctions do not converge at $\\sqrt{n}$-rate. In simulations, our estimators\ndemonstrate higher confidence interval coverage rates than conventional methods\nacross various model misspecifications. In a real data application, our\napproaches reveal an indirect effect of alcohol consumption on depression risk\nmediated by depersonalization symptoms."
                },
                "authors": [
                    {
                        "name": "Yang Bai"
                    },
                    {
                        "name": "Yifan Cui"
                    },
                    {
                        "name": "Baoluo Sun"
                    }
                ],
                "author_detail": {
                    "name": "Baoluo Sun"
                },
                "author": "Baoluo Sun",
                "arxiv_comment": "60 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11844v1",
                "updated": "2025-04-16T08:07:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    8,
                    7,
                    8,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T08:07:08Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    8,
                    7,
                    8,
                    2,
                    106,
                    0
                ],
                "title": "Evaluating the Goal-Directedness of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Goal-Directedness of Large Language Models"
                },
                "summary": "To what extent do LLMs use their capabilities towards their given goal? We\ntake this as a measure of their goal-directedness. We evaluate\ngoal-directedness on tasks that require information gathering, cognitive\neffort, and plan execution, where we use subtasks to infer each model's\nrelevant capabilities. Our evaluations of LLMs from Google DeepMind, OpenAI,\nand Anthropic show that goal-directedness is relatively consistent across\ntasks, differs from task performance, and is only moderately sensitive to\nmotivational prompts. Notably, most models are not fully goal-directed. We hope\nour goal-directedness evaluations will enable better monitoring of LLM\nprogress, and enable more deliberate design choices of agentic properties in\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To what extent do LLMs use their capabilities towards their given goal? We\ntake this as a measure of their goal-directedness. We evaluate\ngoal-directedness on tasks that require information gathering, cognitive\neffort, and plan execution, where we use subtasks to infer each model's\nrelevant capabilities. Our evaluations of LLMs from Google DeepMind, OpenAI,\nand Anthropic show that goal-directedness is relatively consistent across\ntasks, differs from task performance, and is only moderately sensitive to\nmotivational prompts. Notably, most models are not fully goal-directed. We hope\nour goal-directedness evaluations will enable better monitoring of LLM\nprogress, and enable more deliberate design choices of agentic properties in\nLLMs."
                },
                "authors": [
                    {
                        "name": "Tom Everitt"
                    },
                    {
                        "name": "Cristina Garbacea"
                    },
                    {
                        "name": "Alexis Bellot"
                    },
                    {
                        "name": "Jonathan Richens"
                    },
                    {
                        "name": "Henry Papadatos"
                    },
                    {
                        "name": "Simon Campos"
                    },
                    {
                        "name": "Rohin Shah"
                    }
                ],
                "author_detail": {
                    "name": "Rohin Shah"
                },
                "author": "Rohin Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11843v1",
                "updated": "2025-04-16T08:06:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    8,
                    6,
                    46,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T08:06:46Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    8,
                    6,
                    46,
                    2,
                    106,
                    0
                ],
                "title": "Scalable Multi-task Edge Sensing via Task-oriented Joint Information\n  Gathering and Broadcast",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Multi-task Edge Sensing via Task-oriented Joint Information\n  Gathering and Broadcast"
                },
                "summary": "The recent advance of edge computing technology enables significant sensing\nperformance improvement of Internet of Things (IoT) networks. In particular, an\nedge server (ES) is responsible for gathering sensing data from distributed\nsensing devices, and immediately executing different sensing tasks to\naccommodate the heterogeneous service demands of mobile users. However, as the\nnumber of users surges and the sensing tasks become increasingly\ncompute-intensive, the huge amount of computation workloads and data\ntransmissions may overwhelm the edge system of limited resources. Accordingly,\nwe propose in this paper a scalable edge sensing framework for multi-task\nexecution, in the sense that the computation workload and communication\noverhead of the ES do not increase with the number of downstream users or\ntasks. By exploiting the task-relevant correlations, the proposed scheme\nimplements a unified encoder at the ES, which produces a common low-dimensional\nmessage from the sensing data and broadcasts it to all users to execute their\nindividual tasks. To achieve high sensing accuracy, we extend the well-known\ninformation bottleneck theory to a multi-task scenario to jointly optimize the\ninformation gathering and broadcast processes. We also develop an efficient\ntwo-step training procedure to optimize the parameters of the neural\nnetwork-based codecs deployed in the edge sensing system. Experiment results\nshow that the proposed scheme significantly outperforms the considered\nrepresentative benchmark methods in multi-task inference accuracy. Besides, the\nproposed scheme is scalable to the network size, which maintains almost\nconstant computation delay with less than 1% degradation of inference\nperformance when the user number increases by four times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advance of edge computing technology enables significant sensing\nperformance improvement of Internet of Things (IoT) networks. In particular, an\nedge server (ES) is responsible for gathering sensing data from distributed\nsensing devices, and immediately executing different sensing tasks to\naccommodate the heterogeneous service demands of mobile users. However, as the\nnumber of users surges and the sensing tasks become increasingly\ncompute-intensive, the huge amount of computation workloads and data\ntransmissions may overwhelm the edge system of limited resources. Accordingly,\nwe propose in this paper a scalable edge sensing framework for multi-task\nexecution, in the sense that the computation workload and communication\noverhead of the ES do not increase with the number of downstream users or\ntasks. By exploiting the task-relevant correlations, the proposed scheme\nimplements a unified encoder at the ES, which produces a common low-dimensional\nmessage from the sensing data and broadcasts it to all users to execute their\nindividual tasks. To achieve high sensing accuracy, we extend the well-known\ninformation bottleneck theory to a multi-task scenario to jointly optimize the\ninformation gathering and broadcast processes. We also develop an efficient\ntwo-step training procedure to optimize the parameters of the neural\nnetwork-based codecs deployed in the edge sensing system. Experiment results\nshow that the proposed scheme significantly outperforms the considered\nrepresentative benchmark methods in multi-task inference accuracy. Besides, the\nproposed scheme is scalable to the network size, which maintains almost\nconstant computation delay with less than 1% degradation of inference\nperformance when the user number increases by four times."
                },
                "authors": [
                    {
                        "name": "Huawei Hou"
                    },
                    {
                        "name": "Suzhi Bi"
                    },
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Shuoyao Wang"
                    },
                    {
                        "name": "Liping Qian"
                    },
                    {
                        "name": "Zhi Quan"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Quan"
                },
                "author": "Zhi Quan",
                "arxiv_comment": "15 pages, 10 figures. The paper is submitted for potential journal\n  publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08754v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08754v3",
                "updated": "2025-04-16T07:59:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    59,
                    48,
                    2,
                    106,
                    0
                ],
                "published": "2025-03-28T15:49:52Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    49,
                    52,
                    4,
                    87,
                    0
                ],
                "title": "Towards Personalized Conversational Sales Agents : Contextual User\n  Profiling for Strategic Action",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Personalized Conversational Sales Agents : Contextual User\n  Profiling for Strategic Action"
                },
                "summary": "Conversational Recommender Systems (CRSs) aim to engage users in dialogue to\nprovide tailored recommendations. While traditional CRSs focus on eliciting\npreferences and retrieving items, real-world e-commerce interactions involve\nmore complex decision-making, where users consider multiple factors beyond\nsimple attributes. To bridge this gap, we introduce Conversational Sales\n(CSales), a novel task that unifies preference elicitation, recommendation, and\npersuasion to better support user decision-making. For a realistic evaluation\nof CSales, we present CSUser, an LLM-based user simulator constructed from\nreal-world data, modeling diverse user profiles with needs and personalities.\nAdditionally, we propose CSI, a conversational sales agent that proactively\ninfers contextual profiles through dialogue for personalized action planning.\nExtensive experiments demonstrate that CSUser effectively replicates real-world\nusers and emphasize the importance of contextual profiling for strategic action\nselection, ultimately driving successful purchases in e-commerce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Recommender Systems (CRSs) aim to engage users in dialogue to\nprovide tailored recommendations. While traditional CRSs focus on eliciting\npreferences and retrieving items, real-world e-commerce interactions involve\nmore complex decision-making, where users consider multiple factors beyond\nsimple attributes. To bridge this gap, we introduce Conversational Sales\n(CSales), a novel task that unifies preference elicitation, recommendation, and\npersuasion to better support user decision-making. For a realistic evaluation\nof CSales, we present CSUser, an LLM-based user simulator constructed from\nreal-world data, modeling diverse user profiles with needs and personalities.\nAdditionally, we propose CSI, a conversational sales agent that proactively\ninfers contextual profiles through dialogue for personalized action planning.\nExtensive experiments demonstrate that CSUser effectively replicates real-world\nusers and emphasize the importance of contextual profiling for strategic action\nselection, ultimately driving successful purchases in e-commerce."
                },
                "authors": [
                    {
                        "name": "Tongyoung Kim"
                    },
                    {
                        "name": "Jeongeun Lee"
                    },
                    {
                        "name": "Soojin Yoon"
                    },
                    {
                        "name": "Sunghwan Kim"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08754v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08754v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11840v1",
                "updated": "2025-04-16T07:57:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    57,
                    42,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T07:57:42Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    57,
                    42,
                    2,
                    106,
                    0
                ],
                "title": "GT-SVQ: A Linear-Time Graph Transformer for Node Classification Using\n  Spiking Vector Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GT-SVQ: A Linear-Time Graph Transformer for Node Classification Using\n  Spiking Vector Quantization"
                },
                "summary": "Graph Transformers (GTs), which simultaneously integrate message-passing and\nself-attention mechanisms, have achieved promising empirical results in some\ngraph prediction tasks. Although these approaches show the potential of\nTransformers in capturing long-range graph topology information, issues\nconcerning the quadratic complexity and high computing energy consumption\nseverely limit the scalability of GTs on large-scale graphs. Recently, as\nbrain-inspired neural networks, Spiking Neural Networks (SNNs), facilitate the\ndevelopment of graph representation learning methods with lower computational\nand storage overhead through the unique event-driven spiking neurons. Inspired\nby these characteristics, we propose a linear-time Graph Transformer using\nSpiking Vector Quantization (GT-SVQ) for node classification. GT-SVQ\nreconstructs codebooks based on rate coding outputs from spiking neurons, and\ninjects the codebooks into self-attention blocks to aggregate global\ninformation in linear complexity. Besides, spiking vector quantization\neffectively alleviates codebook collapse and the reliance on complex machinery\n(distance measure, auxiliary loss, etc.) present in previous vector\nquantization-based graph learning methods. In experiments, we compare GT-SVQ\nwith other state-of-the-art baselines on node classification datasets ranging\nfrom small to large. Experimental results show that GT-SVQ has achieved\ncompetitive performances on most datasets while maintaining up to 130x faster\ninference speed compared to other GTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Transformers (GTs), which simultaneously integrate message-passing and\nself-attention mechanisms, have achieved promising empirical results in some\ngraph prediction tasks. Although these approaches show the potential of\nTransformers in capturing long-range graph topology information, issues\nconcerning the quadratic complexity and high computing energy consumption\nseverely limit the scalability of GTs on large-scale graphs. Recently, as\nbrain-inspired neural networks, Spiking Neural Networks (SNNs), facilitate the\ndevelopment of graph representation learning methods with lower computational\nand storage overhead through the unique event-driven spiking neurons. Inspired\nby these characteristics, we propose a linear-time Graph Transformer using\nSpiking Vector Quantization (GT-SVQ) for node classification. GT-SVQ\nreconstructs codebooks based on rate coding outputs from spiking neurons, and\ninjects the codebooks into self-attention blocks to aggregate global\ninformation in linear complexity. Besides, spiking vector quantization\neffectively alleviates codebook collapse and the reliance on complex machinery\n(distance measure, auxiliary loss, etc.) present in previous vector\nquantization-based graph learning methods. In experiments, we compare GT-SVQ\nwith other state-of-the-art baselines on node classification datasets ranging\nfrom small to large. Experimental results show that GT-SVQ has achieved\ncompetitive performances on most datasets while maintaining up to 130x faster\ninference speed compared to other GTs."
                },
                "authors": [
                    {
                        "name": "Huizhe Zhang"
                    },
                    {
                        "name": "Jintang Li"
                    },
                    {
                        "name": "Yuchang Zhu"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11837v1",
                "updated": "2025-04-16T07:52:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    52,
                    6,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T07:52:06Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    52,
                    6,
                    2,
                    106,
                    0
                ],
                "title": "FiSMiness: A Finite State Machine Based Paradigm for Emotional Support\n  Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiSMiness: A Finite State Machine Based Paradigm for Emotional Support\n  Conversations"
                },
                "summary": "Emotional support conversation (ESC) aims to alleviate the emotional distress\nof individuals through effective conversations. Although large language models\n(LLMs) have obtained remarkable progress on ESC, most of these studies might\nnot define the diagram from the state model perspective, therefore providing a\nsuboptimal solution for long-term satisfaction. To address such an issue, we\nleverage the Finite State Machine (FSM) on LLMs, and propose a framework called\nFiSMiness. Our framework allows a single LLM to bootstrap the planning during\nESC, and self-reason the seeker's emotion, support strategy and the final\nresponse upon each conversational turn. Substantial experiments on ESC datasets\nsuggest that FiSMiness outperforms many baselines, including direct inference,\nself-refine, chain of thought, finetuning, and external-assisted methods, even\nthose with many more parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotional support conversation (ESC) aims to alleviate the emotional distress\nof individuals through effective conversations. Although large language models\n(LLMs) have obtained remarkable progress on ESC, most of these studies might\nnot define the diagram from the state model perspective, therefore providing a\nsuboptimal solution for long-term satisfaction. To address such an issue, we\nleverage the Finite State Machine (FSM) on LLMs, and propose a framework called\nFiSMiness. Our framework allows a single LLM to bootstrap the planning during\nESC, and self-reason the seeker's emotion, support strategy and the final\nresponse upon each conversational turn. Substantial experiments on ESC datasets\nsuggest that FiSMiness outperforms many baselines, including direct inference,\nself-refine, chain of thought, finetuning, and external-assisted methods, even\nthose with many more parameters."
                },
                "authors": [
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "Qingqing Gu"
                    },
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Teng Chen"
                    },
                    {
                        "name": "Zhonglin Jiang"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Luo Ji"
                    }
                ],
                "author_detail": {
                    "name": "Luo Ji"
                },
                "author": "Luo Ji",
                "arxiv_comment": "accepted by CMCL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11836v1",
                "updated": "2025-04-16T07:46:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    46,
                    46,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T07:46:46Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    46,
                    46,
                    2,
                    106,
                    0
                ],
                "title": "Non-centering for discrete-valued state transition models: an\n  application to ESBL-producing E. coli transmission in Malawi",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-centering for discrete-valued state transition models: an\n  application to ESBL-producing E. coli transmission in Malawi"
                },
                "summary": "Infectious disease transmission is often modelled by discrete-valued\nstochastic state-transition processes. Due to a lack of complete data, Bayesian\ninference for these models often relies on data-augmentation techniques. These\ntechniques are often inefficient or time consuming to implement. We introduce a\nnovel data-augmentation Markov chain Monte Carlo method for discrete-time\nindividual-based epidemic models, which we call the Rippler algorithm. This\nmethod uses the transmission model in the proposal step of the\nMetropolis-Hastings algorithm, rather than in the accept-reject step. We test\nthe Rippler algorithm on simulated data and apply it to data on\nextended-spectrum beta-lactamase (ESBL)-producing E. coli collected in\nBlantyre, Malawi. We compare the Rippler algorithm to two other commonly used\nBayesian inference methods for partially observed epidemic data, and find that\nit has a good balance between mixing speed and computational complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infectious disease transmission is often modelled by discrete-valued\nstochastic state-transition processes. Due to a lack of complete data, Bayesian\ninference for these models often relies on data-augmentation techniques. These\ntechniques are often inefficient or time consuming to implement. We introduce a\nnovel data-augmentation Markov chain Monte Carlo method for discrete-time\nindividual-based epidemic models, which we call the Rippler algorithm. This\nmethod uses the transmission model in the proposal step of the\nMetropolis-Hastings algorithm, rather than in the accept-reject step. We test\nthe Rippler algorithm on simulated data and apply it to data on\nextended-spectrum beta-lactamase (ESBL)-producing E. coli collected in\nBlantyre, Malawi. We compare the Rippler algorithm to two other commonly used\nBayesian inference methods for partially observed epidemic data, and find that\nit has a good balance between mixing speed and computational complexity."
                },
                "authors": [
                    {
                        "name": "James Neill"
                    },
                    {
                        "name": "Rebecca Lester"
                    },
                    {
                        "name": "Winnie Bakali"
                    },
                    {
                        "name": "Gareth Roberts"
                    },
                    {
                        "name": "Nicholas Feasey"
                    },
                    {
                        "name": "Lloyd A. C. Chapman"
                    },
                    {
                        "name": "Chris Jewell"
                    }
                ],
                "author_detail": {
                    "name": "Chris Jewell"
                },
                "author": "Chris Jewell",
                "arxiv_comment": "18 pages, 8 figures (plus supplementary material with an additional\n  18 pages, 12 figures)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11835v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11835v1",
                "updated": "2025-04-16T07:46:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    46,
                    8,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T07:46:08Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    46,
                    8,
                    2,
                    106,
                    0
                ],
                "title": "Particle Data Cloning for Complex Ordinary Differential Equations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Particle Data Cloning for Complex Ordinary Differential Equations"
                },
                "summary": "Ordinary differential equations (ODEs) are fundamental tools for modeling\ncomplex dynamic systems across scientific disciplines. However, parameter\nestimation in ODE models is challenging due to the multimodal nature of the\nlikelihood function, which can lead to local optima and unstable inference. In\nthis paper, we propose particle data cloning (PDC), a novel approach that\nenhances global optimization by leveraging data cloning and annealed sequential\nMonte Carlo (ASMC). PDC mitigates multimodality by refining the likelihood\nthrough data clones and progressively extracting information from the sharpened\nposterior. Compared to standard data cloning, PDC provides more reliable\nfrequentist inference and demonstrates superior global optimization\nperformance. We offer practical guidelines for efficient implementation and\nillustrate the method through simulation studies and an application to a\nprey-predator ODE model. Our implementation is available at\nhttps://github.com/SONDONGHUI/PDC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ordinary differential equations (ODEs) are fundamental tools for modeling\ncomplex dynamic systems across scientific disciplines. However, parameter\nestimation in ODE models is challenging due to the multimodal nature of the\nlikelihood function, which can lead to local optima and unstable inference. In\nthis paper, we propose particle data cloning (PDC), a novel approach that\nenhances global optimization by leveraging data cloning and annealed sequential\nMonte Carlo (ASMC). PDC mitigates multimodality by refining the likelihood\nthrough data clones and progressively extracting information from the sharpened\nposterior. Compared to standard data cloning, PDC provides more reliable\nfrequentist inference and demonstrates superior global optimization\nperformance. We offer practical guidelines for efficient implementation and\nillustrate the method through simulation studies and an application to a\nprey-predator ODE model. Our implementation is available at\nhttps://github.com/SONDONGHUI/PDC."
                },
                "authors": [
                    {
                        "name": "Donghui Son"
                    },
                    {
                        "name": "Liangliang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liangliang Wang"
                },
                "author": "Liangliang Wang",
                "arxiv_comment": "33 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11835v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11835v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11834v1",
                "updated": "2025-04-16T07:45:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    45,
                    44,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T07:45:44Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    45,
                    44,
                    2,
                    106,
                    0
                ],
                "title": "Estimation and inference in error-in-operator model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimation and inference in error-in-operator model"
                },
                "summary": "Many statistical problems can be reduced to a linear inverse problem in which\nonly a noisy version of the operator is available. Particular examples include\nrandom design regression, deconvolution problem, instrumental variable\nregression, functional data analysis, error-in-variable regression, drift\nestimation in stochastic diffusion, and many others. The pragmatic plug-in\napproach can be well justified in the classical asymptotic setup with a growing\nsample size. However, recent developments in high dimensional inference reveal\nsome new features of this problem. In high dimensional linear regression with a\nrandom design, the plug-in approach is questionable but the use of a simple\nridge penalization yields a benign overfitting phenomenon; see\n\\cite{baLoLu2020}, \\cite{ChMo2022}, \\cite{NoPuSp2024}. This paper revisits the\ngeneral Error-in-Operator problem for finite samples and high dimension of the\nsource and image spaces. A particular focus is on the choice of a proper\nregularization. We show that a simple ridge penalty (Tikhonov regularization)\nworks properly in the case when the operator is more regular than the signal.\nIn the opposite case, some model reduction technique like spectral truncation\nshould be applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many statistical problems can be reduced to a linear inverse problem in which\nonly a noisy version of the operator is available. Particular examples include\nrandom design regression, deconvolution problem, instrumental variable\nregression, functional data analysis, error-in-variable regression, drift\nestimation in stochastic diffusion, and many others. The pragmatic plug-in\napproach can be well justified in the classical asymptotic setup with a growing\nsample size. However, recent developments in high dimensional inference reveal\nsome new features of this problem. In high dimensional linear regression with a\nrandom design, the plug-in approach is questionable but the use of a simple\nridge penalization yields a benign overfitting phenomenon; see\n\\cite{baLoLu2020}, \\cite{ChMo2022}, \\cite{NoPuSp2024}. This paper revisits the\ngeneral Error-in-Operator problem for finite samples and high dimension of the\nsource and image spaces. A particular focus is on the choice of a proper\nregularization. We show that a simple ridge penalty (Tikhonov regularization)\nworks properly in the case when the operator is more regular than the signal.\nIn the opposite case, some model reduction technique like spectral truncation\nshould be applied."
                },
                "authors": [
                    {
                        "name": "Vladimir Spokoiny"
                    }
                ],
                "author_detail": {
                    "name": "Vladimir Spokoiny"
                },
                "author": "Vladimir Spokoiny",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F10, 62E17, 62J12",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11833v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11833v1",
                "updated": "2025-04-16T07:45:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    45,
                    10,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T07:45:10Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    45,
                    10,
                    2,
                    106,
                    0
                ],
                "title": "Could Thinking Multilingually Empower LLM Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Could Thinking Multilingually Empower LLM Reasoning?"
                },
                "summary": "Previous work indicates that large language models exhibit a significant\n\"English bias\", i.e. they often perform better when tasks are presented in\nEnglish. Interestingly, we have observed that using certain other languages in\nreasoning tasks can yield better performance than English. However, this\nphenomenon remains under-explored. In this paper, we explore the upper bound of\nharnessing multilingualism in reasoning tasks, suggesting that multilingual\nreasoning promises significantly (by nearly 10 Acc@$k$ points) and robustly\n(tolerance for variations in translation quality and language choice) higher\nupper bounds than English-only reasoning. Besides analyzing the reason behind\nthe upper bound and challenges in reaching it, we also find that common answer\nselection methods cannot achieve this upper bound, due to their limitations and\nbiases. These insights could pave the way for future research aimed at fully\nharnessing the potential of multilingual reasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous work indicates that large language models exhibit a significant\n\"English bias\", i.e. they often perform better when tasks are presented in\nEnglish. Interestingly, we have observed that using certain other languages in\nreasoning tasks can yield better performance than English. However, this\nphenomenon remains under-explored. In this paper, we explore the upper bound of\nharnessing multilingualism in reasoning tasks, suggesting that multilingual\nreasoning promises significantly (by nearly 10 Acc@$k$ points) and robustly\n(tolerance for variations in translation quality and language choice) higher\nupper bounds than English-only reasoning. Besides analyzing the reason behind\nthe upper bound and challenges in reaching it, we also find that common answer\nselection methods cannot achieve this upper bound, due to their limitations and\nbiases. These insights could pave the way for future research aimed at fully\nharnessing the potential of multilingual reasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Changjiang Gao"
                    },
                    {
                        "name": "Xu Huang"
                    },
                    {
                        "name": "Wenhao Zhu"
                    },
                    {
                        "name": "Shujian Huang"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Fei Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Fei Yuan"
                },
                "author": "Fei Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11833v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11833v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12004v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12004v2",
                "updated": "2025-04-16T07:38:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    38,
                    33,
                    2,
                    106,
                    0
                ],
                "published": "2025-03-15T06:00:35Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    6,
                    0,
                    35,
                    5,
                    74,
                    0
                ],
                "title": "WiFi-Diffusion: Achieving Fine-Grained WiFi Radio Map Estimation With\n  Ultra-Low Sampling Rate by Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WiFi-Diffusion: Achieving Fine-Grained WiFi Radio Map Estimation With\n  Ultra-Low Sampling Rate by Diffusion Models"
                },
                "summary": "Fine-grained radio map presents communication parameters of interest, e.g.,\nreceived signal strength, at every point across a large geographical region. It\ncan be leveraged to improve the efficiency of spectrum utilization for a large\narea, particularly critical for the unlicensed WiFi spectrum. The problem of\nfine-grained radio map estimation is to utilize radio samples collected by\nsparsely distributed sensors to infer the map. This problem is challenging due\nto the ultra-low sampling rate, where the number of available samples is far\nless than the fine-grained resolution required for radio map estimation. We\npropose WiFi-Diffusion -- a novel generative framework for achieving\nfine-grained WiFi radio map estimation using diffusion models. WiFi-Diffusion\nemploys the creative power of generative AI to address the ultra-low sampling\nrate challenge and consists of three blocks: 1) a boost block, using prior\ninformation such as the layout of obstacles to optimize the diffusion model; 2)\na generation block, leveraging the diffusion model to generate a candidate set\nof radio maps; and 3) an election block, utilizing the radio propagation model\nas a guide to find the best radio map from the candidate set. Extensive\nsimulations demonstrate that 1) the fine-grained radio map generated by\nWiFi-Diffusion is ten times better than those produced by state-of-the-art\n(SOTA) when they use the same ultra-low sampling rate; and 2) WiFi-Diffusion\nachieves comparable fine-grained radio map quality with only one-fifth of the\nsampling rate required by SOTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained radio map presents communication parameters of interest, e.g.,\nreceived signal strength, at every point across a large geographical region. It\ncan be leveraged to improve the efficiency of spectrum utilization for a large\narea, particularly critical for the unlicensed WiFi spectrum. The problem of\nfine-grained radio map estimation is to utilize radio samples collected by\nsparsely distributed sensors to infer the map. This problem is challenging due\nto the ultra-low sampling rate, where the number of available samples is far\nless than the fine-grained resolution required for radio map estimation. We\npropose WiFi-Diffusion -- a novel generative framework for achieving\nfine-grained WiFi radio map estimation using diffusion models. WiFi-Diffusion\nemploys the creative power of generative AI to address the ultra-low sampling\nrate challenge and consists of three blocks: 1) a boost block, using prior\ninformation such as the layout of obstacles to optimize the diffusion model; 2)\na generation block, leveraging the diffusion model to generate a candidate set\nof radio maps; and 3) an election block, utilizing the radio propagation model\nas a guide to find the best radio map from the candidate set. Extensive\nsimulations demonstrate that 1) the fine-grained radio map generated by\nWiFi-Diffusion is ten times better than those produced by state-of-the-art\n(SOTA) when they use the same ultra-low sampling rate; and 2) WiFi-Diffusion\nachieves comparable fine-grained radio map quality with only one-fifth of the\nsampling rate required by SOTA."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Shuhang Zhang"
                    },
                    {
                        "name": "Qingyu Liu"
                    },
                    {
                        "name": "Hongliang Zhang"
                    },
                    {
                        "name": "Lingyang Song"
                    }
                ],
                "author_detail": {
                    "name": "Lingyang Song"
                },
                "author": "Lingyang Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12004v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12004v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.12285v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12285v1",
                "updated": "2025-04-16T17:51:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    17,
                    51,
                    43,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T17:51:43Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    17,
                    51,
                    43,
                    2,
                    106,
                    0
                ],
                "title": "BitNet b1.58 2B4T Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitNet b1.58 2B4T Technical Report"
                },
                "summary": "We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large\nLanguage Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4\ntrillion tokens, the model has been rigorously evaluated across benchmarks\ncovering language understanding, mathematical reasoning, coding proficiency,\nand conversational ability. Our results demonstrate that BitNet b1.58 2B4T\nachieves performance on par with leading open-weight, full-precision LLMs of\nsimilar size, while offering significant advantages in computational\nefficiency, including substantially reduced memory footprint, energy\nconsumption, and decoding latency. To facilitate further research and adoption,\nthe model weights are released via Hugging Face along with open-source\ninference implementations for both GPU and CPU architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large\nLanguage Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4\ntrillion tokens, the model has been rigorously evaluated across benchmarks\ncovering language understanding, mathematical reasoning, coding proficiency,\nand conversational ability. Our results demonstrate that BitNet b1.58 2B4T\nachieves performance on par with leading open-weight, full-precision LLMs of\nsimilar size, while offering significant advantages in computational\nefficiency, including substantially reduced memory footprint, energy\nconsumption, and decoding latency. To facilitate further research and adoption,\nthe model weights are released via Hugging Face along with open-source\ninference implementations for both GPU and CPU architectures."
                },
                "authors": [
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Hongyu Wang"
                    },
                    {
                        "name": "Shaohan Huang"
                    },
                    {
                        "name": "Xingxing Zhang"
                    },
                    {
                        "name": "Ying Hu"
                    },
                    {
                        "name": "Ting Song"
                    },
                    {
                        "name": "Yan Xia"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12285v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12285v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02108v2",
                "updated": "2025-04-16T17:50:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    17,
                    50,
                    12,
                    2,
                    106,
                    0
                ],
                "published": "2024-10-03T00:09:15Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    0,
                    9,
                    15,
                    3,
                    277,
                    0
                ],
                "title": "ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement"
                },
                "summary": "Post-training Large Language Models (LLMs) with explicit reasoning\ntrajectories can enhance their reasoning abilities. However, acquiring such\nhigh-quality trajectory data typically demands meticulous supervision from\nhumans or superior models, which can be either expensive or\nlicense-constrained. In this paper, we explore how far an LLM can improve its\nreasoning by self-synthesizing reasoning paths as training data without any\nadditional supervision. Existing self-synthesizing methods, such as STaR,\nsuffer from poor generalization to out-of-domain (OOD) reasoning tasks. We\nhypothesize it is due to that their self-synthesized reasoning paths are too\ntask-specific, lacking general task-agnostic reasoning guidance. To address\nthis, we propose Reasoning Generalist via Self-Improvement (ReGenesis), a\nmethod to self-synthesize reasoning paths as post-training data by progressing\nfrom abstract to concrete. More specifically, ReGenesis self-synthesizes\nreasoning paths by converting general reasoning guidelines into task-specific\nones, generating reasoning structures, and subsequently transforming these\nstructures into reasoning paths, without the need for human-designed\ntask-specific examples used in existing methods. We show that ReGenesis\nachieves superior performance on all in-domain and OOD settings tested compared\nto existing methods. For six OOD tasks specifically, while previous methods\nexhibited an average performance decrease of approximately 4.6% after post\ntraining, ReGenesis delivers around 6.1% performance improvement. We also\nconduct in-depth analysis of our framework and show ReGenesis is effective\nacross various LLMs and design choices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training Large Language Models (LLMs) with explicit reasoning\ntrajectories can enhance their reasoning abilities. However, acquiring such\nhigh-quality trajectory data typically demands meticulous supervision from\nhumans or superior models, which can be either expensive or\nlicense-constrained. In this paper, we explore how far an LLM can improve its\nreasoning by self-synthesizing reasoning paths as training data without any\nadditional supervision. Existing self-synthesizing methods, such as STaR,\nsuffer from poor generalization to out-of-domain (OOD) reasoning tasks. We\nhypothesize it is due to that their self-synthesized reasoning paths are too\ntask-specific, lacking general task-agnostic reasoning guidance. To address\nthis, we propose Reasoning Generalist via Self-Improvement (ReGenesis), a\nmethod to self-synthesize reasoning paths as post-training data by progressing\nfrom abstract to concrete. More specifically, ReGenesis self-synthesizes\nreasoning paths by converting general reasoning guidelines into task-specific\nones, generating reasoning structures, and subsequently transforming these\nstructures into reasoning paths, without the need for human-designed\ntask-specific examples used in existing methods. We show that ReGenesis\nachieves superior performance on all in-domain and OOD settings tested compared\nto existing methods. For six OOD tasks specifically, while previous methods\nexhibited an average performance decrease of approximately 4.6% after post\ntraining, ReGenesis delivers around 6.1% performance improvement. We also\nconduct in-depth analysis of our framework and show ReGenesis is effective\nacross various LLMs and design choices."
                },
                "authors": [
                    {
                        "name": "Xiangyu Peng"
                    },
                    {
                        "name": "Congying Xia"
                    },
                    {
                        "name": "Xinyi Yang"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Chien-Sheng Wu"
                    },
                    {
                        "name": "Chen Xing"
                    }
                ],
                "author_detail": {
                    "name": "Chen Xing"
                },
                "author": "Chen Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09775v2",
                "updated": "2025-04-16T17:34:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    17,
                    34,
                    4,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-14T00:29:49Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    0,
                    29,
                    49,
                    0,
                    104,
                    0
                ],
                "title": "Understanding and Optimizing Multi-Stage AI Inference Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Optimizing Multi-Stage AI Inference Pipelines"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads."
                },
                "authors": [
                    {
                        "name": "Abhimanyu Rajeshkumar Bambhaniya"
                    },
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "name": "Sudarshan Srinivasan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Amir Yazdanbakhsh"
                    },
                    {
                        "name": "Midhilesh Elavazhagan"
                    },
                    {
                        "name": "Madhu Kumar"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "arxiv_comment": "Inference System Design for Multi-Stage AI Inference Pipelines. 13\n  Pages, 15 Figues, 3 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12268v1",
                "updated": "2025-04-16T17:30:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    17,
                    30,
                    36,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T17:30:36Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    17,
                    30,
                    36,
                    2,
                    106,
                    0
                ],
                "title": "HLS-Eval: A Benchmark and Framework for Evaluating LLMs on High-Level\n  Synthesis Design Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HLS-Eval: A Benchmark and Framework for Evaluating LLMs on High-Level\n  Synthesis Design Tasks"
                },
                "summary": "The rapid scaling of large language model (LLM) training and inference has\ndriven their adoption in semiconductor design across academia and industry.\nWhile most prior work evaluates LLMs on hardware description language (HDL)\ntasks, particularly Verilog, designers are increasingly using high-level\nsynthesis (HLS) to build domain-specific accelerators and complex hardware\nsystems. However, benchmarks and tooling to comprehensively evaluate LLMs for\nHLS design tasks remain scarce.\n  To address this, we introduce HLS-Eval, the first complete benchmark and\nevaluation framework for LLM-driven HLS design. HLS-Eval targets two core\ntasks: (1) generating HLS code from natural language descriptions, and (2)\nperforming HLS-specific code edits to optimize performance and hardware\nefficiency. The benchmark includes 94 unique designs drawn from standard HLS\nbenchmarks and novel sources. Each case is prepared via a semi-automated flow\nthat produces a natural language description and a paired testbench for\nC-simulation and synthesis validation, ensuring each task is \"LLM-ready.\"\n  Beyond the benchmark, HLS-Eval offers a modular Python framework for\nautomated, parallel evaluation of both local and hosted LLMs. It includes a\nparallel evaluation engine, direct HLS tool integration, and abstractions for\nto support different LLM interaction paradigms, enabling rapid prototyping of\nnew benchmarks, tasks, and LLM methods.\n  We demonstrate HLS-Eval through baseline evaluations of open-source LLMs on\nVitis HLS, measuring outputs across four key metrics - parseability,\ncompilability, runnability, and synthesizability - reflecting the iterative HLS\ndesign cycle. We also report pass@k metrics, establishing clear baselines and\nreusable infrastructure for the broader LLM-for-hardware community.\n  All benchmarks, framework code, and results are open-sourced at\nhttps://github.com/stefanpie/hls-eval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid scaling of large language model (LLM) training and inference has\ndriven their adoption in semiconductor design across academia and industry.\nWhile most prior work evaluates LLMs on hardware description language (HDL)\ntasks, particularly Verilog, designers are increasingly using high-level\nsynthesis (HLS) to build domain-specific accelerators and complex hardware\nsystems. However, benchmarks and tooling to comprehensively evaluate LLMs for\nHLS design tasks remain scarce.\n  To address this, we introduce HLS-Eval, the first complete benchmark and\nevaluation framework for LLM-driven HLS design. HLS-Eval targets two core\ntasks: (1) generating HLS code from natural language descriptions, and (2)\nperforming HLS-specific code edits to optimize performance and hardware\nefficiency. The benchmark includes 94 unique designs drawn from standard HLS\nbenchmarks and novel sources. Each case is prepared via a semi-automated flow\nthat produces a natural language description and a paired testbench for\nC-simulation and synthesis validation, ensuring each task is \"LLM-ready.\"\n  Beyond the benchmark, HLS-Eval offers a modular Python framework for\nautomated, parallel evaluation of both local and hosted LLMs. It includes a\nparallel evaluation engine, direct HLS tool integration, and abstractions for\nto support different LLM interaction paradigms, enabling rapid prototyping of\nnew benchmarks, tasks, and LLM methods.\n  We demonstrate HLS-Eval through baseline evaluations of open-source LLMs on\nVitis HLS, measuring outputs across four key metrics - parseability,\ncompilability, runnability, and synthesizability - reflecting the iterative HLS\ndesign cycle. We also report pass@k metrics, establishing clear baselines and\nreusable infrastructure for the broader LLM-for-hardware community.\n  All benchmarks, framework code, and results are open-sourced at\nhttps://github.com/stefanpie/hls-eval."
                },
                "authors": [
                    {
                        "name": "Stefan Abi-Karam"
                    },
                    {
                        "name": "Cong Hao"
                    }
                ],
                "author_detail": {
                    "name": "Cong Hao"
                },
                "author": "Cong Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12256v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12256v1",
                "updated": "2025-04-16T17:07:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    17,
                    7,
                    16,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T17:07:16Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    17,
                    7,
                    16,
                    2,
                    106,
                    0
                ],
                "title": "FLIP Reasoning Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLIP Reasoning Challenge"
                },
                "summary": "Over the past years, advances in artificial intelligence (AI) have\ndemonstrated how AI can solve many perception and generation tasks, such as\nimage classification and text writing, yet reasoning remains a challenge. This\npaper introduces the FLIP dataset, a benchmark for evaluating AI reasoning\ncapabilities based on human verification tasks on the Idena blockchain. FLIP\nchallenges present users with two orderings of 4 images, requiring them to\nidentify the logically coherent one. By emphasizing sequential reasoning,\nvisual storytelling, and common sense, FLIP provides a unique testbed for\nmultimodal AI systems. Our experiments evaluate state-of-the-art models,\nleveraging both vision-language models (VLMs) and large language models (LLMs).\nResults reveal that even the best open-sourced and closed-sourced models\nachieve maximum accuracies of 75.5% and 77.9%, respectively, in zero-shot\nsettings, compared to human performance of 95.3%. Captioning models aid\nreasoning models by providing text descriptions of images, yielding better\nresults than when using the raw images directly, 69.6% vs. 75.2% for Gemini 1.5\nPro. Combining the predictions from 15 models in an ensemble increases the\naccuracy to 85.2%. These findings highlight the limitations of existing\nreasoning models and the need for robust multimodal benchmarks like FLIP. The\nfull codebase and dataset will be available at\nhttps://github.com/aplesner/FLIP-Reasoning-Challenge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past years, advances in artificial intelligence (AI) have\ndemonstrated how AI can solve many perception and generation tasks, such as\nimage classification and text writing, yet reasoning remains a challenge. This\npaper introduces the FLIP dataset, a benchmark for evaluating AI reasoning\ncapabilities based on human verification tasks on the Idena blockchain. FLIP\nchallenges present users with two orderings of 4 images, requiring them to\nidentify the logically coherent one. By emphasizing sequential reasoning,\nvisual storytelling, and common sense, FLIP provides a unique testbed for\nmultimodal AI systems. Our experiments evaluate state-of-the-art models,\nleveraging both vision-language models (VLMs) and large language models (LLMs).\nResults reveal that even the best open-sourced and closed-sourced models\nachieve maximum accuracies of 75.5% and 77.9%, respectively, in zero-shot\nsettings, compared to human performance of 95.3%. Captioning models aid\nreasoning models by providing text descriptions of images, yielding better\nresults than when using the raw images directly, 69.6% vs. 75.2% for Gemini 1.5\nPro. Combining the predictions from 15 models in an ensemble increases the\naccuracy to 85.2%. These findings highlight the limitations of existing\nreasoning models and the need for robust multimodal benchmarks like FLIP. The\nfull codebase and dataset will be available at\nhttps://github.com/aplesner/FLIP-Reasoning-Challenge."
                },
                "authors": [
                    {
                        "name": "Andreas Plesner"
                    },
                    {
                        "name": "Turlan Kuzhagaliyev"
                    },
                    {
                        "name": "Roger Wattenhofer"
                    }
                ],
                "author_detail": {
                    "name": "Roger Wattenhofer"
                },
                "author": "Roger Wattenhofer",
                "arxiv_comment": "Published at First Workshop on Open Science for Foundation Models at\n  ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12256v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12256v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12250v1",
                "updated": "2025-04-16T16:54:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    54,
                    38,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T16:54:38Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    54,
                    38,
                    2,
                    106,
                    0
                ],
                "title": "AnomalyGen: An Automated Semantic Log Sequence Generation Framework with\n  LLM for Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnomalyGen: An Automated Semantic Log Sequence Generation Framework with\n  LLM for Anomaly Detection"
                },
                "summary": "The scarcity of high-quality public log datasets has become a critical\nbottleneck in advancing log-based anomaly detection techniques. Current\ndatasets exhibit three fundamental limitations: (1) incomplete event coverage,\n(2) artificial patterns introduced by static analysis-based generation\nframeworks, and (3) insufficient semantic awareness. To address these\nchallenges, we present AnomalyGen, the first automated log synthesis framework\nspecifically designed for anomaly detection. Our framework introduces a novel\nfour-phase architecture that integrates enhanced program analysis with\nChain-of-Thought reasoning (CoT reasoning), enabling iterative log generation\nand anomaly annotation without requiring physical system execution. Evaluations\non Hadoop and HDFS distributed systems demonstrate that AnomalyGen achieves\nsubstantially broader log event coverage (38-95 times improvement over existing\ndatasets) while producing more operationally realistic log sequences compared\nto static analysis-based approaches. When augmenting benchmark datasets with\nsynthesized logs, we observe maximum F1-score improvements of 3.7% (average\n1.8% improvement across three state-of-the-art anomaly detection models). This\nwork not only establishes a high-quality benchmarking resource for automated\nlog analysis but also pioneers a new paradigm for applying large language\nmodels (LLMs) in software engineering workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of high-quality public log datasets has become a critical\nbottleneck in advancing log-based anomaly detection techniques. Current\ndatasets exhibit three fundamental limitations: (1) incomplete event coverage,\n(2) artificial patterns introduced by static analysis-based generation\nframeworks, and (3) insufficient semantic awareness. To address these\nchallenges, we present AnomalyGen, the first automated log synthesis framework\nspecifically designed for anomaly detection. Our framework introduces a novel\nfour-phase architecture that integrates enhanced program analysis with\nChain-of-Thought reasoning (CoT reasoning), enabling iterative log generation\nand anomaly annotation without requiring physical system execution. Evaluations\non Hadoop and HDFS distributed systems demonstrate that AnomalyGen achieves\nsubstantially broader log event coverage (38-95 times improvement over existing\ndatasets) while producing more operationally realistic log sequences compared\nto static analysis-based approaches. When augmenting benchmark datasets with\nsynthesized logs, we observe maximum F1-score improvements of 3.7% (average\n1.8% improvement across three state-of-the-art anomaly detection models). This\nwork not only establishes a high-quality benchmarking resource for automated\nlog analysis but also pioneers a new paradigm for applying large language\nmodels (LLMs) in software engineering workflows."
                },
                "authors": [
                    {
                        "name": "Xinyu Li"
                    },
                    {
                        "name": "Yingtong Huo"
                    },
                    {
                        "name": "Chenxi Mao"
                    },
                    {
                        "name": "Shiwen Shan"
                    },
                    {
                        "name": "Yuxin Su"
                    },
                    {
                        "name": "Dan Li"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16660v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16660v4",
                "updated": "2025-04-16T16:49:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    49,
                    34,
                    2,
                    106,
                    0
                ],
                "published": "2025-02-23T17:38:10Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    17,
                    38,
                    10,
                    6,
                    54,
                    0
                ],
                "title": "BioMaze: Benchmarking and Enhancing Large Language Models for Biological\n  Pathway Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BioMaze: Benchmarking and Enhancing Large Language Models for Biological\n  Pathway Reasoning"
                },
                "summary": "The applications of large language models (LLMs) in various biological\ndomains have been explored recently, but their reasoning ability in complex\nbiological systems, such as pathways, remains underexplored, which is crucial\nfor predicting biological phenomena, formulating hypotheses, and designing\nexperiments. This work explores the potential of LLMs in pathway reasoning. We\nintroduce BioMaze, a dataset with 5.1K complex pathway problems derived from\nreal research, covering various biological contexts including natural dynamic\nchanges, disturbances, additional intervention conditions, and multi-scale\nresearch targets. Our evaluation of methods such as CoT and graph-augmented\nreasoning, shows that LLMs struggle with pathway reasoning, especially in\nperturbed systems. To address this, we propose PathSeeker, an LLM agent that\nenhances reasoning through interactive subgraph-based navigation, enabling a\nmore effective approach to handling the complexities of biological systems in a\nscientifically aligned manner. The dataset and code are available at\nhttps://github.com/zhao-ht/BioMaze.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The applications of large language models (LLMs) in various biological\ndomains have been explored recently, but their reasoning ability in complex\nbiological systems, such as pathways, remains underexplored, which is crucial\nfor predicting biological phenomena, formulating hypotheses, and designing\nexperiments. This work explores the potential of LLMs in pathway reasoning. We\nintroduce BioMaze, a dataset with 5.1K complex pathway problems derived from\nreal research, covering various biological contexts including natural dynamic\nchanges, disturbances, additional intervention conditions, and multi-scale\nresearch targets. Our evaluation of methods such as CoT and graph-augmented\nreasoning, shows that LLMs struggle with pathway reasoning, especially in\nperturbed systems. To address this, we propose PathSeeker, an LLM agent that\nenhances reasoning through interactive subgraph-based navigation, enabling a\nmore effective approach to handling the complexities of biological systems in a\nscientifically aligned manner. The dataset and code are available at\nhttps://github.com/zhao-ht/BioMaze."
                },
                "authors": [
                    {
                        "name": "Haiteng Zhao"
                    },
                    {
                        "name": "Chang Ma"
                    },
                    {
                        "name": "Fangzhi Xu"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Zhi-Hong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhi-Hong Deng"
                },
                "author": "Zhi-Hong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16660v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16660v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12244v1",
                "updated": "2025-04-16T16:48:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    48,
                    29,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T16:48:29Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    48,
                    29,
                    2,
                    106,
                    0
                ],
                "title": "Mobile Distributed MIMO (MD-MIMO) for NextG: Mobility Meets Cooperation\n  in Distributed Arrays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Distributed MIMO (MD-MIMO) for NextG: Mobility Meets Cooperation\n  in Distributed Arrays"
                },
                "summary": "Distributed multiple-input multiple-output (D\\mbox{-}MIMO) is a promising\ntechnology to realize the promise of massive MIMO gains by fiber-connecting the\ndistributed antenna arrays, thereby overcoming the form factor limitations of\nco-located MIMO. In this paper, we introduce the concept of mobile D-MIMO\n(MD-MIMO) network, a further extension of the D-MIMO technology where\ndistributed antenna arrays are connected to the base station with a wireless\nlink allowing all radio network nodes to be mobile. This approach significantly\nimproves deployment flexibility and reduces operating costs, enabling the\nnetwork to adapt to the highly dynamic nature of next-generation (NextG)\nnetworks. We discuss use cases, system design, network architecture, and the\nkey enabling technologies for MD-MIMO. Furthermore, we investigate a case study\nof MD-MIMO for vehicular networks, presenting detailed performance evaluations\nfor both downlink and uplink. The results show that an MD-MIMO network can\nprovide substantial improvements in network throughput and reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed multiple-input multiple-output (D\\mbox{-}MIMO) is a promising\ntechnology to realize the promise of massive MIMO gains by fiber-connecting the\ndistributed antenna arrays, thereby overcoming the form factor limitations of\nco-located MIMO. In this paper, we introduce the concept of mobile D-MIMO\n(MD-MIMO) network, a further extension of the D-MIMO technology where\ndistributed antenna arrays are connected to the base station with a wireless\nlink allowing all radio network nodes to be mobile. This approach significantly\nimproves deployment flexibility and reduces operating costs, enabling the\nnetwork to adapt to the highly dynamic nature of next-generation (NextG)\nnetworks. We discuss use cases, system design, network architecture, and the\nkey enabling technologies for MD-MIMO. Furthermore, we investigate a case study\nof MD-MIMO for vehicular networks, presenting detailed performance evaluations\nfor both downlink and uplink. The results show that an MD-MIMO network can\nprovide substantial improvements in network throughput and reliability."
                },
                "authors": [
                    {
                        "name": "Karim A. Said"
                    },
                    {
                        "name": "Yibin Liang"
                    },
                    {
                        "name": "Usama Saeed"
                    },
                    {
                        "name": "Ramin Safavinejad"
                    },
                    {
                        "name": "Kumar Sai Bondada"
                    },
                    {
                        "name": "Evan Allen"
                    },
                    {
                        "name": "Nima Mohammadi"
                    },
                    {
                        "name": "Benjamin Pimentel"
                    },
                    {
                        "name": "Brian Kelley"
                    },
                    {
                        "name": "Jeff Reed"
                    },
                    {
                        "name": "Nishith Tripathi"
                    },
                    {
                        "name": "Michael Buehrer"
                    },
                    {
                        "name": "Yang"
                    },
                    {
                        "name": "Yi"
                    },
                    {
                        "name": "Daniel Jakubisin"
                    },
                    {
                        "name": "Lingjia Liu"
                    }
                ],
                "author_detail": {
                    "name": "Lingjia Liu"
                },
                "arxiv_affiliation": "Cindy",
                "author": "Lingjia Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12234v1",
                "updated": "2025-04-16T16:33:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    33,
                    53,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T16:33:53Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    33,
                    53,
                    2,
                    106,
                    0
                ],
                "title": "MOS: Towards Effective Smart Contract Vulnerability Detection through\n  Mixture-of-Experts Tuning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOS: Towards Effective Smart Contract Vulnerability Detection through\n  Mixture-of-Experts Tuning of Large Language Models"
                },
                "summary": "Smart contract vulnerabilities pose significant security risks to blockchain\nsystems, potentially leading to severe financial losses. Existing methods face\nseveral limitations: (1) Program analysis-based approaches rely on predefined\npatterns, lacking flexibility for new vulnerability types; (2) Deep\nlearning-based methods lack explanations; (3) Large language model-based\napproaches suffer from high false positives. We propose MOS, a smart contract\nvulnerability detection framework based on mixture-of-experts tuning\n(MOE-Tuning) of large language models. First, we conduct continual pre-training\non a large-scale smart contract dataset to provide domain-enhanced\ninitialization. Second, we construct a high-quality MOE-Tuning dataset through\na multi-stage pipeline combining LLM generation and expert verification for\nreliable explanations. Third, we design a vulnerability-aware routing mechanism\nthat activates the most relevant expert networks by analyzing code features and\ntheir matching degree with experts. Finally, we extend the feed-forward layers\ninto multiple parallel expert networks, each specializing in specific\nvulnerability patterns. We employ a dual-objective loss function: one for\noptimizing detection and explanation performance, and another for ensuring\nreasonable distribution of vulnerability types to experts through entropy\ncalculation. Experiments show that MOS significantly outperforms existing\nmethods with average improvements of 6.32% in F1 score and 4.80% in accuracy.\nThe vulnerability explanations achieve positive ratings (scores of 3-4 on a\n4-point scale) of 82.96%, 85.21% and 94.58% for correctness, completeness, and\nconciseness through human and LLM evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart contract vulnerabilities pose significant security risks to blockchain\nsystems, potentially leading to severe financial losses. Existing methods face\nseveral limitations: (1) Program analysis-based approaches rely on predefined\npatterns, lacking flexibility for new vulnerability types; (2) Deep\nlearning-based methods lack explanations; (3) Large language model-based\napproaches suffer from high false positives. We propose MOS, a smart contract\nvulnerability detection framework based on mixture-of-experts tuning\n(MOE-Tuning) of large language models. First, we conduct continual pre-training\non a large-scale smart contract dataset to provide domain-enhanced\ninitialization. Second, we construct a high-quality MOE-Tuning dataset through\na multi-stage pipeline combining LLM generation and expert verification for\nreliable explanations. Third, we design a vulnerability-aware routing mechanism\nthat activates the most relevant expert networks by analyzing code features and\ntheir matching degree with experts. Finally, we extend the feed-forward layers\ninto multiple parallel expert networks, each specializing in specific\nvulnerability patterns. We employ a dual-objective loss function: one for\noptimizing detection and explanation performance, and another for ensuring\nreasonable distribution of vulnerability types to experts through entropy\ncalculation. Experiments show that MOS significantly outperforms existing\nmethods with average improvements of 6.32% in F1 score and 4.80% in accuracy.\nThe vulnerability explanations achieve positive ratings (scores of 3-4 on a\n4-point scale) of 82.96%, 85.21% and 94.58% for correctness, completeness, and\nconciseness through human and LLM evaluation."
                },
                "authors": [
                    {
                        "name": "Hang Yuan"
                    },
                    {
                        "name": "Lei Yu"
                    },
                    {
                        "name": "Zhirong Huang"
                    },
                    {
                        "name": "Jingyuan Zhang"
                    },
                    {
                        "name": "Junyi Lu"
                    },
                    {
                        "name": "Shiqi Cheng"
                    },
                    {
                        "name": "Li Yang"
                    },
                    {
                        "name": "Fengjun Zhang"
                    },
                    {
                        "name": "Jiajia Ma"
                    },
                    {
                        "name": "Chun Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Chun Zuo"
                },
                "author": "Chun Zuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12229v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12229v1",
                "updated": "2025-04-16T16:25:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    25,
                    26,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T16:25:26Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    25,
                    26,
                    2,
                    106,
                    0
                ],
                "title": "Watermarking Needs Input Repetition Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking Needs Input Repetition Masking"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) raised concerns over\npotential misuse, such as for spreading misinformation. In response two counter\nmeasures emerged: machine learning-based detectors that predict if text is\nsynthetic, and LLM watermarking, which subtly marks generated text for\nidentification and attribution. Meanwhile, humans are known to adjust language\nto their conversational partners both syntactically and lexically. By\nimplication, it is possible that humans or unwatermarked LLMs could\nunintentionally mimic properties of LLM generated text, making counter measures\nunreliable. In this work we investigate the extent to which such conversational\nadaptation happens. We call the concept $\\textit{mimicry}$ and demonstrate that\nboth humans and LLMs end up mimicking, including the watermarking signal even\nin seemingly improbable settings. This challenges current academic assumptions\nand suggests that for long-term watermarking to be reliable, the likelihood of\nfalse positives needs to be significantly lower, while longer word sequences\nshould be used for seeding watermarking mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) raised concerns over\npotential misuse, such as for spreading misinformation. In response two counter\nmeasures emerged: machine learning-based detectors that predict if text is\nsynthetic, and LLM watermarking, which subtly marks generated text for\nidentification and attribution. Meanwhile, humans are known to adjust language\nto their conversational partners both syntactically and lexically. By\nimplication, it is possible that humans or unwatermarked LLMs could\nunintentionally mimic properties of LLM generated text, making counter measures\nunreliable. In this work we investigate the extent to which such conversational\nadaptation happens. We call the concept $\\textit{mimicry}$ and demonstrate that\nboth humans and LLMs end up mimicking, including the watermarking signal even\nin seemingly improbable settings. This challenges current academic assumptions\nand suggests that for long-term watermarking to be reliable, the likelihood of\nfalse positives needs to be significantly lower, while longer word sequences\nshould be used for seeding watermarking mechanisms."
                },
                "authors": [
                    {
                        "name": "David Khachaturov"
                    },
                    {
                        "name": "Robert Mullins"
                    },
                    {
                        "name": "Ilia Shumailov"
                    },
                    {
                        "name": "Sumanth Dathathri"
                    }
                ],
                "author_detail": {
                    "name": "Sumanth Dathathri"
                },
                "author": "Sumanth Dathathri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12229v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12229v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11346v2",
                "updated": "2025-04-16T16:23:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    23,
                    31,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-15T16:19:07Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    19,
                    7,
                    1,
                    105,
                    0
                ],
                "title": "Seedream 3.0 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seedream 3.0 Technical Report"
                },
                "summary": "We present Seedream 3.0, a high-performance Chinese-English bilingual image\ngeneration foundation model. We develop several technical improvements to\naddress existing challenges in Seedream 2.0, including alignment with\ncomplicated prompts, fine-grained typography generation, suboptimal visual\naesthetics and fidelity, and limited image resolutions. Specifically, the\nadvancements of Seedream 3.0 stem from improvements across the entire pipeline,\nfrom data construction to model deployment. At the data stratum, we double the\ndataset using a defect-aware training paradigm and a dual-axis collaborative\ndata-sampling framework. Furthermore, we adopt several effective techniques\nsuch as mixed-resolution training, cross-modality RoPE, representation\nalignment loss, and resolution-aware timestep sampling in the pre-training\nphase. During the post-training stage, we utilize diversified aesthetic\ncaptions in SFT, and a VLM-based reward model with scaling, thereby achieving\noutputs that well align with human preferences. Furthermore, Seedream 3.0\npioneers a novel acceleration paradigm. By employing consistent noise\nexpectation and importance-aware timestep sampling, we achieve a 4 to 8 times\nspeedup while maintaining image quality. Seedream 3.0 demonstrates significant\nimprovements over Seedream 2.0: it enhances overall capabilities, in particular\nfor text-rendering in complicated Chinese characters which is important to\nprofessional typography generation. In addition, it provides native\nhigh-resolution output (up to 2K), allowing it to generate images with high\nvisual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Seedream 3.0, a high-performance Chinese-English bilingual image\ngeneration foundation model. We develop several technical improvements to\naddress existing challenges in Seedream 2.0, including alignment with\ncomplicated prompts, fine-grained typography generation, suboptimal visual\naesthetics and fidelity, and limited image resolutions. Specifically, the\nadvancements of Seedream 3.0 stem from improvements across the entire pipeline,\nfrom data construction to model deployment. At the data stratum, we double the\ndataset using a defect-aware training paradigm and a dual-axis collaborative\ndata-sampling framework. Furthermore, we adopt several effective techniques\nsuch as mixed-resolution training, cross-modality RoPE, representation\nalignment loss, and resolution-aware timestep sampling in the pre-training\nphase. During the post-training stage, we utilize diversified aesthetic\ncaptions in SFT, and a VLM-based reward model with scaling, thereby achieving\noutputs that well align with human preferences. Furthermore, Seedream 3.0\npioneers a novel acceleration paradigm. By employing consistent noise\nexpectation and importance-aware timestep sampling, we achieve a 4 to 8 times\nspeedup while maintaining image quality. Seedream 3.0 demonstrates significant\nimprovements over Seedream 2.0: it enhances overall capabilities, in particular\nfor text-rendering in complicated Chinese characters which is important to\nprofessional typography generation. In addition, it provides native\nhigh-resolution output (up to 2K), allowing it to generate images with high\nvisual quality."
                },
                "authors": [
                    {
                        "name": "Yu Gao"
                    },
                    {
                        "name": "Lixue Gong"
                    },
                    {
                        "name": "Qiushan Guo"
                    },
                    {
                        "name": "Xiaoxia Hou"
                    },
                    {
                        "name": "Zhichao Lai"
                    },
                    {
                        "name": "Fanshi Li"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Xiaochen Lian"
                    },
                    {
                        "name": "Chao Liao"
                    },
                    {
                        "name": "Liyang Liu"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Yichun Shi"
                    },
                    {
                        "name": "Shiqi Sun"
                    },
                    {
                        "name": "Yu Tian"
                    },
                    {
                        "name": "Zhi Tian"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Xuanda Wang"
                    },
                    {
                        "name": "Xun Wang"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Guofeng Wu"
                    },
                    {
                        "name": "Jie Wu"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Zhonghua Zhai"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Yuwei Zhang"
                    },
                    {
                        "name": "Shijia Zhao"
                    },
                    {
                        "name": "Jianchao Yang"
                    },
                    {
                        "name": "Weilin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Weilin Huang"
                },
                "author": "Weilin Huang",
                "arxiv_comment": "Seedream 3.0 Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17404v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17404v3",
                "updated": "2025-04-16T16:21:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    21,
                    29,
                    2,
                    106,
                    0
                ],
                "published": "2024-11-26T13:05:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    5,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical\n  Modeling Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical\n  Modeling Problem Solving"
                },
                "summary": "LLMs exhibit advanced reasoning capabilities, offering the potential to\ntransform natural language questions into mathematical models. However,\nexisting open-source datasets in operations research domain lack detailed\nannotations of the modeling process, such as variable definitions, focusing\nsolely on objective values, which hinders reinforcement learning applications.\nTo address this, we release the StructuredOR dataset, annotated with\ncomprehensive labels that capture the complete mathematical modeling process.\nWe further propose BPP-Search, an algorithm that integrates reinforcement\nlearning into a tree-of-thought structure using Beam search, a Process reward\nmodel, and a pairwise Preference algorithm. This approach enables efficient\nexploration of tree structures, avoiding exhaustive search while improving\naccuracy. Extensive experiments on StructuredOR, NL4OPT, and MAMO-ComplexLP\ndatasets show that BPP-Search significantly outperforms state-of-the-art\nmethods. In tree-based reasoning, BPP-Search excels in accuracy and efficiency,\nenabling faster retrieval of correct solutions. The StructuredOR dataset is\navailable at https://github.com/tengwang0318/StructuredOR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs exhibit advanced reasoning capabilities, offering the potential to\ntransform natural language questions into mathematical models. However,\nexisting open-source datasets in operations research domain lack detailed\nannotations of the modeling process, such as variable definitions, focusing\nsolely on objective values, which hinders reinforcement learning applications.\nTo address this, we release the StructuredOR dataset, annotated with\ncomprehensive labels that capture the complete mathematical modeling process.\nWe further propose BPP-Search, an algorithm that integrates reinforcement\nlearning into a tree-of-thought structure using Beam search, a Process reward\nmodel, and a pairwise Preference algorithm. This approach enables efficient\nexploration of tree structures, avoiding exhaustive search while improving\naccuracy. Extensive experiments on StructuredOR, NL4OPT, and MAMO-ComplexLP\ndatasets show that BPP-Search significantly outperforms state-of-the-art\nmethods. In tree-based reasoning, BPP-Search excels in accuracy and efficiency,\nenabling faster retrieval of correct solutions. The StructuredOR dataset is\navailable at https://github.com/tengwang0318/StructuredOR."
                },
                "authors": [
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Wing-Yin Yu"
                    },
                    {
                        "name": "Zhenqi He"
                    },
                    {
                        "name": "Zehua Liu"
                    },
                    {
                        "name": "Hailei Gong"
                    },
                    {
                        "name": "Han Wu"
                    },
                    {
                        "name": "Xiongwei Han"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Ruifeng She"
                    },
                    {
                        "name": "Fangzhou Zhu"
                    },
                    {
                        "name": "Tao Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Tao Zhong"
                },
                "author": "Tao Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17404v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17404v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12216v1",
                "updated": "2025-04-16T16:08:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    8,
                    45,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T16:08:45Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    8,
                    45,
                    2,
                    106,
                    0
                ],
                "title": "d1: Scaling Reasoning in Diffusion Large Language Models via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "d1: Scaling Reasoning in Diffusion Large Language Models via\n  Reinforcement Learning"
                },
                "summary": "Recent large language models (LLMs) have demonstrated strong reasoning\ncapabilities that benefits from online reinforcement learning (RL). These\ncapabilities have primarily been demonstrated within the left-to-right\nautoregressive (AR) generation paradigm. In contrast, non-autoregressive\nparadigms based on diffusion generate text in a coarse-to-fine manner. Although\nrecent diffusion-based large language models (dLLMs) have achieved competitive\nlanguage modeling performance compared to their AR counterparts, it remains\nunclear if dLLMs can also leverage recent advances in LLM reasoning. To this\nend, we propose d1, a framework to adapt pre-trained masked dLLMs into\nreasoning models via a combination of supervised finetuning (SFT) and RL.\nSpecifically, we develop and extend techniques to improve reasoning in\npretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge\nand instill self-improvement behavior directly from existing datasets, and (b)\nwe introduce a novel critic-free, policy-gradient based RL algorithm called\ndiffu-GRPO. Through empirical studies, we investigate the performance of\ndifferent post-training recipes on multiple mathematical and logical reasoning\nbenchmarks. We find that d1 yields the best performance and significantly\nimproves performance of a state-of-the-art dLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) have demonstrated strong reasoning\ncapabilities that benefits from online reinforcement learning (RL). These\ncapabilities have primarily been demonstrated within the left-to-right\nautoregressive (AR) generation paradigm. In contrast, non-autoregressive\nparadigms based on diffusion generate text in a coarse-to-fine manner. Although\nrecent diffusion-based large language models (dLLMs) have achieved competitive\nlanguage modeling performance compared to their AR counterparts, it remains\nunclear if dLLMs can also leverage recent advances in LLM reasoning. To this\nend, we propose d1, a framework to adapt pre-trained masked dLLMs into\nreasoning models via a combination of supervised finetuning (SFT) and RL.\nSpecifically, we develop and extend techniques to improve reasoning in\npretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge\nand instill self-improvement behavior directly from existing datasets, and (b)\nwe introduce a novel critic-free, policy-gradient based RL algorithm called\ndiffu-GRPO. Through empirical studies, we investigate the performance of\ndifferent post-training recipes on multiple mathematical and logical reasoning\nbenchmarks. We find that d1 yields the best performance and significantly\nimproves performance of a state-of-the-art dLLM."
                },
                "authors": [
                    {
                        "name": "Siyan Zhao"
                    },
                    {
                        "name": "Devaansh Gupta"
                    },
                    {
                        "name": "Qinqing Zheng"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "25 pages, project page at https://dllm-reasoning.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12144v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12144v3",
                "updated": "2025-04-16T15:53:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    15,
                    53,
                    3,
                    2,
                    106,
                    0
                ],
                "published": "2024-12-10T09:13:32Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    13,
                    32,
                    1,
                    345,
                    0
                ],
                "title": "Automatic Item Generation for Personality Situational Judgment Tests\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Item Generation for Personality Situational Judgment Tests\n  with Large Language Models"
                },
                "summary": "Personality assessment, particularly through situational judgment tests\n(SJTs), is a vital tool for psychological research, talent selection, and\neducational evaluation. This study explores the potential of GPT-4, a\nstate-of-the-art large language model (LLM), to automate the generation of\npersonality situational judgment tests (PSJTs) in Chinese. Traditional SJT\ndevelopment is labor-intensive and prone to biases, while GPT-4 offers a\nscalable, efficient alternative. Two studies were conducted: Study 1 evaluated\nthe impact of prompt design and temperature settings on content validity,\nfinding that optimized prompts with a temperature of 1.0 produced creative and\naccurate items. Study 2 assessed the psychometric properties of GPT-4-generated\nPSJTs, revealing that they demonstrated satisfactory reliability and validity,\nsurpassing the performance of manually developed tests in measuring the Big\nFive personality traits. This research highlights GPT-4's effectiveness in\ndeveloping high-quality PSJTs, providing a scalable and innovative method for\npsychometric test development. These findings expand the possibilities of\nautomatic item generation and the application of LLMs in psychology, and offer\npractical implications for streamlining test development processes in\nresource-limited settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personality assessment, particularly through situational judgment tests\n(SJTs), is a vital tool for psychological research, talent selection, and\neducational evaluation. This study explores the potential of GPT-4, a\nstate-of-the-art large language model (LLM), to automate the generation of\npersonality situational judgment tests (PSJTs) in Chinese. Traditional SJT\ndevelopment is labor-intensive and prone to biases, while GPT-4 offers a\nscalable, efficient alternative. Two studies were conducted: Study 1 evaluated\nthe impact of prompt design and temperature settings on content validity,\nfinding that optimized prompts with a temperature of 1.0 produced creative and\naccurate items. Study 2 assessed the psychometric properties of GPT-4-generated\nPSJTs, revealing that they demonstrated satisfactory reliability and validity,\nsurpassing the performance of manually developed tests in measuring the Big\nFive personality traits. This research highlights GPT-4's effectiveness in\ndeveloping high-quality PSJTs, providing a scalable and innovative method for\npsychometric test development. These findings expand the possibilities of\nautomatic item generation and the application of LLMs in psychology, and offer\npractical implications for streamlining test development processes in\nresource-limited settings."
                },
                "authors": [
                    {
                        "name": "Chang-Jin Li"
                    },
                    {
                        "name": "Jiyuan Zhang"
                    },
                    {
                        "name": "Yun Tang"
                    },
                    {
                        "name": "Jian Li"
                    }
                ],
                "author_detail": {
                    "name": "Jian Li"
                },
                "author": "Jian Li",
                "arxiv_comment": "Submitted to Psychological Methods. 56 pages (main text), 12 pages\n  (appendix), and 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12144v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12144v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12192v1",
                "updated": "2025-04-16T15:46:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    15,
                    46,
                    56,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T15:46:56Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    15,
                    46,
                    56,
                    2,
                    106,
                    0
                ],
                "title": "From Requirements to Architecture: Semi-Automatically Generating\n  Software Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Requirements to Architecture: Semi-Automatically Generating\n  Software Architectures"
                },
                "summary": "To support junior and senior architects, I propose developing a new\narchitecture creation method that leverages LLMs' evolving capabilities to\nsupport the architect. This method involves the architect's close collaboration\nwith LLM-fueled tooling over the whole process. The architect is guided through\nDomain Model creation, Use Case specification, architectural decisions, and\narchitecture evaluation. While the architect can take complete control of the\nprocess and the results, and use the tooling as a building set, they can follow\nthe intended process for maximum tooling support. The preliminary results\nsuggest the feasibility of this process and indicate major time savings for the\narchitect.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To support junior and senior architects, I propose developing a new\narchitecture creation method that leverages LLMs' evolving capabilities to\nsupport the architect. This method involves the architect's close collaboration\nwith LLM-fueled tooling over the whole process. The architect is guided through\nDomain Model creation, Use Case specification, architectural decisions, and\narchitecture evaluation. While the architect can take complete control of the\nprocess and the results, and use the tooling as a building set, they can follow\nthe intended process for maximum tooling support. The preliminary results\nsuggest the feasibility of this process and indicate major time savings for the\narchitect."
                },
                "authors": [
                    {
                        "name": "Tobias Eisenreich"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Eisenreich"
                },
                "author": "Tobias Eisenreich",
                "arxiv_comment": "to be published in EMISA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12187v1",
                "updated": "2025-04-16T15:42:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    15,
                    42,
                    33,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T15:42:33Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    15,
                    42,
                    33,
                    2,
                    106,
                    0
                ],
                "title": "What Do Large Language Models Know? Tacit Knowledge as a Potential\n  Causal-Explanatory Structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Do Large Language Models Know? Tacit Knowledge as a Potential\n  Causal-Explanatory Structure"
                },
                "summary": "It is sometimes assumed that Large Language Models (LLMs) know language, or\nfor example that they know that Paris is the capital of France. But what -- if\nanything -- do LLMs actually know? In this paper, I argue that LLMs can acquire\ntacit knowledge as defined by Martin Davies (1990). Whereas Davies himself\ndenies that neural networks can acquire tacit knowledge, I demonstrate that\ncertain architectural features of LLMs satisfy the constraints of semantic\ndescription, syntactic structure, and causal systematicity. Thus, tacit\nknowledge may serve as a conceptual framework for describing, explaining, and\nintervening on LLMs and their behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is sometimes assumed that Large Language Models (LLMs) know language, or\nfor example that they know that Paris is the capital of France. But what -- if\nanything -- do LLMs actually know? In this paper, I argue that LLMs can acquire\ntacit knowledge as defined by Martin Davies (1990). Whereas Davies himself\ndenies that neural networks can acquire tacit knowledge, I demonstrate that\ncertain architectural features of LLMs satisfy the constraints of semantic\ndescription, syntactic structure, and causal systematicity. Thus, tacit\nknowledge may serve as a conceptual framework for describing, explaining, and\nintervening on LLMs and their behavior."
                },
                "authors": [
                    {
                        "name": "Cline Budding"
                    }
                ],
                "author_detail": {
                    "name": "Cline Budding"
                },
                "author": "Cline Budding",
                "arxiv_doi": "10.1017/psa.2025.19",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1017/psa.2025.19",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.12187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication in Philosophy of Science",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12185v1",
                "updated": "2025-04-16T15:40:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    15,
                    40,
                    10,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T15:40:10Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    15,
                    40,
                    10,
                    2,
                    106,
                    0
                ],
                "title": "SALAD: Improving Robustness and Generalization through Contrastive\n  Learning with Structure-Aware and LLM-Driven Augmented Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALAD: Improving Robustness and Generalization through Contrastive\n  Learning with Structure-Aware and LLM-Driven Augmented Data"
                },
                "summary": "In various natural language processing (NLP) tasks, fine-tuning Pre-trained\nLanguage Models (PLMs) often leads to the issue of spurious correlations, which\nnegatively impacts performance, particularly when dealing with\nout-of-distribution data. To address this problem, we propose SALAD}(Structure\nAware and LLM-driven Augmented Data), a novel approach designed to enhance\nmodel robustness and generalization by generating structure-aware and\ncounterfactually augmented data for contrastive learning. Our method leverages\na tagging-based approach to generate structure-aware positive samples and\nutilizes large language models (LLMs) to generate counterfactual negative\nsamples with diverse sentence patterns. By applying contrastive learning, SALAD\nenables the model to focus on learning the structural relationships between key\nsentence components while minimizing reliance on spurious correlations. We\nvalidate our approach through experiments on three tasks: Sentiment\nClassification, Sexism Detection, and Natural Language Inference. The results\ndemonstrate that SALAD not only improves model robustness and performance\nacross different environments but also enhances generalization to\nout-of-distribution datasets and cross-domain scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In various natural language processing (NLP) tasks, fine-tuning Pre-trained\nLanguage Models (PLMs) often leads to the issue of spurious correlations, which\nnegatively impacts performance, particularly when dealing with\nout-of-distribution data. To address this problem, we propose SALAD}(Structure\nAware and LLM-driven Augmented Data), a novel approach designed to enhance\nmodel robustness and generalization by generating structure-aware and\ncounterfactually augmented data for contrastive learning. Our method leverages\na tagging-based approach to generate structure-aware positive samples and\nutilizes large language models (LLMs) to generate counterfactual negative\nsamples with diverse sentence patterns. By applying contrastive learning, SALAD\nenables the model to focus on learning the structural relationships between key\nsentence components while minimizing reliance on spurious correlations. We\nvalidate our approach through experiments on three tasks: Sentiment\nClassification, Sexism Detection, and Natural Language Inference. The results\ndemonstrate that SALAD not only improves model robustness and performance\nacross different environments but also enhances generalization to\nout-of-distribution datasets and cross-domain scenarios."
                },
                "authors": [
                    {
                        "name": "Suyoung Bae"
                    },
                    {
                        "name": "Hyojun Kim"
                    },
                    {
                        "name": "YunSeok Choi"
                    },
                    {
                        "name": "Jee-Hyong Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jee-Hyong Lee"
                },
                "author": "Jee-Hyong Lee",
                "arxiv_comment": "Accepted to NAACL 2025 main. 15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11168v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11168v2",
                "updated": "2025-04-16T15:33:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    15,
                    33,
                    6,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-15T13:16:02Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    16,
                    2,
                    1,
                    105,
                    0
                ],
                "title": "Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails"
                },
                "summary": "Large Language Models (LLMs) guardrail systems are designed to protect\nagainst prompt injection and jailbreak attacks. However, they remain vulnerable\nto evasion techniques. We demonstrate two approaches for bypassing LLM prompt\ninjection and jailbreak detection systems via traditional character injection\nmethods and algorithmic Adversarial Machine Learning (AML) evasion techniques.\nThrough testing against six prominent protection systems, including Microsoft's\nAzure Prompt Shield and Meta's Prompt Guard, we show that both methods can be\nused to evade detection while maintaining adversarial utility achieving in some\ninstances up to 100% evasion success. Furthermore, we demonstrate that\nadversaries can enhance Attack Success Rates (ASR) against black-box targets by\nleveraging word importance ranking computed by offline white-box models. Our\nfindings reveal vulnerabilities within current LLM protection mechanisms and\nhighlight the need for more robust guardrail systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) guardrail systems are designed to protect\nagainst prompt injection and jailbreak attacks. However, they remain vulnerable\nto evasion techniques. We demonstrate two approaches for bypassing LLM prompt\ninjection and jailbreak detection systems via traditional character injection\nmethods and algorithmic Adversarial Machine Learning (AML) evasion techniques.\nThrough testing against six prominent protection systems, including Microsoft's\nAzure Prompt Shield and Meta's Prompt Guard, we show that both methods can be\nused to evade detection while maintaining adversarial utility achieving in some\ninstances up to 100% evasion success. Furthermore, we demonstrate that\nadversaries can enhance Attack Success Rates (ASR) against black-box targets by\nleveraging word importance ranking computed by offline white-box models. Our\nfindings reveal vulnerabilities within current LLM protection mechanisms and\nhighlight the need for more robust guardrail systems."
                },
                "authors": [
                    {
                        "name": "William Hackett"
                    },
                    {
                        "name": "Lewis Birch"
                    },
                    {
                        "name": "Stefan Trawicki"
                    },
                    {
                        "name": "Neeraj Suri"
                    },
                    {
                        "name": "Peter Garraghan"
                    }
                ],
                "author_detail": {
                    "name": "Peter Garraghan"
                },
                "author": "Peter Garraghan",
                "arxiv_comment": "12 pages, 5 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11168v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11168v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04285v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04285v3",
                "updated": "2025-04-16T15:24:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    15,
                    24,
                    31,
                    2,
                    106,
                    0
                ],
                "published": "2025-01-08T05:17:09Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    5,
                    17,
                    9,
                    2,
                    8,
                    0
                ],
                "title": "Separate Source Channel Coding Is Still What You Need: An LLM-based\n  Rethinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Separate Source Channel Coding Is Still What You Need: An LLM-based\n  Rethinking"
                },
                "summary": "Along with the proliferating research interest in Semantic Communication\n(SemCom), Joint Source Channel Coding (JSCC) has dominated the attention due to\nthe widely assumed existence in efficiently delivering information semantics.\n%has emerged as a pivotal area of research, aiming to enhance the efficiency\nand reliability of information transmission through deep learning-based\nmethods. Nevertheless, this paper challenges the conventional JSCC paradigm,\nand advocates for adoption of Separate Source Channel Coding (SSCC) to enjoy\nthe underlying more degree of freedom for optimization. We demonstrate that\nSSCC, after leveraging the strengths of Large Language Model (LLM) for source\ncoding and Error Correction Code Transformer (ECCT) complemented for channel\ndecoding, offers superior performance over JSCC. Our proposed framework also\neffectively highlights the compatibility challenges between SemCom approaches\nand digital communication systems, particularly concerning the resource costs\nassociated with the transmission of high precision floating point numbers.\nThrough comprehensive evaluations, we establish that empowered by LLM-based\ncompression and ECCT-enhanced error correction, SSCC remains a viable and\neffective solution for modern communication systems. In other words, separate\nsource and channel coding is still what we need!",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Along with the proliferating research interest in Semantic Communication\n(SemCom), Joint Source Channel Coding (JSCC) has dominated the attention due to\nthe widely assumed existence in efficiently delivering information semantics.\n%has emerged as a pivotal area of research, aiming to enhance the efficiency\nand reliability of information transmission through deep learning-based\nmethods. Nevertheless, this paper challenges the conventional JSCC paradigm,\nand advocates for adoption of Separate Source Channel Coding (SSCC) to enjoy\nthe underlying more degree of freedom for optimization. We demonstrate that\nSSCC, after leveraging the strengths of Large Language Model (LLM) for source\ncoding and Error Correction Code Transformer (ECCT) complemented for channel\ndecoding, offers superior performance over JSCC. Our proposed framework also\neffectively highlights the compatibility challenges between SemCom approaches\nand digital communication systems, particularly concerning the resource costs\nassociated with the transmission of high precision floating point numbers.\nThrough comprehensive evaluations, we establish that empowered by LLM-based\ncompression and ECCT-enhanced error correction, SSCC remains a viable and\neffective solution for modern communication systems. In other words, separate\nsource and channel coding is still what we need!"
                },
                "authors": [
                    {
                        "name": "Tianqi Ren"
                    },
                    {
                        "name": "Rongpeng Li"
                    },
                    {
                        "name": "Ming-min Zhao"
                    },
                    {
                        "name": "Xianfu Chen"
                    },
                    {
                        "name": "Guangyi Liu"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Zhifeng Zhao"
                    },
                    {
                        "name": "Honggang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Honggang Zhang"
                },
                "author": "Honggang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04285v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04285v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12170v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12170v1",
                "updated": "2025-04-16T15:21:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    15,
                    21,
                    13,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T15:21:13Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    15,
                    21,
                    13,
                    2,
                    106,
                    0
                ],
                "title": "AI Behind Closed Doors: a Primer on The Governance of Internal\n  Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Behind Closed Doors: a Primer on The Governance of Internal\n  Deployment"
                },
                "summary": "The most advanced future AI systems will first be deployed inside the\nfrontier AI companies developing them. According to these companies and\nindependent experts, AI systems may reach or even surpass human intelligence\nand capabilities by 2030. Internal deployment is, therefore, a key source of\nbenefits and risks from frontier AI systems. Despite this, the governance of\nthe internal deployment of highly advanced frontier AI systems appears absent.\nThis report aims to address this absence by priming a conversation around the\ngovernance of internal deployment. It presents a conceptualization of internal\ndeployment, learnings from other sectors, reviews of existing legal frameworks\nand their applicability, and illustrative examples of the type of scenarios we\nare most concerned about. Specifically, it discusses the risks correlated to\nthe loss of control via the internal application of a misaligned AI system to\nthe AI research and development pipeline, and unconstrained and undetected\npower concentration behind closed doors. The report culminates with a small\nnumber of targeted recommendations that provide a first blueprint for the\ngovernance of internal deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The most advanced future AI systems will first be deployed inside the\nfrontier AI companies developing them. According to these companies and\nindependent experts, AI systems may reach or even surpass human intelligence\nand capabilities by 2030. Internal deployment is, therefore, a key source of\nbenefits and risks from frontier AI systems. Despite this, the governance of\nthe internal deployment of highly advanced frontier AI systems appears absent.\nThis report aims to address this absence by priming a conversation around the\ngovernance of internal deployment. It presents a conceptualization of internal\ndeployment, learnings from other sectors, reviews of existing legal frameworks\nand their applicability, and illustrative examples of the type of scenarios we\nare most concerned about. Specifically, it discusses the risks correlated to\nthe loss of control via the internal application of a misaligned AI system to\nthe AI research and development pipeline, and unconstrained and undetected\npower concentration behind closed doors. The report culminates with a small\nnumber of targeted recommendations that provide a first blueprint for the\ngovernance of internal deployment."
                },
                "authors": [
                    {
                        "name": "Charlotte Stix"
                    },
                    {
                        "name": "Matteo Pistillo"
                    },
                    {
                        "name": "Girish Sastry"
                    },
                    {
                        "name": "Marius Hobbhahn"
                    },
                    {
                        "name": "Alejandro Ortega"
                    },
                    {
                        "name": "Mikita Balesni"
                    },
                    {
                        "name": "Annika Hallensleben"
                    },
                    {
                        "name": "Nix Goldowsky-Dill"
                    },
                    {
                        "name": "Lee Sharkey"
                    }
                ],
                "author_detail": {
                    "name": "Lee Sharkey"
                },
                "author": "Lee Sharkey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12170v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12170v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09063v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09063v2",
                "updated": "2025-04-16T15:15:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    15,
                    15,
                    56,
                    2,
                    106,
                    0
                ],
                "published": "2024-02-14T10:20:03Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    10,
                    20,
                    3,
                    2,
                    45,
                    0
                ],
                "title": "Soft Prompt Threats: Attacking Safety Alignment and Unlearning in\n  Open-Source LLMs through the Embedding Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft Prompt Threats: Attacking Safety Alignment and Unlearning in\n  Open-Source LLMs through the Embedding Space"
                },
                "summary": "Current research in adversarial robustness of LLMs focuses on discrete input\nmanipulations in the natural language space, which can be directly transferred\nto closed-source models. However, this approach neglects the steady progression\nof open-source models. As open-source models advance in capability, ensuring\ntheir safety also becomes increasingly imperative. Yet, attacks tailored to\nopen-source LLMs that exploit full model access remain largely unexplored. We\naddress this research gap and propose the embedding space attack, which\ndirectly attacks the continuous embedding representation of input tokens. We\nfind that embedding space attacks circumvent model alignments and trigger\nharmful behaviors more efficiently than discrete attacks or model fine-tuning.\nFurthermore, we present a novel threat model in the context of unlearning and\nshow that embedding space attacks can extract supposedly deleted information\nfrom unlearned LLMs across multiple datasets and models. Our findings highlight\nembedding space attacks as an important threat model in open-source LLMs.\nTrigger Warning: the appendix contains LLM-generated text with violence and\nharassment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current research in adversarial robustness of LLMs focuses on discrete input\nmanipulations in the natural language space, which can be directly transferred\nto closed-source models. However, this approach neglects the steady progression\nof open-source models. As open-source models advance in capability, ensuring\ntheir safety also becomes increasingly imperative. Yet, attacks tailored to\nopen-source LLMs that exploit full model access remain largely unexplored. We\naddress this research gap and propose the embedding space attack, which\ndirectly attacks the continuous embedding representation of input tokens. We\nfind that embedding space attacks circumvent model alignments and trigger\nharmful behaviors more efficiently than discrete attacks or model fine-tuning.\nFurthermore, we present a novel threat model in the context of unlearning and\nshow that embedding space attacks can extract supposedly deleted information\nfrom unlearned LLMs across multiple datasets and models. Our findings highlight\nembedding space attacks as an important threat model in open-source LLMs.\nTrigger Warning: the appendix contains LLM-generated text with violence and\nharassment."
                },
                "authors": [
                    {
                        "name": "Leo Schwinn"
                    },
                    {
                        "name": "David Dobre"
                    },
                    {
                        "name": "Sophie Xhonneux"
                    },
                    {
                        "name": "Gauthier Gidel"
                    },
                    {
                        "name": "Stephan Gunnemann"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Gunnemann"
                },
                "author": "Stephan Gunnemann",
                "arxiv_comment": "Trigger Warning: the appendix contains LLM-generated text with\n  violence and harassment",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09063v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09063v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.05308v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.05308v2",
                "updated": "2025-04-16T15:14:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    15,
                    14,
                    34,
                    2,
                    106,
                    0
                ],
                "published": "2024-01-10T18:22:00Z",
                "published_parsed": [
                    2024,
                    1,
                    10,
                    18,
                    22,
                    0,
                    2,
                    10,
                    0
                ],
                "title": "Strategic Client Selection to Address Non-IIDness in HAPS-enabled FL\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategic Client Selection to Address Non-IIDness in HAPS-enabled FL\n  Networks"
                },
                "summary": "The deployment of federated learning (FL) in non-terrestrial networks (NTN)\nthat are supported by high-altitude platform stations (HAPS) offers numerous\nadvantages. Due to its large footprint, it facilitates interaction with a large\nnumber of line-of-sight (LoS) ground clients, each possessing diverse datasets\nalong with distinct communication and computational capabilities. The presence\nof many clients enhances the accuracy of the FL model and speeds up\nconvergence. However, the variety of datasets among these clients poses a\nsignificant challenge, as it leads to pervasive non-independent and identically\ndistributed (non-IID) data. The data non-IIDness results in markedly reduced\ntraining accuracy and slower convergence rates. To address this issue, we\npropose a novel weighted attribute-based client selection strategy that\nleverages multiple user-specific attributes, including historical traffic\npatterns, instantaneous channel conditions, computational capabilities, and\nprevious-round learning performance. By combining these attributes into a\ncomposite score for each user at every FL round and selecting users with higher\nscores as FL clients, the framework ensures more uniform and representative\ndata distributions, effectively mitigating the adverse effects of non-IID data.\nSimulation results corroborate the effectiveness of the proposed client\nselection strategy in enhancing FL model accuracy and convergence rate, as well\nas reducing training loss, by effectively addressing the critical challenge of\ndata non-IIDness in large-scale FL system implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of federated learning (FL) in non-terrestrial networks (NTN)\nthat are supported by high-altitude platform stations (HAPS) offers numerous\nadvantages. Due to its large footprint, it facilitates interaction with a large\nnumber of line-of-sight (LoS) ground clients, each possessing diverse datasets\nalong with distinct communication and computational capabilities. The presence\nof many clients enhances the accuracy of the FL model and speeds up\nconvergence. However, the variety of datasets among these clients poses a\nsignificant challenge, as it leads to pervasive non-independent and identically\ndistributed (non-IID) data. The data non-IIDness results in markedly reduced\ntraining accuracy and slower convergence rates. To address this issue, we\npropose a novel weighted attribute-based client selection strategy that\nleverages multiple user-specific attributes, including historical traffic\npatterns, instantaneous channel conditions, computational capabilities, and\nprevious-round learning performance. By combining these attributes into a\ncomposite score for each user at every FL round and selecting users with higher\nscores as FL clients, the framework ensures more uniform and representative\ndata distributions, effectively mitigating the adverse effects of non-IID data.\nSimulation results corroborate the effectiveness of the proposed client\nselection strategy in enhancing FL model accuracy and convergence rate, as well\nas reducing training loss, by effectively addressing the critical challenge of\ndata non-IIDness in large-scale FL system implementations."
                },
                "authors": [
                    {
                        "name": "Amin Farajzadeh"
                    },
                    {
                        "name": "Animesh Yadav"
                    },
                    {
                        "name": "Halim Yanikomeroglu"
                    }
                ],
                "author_detail": {
                    "name": "Halim Yanikomeroglu"
                },
                "author": "Halim Yanikomeroglu",
                "arxiv_comment": "Submitted to IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.05308v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.05308v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01533v2",
                "updated": "2025-04-16T15:12:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    15,
                    12,
                    21,
                    2,
                    106,
                    0
                ],
                "published": "2024-05-02T17:59:24Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    17,
                    59,
                    24,
                    3,
                    123,
                    0
                ],
                "title": "OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving\n  with Counterfactual Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving\n  with Counterfactual Reasoning"
                },
                "summary": "The advances in vision-language models (VLMs) have led to a growing interest\nin autonomous driving to leverage their strong reasoning capabilities. However,\nextending these capabilities from 2D to full 3D understanding is crucial for\nreal-world applications. To address this challenge, we propose OmniDrive, a\nholistic vision-language dataset that aligns agent models with 3D driving tasks\nthrough counterfactual reasoning. This approach enhances decision-making by\nevaluating potential scenarios and their outcomes, similar to human drivers\nconsidering alternative actions. Our counterfactual-based synthetic data\nannotation process generates large-scale, high-quality datasets, providing\ndenser supervision signals that bridge planning trajectories and language-based\nreasoning. Futher, we explore two advanced OmniDrive-Agent frameworks, namely\nOmni-L and Omni-Q, to assess the importance of vision-language alignment versus\n3D perception, revealing critical insights into designing effective LLM-agents.\nSignificant improvements on the DriveLM Q\\&A benchmark and nuScenes open-loop\nplanning demonstrate the effectiveness of our dataset and methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advances in vision-language models (VLMs) have led to a growing interest\nin autonomous driving to leverage their strong reasoning capabilities. However,\nextending these capabilities from 2D to full 3D understanding is crucial for\nreal-world applications. To address this challenge, we propose OmniDrive, a\nholistic vision-language dataset that aligns agent models with 3D driving tasks\nthrough counterfactual reasoning. This approach enhances decision-making by\nevaluating potential scenarios and their outcomes, similar to human drivers\nconsidering alternative actions. Our counterfactual-based synthetic data\nannotation process generates large-scale, high-quality datasets, providing\ndenser supervision signals that bridge planning trajectories and language-based\nreasoning. Futher, we explore two advanced OmniDrive-Agent frameworks, namely\nOmni-L and Omni-Q, to assess the importance of vision-language alignment versus\n3D perception, revealing critical insights into designing effective LLM-agents.\nSignificant improvements on the DriveLM Q\\&A benchmark and nuScenes open-loop\nplanning demonstrate the effectiveness of our dataset and methods."
                },
                "authors": [
                    {
                        "name": "Shihao Wang"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Xiaohui Jiang"
                    },
                    {
                        "name": "Shiyi Lan"
                    },
                    {
                        "name": "Min Shi"
                    },
                    {
                        "name": "Nadine Chang"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Jose M. Alvarez"
                    }
                ],
                "author_detail": {
                    "name": "Jose M. Alvarez"
                },
                "author": "Jose M. Alvarez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04348v2",
                "updated": "2025-04-16T15:00:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    15,
                    0,
                    11,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-06T03:54:21Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    3,
                    54,
                    21,
                    6,
                    96,
                    0
                ],
                "title": "OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving\n  with Counterfactual Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving\n  with Counterfactual Reasoning"
                },
                "summary": "The advances in vision-language models (VLMs) have led to a growing interest\nin autonomous driving to leverage their strong reasoning capabilities. However,\nextending these capabilities from 2D to full 3D understanding is crucial for\nreal-world applications. To address this challenge, we propose OmniDrive, a\nholistic vision-language dataset that aligns agent models with 3D driving tasks\nthrough counterfactual reasoning. This approach enhances decision-making by\nevaluating potential scenarios and their outcomes, similar to human drivers\nconsidering alternative actions. Our counterfactual-based synthetic data\nannotation process generates large-scale, high-quality datasets, providing\ndenser supervision signals that bridge planning trajectories and language-based\nreasoning. Futher, we explore two advanced OmniDrive-Agent frameworks, namely\nOmni-L and Omni-Q, to assess the importance of vision-language alignment versus\n3D perception, revealing critical insights into designing effective LLM-agents.\nSignificant improvements on the DriveLM Q\\&A benchmark and nuScenes open-loop\nplanning demonstrate the effectiveness of our dataset and methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advances in vision-language models (VLMs) have led to a growing interest\nin autonomous driving to leverage their strong reasoning capabilities. However,\nextending these capabilities from 2D to full 3D understanding is crucial for\nreal-world applications. To address this challenge, we propose OmniDrive, a\nholistic vision-language dataset that aligns agent models with 3D driving tasks\nthrough counterfactual reasoning. This approach enhances decision-making by\nevaluating potential scenarios and their outcomes, similar to human drivers\nconsidering alternative actions. Our counterfactual-based synthetic data\nannotation process generates large-scale, high-quality datasets, providing\ndenser supervision signals that bridge planning trajectories and language-based\nreasoning. Futher, we explore two advanced OmniDrive-Agent frameworks, namely\nOmni-L and Omni-Q, to assess the importance of vision-language alignment versus\n3D perception, revealing critical insights into designing effective LLM-agents.\nSignificant improvements on the DriveLM Q\\&A benchmark and nuScenes open-loop\nplanning demonstrate the effectiveness of our dataset and methods."
                },
                "authors": [
                    {
                        "name": "Shihao Wang"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Xiaohui Jiang"
                    },
                    {
                        "name": "Shiyi Lan"
                    },
                    {
                        "name": "Min Shi"
                    },
                    {
                        "name": "Nadine Chang"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Jose M. Alvarez"
                    }
                ],
                "author_detail": {
                    "name": "Jose M. Alvarez"
                },
                "author": "Jose M. Alvarez",
                "arxiv_comment": "Mistaken resubmission. The original version is at arXiv:2405.01533",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16514v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16514v3",
                "updated": "2025-04-16T14:58:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    58,
                    48,
                    2,
                    106,
                    0
                ],
                "published": "2025-03-15T23:43:06Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    23,
                    43,
                    6,
                    5,
                    74,
                    0
                ],
                "title": "VeriMind: Agentic LLM for Automated Verilog Generation with a Novel\n  Evaluation Metric",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriMind: Agentic LLM for Automated Verilog Generation with a Novel\n  Evaluation Metric"
                },
                "summary": "Designing Verilog modules requires meticulous attention to correctness,\nefficiency, and adherence to design specifications. However, manually writing\nVerilog code remains a complex and time-consuming task that demands both expert\nknowledge and iterative refinement. Leveraging recent advancements in large\nlanguage models (LLMs) and their structured text generation capabilities, we\npropose VeriMind, an agentic LLM framework for Verilog code generation that\nsignificantly automates and optimizes the synthesis process. Unlike traditional\nLLM-based code generators, VeriMind employs a structured reasoning approach:\ngiven a user-provided prompt describing design requirements, the system first\nformulates a detailed train of thought before the final Verilog code is\ngenerated. This multi-step methodology enhances interpretability, accuracy, and\nadaptability in hardware design. In addition, we introduce a novel evaluation\nmetric-pass@ARC-which combines the conventional pass@k measure with Average\nRefinement Cycles (ARC) to capture both success rate and the efficiency of\niterative refinement. Experimental results on diverse hardware design tasks\ndemonstrated that our approach achieved up to $8.3\\%$ improvement on pass@k\nmetric and $8.1\\%$ on pass@ARC metric. These findings underscore the\ntransformative potential of agentic LLMs in automated hardware design, RTL\ndevelopment, and digital system synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing Verilog modules requires meticulous attention to correctness,\nefficiency, and adherence to design specifications. However, manually writing\nVerilog code remains a complex and time-consuming task that demands both expert\nknowledge and iterative refinement. Leveraging recent advancements in large\nlanguage models (LLMs) and their structured text generation capabilities, we\npropose VeriMind, an agentic LLM framework for Verilog code generation that\nsignificantly automates and optimizes the synthesis process. Unlike traditional\nLLM-based code generators, VeriMind employs a structured reasoning approach:\ngiven a user-provided prompt describing design requirements, the system first\nformulates a detailed train of thought before the final Verilog code is\ngenerated. This multi-step methodology enhances interpretability, accuracy, and\nadaptability in hardware design. In addition, we introduce a novel evaluation\nmetric-pass@ARC-which combines the conventional pass@k measure with Average\nRefinement Cycles (ARC) to capture both success rate and the efficiency of\niterative refinement. Experimental results on diverse hardware design tasks\ndemonstrated that our approach achieved up to $8.3\\%$ improvement on pass@k\nmetric and $8.1\\%$ on pass@ARC metric. These findings underscore the\ntransformative potential of agentic LLMs in automated hardware design, RTL\ndevelopment, and digital system synthesis."
                },
                "authors": [
                    {
                        "name": "Bardia Nadimi"
                    },
                    {
                        "name": "Ghali Omar Boutaib"
                    },
                    {
                        "name": "Hao Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zheng"
                },
                "author": "Hao Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16514v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16514v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10337v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10337v2",
                "updated": "2025-04-16T14:58:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    58,
                    26,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-14T15:46:33Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    46,
                    33,
                    0,
                    104,
                    0
                ],
                "title": "Heimdall: test-time scaling on the generative verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heimdall: test-time scaling on the generative verification"
                },
                "summary": "An AI system can create and maintain knowledge only to the extent that it can\nverify that knowledge itself. Recent work on long Chain-of-Thought reasoning\nhas demonstrated great potential of LLMs on solving competitive problems, but\ntheir verification ability remains to be weak and not sufficiently\ninvestigated. In this paper, we propose Heimdall, the long CoT verification LLM\nthat can accurately judge the correctness of solutions. With pure reinforcement\nlearning, we boost the verification accuracy from 62.5% to 94.5% on competitive\nmath problems. By scaling with repeated sampling, the accuracy further\nincreases to 97.5%. Through human evaluation, Heimdall demonstrates impressive\ngeneralization capabilities, successfully detecting most issues in challenging\nmath proofs, the type of which is not included during training. Furthermore, we\npropose Pessimistic Verification to extend the functionality of Heimdall to\nscaling up the problem solving. It calls Heimdall to judge the solutions from a\nsolver model and based on the pessimistic principle, selects the most likely\ncorrect solution with the least uncertainty. Taking\nDeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification\nimproves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute\nbudget and to 83.3% with more compute budget. With the stronger solver Gemini\n2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge\ndiscovery system, a ternary system where one poses questions, another provides\nsolutions, and the third verifies the solutions. Using the data synthesis work\nNuminaMath for the first two components, Heimdall effectively identifies\nproblematic records within the dataset and reveals that nearly half of the data\nis flawed, which interestingly aligns with the recent ablation studies from\nNuminaMath.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An AI system can create and maintain knowledge only to the extent that it can\nverify that knowledge itself. Recent work on long Chain-of-Thought reasoning\nhas demonstrated great potential of LLMs on solving competitive problems, but\ntheir verification ability remains to be weak and not sufficiently\ninvestigated. In this paper, we propose Heimdall, the long CoT verification LLM\nthat can accurately judge the correctness of solutions. With pure reinforcement\nlearning, we boost the verification accuracy from 62.5% to 94.5% on competitive\nmath problems. By scaling with repeated sampling, the accuracy further\nincreases to 97.5%. Through human evaluation, Heimdall demonstrates impressive\ngeneralization capabilities, successfully detecting most issues in challenging\nmath proofs, the type of which is not included during training. Furthermore, we\npropose Pessimistic Verification to extend the functionality of Heimdall to\nscaling up the problem solving. It calls Heimdall to judge the solutions from a\nsolver model and based on the pessimistic principle, selects the most likely\ncorrect solution with the least uncertainty. Taking\nDeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification\nimproves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute\nbudget and to 83.3% with more compute budget. With the stronger solver Gemini\n2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge\ndiscovery system, a ternary system where one poses questions, another provides\nsolutions, and the third verifies the solutions. Using the data synthesis work\nNuminaMath for the first two components, Heimdall effectively identifies\nproblematic records within the dataset and reveals that nearly half of the data\nis flawed, which interestingly aligns with the recent ablation studies from\nNuminaMath."
                },
                "authors": [
                    {
                        "name": "Wenlei Shi"
                    },
                    {
                        "name": "Xing Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xing Jin"
                },
                "author": "Xing Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10337v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10337v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12143v1",
                "updated": "2025-04-16T14:53:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    53,
                    28,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T14:53:28Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    53,
                    28,
                    2,
                    106,
                    0
                ],
                "title": "ARCeR: an Agentic RAG for the Automated Definition of Cyber Ranges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCeR: an Agentic RAG for the Automated Definition of Cyber Ranges"
                },
                "summary": "The growing and evolving landscape of cybersecurity threats necessitates the\ndevelopment of supporting tools and platforms that allow for the creation of\nrealistic IT environments operating within virtual, controlled settings as\nCyber Ranges (CRs). CRs can be exploited for analyzing vulnerabilities and\nexperimenting with the effectiveness of devised countermeasures, as well as\nserving as training environments for building cyber security skills and\nabilities for IT operators. This paper proposes ARCeR as an innovative solution\nfor the automatic generation and deployment of CRs, starting from user-provided\ndescriptions in a natural language. ARCeR relies on the Agentic RAG paradigm,\nwhich allows it to fully exploit state-of-art AI technologies. Experimental\nresults show that ARCeR is able to successfully process prompts even in cases\nthat LLMs or basic RAG systems are not able to cope with. Furthermore, ARCeR is\nable to target any CR framework provided that specific knowledge is made\navailable to it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing and evolving landscape of cybersecurity threats necessitates the\ndevelopment of supporting tools and platforms that allow for the creation of\nrealistic IT environments operating within virtual, controlled settings as\nCyber Ranges (CRs). CRs can be exploited for analyzing vulnerabilities and\nexperimenting with the effectiveness of devised countermeasures, as well as\nserving as training environments for building cyber security skills and\nabilities for IT operators. This paper proposes ARCeR as an innovative solution\nfor the automatic generation and deployment of CRs, starting from user-provided\ndescriptions in a natural language. ARCeR relies on the Agentic RAG paradigm,\nwhich allows it to fully exploit state-of-art AI technologies. Experimental\nresults show that ARCeR is able to successfully process prompts even in cases\nthat LLMs or basic RAG systems are not able to cope with. Furthermore, ARCeR is\nable to target any CR framework provided that specific knowledge is made\navailable to it."
                },
                "authors": [
                    {
                        "name": "Matteo Lupinacci"
                    },
                    {
                        "name": "Francesco Blefari"
                    },
                    {
                        "name": "Francesco Romeo"
                    },
                    {
                        "name": "Francesco Aurelio Pironti"
                    },
                    {
                        "name": "Angelo Furfaro"
                    }
                ],
                "author_detail": {
                    "name": "Angelo Furfaro"
                },
                "author": "Angelo Furfaro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12140v1",
                "updated": "2025-04-16T14:52:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    52,
                    22,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T14:52:22Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    52,
                    22,
                    2,
                    106,
                    0
                ],
                "title": "Multilingual Contextualization of Large Language Models for\n  Document-Level Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Contextualization of Large Language Models for\n  Document-Level Machine Translation"
                },
                "summary": "Large language models (LLMs) have demonstrated strong performance in\nsentence-level machine translation, but scaling to document-level translation\nremains challenging, particularly in modeling long-range dependencies and\ndiscourse phenomena across sentences and paragraphs. In this work, we propose a\nmethod to improve LLM-based long-document translation through targeted\nfine-tuning on high-quality document-level data, which we curate and introduce\nas DocBlocks. Our approach supports multiple translation paradigms, including\ndirect document-to-document and chunk-level translation, by integrating\ninstructions both with and without surrounding context. This enables models to\nbetter capture cross-sentence dependencies while maintaining strong\nsentence-level translation performance. Experimental results show that\nincorporating multiple translation paradigms improves document-level\ntranslation quality and inference speed compared to prompting and agent-based\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong performance in\nsentence-level machine translation, but scaling to document-level translation\nremains challenging, particularly in modeling long-range dependencies and\ndiscourse phenomena across sentences and paragraphs. In this work, we propose a\nmethod to improve LLM-based long-document translation through targeted\nfine-tuning on high-quality document-level data, which we curate and introduce\nas DocBlocks. Our approach supports multiple translation paradigms, including\ndirect document-to-document and chunk-level translation, by integrating\ninstructions both with and without surrounding context. This enables models to\nbetter capture cross-sentence dependencies while maintaining strong\nsentence-level translation performance. Experimental results show that\nincorporating multiple translation paradigms improves document-level\ntranslation quality and inference speed compared to prompting and agent-based\nmethods."
                },
                "authors": [
                    {
                        "name": "Miguel Moura Ramos"
                    },
                    {
                        "name": "Patrick Fernandes"
                    },
                    {
                        "name": "Sweta Agrawal"
                    },
                    {
                        "name": "Andr F. T. Martins"
                    }
                ],
                "author_detail": {
                    "name": "Andr F. T. Martins"
                },
                "author": "Andr F. T. Martins",
                "arxiv_comment": "9 pages, work-in-progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08525v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08525v3",
                "updated": "2025-04-16T14:48:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    48,
                    13,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-11T13:38:36Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    13,
                    38,
                    36,
                    4,
                    101,
                    0
                ],
                "title": "Task Memory Engine (TME): A Structured Memory Framework with Graph-Aware\n  Extensions for Multi-Step LLM Agent Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task Memory Engine (TME): A Structured Memory Framework with Graph-Aware\n  Extensions for Multi-Step LLM Agent Tasks"
                },
                "summary": "Large Language Models (LLMs) are increasingly used as autonomous agents for\nmulti-step tasks. However, most existing frameworks fail to maintain a\nstructured understanding of the task state, often relying on linear prompt\nconcatenation or shallow memory buffers. This leads to brittle performance,\nfrequent hallucinations, and poor long-range coherence. In this work, we\npropose the Task Memory Engine (TME), a lightweight and structured memory\nmodule that tracks task execution using a hierarchical Task Memory Tree (TMT).\nEach node in the tree corresponds to a task step, storing relevant input,\noutput, status, and sub-task relationships. We introduce a prompt synthesis\nmethod that dynamically generates LLM prompts based on the active node path,\nsignificantly improving execution consistency and contextual grounding. Through\ncase studies and comparative experiments on multi-step agent tasks, we\ndemonstrate that TME leads to better task completion accuracy and more\ninterpretable behavior with minimal implementation overhead. A reference\nimplementation of the core TME components is available at\nhttps://github.com/biubiutomato/TME-Agent, including basic examples and\nstructured memory integration. While the current implementation uses a\ntree-based structure, TME is designed to be graph-aware, supporting reusable\nsubsteps, converging task paths, and shared dependencies. This lays the\ngroundwork for future DAG-based memory architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used as autonomous agents for\nmulti-step tasks. However, most existing frameworks fail to maintain a\nstructured understanding of the task state, often relying on linear prompt\nconcatenation or shallow memory buffers. This leads to brittle performance,\nfrequent hallucinations, and poor long-range coherence. In this work, we\npropose the Task Memory Engine (TME), a lightweight and structured memory\nmodule that tracks task execution using a hierarchical Task Memory Tree (TMT).\nEach node in the tree corresponds to a task step, storing relevant input,\noutput, status, and sub-task relationships. We introduce a prompt synthesis\nmethod that dynamically generates LLM prompts based on the active node path,\nsignificantly improving execution consistency and contextual grounding. Through\ncase studies and comparative experiments on multi-step agent tasks, we\ndemonstrate that TME leads to better task completion accuracy and more\ninterpretable behavior with minimal implementation overhead. A reference\nimplementation of the core TME components is available at\nhttps://github.com/biubiutomato/TME-Agent, including basic examples and\nstructured memory integration. While the current implementation uses a\ntree-based structure, TME is designed to be graph-aware, supporting reusable\nsubsteps, converging task paths, and shared dependencies. This lays the\ngroundwork for future DAG-based memory architectures."
                },
                "authors": [
                    {
                        "name": "Ye Ye"
                    }
                ],
                "author_detail": {
                    "name": "Ye Ye"
                },
                "author": "Ye Ye",
                "arxiv_comment": "14 pages, 5 figures. Preprint prepared for future submission.\n  Includes implementation and token-efficiency analysis. Code at\n  https://github.com/biubiutomato/TME-Agent",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08525v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08525v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.8; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10185v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10185v2",
                "updated": "2025-04-16T14:45:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    45,
                    55,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-14T12:38:37Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    38,
                    37,
                    0,
                    104,
                    0
                ],
                "title": "LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in\n  Current Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in\n  Current Benchmarks"
                },
                "summary": "Large language model unlearning has become a critical challenge in ensuring\nsafety and controlled model behavior by removing undesired data-model\ninfluences from the pretrained model while preserving general utility.\nSignificant recent efforts have been dedicated to developing LLM unlearning\nbenchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine\nUnlearning Six-way Evaluation), facilitating standardized unlearning\nperformance assessment and method comparison. Despite their usefulness, we\nuncover for the first time a novel coreset effect within these benchmarks.\nSpecifically, we find that LLM unlearning achieved with the original (full)\nforget set can be effectively maintained using a significantly smaller subset\n(functioning as a \"coreset\"), e.g., as little as 5% of the forget set, even\nwhen selected at random. This suggests that LLM unlearning in these benchmarks\ncan be performed surprisingly easily, even in an extremely low-data regime. We\ndemonstrate that this coreset effect remains strong, regardless of the LLM\nunlearning method used, such as NPO (Negative Preference Optimization) and RMU\n(Representation Misdirection Unlearning), the popular ones in these benchmarks.\nThe surprisingly strong coreset effect is also robust across various data\nselection methods, ranging from random selection to more sophisticated\nheuristic approaches. We explain the coreset effect in LLM unlearning through a\nkeyword-based perspective, showing that keywords extracted from the forget set\nalone contribute significantly to unlearning effectiveness and indicating that\ncurrent unlearning is driven by a compact set of high-impact tokens rather than\nthe entire dataset. We further justify the faithfulness of coreset-unlearned\nmodels along additional dimensions, such as mode connectivity and robustness to\njailbreaking attacks. Codes are available at\nhttps://github.com/OPTML-Group/MU-Coreset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model unlearning has become a critical challenge in ensuring\nsafety and controlled model behavior by removing undesired data-model\ninfluences from the pretrained model while preserving general utility.\nSignificant recent efforts have been dedicated to developing LLM unlearning\nbenchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine\nUnlearning Six-way Evaluation), facilitating standardized unlearning\nperformance assessment and method comparison. Despite their usefulness, we\nuncover for the first time a novel coreset effect within these benchmarks.\nSpecifically, we find that LLM unlearning achieved with the original (full)\nforget set can be effectively maintained using a significantly smaller subset\n(functioning as a \"coreset\"), e.g., as little as 5% of the forget set, even\nwhen selected at random. This suggests that LLM unlearning in these benchmarks\ncan be performed surprisingly easily, even in an extremely low-data regime. We\ndemonstrate that this coreset effect remains strong, regardless of the LLM\nunlearning method used, such as NPO (Negative Preference Optimization) and RMU\n(Representation Misdirection Unlearning), the popular ones in these benchmarks.\nThe surprisingly strong coreset effect is also robust across various data\nselection methods, ranging from random selection to more sophisticated\nheuristic approaches. We explain the coreset effect in LLM unlearning through a\nkeyword-based perspective, showing that keywords extracted from the forget set\nalone contribute significantly to unlearning effectiveness and indicating that\ncurrent unlearning is driven by a compact set of high-impact tokens rather than\nthe entire dataset. We further justify the faithfulness of coreset-unlearned\nmodels along additional dimensions, such as mode connectivity and robustness to\njailbreaking attacks. Codes are available at\nhttps://github.com/OPTML-Group/MU-Coreset."
                },
                "authors": [
                    {
                        "name": "Soumyadeep Pal"
                    },
                    {
                        "name": "Changsheng Wang"
                    },
                    {
                        "name": "James Diffenderfer"
                    },
                    {
                        "name": "Bhavya Kailkhura"
                    },
                    {
                        "name": "Sijia Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sijia Liu"
                },
                "author": "Sijia Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10185v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10185v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12113v1",
                "updated": "2025-04-16T14:21:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    21,
                    2,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T14:21:02Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    21,
                    2,
                    2,
                    106,
                    0
                ],
                "title": "Clarifying Ambiguities: on the Role of Ambiguity Types in Prompting\n  Methods for Clarification Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clarifying Ambiguities: on the Role of Ambiguity Types in Prompting\n  Methods for Clarification Generation"
                },
                "summary": "In information retrieval (IR), providing appropriate clarifications to better\nunderstand users' information needs is crucial for building a proactive\nsearch-oriented dialogue system. Due to the strong in-context learning ability\nof large language models (LLMs), recent studies investigate prompting methods\nto generate clarifications using few-shot or Chain of Thought (CoT) prompts.\nHowever, vanilla CoT prompting does not distinguish the characteristics of\ndifferent information needs, making it difficult to understand how LLMs resolve\nambiguities in user queries. In this work, we focus on the concept of ambiguity\nfor clarification, seeking to model and integrate ambiguities in the\nclarification process. To this end, we comprehensively study the impact of\nprompting schemes based on reasoning and ambiguity for clarification. The idea\nis to enhance the reasoning abilities of LLMs by limiting CoT to predict first\nambiguity types that can be interpreted as instructions to clarify, then\ncorrespondingly generate clarifications. We name this new prompting scheme\nAmbiguity Type-Chain of Thought (AT-CoT). Experiments are conducted on various\ndatasets containing human-annotated clarifying questions to compare AT-CoT with\nmultiple baselines. We also perform user simulations to implicitly measure the\nquality of generated clarifications under various IR scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In information retrieval (IR), providing appropriate clarifications to better\nunderstand users' information needs is crucial for building a proactive\nsearch-oriented dialogue system. Due to the strong in-context learning ability\nof large language models (LLMs), recent studies investigate prompting methods\nto generate clarifications using few-shot or Chain of Thought (CoT) prompts.\nHowever, vanilla CoT prompting does not distinguish the characteristics of\ndifferent information needs, making it difficult to understand how LLMs resolve\nambiguities in user queries. In this work, we focus on the concept of ambiguity\nfor clarification, seeking to model and integrate ambiguities in the\nclarification process. To this end, we comprehensively study the impact of\nprompting schemes based on reasoning and ambiguity for clarification. The idea\nis to enhance the reasoning abilities of LLMs by limiting CoT to predict first\nambiguity types that can be interpreted as instructions to clarify, then\ncorrespondingly generate clarifications. We name this new prompting scheme\nAmbiguity Type-Chain of Thought (AT-CoT). Experiments are conducted on various\ndatasets containing human-annotated clarifying questions to compare AT-CoT with\nmultiple baselines. We also perform user simulations to implicitly measure the\nquality of generated clarifications under various IR scenarios."
                },
                "authors": [
                    {
                        "name": "Anfu Tang"
                    },
                    {
                        "name": "Laure Soulier"
                    },
                    {
                        "name": "Vincent Guigue"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Guigue"
                },
                "author": "Vincent Guigue",
                "arxiv_comment": "11 pages, 3 figures. Accepted at SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12110v1",
                "updated": "2025-04-16T14:19:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    19,
                    25,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T14:19:25Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    19,
                    25,
                    2,
                    106,
                    0
                ],
                "title": "Towards LLM Agents for Earth Observation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards LLM Agents for Earth Observation"
                },
                "summary": "Earth Observation (EO) provides critical planetary data for environmental\nmonitoring, disaster management, climate science, and other scientific domains.\nHere we ask: Are AI systems ready for reliable Earth Observation? We introduce\n\\datasetnamenospace, a benchmark of 140 yes/no questions from NASA Earth\nObservatory articles across 13 topics and 17 satellite sensors. Using Google\nEarth Engine API as a tool, LLM agents can only achieve an accuracy of 33%\nbecause the code fails to run over 58% of the time. We improve the failure rate\nfor open models by fine-tuning synthetic data, allowing much smaller models\n(Llama-3.1-8B) to achieve comparable accuracy to much larger ones (e.g.,\nDeepSeek-R1). Taken together, our findings identify significant challenges to\nbe solved before AI agents can automate earth observation, and suggest paths\nforward. The project page is available at\nhttps://iandrover.github.io/UnivEarth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Earth Observation (EO) provides critical planetary data for environmental\nmonitoring, disaster management, climate science, and other scientific domains.\nHere we ask: Are AI systems ready for reliable Earth Observation? We introduce\n\\datasetnamenospace, a benchmark of 140 yes/no questions from NASA Earth\nObservatory articles across 13 topics and 17 satellite sensors. Using Google\nEarth Engine API as a tool, LLM agents can only achieve an accuracy of 33%\nbecause the code fails to run over 58% of the time. We improve the failure rate\nfor open models by fine-tuning synthetic data, allowing much smaller models\n(Llama-3.1-8B) to achieve comparable accuracy to much larger ones (e.g.,\nDeepSeek-R1). Taken together, our findings identify significant challenges to\nbe solved before AI agents can automate earth observation, and suggest paths\nforward. The project page is available at\nhttps://iandrover.github.io/UnivEarth."
                },
                "authors": [
                    {
                        "name": "Chia Hsiang Kao"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Shreelekha Revankar"
                    },
                    {
                        "name": "Samuel Speas"
                    },
                    {
                        "name": "Snehal Bhagat"
                    },
                    {
                        "name": "Rajeev Datta"
                    },
                    {
                        "name": "Cheng Perng Phoo"
                    },
                    {
                        "name": "Utkarsh Mall"
                    },
                    {
                        "name": "Carl Vondrick"
                    },
                    {
                        "name": "Kavita Bala"
                    },
                    {
                        "name": "Bharath Hariharan"
                    }
                ],
                "author_detail": {
                    "name": "Bharath Hariharan"
                },
                "author": "Bharath Hariharan",
                "arxiv_comment": "36 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12108v1",
                "updated": "2025-04-16T14:16:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    16,
                    38,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T14:16:38Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    16,
                    38,
                    2,
                    106,
                    0
                ],
                "title": "Entropy-Guided Watermarking for LLMs: A Test-Time Framework for Robust\n  and Traceable Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entropy-Guided Watermarking for LLMs: A Test-Time Framework for Robust\n  and Traceable Text Generation"
                },
                "summary": "The rapid development of Large Language Models (LLMs) has intensified\nconcerns about content traceability and potential misuse. Existing watermarking\nschemes for sampled text often face trade-offs between maintaining text quality\nand ensuring robust detection against various attacks. To address these issues,\nwe propose a novel watermarking scheme that improves both detectability and\ntext quality by introducing a cumulative watermark entropy threshold. Our\napproach is compatible with and generalizes existing sampling functions,\nenhancing adaptability. Experimental results across multiple LLMs show that our\nscheme significantly outperforms existing methods, achieving over 80\\%\nimprovements on widely-used datasets, e.g., MATH and GSM8K, while maintaining\nhigh detection accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Large Language Models (LLMs) has intensified\nconcerns about content traceability and potential misuse. Existing watermarking\nschemes for sampled text often face trade-offs between maintaining text quality\nand ensuring robust detection against various attacks. To address these issues,\nwe propose a novel watermarking scheme that improves both detectability and\ntext quality by introducing a cumulative watermark entropy threshold. Our\napproach is compatible with and generalizes existing sampling functions,\nenhancing adaptability. Experimental results across multiple LLMs show that our\nscheme significantly outperforms existing methods, achieving over 80\\%\nimprovements on widely-used datasets, e.g., MATH and GSM8K, while maintaining\nhigh detection accuracy."
                },
                "authors": [
                    {
                        "name": "Shizhan Cai"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02699v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02699v2",
                "updated": "2025-04-16T14:03:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    3,
                    37,
                    2,
                    106,
                    0
                ],
                "published": "2024-09-04T13:35:15Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    35,
                    15,
                    2,
                    248,
                    0
                ],
                "title": "Collaborative Learning for Enhanced Unsupervised Domain Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Learning for Enhanced Unsupervised Domain Adaptation"
                },
                "summary": "Unsupervised Domain Adaptation (UDA) endeavors to bridge the gap between a\nmodel trained on a labeled source domain and its deployment in an unlabeled\ntarget domain. However, current high-performance models demand significant\nresources, making deployment costs prohibitive and highlighting the need for\ncompact, yet effective models. For UDA of lightweight models, Knowledge\nDistillation (KD) leveraging a Teacher-Student framework could be a common\napproach, but we found that domain shift in UDA leads to a significant increase\nin non-salient parameters in the teacher model, degrading model's\ngeneralization ability and transferring misleading information to the student\nmodel. Interestingly, we observed that this phenomenon occurs considerably less\nin the student model. Driven by this insight, we introduce Collaborative\nLearning for UDA (CLDA), a method that updates the teacher's non-salient\nparameters using the student model and at the same time utilizes the updated\nteacher model to improve UDA performance of the student model. Experiments show\nconsistent performance improvements for both student and teacher models. For\nexample, in semantic segmentation, CLDA achieves an improvement of +0.7% mIoU\nfor the teacher model and +1.4% mIoU for the student model compared to the\nbaseline model in the GTA-to-Cityscapes datasets. In the Synthia-to-Cityscapes\ndataset, it achieves an improvement of +0.8% mIoU and +2.0% mIoU for the\nteacher and student models, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised Domain Adaptation (UDA) endeavors to bridge the gap between a\nmodel trained on a labeled source domain and its deployment in an unlabeled\ntarget domain. However, current high-performance models demand significant\nresources, making deployment costs prohibitive and highlighting the need for\ncompact, yet effective models. For UDA of lightweight models, Knowledge\nDistillation (KD) leveraging a Teacher-Student framework could be a common\napproach, but we found that domain shift in UDA leads to a significant increase\nin non-salient parameters in the teacher model, degrading model's\ngeneralization ability and transferring misleading information to the student\nmodel. Interestingly, we observed that this phenomenon occurs considerably less\nin the student model. Driven by this insight, we introduce Collaborative\nLearning for UDA (CLDA), a method that updates the teacher's non-salient\nparameters using the student model and at the same time utilizes the updated\nteacher model to improve UDA performance of the student model. Experiments show\nconsistent performance improvements for both student and teacher models. For\nexample, in semantic segmentation, CLDA achieves an improvement of +0.7% mIoU\nfor the teacher model and +1.4% mIoU for the student model compared to the\nbaseline model in the GTA-to-Cityscapes datasets. In the Synthia-to-Cityscapes\ndataset, it achieves an improvement of +0.8% mIoU and +2.0% mIoU for the\nteacher and student models, respectively."
                },
                "authors": [
                    {
                        "name": "Minhee Cho"
                    },
                    {
                        "name": "Hyesong Choi"
                    },
                    {
                        "name": "Hayeon Jo"
                    },
                    {
                        "name": "Dongbo Min"
                    }
                ],
                "author_detail": {
                    "name": "Dongbo Min"
                },
                "author": "Dongbo Min",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02699v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02699v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12098v1",
                "updated": "2025-04-16T14:02:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    2,
                    21,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T14:02:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    2,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Gauging Overprecision in LLMs: An Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gauging Overprecision in LLMs: An Empirical Study"
                },
                "summary": "Recently, overconfidence in large language models (LLMs) has garnered\nconsiderable attention due to its fundamental importance in quantifying the\ntrustworthiness of LLM generation. However, existing approaches prompt the\n\\textit{black box LLMs} to produce their confidence (\\textit{verbalized\nconfidence}), which can be subject to many biases and hallucinations. Inspired\nby a different aspect of overconfidence in cognitive science called\n\\textit{overprecision}, we designed a framework for its study in black box\nLLMs. This framework contains three main phases: 1) generation, 2) refinement\nand 3) evaluation. In the generation phase we prompt the LLM to generate\nanswers to numerical questions in the form of intervals with a certain level of\nconfidence. This confidence level is imposed in the prompt and not required for\nthe LLM to generate as in previous approaches. We use various prompting\ntechniques and use the same prompt multiple times to gauge the effects of\nrandomness in the generation process. In the refinement phase, answers from the\nprevious phase are refined to generate better answers. The LLM answers are\nevaluated and studied in the evaluation phase to understand its internal\nworkings. This study allowed us to gain various insights into LLM\noverprecision: 1) LLMs are highly uncalibrated for numerical tasks 2)\n{\\color{blue}there is no correlation between the length of the interval and the\nimposed confidence level, which can be symptomatic of a a) lack of\nunderstanding of the concept of confidence or b) inability to adjust\nself-confidence by following instructions}, {\\color{blue}3)} LLM numerical\nprecision differs depending on the task, scale of answer and prompting\ntechnique {\\color{blue}4) Refinement of answers doesn't improve precision in\nmost cases}. We believe this study offers new perspectives on LLM\noverconfidence and serves as a strong baseline for overprecision in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, overconfidence in large language models (LLMs) has garnered\nconsiderable attention due to its fundamental importance in quantifying the\ntrustworthiness of LLM generation. However, existing approaches prompt the\n\\textit{black box LLMs} to produce their confidence (\\textit{verbalized\nconfidence}), which can be subject to many biases and hallucinations. Inspired\nby a different aspect of overconfidence in cognitive science called\n\\textit{overprecision}, we designed a framework for its study in black box\nLLMs. This framework contains three main phases: 1) generation, 2) refinement\nand 3) evaluation. In the generation phase we prompt the LLM to generate\nanswers to numerical questions in the form of intervals with a certain level of\nconfidence. This confidence level is imposed in the prompt and not required for\nthe LLM to generate as in previous approaches. We use various prompting\ntechniques and use the same prompt multiple times to gauge the effects of\nrandomness in the generation process. In the refinement phase, answers from the\nprevious phase are refined to generate better answers. The LLM answers are\nevaluated and studied in the evaluation phase to understand its internal\nworkings. This study allowed us to gain various insights into LLM\noverprecision: 1) LLMs are highly uncalibrated for numerical tasks 2)\n{\\color{blue}there is no correlation between the length of the interval and the\nimposed confidence level, which can be symptomatic of a a) lack of\nunderstanding of the concept of confidence or b) inability to adjust\nself-confidence by following instructions}, {\\color{blue}3)} LLM numerical\nprecision differs depending on the task, scale of answer and prompting\ntechnique {\\color{blue}4) Refinement of answers doesn't improve precision in\nmost cases}. We believe this study offers new perspectives on LLM\noverconfidence and serves as a strong baseline for overprecision in LLMs."
                },
                "authors": [
                    {
                        "name": "Adil Bahaj"
                    },
                    {
                        "name": "Hamed Rahimi"
                    },
                    {
                        "name": "Mohamed Chetouani"
                    },
                    {
                        "name": "Mounir Ghogho"
                    }
                ],
                "author_detail": {
                    "name": "Mounir Ghogho"
                },
                "author": "Mounir Ghogho",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12090v1",
                "updated": "2025-04-16T13:53:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    13,
                    53,
                    42,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T13:53:42Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    13,
                    53,
                    42,
                    2,
                    106,
                    0
                ],
                "title": "Reasoning-Based AI for Startup Evaluation (R.A.I.S.E.): A\n  Memory-Augmented, Multi-Step Decision Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-Based AI for Startup Evaluation (R.A.I.S.E.): A\n  Memory-Augmented, Multi-Step Decision Framework"
                },
                "summary": "We present a novel framework that bridges the gap between the\ninterpretability of decision trees and the advanced reasoning capabilities of\nlarge language models (LLMs) to predict startup success. Our approach leverages\nchain-of-thought prompting to generate detailed reasoning logs, which are\nsubsequently distilled into structured, human-understandable logical rules. The\npipeline integrates multiple enhancements - efficient data ingestion, a\ntwo-step refinement process, ensemble candidate sampling, simulated\nreinforcement learning scoring, and persistent memory - to ensure both stable\ndecision-making and transparent output. Experimental evaluations on curated\nstartup datasets demonstrate that our combined pipeline improves precision by\n54% from 0.225 to 0.346 and accuracy by 50% from 0.46 to 0.70 compared to a\nstandalone OpenAI o3 model. Notably, our model achieves over 2x the precision\nof a random classifier (16%). By combining state-of-the-art AI reasoning with\nexplicit rule-based explanations, our method not only augments traditional\ndecision-making processes but also facilitates expert intervention and\ncontinuous policy refinement. This work lays the foundation for the\nimplementation of interpretable LLM-powered decision frameworks in high-stakes\ninvestment environments and other domains that require transparent and\ndata-driven insights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel framework that bridges the gap between the\ninterpretability of decision trees and the advanced reasoning capabilities of\nlarge language models (LLMs) to predict startup success. Our approach leverages\nchain-of-thought prompting to generate detailed reasoning logs, which are\nsubsequently distilled into structured, human-understandable logical rules. The\npipeline integrates multiple enhancements - efficient data ingestion, a\ntwo-step refinement process, ensemble candidate sampling, simulated\nreinforcement learning scoring, and persistent memory - to ensure both stable\ndecision-making and transparent output. Experimental evaluations on curated\nstartup datasets demonstrate that our combined pipeline improves precision by\n54% from 0.225 to 0.346 and accuracy by 50% from 0.46 to 0.70 compared to a\nstandalone OpenAI o3 model. Notably, our model achieves over 2x the precision\nof a random classifier (16%). By combining state-of-the-art AI reasoning with\nexplicit rule-based explanations, our method not only augments traditional\ndecision-making processes but also facilitates expert intervention and\ncontinuous policy refinement. This work lays the foundation for the\nimplementation of interpretable LLM-powered decision frameworks in high-stakes\ninvestment environments and other domains that require transparent and\ndata-driven insights."
                },
                "authors": [
                    {
                        "name": "Jack Preuveneers"
                    },
                    {
                        "name": "Joseph Ternasky"
                    },
                    {
                        "name": "Fuat Alican"
                    },
                    {
                        "name": "Yigit Ihlamur"
                    }
                ],
                "author_detail": {
                    "name": "Yigit Ihlamur"
                },
                "author": "Yigit Ihlamur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12083v1",
                "updated": "2025-04-16T13:43:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    13,
                    43,
                    56,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T13:43:56Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    13,
                    43,
                    56,
                    2,
                    106,
                    0
                ],
                "title": "Self-alignment of Large Video Language Models with Refined Regularized\n  Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-alignment of Large Video Language Models with Refined Regularized\n  Preference Optimization"
                },
                "summary": "Despite recent advances in Large Video Language Models (LVLMs), they still\nstruggle with fine-grained temporal understanding, hallucinate, and often make\nsimple mistakes on even simple video question-answering tasks, all of which\npose significant challenges to their safe and reliable deployment in real-world\napplications. To address these limitations, we propose a self-alignment\nframework that enables LVLMs to learn from their own errors. Our proposed\nframework first obtains a training set of preferred and non-preferred response\npairs, where non-preferred responses are generated by incorporating common\nerror patterns that often occur due to inadequate spatio-temporal\nunderstanding, spurious correlations between co-occurring concepts, and\nover-reliance on linguistic cues while neglecting the vision modality, among\nothers. To facilitate self-alignment of LVLMs with the constructed preferred\nand non-preferred response pairs, we introduce Refined Regularized Preference\nOptimization (RRPO), a novel preference optimization method that utilizes\nsub-sequence-level refined rewards and token-wise KL regularization to address\nthe limitations of Direct Preference Optimization (DPO). We demonstrate that\nRRPO achieves more precise alignment and more stable training compared to DPO.\nOur experiments and analysis validate the effectiveness of our approach across\ndiverse video tasks, including video hallucination, short- and long-video\nunderstanding, and fine-grained temporal reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advances in Large Video Language Models (LVLMs), they still\nstruggle with fine-grained temporal understanding, hallucinate, and often make\nsimple mistakes on even simple video question-answering tasks, all of which\npose significant challenges to their safe and reliable deployment in real-world\napplications. To address these limitations, we propose a self-alignment\nframework that enables LVLMs to learn from their own errors. Our proposed\nframework first obtains a training set of preferred and non-preferred response\npairs, where non-preferred responses are generated by incorporating common\nerror patterns that often occur due to inadequate spatio-temporal\nunderstanding, spurious correlations between co-occurring concepts, and\nover-reliance on linguistic cues while neglecting the vision modality, among\nothers. To facilitate self-alignment of LVLMs with the constructed preferred\nand non-preferred response pairs, we introduce Refined Regularized Preference\nOptimization (RRPO), a novel preference optimization method that utilizes\nsub-sequence-level refined rewards and token-wise KL regularization to address\nthe limitations of Direct Preference Optimization (DPO). We demonstrate that\nRRPO achieves more precise alignment and more stable training compared to DPO.\nOur experiments and analysis validate the effectiveness of our approach across\ndiverse video tasks, including video hallucination, short- and long-video\nunderstanding, and fine-grained temporal reasoning."
                },
                "authors": [
                    {
                        "name": "Pritam Sarkar"
                    },
                    {
                        "name": "Ali Etemad"
                    }
                ],
                "author_detail": {
                    "name": "Ali Etemad"
                },
                "author": "Ali Etemad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12076v1",
                "updated": "2025-04-16T13:35:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    13,
                    35,
                    41,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T13:35:41Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    13,
                    35,
                    41,
                    2,
                    106,
                    0
                ],
                "title": "Subitizing-Inspired_Large_Language_Models_for_Floorplanning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Subitizing-Inspired_Large_Language_Models_for_Floorplanning"
                },
                "summary": "We present a novel approach to solving the floorplanning problem by\nleveraging fine-tuned Large Language Models (LLMs). Inspired by subitizing--the\nhuman ability to instantly and accurately count small numbers of items at a\nglance--we hypothesize that LLMs can similarly address floorplanning challenges\nswiftly and accurately. We propose an efficient representation of the\nfloorplanning problem and introduce a method for generating high-quality\ndatasets tailored for model fine-tuning. We fine-tune LLMs on datasets with a\nspecified number of modules to test whether LLMs can emulate the human ability\nto quickly count and arrange items. Our experimental results demonstrate that\nfine-tuned LLMs, particularly GPT4o-mini, achieve high success and optimal\nrates while attaining relatively low average dead space. These findings\nunderscore the potential of LLMs as promising solutions for complex\noptimization tasks in VLSI design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach to solving the floorplanning problem by\nleveraging fine-tuned Large Language Models (LLMs). Inspired by subitizing--the\nhuman ability to instantly and accurately count small numbers of items at a\nglance--we hypothesize that LLMs can similarly address floorplanning challenges\nswiftly and accurately. We propose an efficient representation of the\nfloorplanning problem and introduce a method for generating high-quality\ndatasets tailored for model fine-tuning. We fine-tune LLMs on datasets with a\nspecified number of modules to test whether LLMs can emulate the human ability\nto quickly count and arrange items. Our experimental results demonstrate that\nfine-tuned LLMs, particularly GPT4o-mini, achieve high success and optimal\nrates while attaining relatively low average dead space. These findings\nunderscore the potential of LLMs as promising solutions for complex\noptimization tasks in VLSI design."
                },
                "authors": [
                    {
                        "name": "Shao-Chien Lu"
                    },
                    {
                        "name": "Chen-Chen Yeh"
                    },
                    {
                        "name": "Hui-Lin Cho"
                    },
                    {
                        "name": "Yu-Cheng Lin"
                    },
                    {
                        "name": "Rung-Bin Lin"
                    }
                ],
                "author_detail": {
                    "name": "Rung-Bin Lin"
                },
                "author": "Rung-Bin Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15065v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15065v2",
                "updated": "2025-04-16T13:23:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    13,
                    23,
                    25,
                    2,
                    106,
                    0
                ],
                "published": "2024-04-23T14:12:48Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    14,
                    12,
                    48,
                    1,
                    114,
                    0
                ],
                "title": "Formal Verification of Graph Convolutional Networks with Uncertain Node\n  Features and Uncertain Graph Structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal Verification of Graph Convolutional Networks with Uncertain Node\n  Features and Uncertain Graph Structure"
                },
                "summary": "Graph neural networks are becoming increasingly popular in the field of\nmachine learning due to their unique ability to process data structured in\ngraphs. They have also been applied in safety-critical environments where\nperturbations inherently occur. However, these perturbations require us to\nformally verify neural networks before their deployment in safety-critical\nenvironments as neural networks are prone to adversarial attacks. While there\nexists research on the formal verification of neural networks, there is no work\nverifying the robustness of generic graph convolutional network architectures\nwith uncertainty in the node features and in the graph structure over multiple\nmessage-passing steps. This work addresses this research gap by explicitly\npreserving the non-convex dependencies of all elements in the underlying\ncomputations through reachability analysis with (matrix) polynomial zonotopes.\nWe demonstrate our approach on three popular benchmark datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks are becoming increasingly popular in the field of\nmachine learning due to their unique ability to process data structured in\ngraphs. They have also been applied in safety-critical environments where\nperturbations inherently occur. However, these perturbations require us to\nformally verify neural networks before their deployment in safety-critical\nenvironments as neural networks are prone to adversarial attacks. While there\nexists research on the formal verification of neural networks, there is no work\nverifying the robustness of generic graph convolutional network architectures\nwith uncertainty in the node features and in the graph structure over multiple\nmessage-passing steps. This work addresses this research gap by explicitly\npreserving the non-convex dependencies of all elements in the underlying\ncomputations through reachability analysis with (matrix) polynomial zonotopes.\nWe demonstrate our approach on three popular benchmark datasets."
                },
                "authors": [
                    {
                        "name": "Tobias Ladner"
                    },
                    {
                        "name": "Michael Eichelbeck"
                    },
                    {
                        "name": "Matthias Althoff"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Althoff"
                },
                "author": "Matthias Althoff",
                "arxiv_comment": "published at Transactions on Machine Learning Research (TMLR) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15065v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15065v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12063v1",
                "updated": "2025-04-16T13:18:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    13,
                    18,
                    16,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T13:18:16Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    13,
                    18,
                    16,
                    2,
                    106,
                    0
                ],
                "title": "Optimizing Compound Retrieval Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Compound Retrieval Systems"
                },
                "summary": "Modern retrieval systems do not rely on a single ranking model to construct\ntheir rankings. Instead, they generally take a cascading approach where a\nsequence of ranking models are applied in multiple re-ranking stages. Thereby,\nthey balance the quality of the top-K ranking with computational costs by\nlimiting the number of documents each model re-ranks. However, the cascading\napproach is not the only way models can interact to form a retrieval system.\n  We propose the concept of compound retrieval systems as a broader class of\nretrieval systems that apply multiple prediction models. This encapsulates\ncascading models but also allows other types of interactions than top-K\nre-ranking. In particular, we enable interactions with large language models\n(LLMs) which can provide relative relevance comparisons. We focus on the\noptimization of compound retrieval system design which uniquely involves\nlearning where to apply the component models and how to aggregate their\npredictions into a final ranking. This work shows how our compound approach can\ncombine the classic BM25 retrieval model with state-of-the-art (pairwise) LLM\nrelevance predictions, while optimizing a given ranking metric and efficiency\ntarget. Our experimental results show optimized compound retrieval systems\nprovide better trade-offs between effectiveness and efficiency than cascading\napproaches, even when applied in a self-supervised manner.\n  With the introduction of compound retrieval systems, we hope to inspire the\ninformation retrieval field to more out-of-the-box thinking on how prediction\nmodels can interact to form rankings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern retrieval systems do not rely on a single ranking model to construct\ntheir rankings. Instead, they generally take a cascading approach where a\nsequence of ranking models are applied in multiple re-ranking stages. Thereby,\nthey balance the quality of the top-K ranking with computational costs by\nlimiting the number of documents each model re-ranks. However, the cascading\napproach is not the only way models can interact to form a retrieval system.\n  We propose the concept of compound retrieval systems as a broader class of\nretrieval systems that apply multiple prediction models. This encapsulates\ncascading models but also allows other types of interactions than top-K\nre-ranking. In particular, we enable interactions with large language models\n(LLMs) which can provide relative relevance comparisons. We focus on the\noptimization of compound retrieval system design which uniquely involves\nlearning where to apply the component models and how to aggregate their\npredictions into a final ranking. This work shows how our compound approach can\ncombine the classic BM25 retrieval model with state-of-the-art (pairwise) LLM\nrelevance predictions, while optimizing a given ranking metric and efficiency\ntarget. Our experimental results show optimized compound retrieval systems\nprovide better trade-offs between effectiveness and efficiency than cascading\napproaches, even when applied in a self-supervised manner.\n  With the introduction of compound retrieval systems, we hope to inspire the\ninformation retrieval field to more out-of-the-box thinking on how prediction\nmodels can interact to form rankings."
                },
                "authors": [
                    {
                        "name": "Harrie Oosterhuis"
                    },
                    {
                        "name": "Rolf Jagerman"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Xuanhui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanhui Wang"
                },
                "author": "Xuanhui Wang",
                "arxiv_doi": "10.1145/3726302.3730051",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730051",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.12063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SIGIR 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10149v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10149v2",
                "updated": "2025-04-16T13:16:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    13,
                    16,
                    19,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-14T12:00:00Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    0,
                    0,
                    0,
                    104,
                    0
                ],
                "title": "BoTTA: Benchmarking on-device Test Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BoTTA: Benchmarking on-device Test Time Adaptation"
                },
                "summary": "The performance of deep learning models depends heavily on test samples at\nruntime, and shifts from the training data distribution can significantly\nreduce accuracy. Test-time adaptation (TTA) addresses this by adapting models\nduring inference without requiring labeled test data or access to the original\ntraining set. While research has explored TTA from various perspectives like\nalgorithmic complexity, data and class distribution shifts, model\narchitectures, and offline versus continuous learning, constraints specific to\nmobile and edge devices remain underexplored. We propose BoTTA, a benchmark\ndesigned to evaluate TTA methods under practical constraints on mobile and edge\ndevices. Our evaluation targets four key challenges caused by limited resources\nand usage conditions: (i) limited test samples, (ii) limited exposure to\ncategories, (iii) diverse distribution shifts, and (iv) overlapping shifts\nwithin a sample. We assess state-of-the-art TTA methods under these scenarios\nusing benchmark datasets and report system-level metrics on a real testbed.\nFurthermore, unlike prior work, we align with on-device requirements by\nadvocating periodic adaptation instead of continuous inference-time adaptation.\nExperiments reveal key insights: many recent TTA algorithms struggle with small\ndatasets, fail to generalize to unseen categories, and depend on the diversity\nand complexity of distribution shifts. BoTTA also reports device-specific\nresource use. For example, while SHOT improves accuracy by $2.25\\times$ with\n$512$ adaptation samples, it uses $1.08\\times$ peak memory on Raspberry Pi\nversus the base model. BoTTA offers actionable guidance for TTA in real-world,\nresource-constrained deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of deep learning models depends heavily on test samples at\nruntime, and shifts from the training data distribution can significantly\nreduce accuracy. Test-time adaptation (TTA) addresses this by adapting models\nduring inference without requiring labeled test data or access to the original\ntraining set. While research has explored TTA from various perspectives like\nalgorithmic complexity, data and class distribution shifts, model\narchitectures, and offline versus continuous learning, constraints specific to\nmobile and edge devices remain underexplored. We propose BoTTA, a benchmark\ndesigned to evaluate TTA methods under practical constraints on mobile and edge\ndevices. Our evaluation targets four key challenges caused by limited resources\nand usage conditions: (i) limited test samples, (ii) limited exposure to\ncategories, (iii) diverse distribution shifts, and (iv) overlapping shifts\nwithin a sample. We assess state-of-the-art TTA methods under these scenarios\nusing benchmark datasets and report system-level metrics on a real testbed.\nFurthermore, unlike prior work, we align with on-device requirements by\nadvocating periodic adaptation instead of continuous inference-time adaptation.\nExperiments reveal key insights: many recent TTA algorithms struggle with small\ndatasets, fail to generalize to unseen categories, and depend on the diversity\nand complexity of distribution shifts. BoTTA also reports device-specific\nresource use. For example, while SHOT improves accuracy by $2.25\\times$ with\n$512$ adaptation samples, it uses $1.08\\times$ peak memory on Raspberry Pi\nversus the base model. BoTTA offers actionable guidance for TTA in real-world,\nresource-constrained deployments."
                },
                "authors": [
                    {
                        "name": "Michal Danilowski"
                    },
                    {
                        "name": "Soumyajit Chatterjee"
                    },
                    {
                        "name": "Abhirup Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Abhirup Ghosh"
                },
                "author": "Abhirup Ghosh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10149v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10149v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12048v1",
                "updated": "2025-04-16T13:04:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    13,
                    4,
                    1,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T13:04:01Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    13,
                    4,
                    1,
                    2,
                    106,
                    0
                ],
                "title": "Modular-Cam: Modular Dynamic Camera-view Video Generation with LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modular-Cam: Modular Dynamic Camera-view Video Generation with LLM"
                },
                "summary": "Text-to-Video generation, which utilizes the provided text prompt to generate\nhigh-quality videos, has drawn increasing attention and achieved great success\ndue to the development of diffusion models recently. Existing methods mainly\nrely on a pre-trained text encoder to capture the semantic information and\nperform cross attention with the encoded text prompt to guide the generation of\nvideo. However, when it comes to complex prompts that contain dynamic scenes\nand multiple camera-view transformations, these methods can not decompose the\noverall information into separate scenes, as well as fail to smoothly change\nscenes based on the corresponding camera-views. To solve these problems, we\npropose a novel method, i.e., Modular-Cam. Specifically, to better understand a\ngiven complex prompt, we utilize a large language model to analyze user\ninstructions and decouple them into multiple scenes together with transition\nactions. To generate a video containing dynamic scenes that match the given\ncamera-views, we incorporate the widely-used temporal transformer into the\ndiffusion model to ensure continuity within a single scene and propose\nCamOperator, a modular network based module that well controls the camera\nmovements. Moreover, we propose AdaControlNet, which utilizes ControlNet to\nensure consistency across scenes and adaptively adjusts the color tone of the\ngenerated video. Extensive qualitative and quantitative experiments prove our\nproposed Modular-Cam's strong capability of generating multi-scene videos\ntogether with its ability to achieve fine-grained control of camera movements.\nGenerated results are available at https://modular-cam.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-Video generation, which utilizes the provided text prompt to generate\nhigh-quality videos, has drawn increasing attention and achieved great success\ndue to the development of diffusion models recently. Existing methods mainly\nrely on a pre-trained text encoder to capture the semantic information and\nperform cross attention with the encoded text prompt to guide the generation of\nvideo. However, when it comes to complex prompts that contain dynamic scenes\nand multiple camera-view transformations, these methods can not decompose the\noverall information into separate scenes, as well as fail to smoothly change\nscenes based on the corresponding camera-views. To solve these problems, we\npropose a novel method, i.e., Modular-Cam. Specifically, to better understand a\ngiven complex prompt, we utilize a large language model to analyze user\ninstructions and decouple them into multiple scenes together with transition\nactions. To generate a video containing dynamic scenes that match the given\ncamera-views, we incorporate the widely-used temporal transformer into the\ndiffusion model to ensure continuity within a single scene and propose\nCamOperator, a modular network based module that well controls the camera\nmovements. Moreover, we propose AdaControlNet, which utilizes ControlNet to\nensure consistency across scenes and adaptively adjusts the color tone of the\ngenerated video. Extensive qualitative and quantitative experiments prove our\nproposed Modular-Cam's strong capability of generating multi-scene videos\ntogether with its ability to achieve fine-grained control of camera movements.\nGenerated results are available at https://modular-cam.github.io."
                },
                "authors": [
                    {
                        "name": "Zirui Pan"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Yipeng Zhang"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Kwan Man Cheng"
                    },
                    {
                        "name": "Yaofei Wu"
                    },
                    {
                        "name": "Wenwu Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Wenwu Zhu"
                },
                "author": "Wenwu Zhu",
                "arxiv_comment": "AAAI 2025 Poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14744v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14744v3",
                "updated": "2025-04-16T13:02:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    13,
                    2,
                    25,
                    2,
                    106,
                    0
                ],
                "published": "2024-08-27T02:45:26Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    45,
                    26,
                    1,
                    240,
                    0
                ],
                "title": "RSTeller: Scaling Up Visual Language Modeling in Remote Sensing with\n  Rich Linguistic Semantics from Openly Available Data and Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RSTeller: Scaling Up Visual Language Modeling in Remote Sensing with\n  Rich Linguistic Semantics from Openly Available Data and Large Language\n  Models"
                },
                "summary": "Abundant, well-annotated multimodal data in remote sensing are pivotal for\naligning complex visual remote sensing (RS) scenes with human language,\nenabling the development of specialized vision language models across diverse\nRS interpretation tasks. However, annotating RS images with rich linguistic\nsemantics at scale demands expertise in RS and substantial human labor, making\nit costly and often impractical. In this study, we propose a workflow that\nleverages large language models (LLMs) to generate multimodal datasets with\nsemantically rich captions at scale from plain OpenStreetMap (OSM) data for\nimages sourced from the Google Earth Engine (GEE) platform. This approach\nfacilitates the generation of paired remote sensing data and can be readily\nscaled up using openly available data. Within this framework, we present\nRSTeller, a multimodal dataset comprising over 1.3 million RS images, each\naccompanied by two descriptive captions. Extensive experiments demonstrate that\nRSTeller enhances the performance of multiple existing vision language models\nfor RS scene understanding through continual pre-training. Our methodology\nsignificantly reduces the manual effort and expertise needed for annotating\nremote sensing imagery while democratizing access to high-quality annotated\ndata. This advancement fosters progress in visual language modeling and\nencourages broader participation in remote sensing research and applications.\nThe RSTeller dataset is available at https://github.com/SlytherinGe/RSTeller.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abundant, well-annotated multimodal data in remote sensing are pivotal for\naligning complex visual remote sensing (RS) scenes with human language,\nenabling the development of specialized vision language models across diverse\nRS interpretation tasks. However, annotating RS images with rich linguistic\nsemantics at scale demands expertise in RS and substantial human labor, making\nit costly and often impractical. In this study, we propose a workflow that\nleverages large language models (LLMs) to generate multimodal datasets with\nsemantically rich captions at scale from plain OpenStreetMap (OSM) data for\nimages sourced from the Google Earth Engine (GEE) platform. This approach\nfacilitates the generation of paired remote sensing data and can be readily\nscaled up using openly available data. Within this framework, we present\nRSTeller, a multimodal dataset comprising over 1.3 million RS images, each\naccompanied by two descriptive captions. Extensive experiments demonstrate that\nRSTeller enhances the performance of multiple existing vision language models\nfor RS scene understanding through continual pre-training. Our methodology\nsignificantly reduces the manual effort and expertise needed for annotating\nremote sensing imagery while democratizing access to high-quality annotated\ndata. This advancement fosters progress in visual language modeling and\nencourages broader participation in remote sensing research and applications.\nThe RSTeller dataset is available at https://github.com/SlytherinGe/RSTeller."
                },
                "authors": [
                    {
                        "name": "Junyao Ge"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Yang Zheng"
                    },
                    {
                        "name": "Kaitai Guo"
                    },
                    {
                        "name": "Jimin Liang"
                    }
                ],
                "author_detail": {
                    "name": "Jimin Liang"
                },
                "author": "Jimin Liang",
                "arxiv_comment": "Submitted to ISPRS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14744v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14744v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.8; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12039v1",
                "updated": "2025-04-16T12:54:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    12,
                    54,
                    11,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T12:54:11Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    12,
                    54,
                    11,
                    2,
                    106,
                    0
                ],
                "title": "RadMamba: Efficient Human Activity Recognition through Radar-based\n  Micro-Doppler-Oriented Mamba State-Space Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RadMamba: Efficient Human Activity Recognition through Radar-based\n  Micro-Doppler-Oriented Mamba State-Space Model"
                },
                "summary": "Radar-based HAR has emerged as a promising alternative to conventional\nmonitoring approaches, such as wearable devices and camera-based systems, due\nto its unique privacy preservation and robustness advantages. However, existing\nsolutions based on convolutional and recurrent neural networks, although\neffective, are computationally demanding during deployment. This limits their\napplicability in scenarios with constrained resources or those requiring\nmultiple sensors. Advanced architectures, such as ViT and SSM architectures,\noffer improved modeling capabilities and have made efforts toward lightweight\ndesigns. However, their computational complexity remains relatively high. To\nleverage the strengths of transformer architectures while simultaneously\nenhancing accuracy and reducing computational complexity, this paper introduces\nRadMamba, a parameter-efficient, radar micro-Doppler-oriented Mamba SSM\nspecifically tailored for radar-based HAR. Across three diverse datasets,\nRadMamba matches the top-performing previous model's 99.8% classification\naccuracy on Dataset DIAT with only 1/400 of its parameters and equals the\nleading models' 92.0% accuracy on Dataset CI4R with merely 1/10 of their\nparameters. In scenarios with continuous sequences of actions evaluated on\nDataset UoG2020, RadMamba surpasses other models with significantly higher\nparameter counts by at least 3%, achieving this with only 6.7k parameters. Our\ncode is available at: https://github.com/lab-emi/AIRHAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radar-based HAR has emerged as a promising alternative to conventional\nmonitoring approaches, such as wearable devices and camera-based systems, due\nto its unique privacy preservation and robustness advantages. However, existing\nsolutions based on convolutional and recurrent neural networks, although\neffective, are computationally demanding during deployment. This limits their\napplicability in scenarios with constrained resources or those requiring\nmultiple sensors. Advanced architectures, such as ViT and SSM architectures,\noffer improved modeling capabilities and have made efforts toward lightweight\ndesigns. However, their computational complexity remains relatively high. To\nleverage the strengths of transformer architectures while simultaneously\nenhancing accuracy and reducing computational complexity, this paper introduces\nRadMamba, a parameter-efficient, radar micro-Doppler-oriented Mamba SSM\nspecifically tailored for radar-based HAR. Across three diverse datasets,\nRadMamba matches the top-performing previous model's 99.8% classification\naccuracy on Dataset DIAT with only 1/400 of its parameters and equals the\nleading models' 92.0% accuracy on Dataset CI4R with merely 1/10 of their\nparameters. In scenarios with continuous sequences of actions evaluated on\nDataset UoG2020, RadMamba surpasses other models with significantly higher\nparameter counts by at least 3%, achieving this with only 6.7k parameters. Our\ncode is available at: https://github.com/lab-emi/AIRHAR."
                },
                "authors": [
                    {
                        "name": "Yizhuo Wu"
                    },
                    {
                        "name": "Francesco Fioranelli"
                    },
                    {
                        "name": "Chang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Chang Gao"
                },
                "author": "Chang Gao",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17461v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17461v3",
                "updated": "2025-04-16T12:51:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    12,
                    51,
                    11,
                    2,
                    106,
                    0
                ],
                "published": "2024-11-26T14:28:25Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    28,
                    25,
                    1,
                    331,
                    0
                ],
                "title": "SoK: Decentralized AI (DeAI)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: Decentralized AI (DeAI)"
                },
                "summary": "Centralization enhances the efficiency of Artificial Intelligence (AI), but\nit also brings critical challenges, such as single points of failure, inherent\nbiases, data privacy concerns, and scalability issues, for AI systems. These\nproblems are especially common in closed-source large language models (LLMs),\nwhere user data is collected and used with full transparency. To address these\nissues, blockchain-based decentralized AI (DeAI) has been introduced. DeAI\nleverages the strengths of blockchain technologies to enhance the transparency,\nsecurity, decentralization, as well as trustworthiness of AI systems. Although\nDeAI has been widely developed in industry, a comprehensive understanding of\nstate-of-the-art practical DeAI solutions is still lacking. In this work, we\npresent a Systematization of Knowledge (SoK) for blockchain-based DeAI\nsolutions. We propose a taxonomy to classify existing DeAI protocols based on\nthe model lifecycle. Based on this taxonomy, we provide a structured way to\nclarify the landscape of DeAI protocols and identify their similarities and\ndifferences. Specifically, we analyze the functionalities of blockchain in\nDeAI, investigate how blockchain features contribute to enhancing the security,\ntransparency, and trustworthiness of AI processes, and also ensure fair\nincentives for AI data and model contributors. In addition, we provide key\ninsights and research gaps in developing DeAI protocols for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Centralization enhances the efficiency of Artificial Intelligence (AI), but\nit also brings critical challenges, such as single points of failure, inherent\nbiases, data privacy concerns, and scalability issues, for AI systems. These\nproblems are especially common in closed-source large language models (LLMs),\nwhere user data is collected and used with full transparency. To address these\nissues, blockchain-based decentralized AI (DeAI) has been introduced. DeAI\nleverages the strengths of blockchain technologies to enhance the transparency,\nsecurity, decentralization, as well as trustworthiness of AI systems. Although\nDeAI has been widely developed in industry, a comprehensive understanding of\nstate-of-the-art practical DeAI solutions is still lacking. In this work, we\npresent a Systematization of Knowledge (SoK) for blockchain-based DeAI\nsolutions. We propose a taxonomy to classify existing DeAI protocols based on\nthe model lifecycle. Based on this taxonomy, we provide a structured way to\nclarify the landscape of DeAI protocols and identify their similarities and\ndifferences. Specifically, we analyze the functionalities of blockchain in\nDeAI, investigate how blockchain features contribute to enhancing the security,\ntransparency, and trustworthiness of AI processes, and also ensure fair\nincentives for AI data and model contributors. In addition, we provide key\ninsights and research gaps in developing DeAI protocols for future research."
                },
                "authors": [
                    {
                        "name": "Zhipeng Wang"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Elizabeth Lui"
                    },
                    {
                        "name": "Vatsal Shah"
                    },
                    {
                        "name": "Xihan Xiong"
                    },
                    {
                        "name": "Jiahao Sun"
                    },
                    {
                        "name": "Davide Crapis"
                    },
                    {
                        "name": "William Knottenbelt"
                    }
                ],
                "author_detail": {
                    "name": "William Knottenbelt"
                },
                "author": "William Knottenbelt",
                "arxiv_comment": "This is a Systematization of Knowledge (SoK) for the rapidly evolving\n  field of Decentralized AI (DeAI). We welcome valuable comments, suggestions,\n  and collaboration to further refine and enhance this work. We hope our\n  contribution will help accelerate the advancement of DeAI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17461v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17461v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12034v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12034v1",
                "updated": "2025-04-16T12:48:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    12,
                    48,
                    0,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T12:48:00Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    12,
                    48,
                    0,
                    2,
                    106,
                    0
                ],
                "title": "OpDiffer: LLM-Assisted Opcode-Level Differential Testing of Ethereum\n  Virtual Machine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpDiffer: LLM-Assisted Opcode-Level Differential Testing of Ethereum\n  Virtual Machine"
                },
                "summary": "As Ethereum continues to thrive, the Ethereum Virtual Machine (EVM) has\nbecome the cornerstone powering tens of millions of active smart contracts.\nIntuitively, security issues in EVMs could lead to inconsistent behaviors among\nsmart contracts or even denial-of-service of the entire blockchain network.\nHowever, to the best of our knowledge, only a limited number of studies focus\non the security of EVMs. Moreover, they suffer from 1) insufficient test input\ndiversity and invalid semantics; and 2) the inability to automatically identify\nbugs and locate root causes. To bridge this gap, we propose OpDiffer, a\ndifferential testing framework for EVM, which takes advantage of LLMs and\nstatic analysis methods to address the above two limitations. We conducted the\nlargest-scale evaluation, covering nine EVMs and uncovering 26 previously\nunknown bugs, 22 of which have been confirmed by developers and three have been\nassigned CNVD IDs. Compared to state-of-the-art baselines, OpDiffer can improve\ncode coverage by at most 71.06%, 148.40% and 655.56%, respectively. Through an\nanalysis of real-world deployed Ethereum contracts, we estimate that 7.21% of\nthe contracts could trigger our identified EVM bugs under certain environmental\nsettings, potentially resulting in severe negative impact on the Ethereum\necosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Ethereum continues to thrive, the Ethereum Virtual Machine (EVM) has\nbecome the cornerstone powering tens of millions of active smart contracts.\nIntuitively, security issues in EVMs could lead to inconsistent behaviors among\nsmart contracts or even denial-of-service of the entire blockchain network.\nHowever, to the best of our knowledge, only a limited number of studies focus\non the security of EVMs. Moreover, they suffer from 1) insufficient test input\ndiversity and invalid semantics; and 2) the inability to automatically identify\nbugs and locate root causes. To bridge this gap, we propose OpDiffer, a\ndifferential testing framework for EVM, which takes advantage of LLMs and\nstatic analysis methods to address the above two limitations. We conducted the\nlargest-scale evaluation, covering nine EVMs and uncovering 26 previously\nunknown bugs, 22 of which have been confirmed by developers and three have been\nassigned CNVD IDs. Compared to state-of-the-art baselines, OpDiffer can improve\ncode coverage by at most 71.06%, 148.40% and 655.56%, respectively. Through an\nanalysis of real-world deployed Ethereum contracts, we estimate that 7.21% of\nthe contracts could trigger our identified EVM bugs under certain environmental\nsettings, potentially resulting in severe negative impact on the Ethereum\necosystem."
                },
                "authors": [
                    {
                        "name": "Jie Ma"
                    },
                    {
                        "name": "Ningyu He"
                    },
                    {
                        "name": "Jinwen Xi"
                    },
                    {
                        "name": "Mingzhe Xing"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Ying Gao"
                    },
                    {
                        "name": "Yinliang Yue"
                    }
                ],
                "author_detail": {
                    "name": "Yinliang Yue"
                },
                "author": "Yinliang Yue",
                "arxiv_doi": "10.1145/3728946",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3728946",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.12034v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12034v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To appear in ISSTA'25",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12016v1",
                "updated": "2025-04-16T12:16:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    12,
                    16,
                    10,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T12:16:10Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    12,
                    16,
                    10,
                    2,
                    106,
                    0
                ],
                "title": "Active Human Feedback Collection via Neural Contextual Dueling Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Human Feedback Collection via Neural Contextual Dueling Bandits"
                },
                "summary": "Collecting human preference feedback is often expensive, leading recent works\nto develop principled algorithms to select them more efficiently. However,\nthese works assume that the underlying reward function is linear, an assumption\nthat does not hold in many real-life applications, such as online\nrecommendation and LLM alignment. To address this limitation, we propose\nNeural-ADB, an algorithm based on the neural contextual dueling bandit\nframework that provides a principled and practical method for collecting human\npreference feedback when the underlying latent reward function is non-linear.\nWe theoretically show that when preference feedback follows the\nBradley-Terry-Luce model, the worst sub-optimality gap of the policy learned by\nNeural-ADB decreases at a sub-linear rate as the preference dataset increases.\nOur experimental results on problem instances derived from synthetic preference\ndatasets further validate the effectiveness of Neural-ADB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collecting human preference feedback is often expensive, leading recent works\nto develop principled algorithms to select them more efficiently. However,\nthese works assume that the underlying reward function is linear, an assumption\nthat does not hold in many real-life applications, such as online\nrecommendation and LLM alignment. To address this limitation, we propose\nNeural-ADB, an algorithm based on the neural contextual dueling bandit\nframework that provides a principled and practical method for collecting human\npreference feedback when the underlying latent reward function is non-linear.\nWe theoretically show that when preference feedback follows the\nBradley-Terry-Luce model, the worst sub-optimality gap of the policy learned by\nNeural-ADB decreases at a sub-linear rate as the preference dataset increases.\nOur experimental results on problem instances derived from synthetic preference\ndatasets further validate the effectiveness of Neural-ADB."
                },
                "authors": [
                    {
                        "name": "Arun Verma"
                    },
                    {
                        "name": "Xiaoqiang Lin"
                    },
                    {
                        "name": "Zhongxiang Dai"
                    },
                    {
                        "name": "Daniela Rus"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Kian Hsiang Low"
                },
                "author": "Bryan Kian Hsiang Low",
                "arxiv_comment": "Accepted at ICLR 2025 Workshop on Bidirectional Human-AI Alignment\n  (BiAlign)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12012v1",
                "updated": "2025-04-16T12:13:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    12,
                    13,
                    2,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T12:13:02Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    12,
                    13,
                    2,
                    2,
                    106,
                    0
                ],
                "title": "Purposefully Induced Psychosis (PIP): Embracing Hallucination as\n  Imagination in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purposefully Induced Psychosis (PIP): Embracing Hallucination as\n  Imagination in Large Language Models"
                },
                "summary": "Hallucinations in Large Language Models (LLMs) are widely regarded as errors\n- outputs that deviate from factual accuracy. However, in creative or\nexploratory contexts, these \"mistakes\" may represent unexpected avenues for\ninnovation. We introduce Purposefully Induced Psychosis (PIP), a novel approach\nthat amplifies LLM hallucinations for imaginative tasks such as speculative\nfiction, interactive storytelling, and mixed-reality simulations. Drawing on\nHerman Melville's Moby-Dick, where Pip's \"madness\" reveals profound insight, we\nreframe hallucinations as a source of computational imagination rather than a\nflaw. Our method fine-tunes LLMs to encourage speculative, metaphorical, and\nsurreal outputs - hallucinations that are useful when factual accuracy is not\nthe chief objective. Inspired by the consensual illusions of theater and stage\nmagic, PIP situates these creative missteps in contexts where users willingly\nsuspend disbelief, thereby transforming \"errors\" into catalysts for new ways of\nthinking. We discuss potential applications, design principles for ensuring\nuser consent, preliminary observations, and implications for broader AI ethics\nand human-AI collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations in Large Language Models (LLMs) are widely regarded as errors\n- outputs that deviate from factual accuracy. However, in creative or\nexploratory contexts, these \"mistakes\" may represent unexpected avenues for\ninnovation. We introduce Purposefully Induced Psychosis (PIP), a novel approach\nthat amplifies LLM hallucinations for imaginative tasks such as speculative\nfiction, interactive storytelling, and mixed-reality simulations. Drawing on\nHerman Melville's Moby-Dick, where Pip's \"madness\" reveals profound insight, we\nreframe hallucinations as a source of computational imagination rather than a\nflaw. Our method fine-tunes LLMs to encourage speculative, metaphorical, and\nsurreal outputs - hallucinations that are useful when factual accuracy is not\nthe chief objective. Inspired by the consensual illusions of theater and stage\nmagic, PIP situates these creative missteps in contexts where users willingly\nsuspend disbelief, thereby transforming \"errors\" into catalysts for new ways of\nthinking. We discuss potential applications, design principles for ensuring\nuser consent, preliminary observations, and implications for broader AI ethics\nand human-AI collaboration."
                },
                "authors": [
                    {
                        "name": "Kris Pilcher"
                    },
                    {
                        "name": "Esen K. Ttnc"
                    }
                ],
                "author_detail": {
                    "name": "Esen K. Ttnc"
                },
                "author": "Esen K. Ttnc",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12007v1",
                "updated": "2025-04-16T12:01:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    12,
                    1,
                    3,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T12:01:03Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    12,
                    1,
                    3,
                    2,
                    106,
                    0
                ],
                "title": "Generative Recommendation with Continuous-Token Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Recommendation with Continuous-Token Diffusion"
                },
                "summary": "In recent years, there has been a significant trend toward using large\nlanguage model (LLM)-based recommender systems (RecSys). Current research\nprimarily focuses on representing complex user-item interactions within a\ndiscrete space to align with the inherent discrete nature of language models.\nHowever, this approach faces limitations due to its discrete nature: (i)\ninformation is often compressed during discretization; (ii) the tokenization\nand generation for the vast number of users and items in real-world scenarios\nare constrained by a limited vocabulary. Embracing continuous data presents a\npromising alternative to enhance expressive capabilities, though this approach\nis still in its early stages. To address this gap, we propose a novel\nframework, DeftRec, which incorporates \\textbf{de}noising di\\textbf{f}fusion\nmodels to enable LLM-based RecSys to seamlessly support continuous\n\\textbf{t}oken as input and target. First, we introduce a robust tokenizer with\na masking operation and an additive K-way architecture to index users and\nitems, capturing their complex collaborative relationships into continuous\ntokens. Crucially, we develop a denoising diffusion model to process user\npreferences within continuous domains by conditioning on reasoning content from\npre-trained large language model. During the denoising process, we reformulate\nthe objective to include negative interactions, building a comprehensive\nunderstanding of user preferences for effective and accurate recommendation\ngeneration. Finally, given a continuous token as output, recommendations can be\neasily generated through score-based retrieval. Extensive experiments\ndemonstrate the effectiveness of the proposed methods, showing that DeftRec\nsurpasses competitive benchmarks, including both traditional and emerging\nLLM-based RecSys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, there has been a significant trend toward using large\nlanguage model (LLM)-based recommender systems (RecSys). Current research\nprimarily focuses on representing complex user-item interactions within a\ndiscrete space to align with the inherent discrete nature of language models.\nHowever, this approach faces limitations due to its discrete nature: (i)\ninformation is often compressed during discretization; (ii) the tokenization\nand generation for the vast number of users and items in real-world scenarios\nare constrained by a limited vocabulary. Embracing continuous data presents a\npromising alternative to enhance expressive capabilities, though this approach\nis still in its early stages. To address this gap, we propose a novel\nframework, DeftRec, which incorporates \\textbf{de}noising di\\textbf{f}fusion\nmodels to enable LLM-based RecSys to seamlessly support continuous\n\\textbf{t}oken as input and target. First, we introduce a robust tokenizer with\na masking operation and an additive K-way architecture to index users and\nitems, capturing their complex collaborative relationships into continuous\ntokens. Crucially, we develop a denoising diffusion model to process user\npreferences within continuous domains by conditioning on reasoning content from\npre-trained large language model. During the denoising process, we reformulate\nthe objective to include negative interactions, building a comprehensive\nunderstanding of user preferences for effective and accurate recommendation\ngeneration. Finally, given a continuous token as output, recommendations can be\neasily generated through score-based retrieval. Extensive experiments\ndemonstrate the effectiveness of the proposed methods, showing that DeftRec\nsurpasses competitive benchmarks, including both traditional and emerging\nLLM-based RecSys."
                },
                "authors": [
                    {
                        "name": "Haohao Qu"
                    },
                    {
                        "name": "Wenqi Fan"
                    },
                    {
                        "name": "Shanru Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shanru Lin"
                },
                "author": "Shanru Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11986v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11986v1",
                "updated": "2025-04-16T11:27:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    11,
                    27,
                    47,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T11:27:47Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    11,
                    27,
                    47,
                    2,
                    106,
                    0
                ],
                "title": "Language Models as Quasi-Crystalline Thought: Structure, Constraint, and\n  Emergence in Generative Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models as Quasi-Crystalline Thought: Structure, Constraint, and\n  Emergence in Generative Systems"
                },
                "summary": "This essay proposes an analogy between large language models (LLMs) and\nquasicrystals: systems that exhibit global coherence without periodic\nrepetition and that are generated through local constraints. While LLMs are\noften evaluated in terms of predictive accuracy, factuality, or alignment, this\nstructural perspective suggests that their most characteristic behavior is the\nproduction of internally resonant linguistic patterns. Just as quasicrystals\nforced a redefinition of order in physical systems, viewing LLMs as generators\nof quasi-structured language opens new paths for evaluation and design:\nprivileging propagation of constraint over token-level accuracy, and coherence\nof form over fixed meaning. LLM outputs should be read not only for what they\nsay, but for the patterns of constraint and coherence that organize them. This\nshift reframes generative language as a space of emergent patterning: LLMs are\nneither fully random nor strictly rule-based, but defined by a logic of\nconstraint, resonance, and structural depth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This essay proposes an analogy between large language models (LLMs) and\nquasicrystals: systems that exhibit global coherence without periodic\nrepetition and that are generated through local constraints. While LLMs are\noften evaluated in terms of predictive accuracy, factuality, or alignment, this\nstructural perspective suggests that their most characteristic behavior is the\nproduction of internally resonant linguistic patterns. Just as quasicrystals\nforced a redefinition of order in physical systems, viewing LLMs as generators\nof quasi-structured language opens new paths for evaluation and design:\nprivileging propagation of constraint over token-level accuracy, and coherence\nof form over fixed meaning. LLM outputs should be read not only for what they\nsay, but for the patterns of constraint and coherence that organize them. This\nshift reframes generative language as a space of emergent patterning: LLMs are\nneither fully random nor strictly rule-based, but defined by a logic of\nconstraint, resonance, and structural depth."
                },
                "authors": [
                    {
                        "name": "Jose Manuel Guevara-Vela"
                    }
                ],
                "author_detail": {
                    "name": "Jose Manuel Guevara-Vela"
                },
                "author": "Jose Manuel Guevara-Vela",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11986v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09310v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09310v2",
                "updated": "2025-04-16T11:25:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    11,
                    25,
                    54,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-12T19:05:00Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    19,
                    5,
                    0,
                    5,
                    102,
                    0
                ],
                "title": "Conformal Calibration: Ensuring the Reliability of Black-Box AI in\n  Wireless Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Calibration: Ensuring the Reliability of Black-Box AI in\n  Wireless Systems"
                },
                "summary": "AI is poised to revolutionize telecommunication networks by boosting\nefficiency, automation, and decision-making. However, the black-box nature of\nmost AI models introduces substantial risk, possibly deterring adoption by\nnetwork operators. These risks are not addressed by the current prevailing\ndeployment strategy, which typically follows a best-effort train-and-deploy\nparadigm. This paper reviews conformal calibration, a general framework that\nmoves beyond the state of the art by adopting computationally lightweight,\nadvanced statistical tools that offer formal reliability guarantees without\nrequiring further training or fine-tuning. Conformal calibration encompasses\npre-deployment calibration via uncertainty quantification or hyperparameter\nselection; online monitoring to detect and mitigate failures in real time; and\ncounterfactual post-deployment performance analysis to address \"what if\"\ndiagnostic questions after deployment. By weaving conformal calibration into\nthe AI model lifecycle, network operators can establish confidence in black-box\nAI models as a dependable enabling technology for wireless systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI is poised to revolutionize telecommunication networks by boosting\nefficiency, automation, and decision-making. However, the black-box nature of\nmost AI models introduces substantial risk, possibly deterring adoption by\nnetwork operators. These risks are not addressed by the current prevailing\ndeployment strategy, which typically follows a best-effort train-and-deploy\nparadigm. This paper reviews conformal calibration, a general framework that\nmoves beyond the state of the art by adopting computationally lightweight,\nadvanced statistical tools that offer formal reliability guarantees without\nrequiring further training or fine-tuning. Conformal calibration encompasses\npre-deployment calibration via uncertainty quantification or hyperparameter\nselection; online monitoring to detect and mitigate failures in real time; and\ncounterfactual post-deployment performance analysis to address \"what if\"\ndiagnostic questions after deployment. By weaving conformal calibration into\nthe AI model lifecycle, network operators can establish confidence in black-box\nAI models as a dependable enabling technology for wireless systems."
                },
                "authors": [
                    {
                        "name": "Osvaldo Simeone"
                    },
                    {
                        "name": "Sangwoo Park"
                    },
                    {
                        "name": "Matteo Zecchin"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Zecchin"
                },
                "author": "Matteo Zecchin",
                "arxiv_comment": "submitted for a journal publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09310v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09310v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20791v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20791v2",
                "updated": "2025-04-16T11:18:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    11,
                    18,
                    52,
                    2,
                    106,
                    0
                ],
                "published": "2025-02-28T07:16:09Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    7,
                    16,
                    9,
                    4,
                    59,
                    0
                ],
                "title": "Cyber Defense Reinvented: Large Language Models as Threat Intelligence\n  Copilots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyber Defense Reinvented: Large Language Models as Threat Intelligence\n  Copilots"
                },
                "summary": "The exponential growth of cyber threat knowledge, exemplified by the\nexpansion of databases such as MITRE-CVE and NVD, poses significant challenges\nfor cyber threat analysis. Security professionals are increasingly burdened by\nthe sheer volume and complexity of information, creating an urgent need for\neffective tools to navigate, synthesize, and act on large-scale data to counter\nevolving threats proactively. However, conventional threat intelligence tools\noften fail to scale with the dynamic nature of this data and lack the\nadaptability to support diverse threat intelligence tasks.\n  In this work, we introduce CYLENS, a cyber threat intelligence copilot\npowered by large language models (LLMs). CYLENS is designed to assist security\nprofessionals throughout the entire threat management lifecycle, supporting\nthreat attribution, contextualization, detection, correlation, prioritization,\nand remediation. To ensure domain expertise, CYLENS integrates knowledge from\n271,570 threat reports into its model parameters and incorporates six\nspecialized NLP modules to enhance reasoning capabilities. Furthermore, CYLENS\ncan be customized to meet the unique needs of different or ganizations,\nunderscoring its adaptability. Through extensive evaluations, we demonstrate\nthat CYLENS consistently outperforms industry-leading LLMs and state-of-the-art\ncybersecurity agents. By detailing its design, development, and evaluation,\nthis work provides a blueprint for leveraging LLMs to address complex,\ndata-intensive cybersecurity challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of cyber threat knowledge, exemplified by the\nexpansion of databases such as MITRE-CVE and NVD, poses significant challenges\nfor cyber threat analysis. Security professionals are increasingly burdened by\nthe sheer volume and complexity of information, creating an urgent need for\neffective tools to navigate, synthesize, and act on large-scale data to counter\nevolving threats proactively. However, conventional threat intelligence tools\noften fail to scale with the dynamic nature of this data and lack the\nadaptability to support diverse threat intelligence tasks.\n  In this work, we introduce CYLENS, a cyber threat intelligence copilot\npowered by large language models (LLMs). CYLENS is designed to assist security\nprofessionals throughout the entire threat management lifecycle, supporting\nthreat attribution, contextualization, detection, correlation, prioritization,\nand remediation. To ensure domain expertise, CYLENS integrates knowledge from\n271,570 threat reports into its model parameters and incorporates six\nspecialized NLP modules to enhance reasoning capabilities. Furthermore, CYLENS\ncan be customized to meet the unique needs of different or ganizations,\nunderscoring its adaptability. Through extensive evaluations, we demonstrate\nthat CYLENS consistently outperforms industry-leading LLMs and state-of-the-art\ncybersecurity agents. By detailing its design, development, and evaluation,\nthis work provides a blueprint for leveraging LLMs to address complex,\ndata-intensive cybersecurity challenges."
                },
                "authors": [
                    {
                        "name": "Xiaoqun Liu"
                    },
                    {
                        "name": "Jiacheng Liang"
                    },
                    {
                        "name": "Qiben Yan"
                    },
                    {
                        "name": "Jiyong Jang"
                    },
                    {
                        "name": "Sicheng Mao"
                    },
                    {
                        "name": "Muchao Ye"
                    },
                    {
                        "name": "Jinyuan Jia"
                    },
                    {
                        "name": "Zhaohan Xi"
                    }
                ],
                "author_detail": {
                    "name": "Zhaohan Xi"
                },
                "author": "Zhaohan Xi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20791v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20791v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11975v1",
                "updated": "2025-04-16T11:15:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    11,
                    15,
                    26,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T11:15:26Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    11,
                    15,
                    26,
                    2,
                    106,
                    0
                ],
                "title": "SemEval-2025 Task 3: Mu-SHROOM, the Multilingual Shared Task on\n  Hallucinations and Related Observable Overgeneration Mistakes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemEval-2025 Task 3: Mu-SHROOM, the Multilingual Shared Task on\n  Hallucinations and Related Observable Overgeneration Mistakes"
                },
                "summary": "We present the Mu-SHROOM shared task which is focused on detecting\nhallucinations and other overgeneration mistakes in the output of\ninstruction-tuned large language models (LLMs). Mu-SHROOM addresses\ngeneral-purpose LLMs in 14 languages, and frames the hallucination detection\nproblem as a span-labeling task. We received 2,618 submissions from 43\nparticipating teams employing diverse methodologies. The large number of\nsubmissions underscores the interest of the community in hallucination\ndetection. We present the results of the participating systems and conduct an\nempirical analysis to identify key factors contributing to strong performance\nin this task. We also emphasize relevant current challenges, notably the\nvarying degree of hallucinations across languages and the high annotator\ndisagreement when labeling hallucination spans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the Mu-SHROOM shared task which is focused on detecting\nhallucinations and other overgeneration mistakes in the output of\ninstruction-tuned large language models (LLMs). Mu-SHROOM addresses\ngeneral-purpose LLMs in 14 languages, and frames the hallucination detection\nproblem as a span-labeling task. We received 2,618 submissions from 43\nparticipating teams employing diverse methodologies. The large number of\nsubmissions underscores the interest of the community in hallucination\ndetection. We present the results of the participating systems and conduct an\nempirical analysis to identify key factors contributing to strong performance\nin this task. We also emphasize relevant current challenges, notably the\nvarying degree of hallucinations across languages and the high annotator\ndisagreement when labeling hallucination spans."
                },
                "authors": [
                    {
                        "name": "Ral Vzquez"
                    },
                    {
                        "name": "Timothee Mickus"
                    },
                    {
                        "name": "Elaine Zosa"
                    },
                    {
                        "name": "Teemu Vahtola"
                    },
                    {
                        "name": "Jrg Tiedemann"
                    },
                    {
                        "name": "Aman Sinha"
                    },
                    {
                        "name": "Vincent Segonne"
                    },
                    {
                        "name": "Fernando Snchez-Vega"
                    },
                    {
                        "name": "Alessandro Raganato"
                    },
                    {
                        "name": "Jindich Libovick"
                    },
                    {
                        "name": "Jussi Karlgren"
                    },
                    {
                        "name": "Shaoxiong Ji"
                    },
                    {
                        "name": "Jindich Helcl"
                    },
                    {
                        "name": "Liane Guillou"
                    },
                    {
                        "name": "Ona de Gibert"
                    },
                    {
                        "name": "Jaione Bengoetxea"
                    },
                    {
                        "name": "Joseph Attieh"
                    },
                    {
                        "name": "Marianna Apidianaki"
                    }
                ],
                "author_detail": {
                    "name": "Marianna Apidianaki"
                },
                "author": "Marianna Apidianaki",
                "arxiv_comment": "Mu-SHROOM is part of SemEval-2025 (Task 3). TBP: Proceedings of the\n  19th International Workshop on Semantic Evaluation (SemEval-2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11972v1",
                "updated": "2025-04-16T11:08:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    11,
                    8,
                    46,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T11:08:46Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    11,
                    8,
                    46,
                    2,
                    106,
                    0
                ],
                "title": "LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA"
                },
                "summary": "Extractive reading comprehension question answering (QA) datasets are\ntypically evaluated using Exact Match (EM) and F1-score, but these metrics\noften fail to fully capture model performance. With the success of large\nlanguage models (LLMs), they have been employed in various tasks, including\nserving as judges (LLM-as-a-judge). In this paper, we reassess the performance\nof QA models using LLM-as-a-judge across four reading comprehension QA\ndatasets. We examine different families of LLMs and various answer types to\nevaluate the effectiveness of LLM-as-a-judge in these tasks. Our results show\nthat LLM-as-a-judge is highly correlated with human judgments and can replace\ntraditional EM/F1 metrics. By using LLM-as-a-judge, the correlation with human\njudgments improves significantly, from 0.17 (EM) and 0.36 (F1-score) to 0.85.\nThese findings confirm that EM and F1 metrics underestimate the true\nperformance of the QA models. While LLM-as-a-judge is not perfect for more\ndifficult answer types (e.g., job), it still outperforms EM/F1, and we observe\nno bias issues, such as self-preference, when the same model is used for both\nthe QA and judgment tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extractive reading comprehension question answering (QA) datasets are\ntypically evaluated using Exact Match (EM) and F1-score, but these metrics\noften fail to fully capture model performance. With the success of large\nlanguage models (LLMs), they have been employed in various tasks, including\nserving as judges (LLM-as-a-judge). In this paper, we reassess the performance\nof QA models using LLM-as-a-judge across four reading comprehension QA\ndatasets. We examine different families of LLMs and various answer types to\nevaluate the effectiveness of LLM-as-a-judge in these tasks. Our results show\nthat LLM-as-a-judge is highly correlated with human judgments and can replace\ntraditional EM/F1 metrics. By using LLM-as-a-judge, the correlation with human\njudgments improves significantly, from 0.17 (EM) and 0.36 (F1-score) to 0.85.\nThese findings confirm that EM and F1 metrics underestimate the true\nperformance of the QA models. While LLM-as-a-judge is not perfect for more\ndifficult answer types (e.g., job), it still outperforms EM/F1, and we observe\nno bias issues, such as self-preference, when the same model is used for both\nthe QA and judgment tasks."
                },
                "authors": [
                    {
                        "name": "Xanh Ho"
                    },
                    {
                        "name": "Jiahao Huang"
                    },
                    {
                        "name": "Florian Boudin"
                    },
                    {
                        "name": "Akiko Aizawa"
                    }
                ],
                "author_detail": {
                    "name": "Akiko Aizawa"
                },
                "author": "Akiko Aizawa",
                "arxiv_comment": "17 pages; code and data are available at\n  https://github.com/Alab-NII/llm-judge-extract-qa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11952v1",
                "updated": "2025-04-16T10:29:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    10,
                    29,
                    30,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T10:29:30Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    10,
                    29,
                    30,
                    2,
                    106,
                    0
                ],
                "title": "Robust and Fine-Grained Detection of AI Generated Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust and Fine-Grained Detection of AI Generated Texts"
                },
                "summary": "An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts."
                },
                "authors": [
                    {
                        "name": "Ram Mohan Rao Kadiyala"
                    },
                    {
                        "name": "Siddartha Pullakhandam"
                    },
                    {
                        "name": "Kanwal Mehreen"
                    },
                    {
                        "name": "Drishti Sharma"
                    },
                    {
                        "name": "Siddhant Gupta"
                    },
                    {
                        "name": "Jebish Purbey"
                    },
                    {
                        "name": "Ashay Srivastava"
                    },
                    {
                        "name": "Subhasya TippaReddy"
                    },
                    {
                        "name": "Arvind Reddy Bobbili"
                    },
                    {
                        "name": "Suraj Telugara Chandrashekhar"
                    },
                    {
                        "name": "Modabbir Adeeb"
                    },
                    {
                        "name": "Srinadh Vura"
                    },
                    {
                        "name": "Hamza Farooq"
                    }
                ],
                "author_detail": {
                    "name": "Hamza Farooq"
                },
                "author": "Hamza Farooq",
                "arxiv_comment": "ACL 2025 Feb ARR Submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03104v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03104v3",
                "updated": "2025-04-16T10:18:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    10,
                    18,
                    1,
                    2,
                    106,
                    0
                ],
                "published": "2024-12-04T08:06:15Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    6,
                    15,
                    2,
                    339,
                    0
                ],
                "title": "ChatTS: Aligning Time Series with LLMs via Synthetic Data for Enhanced\n  Understanding and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatTS: Aligning Time Series with LLMs via Synthetic Data for Enhanced\n  Understanding and Reasoning"
                },
                "summary": "Understanding time series is crucial for its application in real-world\nscenarios. Recently, large language models (LLMs) have been increasingly\napplied to time series tasks, leveraging their strong language capabilities to\nenhance various applications. However, research on multimodal LLMs (MLLMs) for\ntime series understanding and reasoning remains limited, primarily due to the\nscarcity of high-quality datasets that align time series with textual\ninformation. This paper introduces ChatTS, a novel MLLM designed for time\nseries analysis. ChatTS treats time series as a modality, similar to how vision\nMLLMs process images, enabling it to perform both understanding and reasoning\nwith time series. To address the scarcity of training data, we propose an\nattribute-based method for generating synthetic time series with detailed\nattribute descriptions. We further introduce Time Series Evol-Instruct, a novel\napproach that generates diverse time series Q&As, enhancing the model's\nreasoning capabilities. To the best of our knowledge, ChatTS is the first\nTS-MLLM that takes multivariate time series as input for understanding and\nreasoning, which is fine-tuned exclusively on synthetic datasets. We evaluate\nits performance using benchmark datasets with real-world data, including six\nalignment tasks and four reasoning tasks. Our results show that ChatTS\nsignificantly outperforms existing vision-based MLLMs (e.g., GPT-4o) and\ntext/agent-based LLMs, achieving a 46.0% improvement in alignment tasks and a\n25.8% improvement in reasoning tasks. We have open-sourced the source code,\nmodel checkpoint and datasets at https://github.com/NetManAIOps/ChatTS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding time series is crucial for its application in real-world\nscenarios. Recently, large language models (LLMs) have been increasingly\napplied to time series tasks, leveraging their strong language capabilities to\nenhance various applications. However, research on multimodal LLMs (MLLMs) for\ntime series understanding and reasoning remains limited, primarily due to the\nscarcity of high-quality datasets that align time series with textual\ninformation. This paper introduces ChatTS, a novel MLLM designed for time\nseries analysis. ChatTS treats time series as a modality, similar to how vision\nMLLMs process images, enabling it to perform both understanding and reasoning\nwith time series. To address the scarcity of training data, we propose an\nattribute-based method for generating synthetic time series with detailed\nattribute descriptions. We further introduce Time Series Evol-Instruct, a novel\napproach that generates diverse time series Q&As, enhancing the model's\nreasoning capabilities. To the best of our knowledge, ChatTS is the first\nTS-MLLM that takes multivariate time series as input for understanding and\nreasoning, which is fine-tuned exclusively on synthetic datasets. We evaluate\nits performance using benchmark datasets with real-world data, including six\nalignment tasks and four reasoning tasks. Our results show that ChatTS\nsignificantly outperforms existing vision-based MLLMs (e.g., GPT-4o) and\ntext/agent-based LLMs, achieving a 46.0% improvement in alignment tasks and a\n25.8% improvement in reasoning tasks. We have open-sourced the source code,\nmodel checkpoint and datasets at https://github.com/NetManAIOps/ChatTS."
                },
                "authors": [
                    {
                        "name": "Zhe Xie"
                    },
                    {
                        "name": "Zeyan Li"
                    },
                    {
                        "name": "Xiao He"
                    },
                    {
                        "name": "Longlong Xu"
                    },
                    {
                        "name": "Xidao Wen"
                    },
                    {
                        "name": "Tieying Zhang"
                    },
                    {
                        "name": "Jianjun Chen"
                    },
                    {
                        "name": "Rui Shi"
                    },
                    {
                        "name": "Dan Pei"
                    }
                ],
                "author_detail": {
                    "name": "Dan Pei"
                },
                "author": "Dan Pei",
                "arxiv_comment": "accepted by VLDB' 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03104v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03104v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11934v1",
                "updated": "2025-04-16T10:14:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    10,
                    14,
                    27,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T10:14:27Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    10,
                    14,
                    27,
                    2,
                    106,
                    0
                ],
                "title": "An LLM-as-a-judge Approach for Scalable Gender-Neutral Translation\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM-as-a-judge Approach for Scalable Gender-Neutral Translation\n  Evaluation"
                },
                "summary": "Gender-neutral translation (GNT) aims to avoid expressing the gender of human\nreferents when the source text lacks explicit cues about the gender of those\nreferents. Evaluating GNT automatically is particularly challenging, with\ncurrent solutions being limited to monolingual classifiers. Such solutions are\nnot ideal because they do not factor in the source sentence and require\ndedicated data and fine-tuning to scale to new languages. In this work, we\naddress such limitations by investigating the use of large language models\n(LLMs) as evaluators of GNT. Specifically, we explore two prompting approaches:\none in which LLMs generate sentence-level assessments only, and another, akin\nto a chain-of-thought approach, where they first produce detailed phrase-level\nannotations before a sentence-level judgment. Through extensive experiments on\nmultiple languages with five models, both open and proprietary, we show that\nLLMs can serve as evaluators of GNT. Moreover, we find that prompting for\nphrase-level annotations before sentence-level assessments consistently\nimproves the accuracy of all models, providing a better and more scalable\nalternative to current solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gender-neutral translation (GNT) aims to avoid expressing the gender of human\nreferents when the source text lacks explicit cues about the gender of those\nreferents. Evaluating GNT automatically is particularly challenging, with\ncurrent solutions being limited to monolingual classifiers. Such solutions are\nnot ideal because they do not factor in the source sentence and require\ndedicated data and fine-tuning to scale to new languages. In this work, we\naddress such limitations by investigating the use of large language models\n(LLMs) as evaluators of GNT. Specifically, we explore two prompting approaches:\none in which LLMs generate sentence-level assessments only, and another, akin\nto a chain-of-thought approach, where they first produce detailed phrase-level\nannotations before a sentence-level judgment. Through extensive experiments on\nmultiple languages with five models, both open and proprietary, we show that\nLLMs can serve as evaluators of GNT. Moreover, we find that prompting for\nphrase-level annotations before sentence-level assessments consistently\nimproves the accuracy of all models, providing a better and more scalable\nalternative to current solutions."
                },
                "authors": [
                    {
                        "name": "Andrea Piergentili"
                    },
                    {
                        "name": "Beatrice Savoldi"
                    },
                    {
                        "name": "Matteo Negri"
                    },
                    {
                        "name": "Luisa Bentivogli"
                    }
                ],
                "author_detail": {
                    "name": "Luisa Bentivogli"
                },
                "author": "Luisa Bentivogli",
                "arxiv_comment": "Accepted at GITT 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11919v1",
                "updated": "2025-04-16T09:55:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    55,
                    34,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T09:55:34Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    55,
                    34,
                    2,
                    106,
                    0
                ],
                "title": "Rethinking the Generation of High-Quality CoT Data from the Perspective\n  of LLM-Adaptive Question Difficulty Grading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking the Generation of High-Quality CoT Data from the Perspective\n  of LLM-Adaptive Question Difficulty Grading"
                },
                "summary": "Recently, DeepSeek-R1 (671B) (DeepSeek-AIet al., 2025) has demonstrated its\nexcellent reasoning ability in complex tasks and has publiclyshared its\nmethodology. This provides potentially high-quality chain-of-thought (CoT) data\nfor stimulating the reasoning abilities of small-sized large language models\n(LLMs). To generate high-quality CoT data for different LLMs, we seek an\nefficient method for generating high-quality CoT data with LLM-Adaptive\nquestiondifficulty levels. First, we grade the difficulty of the questions\naccording to the reasoning ability of the LLMs themselves and construct a\nLLM-Adaptive question database. Second, we sample the problem database based on\na distribution of difficulty levels of the questions and then use DeepSeek-R1\n(671B) (DeepSeek-AI et al., 2025) to generate the corresponding high-quality\nCoT data with correct answers. Thanks to the construction of CoT data with\nLLM-Adaptive difficulty levels, we have significantly reduced the cost of data\ngeneration and enhanced the efficiency of model supervised fine-tuning (SFT).\nFinally, we have validated the effectiveness and generalizability of the\nproposed method in the fields of complex mathematical competitions and code\ngeneration tasks. Notably, with only 2k high-quality mathematical CoT data, our\nZMath-32B surpasses DeepSeek-Distill-32B in math reasoning task. Similarly,\nwith only 2k high-quality code CoT data, our ZCode-32B surpasses\nDeepSeek-Distill-32B in code reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, DeepSeek-R1 (671B) (DeepSeek-AIet al., 2025) has demonstrated its\nexcellent reasoning ability in complex tasks and has publiclyshared its\nmethodology. This provides potentially high-quality chain-of-thought (CoT) data\nfor stimulating the reasoning abilities of small-sized large language models\n(LLMs). To generate high-quality CoT data for different LLMs, we seek an\nefficient method for generating high-quality CoT data with LLM-Adaptive\nquestiondifficulty levels. First, we grade the difficulty of the questions\naccording to the reasoning ability of the LLMs themselves and construct a\nLLM-Adaptive question database. Second, we sample the problem database based on\na distribution of difficulty levels of the questions and then use DeepSeek-R1\n(671B) (DeepSeek-AI et al., 2025) to generate the corresponding high-quality\nCoT data with correct answers. Thanks to the construction of CoT data with\nLLM-Adaptive difficulty levels, we have significantly reduced the cost of data\ngeneration and enhanced the efficiency of model supervised fine-tuning (SFT).\nFinally, we have validated the effectiveness and generalizability of the\nproposed method in the fields of complex mathematical competitions and code\ngeneration tasks. Notably, with only 2k high-quality mathematical CoT data, our\nZMath-32B surpasses DeepSeek-Distill-32B in math reasoning task. Similarly,\nwith only 2k high-quality code CoT data, our ZCode-32B surpasses\nDeepSeek-Distill-32B in code reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Qianjin Yu"
                    },
                    {
                        "name": "Keyu Wu"
                    },
                    {
                        "name": "Zihan Chen"
                    },
                    {
                        "name": "Chushu Zhang"
                    },
                    {
                        "name": "Manlin Mei"
                    },
                    {
                        "name": "Lingjun Huang"
                    },
                    {
                        "name": "Fang Tan"
                    },
                    {
                        "name": "Yongsheng Du"
                    },
                    {
                        "name": "Kunlin Liu"
                    },
                    {
                        "name": "Yurui Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yurui Zhu"
                },
                "author": "Yurui Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01329v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01329v2",
                "updated": "2025-04-16T09:54:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    54,
                    20,
                    2,
                    106,
                    0
                ],
                "published": "2025-03-03T09:12:14Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    12,
                    14,
                    0,
                    62,
                    0
                ],
                "title": "Neural ODE Transformers: Analyzing Internal Dynamics and Adaptive\n  Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural ODE Transformers: Analyzing Internal Dynamics and Adaptive\n  Fine-tuning"
                },
                "summary": "Recent advancements in large language models (LLMs) based on transformer\narchitectures have sparked significant interest in understanding their inner\nworkings. In this paper, we introduce a novel approach to modeling transformer\narchitectures using highly flexible non-autonomous neural ordinary differential\nequations (ODEs). Our proposed model parameterizes all weights of attention and\nfeed-forward blocks through neural networks, expressing these weights as\nfunctions of a continuous layer index. Through spectral analysis of the model's\ndynamics, we uncover an increase in eigenvalue magnitude that challenges the\nweight-sharing assumption prevalent in existing theoretical studies. We also\nleverage the Lyapunov exponent to examine token-level sensitivity, enhancing\nmodel interpretability. Our neural ODE transformer demonstrates performance\ncomparable to or better than vanilla transformers across various configurations\nand datasets, while offering flexible fine-tuning capabilities that can adapt\nto different architectural constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) based on transformer\narchitectures have sparked significant interest in understanding their inner\nworkings. In this paper, we introduce a novel approach to modeling transformer\narchitectures using highly flexible non-autonomous neural ordinary differential\nequations (ODEs). Our proposed model parameterizes all weights of attention and\nfeed-forward blocks through neural networks, expressing these weights as\nfunctions of a continuous layer index. Through spectral analysis of the model's\ndynamics, we uncover an increase in eigenvalue magnitude that challenges the\nweight-sharing assumption prevalent in existing theoretical studies. We also\nleverage the Lyapunov exponent to examine token-level sensitivity, enhancing\nmodel interpretability. Our neural ODE transformer demonstrates performance\ncomparable to or better than vanilla transformers across various configurations\nand datasets, while offering flexible fine-tuning capabilities that can adapt\nto different architectural constraints."
                },
                "authors": [
                    {
                        "name": "Anh Tong"
                    },
                    {
                        "name": "Thanh Nguyen-Tang"
                    },
                    {
                        "name": "Dongeun Lee"
                    },
                    {
                        "name": "Duc Nguyen"
                    },
                    {
                        "name": "Toan Tran"
                    },
                    {
                        "name": "David Hall"
                    },
                    {
                        "name": "Cheongwoong Kang"
                    },
                    {
                        "name": "Jaesik Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jaesik Choi"
                },
                "author": "Jaesik Choi",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01329v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01329v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07826v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07826v2",
                "updated": "2025-04-16T09:51:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    51,
                    33,
                    2,
                    106,
                    0
                ],
                "published": "2024-11-12T14:22:16Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    14,
                    22,
                    16,
                    1,
                    317,
                    0
                ],
                "title": "Efficient Federated Finetuning of Tiny Transformers with\n  Resource-Constrained Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Federated Finetuning of Tiny Transformers with\n  Resource-Constrained Devices"
                },
                "summary": "In recent years, Large Language Models (LLMs) through Transformer structures\nhave dominated many machine learning tasks, especially text processing.\nHowever, these models require massive amounts of data for training and induce\nhigh resource requirements, particularly in terms of the large number of\nFloating Point Operations (FLOPs) and the high amounts of memory needed. To\nfine-tune such a model in a parameter-efficient way, techniques like Adapter or\nLoRA have been developed. However, we observe that the application of LoRA,\nwhen used in federated learning (FL), while still being parameter-efficient, is\nmemory and FLOP inefficient. Based on that observation, we develop a novel\nlayer finetuning scheme that allows devices in cross-device FL to make use of\npretrained neural networks (NNs) while adhering to given resource constraints.\nWe show that our presented scheme outperforms the current state of the art when\ndealing with homogeneous or heterogeneous computation and memory constraints\nand is on par with LoRA regarding limited communication, thereby achieving\nsignificantly higher accuracies in FL training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) through Transformer structures\nhave dominated many machine learning tasks, especially text processing.\nHowever, these models require massive amounts of data for training and induce\nhigh resource requirements, particularly in terms of the large number of\nFloating Point Operations (FLOPs) and the high amounts of memory needed. To\nfine-tune such a model in a parameter-efficient way, techniques like Adapter or\nLoRA have been developed. However, we observe that the application of LoRA,\nwhen used in federated learning (FL), while still being parameter-efficient, is\nmemory and FLOP inefficient. Based on that observation, we develop a novel\nlayer finetuning scheme that allows devices in cross-device FL to make use of\npretrained neural networks (NNs) while adhering to given resource constraints.\nWe show that our presented scheme outperforms the current state of the art when\ndealing with homogeneous or heterogeneous computation and memory constraints\nand is on par with LoRA regarding limited communication, thereby achieving\nsignificantly higher accuracies in FL training."
                },
                "authors": [
                    {
                        "name": "Kilian Pfeiffer"
                    },
                    {
                        "name": "Mohamed Aboelenien Ahmed"
                    },
                    {
                        "name": "Ramin Khalili"
                    },
                    {
                        "name": "Jrg Henkel"
                    }
                ],
                "author_detail": {
                    "name": "Jrg Henkel"
                },
                "author": "Jrg Henkel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07826v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07826v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07157v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07157v3",
                "updated": "2025-04-16T09:41:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    41,
                    16,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-09T11:19:42Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    11,
                    19,
                    42,
                    2,
                    99,
                    0
                ],
                "title": "GAAPO: Genetic Algorithmic Applied to Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GAAPO: Genetic Algorithmic Applied to Prompt Optimization"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, with their performance heavily dependent on the quality of input\nprompts. While prompt engineering has proven effective, it typically relies on\nmanual adjustments, making it time-consuming and potentially suboptimal. This\npaper introduces GAAPO (Genetic Algorithm Applied to Prompt Optimization), a\nnovel hybrid optimization framework that leverages genetic algorithm principles\nto evolve prompts through successive generations. Unlike traditional genetic\napproaches that rely solely on mutation and crossover operations, GAAPO\nintegrates multiple specialized prompt generation strategies within its\nevolutionary framework. Through extensive experimentation on diverse datasets\nincluding ETHOS, MMLU-Pro, and GPQA, our analysis reveals several important\npoint for the future development of automatic prompt optimization methods:\nimportance of the tradeoff between the population size and the number of\ngenerations, effect of selection methods on stability results, capacity of\ndifferent LLMs and especially reasoning models to be able to automatically\ngenerate prompts from similar queries... Furthermore, we provide insights into\nthe relative effectiveness of different prompt generation strategies and their\nevolution across optimization phases. These findings contribute to both the\ntheoretical understanding of prompt optimization and practical applications in\nimproving LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, with their performance heavily dependent on the quality of input\nprompts. While prompt engineering has proven effective, it typically relies on\nmanual adjustments, making it time-consuming and potentially suboptimal. This\npaper introduces GAAPO (Genetic Algorithm Applied to Prompt Optimization), a\nnovel hybrid optimization framework that leverages genetic algorithm principles\nto evolve prompts through successive generations. Unlike traditional genetic\napproaches that rely solely on mutation and crossover operations, GAAPO\nintegrates multiple specialized prompt generation strategies within its\nevolutionary framework. Through extensive experimentation on diverse datasets\nincluding ETHOS, MMLU-Pro, and GPQA, our analysis reveals several important\npoint for the future development of automatic prompt optimization methods:\nimportance of the tradeoff between the population size and the number of\ngenerations, effect of selection methods on stability results, capacity of\ndifferent LLMs and especially reasoning models to be able to automatically\ngenerate prompts from similar queries... Furthermore, we provide insights into\nthe relative effectiveness of different prompt generation strategies and their\nevolution across optimization phases. These findings contribute to both the\ntheoretical understanding of prompt optimization and practical applications in\nimproving LLM performance."
                },
                "authors": [
                    {
                        "name": "Xavier Scheresse"
                    },
                    {
                        "name": "Jacques-Yves Guilbert--Ly"
                    },
                    {
                        "name": "Antoine Villedieu de Torcy"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Villedieu de Torcy"
                },
                "author": "Antoine Villedieu de Torcy",
                "arxiv_comment": "26 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07157v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07157v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11900v1",
                "updated": "2025-04-16T09:25:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    25,
                    54,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T09:25:54Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    25,
                    54,
                    2,
                    106,
                    0
                ],
                "title": "Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models\n  via Plot Hole Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models\n  via Plot Hole Detection"
                },
                "summary": "Stories are a fundamental aspect of human experience. Engaging deeply with\nstories and spotting plot holes -- inconsistencies in a storyline that break\nthe internal logic or rules of a story's world -- requires nuanced reasoning\nskills, including tracking entities and events and their interplay, abstract\nthinking, pragmatic narrative understanding, commonsense and social reasoning,\nand theory of mind. As Large Language Models (LLMs) increasingly generate,\ninterpret, and modify text, rigorously assessing their narrative consistency\nand deeper language understanding becomes critical. However, existing\nbenchmarks focus mainly on surface-level comprehension. In this work, we\npropose plot hole detection in stories as a proxy to evaluate language\nunderstanding and reasoning in LLMs. We introduce FlawedFictionsMaker, a novel\nalgorithm to controllably and carefully synthesize plot holes in human-written\nstories. Using this algorithm, we construct a benchmark to evaluate LLMs' plot\nhole detection abilities in stories -- FlawedFictions -- , which is robust to\ncontamination, with human filtering ensuring high quality. We find that\nstate-of-the-art LLMs struggle in accurately solving FlawedFictions regardless\nof the reasoning effort allowed, with performance significantly degrading as\nstory length increases. Finally, we show that LLM-based story summarization and\nstory generation are prone to introducing plot holes, with more than 50% and\n100% increases in plot hole detection rates with respect to human-written\noriginals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stories are a fundamental aspect of human experience. Engaging deeply with\nstories and spotting plot holes -- inconsistencies in a storyline that break\nthe internal logic or rules of a story's world -- requires nuanced reasoning\nskills, including tracking entities and events and their interplay, abstract\nthinking, pragmatic narrative understanding, commonsense and social reasoning,\nand theory of mind. As Large Language Models (LLMs) increasingly generate,\ninterpret, and modify text, rigorously assessing their narrative consistency\nand deeper language understanding becomes critical. However, existing\nbenchmarks focus mainly on surface-level comprehension. In this work, we\npropose plot hole detection in stories as a proxy to evaluate language\nunderstanding and reasoning in LLMs. We introduce FlawedFictionsMaker, a novel\nalgorithm to controllably and carefully synthesize plot holes in human-written\nstories. Using this algorithm, we construct a benchmark to evaluate LLMs' plot\nhole detection abilities in stories -- FlawedFictions -- , which is robust to\ncontamination, with human filtering ensuring high quality. We find that\nstate-of-the-art LLMs struggle in accurately solving FlawedFictions regardless\nof the reasoning effort allowed, with performance significantly degrading as\nstory length increases. Finally, we show that LLM-based story summarization and\nstory generation are prone to introducing plot holes, with more than 50% and\n100% increases in plot hole detection rates with respect to human-written\noriginals."
                },
                "authors": [
                    {
                        "name": "Kabir Ahuja"
                    },
                    {
                        "name": "Melanie Sclar"
                    },
                    {
                        "name": "Yulia Tsvetkov"
                    }
                ],
                "author_detail": {
                    "name": "Yulia Tsvetkov"
                },
                "author": "Yulia Tsvetkov",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19887v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19887v5",
                "updated": "2025-04-16T09:24:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    24,
                    21,
                    2,
                    106,
                    0
                ],
                "published": "2025-03-25T17:51:50Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    51,
                    50,
                    1,
                    84,
                    0
                ],
                "title": "AI threats to national security can be countered through an incident\n  regime",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI threats to national security can be countered through an incident\n  regime"
                },
                "summary": "Recent progress in AI capabilities has heightened concerns that AI systems\ncould pose a threat to national security, for example, by making it easier for\nmalicious actors to perform cyberattacks on critical national infrastructure,\nor through loss of control of autonomous AI systems. In parallel, federal\nlegislators in the US have proposed nascent 'AI incident regimes' to identify\nand counter similar threats. In this paper, we consolidate these two trends and\npresent a timely proposal for a legally mandated post-deployment AI incident\nregime that aims to counter potential national security threats from AI\nsystems. We start the paper by introducing the concept of 'security-critical'\nto describe sectors that pose extreme risks to national security, before\narguing that 'security-critical' describes civilian nuclear power, aviation,\nlife science dual-use research of concern, and frontier AI development. We then\npresent in detail our AI incident regime proposal, justifying each component of\nthe proposal by demonstrating its similarity to US domestic incident regimes in\nother 'security-critical' sectors. Finally, we sketch a hypothetical scenario\nwhere our proposed AI incident regime deals with an AI cyber incident. Our\nproposed AI incident regime is split into three phases. The first phase\nrevolves around a novel operationalization of what counts as an 'AI incident'\nand we suggest that AI providers must create a 'national security case' before\ndeploying a frontier AI system. The second and third phases spell out that AI\nproviders should notify a government agency about incidents, and that the\ngovernment agency should be involved in amending AI providers' security and\nsafety procedures, in order to counter future threats to national security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in AI capabilities has heightened concerns that AI systems\ncould pose a threat to national security, for example, by making it easier for\nmalicious actors to perform cyberattacks on critical national infrastructure,\nor through loss of control of autonomous AI systems. In parallel, federal\nlegislators in the US have proposed nascent 'AI incident regimes' to identify\nand counter similar threats. In this paper, we consolidate these two trends and\npresent a timely proposal for a legally mandated post-deployment AI incident\nregime that aims to counter potential national security threats from AI\nsystems. We start the paper by introducing the concept of 'security-critical'\nto describe sectors that pose extreme risks to national security, before\narguing that 'security-critical' describes civilian nuclear power, aviation,\nlife science dual-use research of concern, and frontier AI development. We then\npresent in detail our AI incident regime proposal, justifying each component of\nthe proposal by demonstrating its similarity to US domestic incident regimes in\nother 'security-critical' sectors. Finally, we sketch a hypothetical scenario\nwhere our proposed AI incident regime deals with an AI cyber incident. Our\nproposed AI incident regime is split into three phases. The first phase\nrevolves around a novel operationalization of what counts as an 'AI incident'\nand we suggest that AI providers must create a 'national security case' before\ndeploying a frontier AI system. The second and third phases spell out that AI\nproviders should notify a government agency about incidents, and that the\ngovernment agency should be involved in amending AI providers' security and\nsafety procedures, in order to counter future threats to national security."
                },
                "authors": [
                    {
                        "name": "Alejandro Ortega"
                    }
                ],
                "author_detail": {
                    "name": "Alejandro Ortega"
                },
                "author": "Alejandro Ortega",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19887v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19887v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11889v1",
                "updated": "2025-04-16T09:17:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    17,
                    45,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T09:17:45Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    17,
                    45,
                    2,
                    106,
                    0
                ],
                "title": "Rethinking LLM-Based Recommendations: A Query Generation-Based,\n  Training-Free Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking LLM-Based Recommendations: A Query Generation-Based,\n  Training-Free Approach"
                },
                "summary": "Existing large language model LLM-based recommendation methods face several\nchallenges, including inefficiency in handling large candidate pools,\nsensitivity to item order within prompts (\"lost in the middle\" phenomenon) poor\nscalability, and unrealistic evaluation due to random negative sampling. To\naddress these issues, we propose a Query-to-Recommendation approach that\nleverages LLMs to generate personalized queries for retrieving relevant items\nfrom the entire candidate pool, eliminating the need for candidate\npre-selection. This method can be integrated into an ID-based recommendation\nsystem without additional training, enhances recommendation performance and\ndiversity through LLMs' world knowledge, and performs well even for less\npopular item groups. Experiments on three datasets show up to 57 percent\nimprovement, with an average gain of 31 percent, demonstrating strong zero-shot\nperformance and further gains when ensembled with existing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large language model LLM-based recommendation methods face several\nchallenges, including inefficiency in handling large candidate pools,\nsensitivity to item order within prompts (\"lost in the middle\" phenomenon) poor\nscalability, and unrealistic evaluation due to random negative sampling. To\naddress these issues, we propose a Query-to-Recommendation approach that\nleverages LLMs to generate personalized queries for retrieving relevant items\nfrom the entire candidate pool, eliminating the need for candidate\npre-selection. This method can be integrated into an ID-based recommendation\nsystem without additional training, enhances recommendation performance and\ndiversity through LLMs' world knowledge, and performs well even for less\npopular item groups. Experiments on three datasets show up to 57 percent\nimprovement, with an average gain of 31 percent, demonstrating strong zero-shot\nperformance and further gains when ensembled with existing models."
                },
                "authors": [
                    {
                        "name": "Donghee Han"
                    },
                    {
                        "name": "Hwanjun Song"
                    },
                    {
                        "name": "Mun Yong Yi"
                    }
                ],
                "author_detail": {
                    "name": "Mun Yong Yi"
                },
                "author": "Mun Yong Yi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02644v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02644v3",
                "updated": "2025-04-16T09:10:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    10,
                    17,
                    2,
                    106,
                    0
                ],
                "published": "2024-10-03T16:30:47Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    30,
                    47,
                    3,
                    277,
                    0
                ],
                "title": "Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and\n  Defenses in LLM-based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and\n  Defenses in LLM-based Agents"
                },
                "summary": "Although LLM-based agents, powered by Large Language Models (LLMs), can use\nexternal tools and memory mechanisms to solve complex real-world tasks, they\nmay also introduce critical security vulnerabilities. However, the existing\nliterature does not comprehensively evaluate attacks and defenses against\nLLM-based agents. To address this, we introduce Agent Security Bench (ASB), a\ncomprehensive framework designed to formalize, benchmark, and evaluate the\nattacks and defenses of LLM-based agents, including 10 scenarios (e.g.,\ne-commerce, autonomous driving, finance), 10 agents targeting the scenarios,\nover 400 tools, 27 different types of attack/defense methods, and 7 evaluation\nmetrics. Based on ASB, we benchmark 10 prompt injection attacks, a memory\npoisoning attack, a novel Plan-of-Thought backdoor attack, 4 mixed attacks, and\n11 corresponding defenses across 13 LLM backbones. Our benchmark results reveal\ncritical vulnerabilities in different stages of agent operation, including\nsystem prompt, user prompt handling, tool usage, and memory retrieval, with the\nhighest average attack success rate of 84.30\\%, but limited effectiveness shown\nin current defenses, unveiling important works to be done in terms of agent\nsecurity for the community. We also introduce a new metric to evaluate the\nagents' capability to balance utility and security. Our code can be found at\nhttps://github.com/agiresearch/ASB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although LLM-based agents, powered by Large Language Models (LLMs), can use\nexternal tools and memory mechanisms to solve complex real-world tasks, they\nmay also introduce critical security vulnerabilities. However, the existing\nliterature does not comprehensively evaluate attacks and defenses against\nLLM-based agents. To address this, we introduce Agent Security Bench (ASB), a\ncomprehensive framework designed to formalize, benchmark, and evaluate the\nattacks and defenses of LLM-based agents, including 10 scenarios (e.g.,\ne-commerce, autonomous driving, finance), 10 agents targeting the scenarios,\nover 400 tools, 27 different types of attack/defense methods, and 7 evaluation\nmetrics. Based on ASB, we benchmark 10 prompt injection attacks, a memory\npoisoning attack, a novel Plan-of-Thought backdoor attack, 4 mixed attacks, and\n11 corresponding defenses across 13 LLM backbones. Our benchmark results reveal\ncritical vulnerabilities in different stages of agent operation, including\nsystem prompt, user prompt handling, tool usage, and memory retrieval, with the\nhighest average attack success rate of 84.30\\%, but limited effectiveness shown\nin current defenses, unveiling important works to be done in terms of agent\nsecurity for the community. We also introduce a new metric to evaluate the\nagents' capability to balance utility and security. Our code can be found at\nhttps://github.com/agiresearch/ASB."
                },
                "authors": [
                    {
                        "name": "Hanrong Zhang"
                    },
                    {
                        "name": "Jingyuan Huang"
                    },
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Yifei Yao"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Chenlu Zhan"
                    },
                    {
                        "name": "Hongwei Wang"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02644v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02644v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11879v1",
                "updated": "2025-04-16T08:59:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    8,
                    59,
                    47,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T08:59:47Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    8,
                    59,
                    47,
                    2,
                    106,
                    0
                ],
                "title": "Learning Compatible Multi-Prize Subnetworks for Asymmetric Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Compatible Multi-Prize Subnetworks for Asymmetric Retrieval"
                },
                "summary": "Asymmetric retrieval is a typical scenario in real-world retrieval systems,\nwhere compatible models of varying capacities are deployed on platforms with\ndifferent resource configurations. Existing methods generally train pre-defined\nnetworks or subnetworks with capacities specifically designed for\npre-determined platforms, using compatible learning. Nevertheless, these\nmethods suffer from limited flexibility for multi-platform deployment. For\nexample, when introducing a new platform into the retrieval systems, developers\nhave to train an additional model at an appropriate capacity that is compatible\nwith existing models via backward-compatible learning. In this paper, we\npropose a Prunable Network with self-compatibility, which allows developers to\ngenerate compatible subnetworks at any desired capacity through post-training\npruning. Thus it allows the creation of a sparse subnetwork matching the\nresources of the new platform without additional training. Specifically, we\noptimize both the architecture and weight of subnetworks at different\ncapacities within a dense network in compatible learning. We also design a\nconflict-aware gradient integration scheme to handle the gradient conflicts\nbetween the dense network and subnetworks during compatible learning. Extensive\nexperiments on diverse benchmarks and visual backbones demonstrate the\neffectiveness of our method. Our code and model are available at\nhttps://github.com/Bunny-Black/PrunNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymmetric retrieval is a typical scenario in real-world retrieval systems,\nwhere compatible models of varying capacities are deployed on platforms with\ndifferent resource configurations. Existing methods generally train pre-defined\nnetworks or subnetworks with capacities specifically designed for\npre-determined platforms, using compatible learning. Nevertheless, these\nmethods suffer from limited flexibility for multi-platform deployment. For\nexample, when introducing a new platform into the retrieval systems, developers\nhave to train an additional model at an appropriate capacity that is compatible\nwith existing models via backward-compatible learning. In this paper, we\npropose a Prunable Network with self-compatibility, which allows developers to\ngenerate compatible subnetworks at any desired capacity through post-training\npruning. Thus it allows the creation of a sparse subnetwork matching the\nresources of the new platform without additional training. Specifically, we\noptimize both the architecture and weight of subnetworks at different\ncapacities within a dense network in compatible learning. We also design a\nconflict-aware gradient integration scheme to handle the gradient conflicts\nbetween the dense network and subnetworks during compatible learning. Extensive\nexperiments on diverse benchmarks and visual backbones demonstrate the\neffectiveness of our method. Our code and model are available at\nhttps://github.com/Bunny-Black/PrunNet."
                },
                "authors": [
                    {
                        "name": "Yushuai Sun"
                    },
                    {
                        "name": "Zikun Zhou"
                    },
                    {
                        "name": "Dongmei Jiang"
                    },
                    {
                        "name": "Yaowei Wang"
                    },
                    {
                        "name": "Jun Yu"
                    },
                    {
                        "name": "Guangming Lu"
                    },
                    {
                        "name": "Wenjie Pei"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Pei"
                },
                "author": "Wenjie Pei",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11873v1",
                "updated": "2025-04-16T08:50:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    8,
                    50,
                    51,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T08:50:51Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    8,
                    50,
                    51,
                    2,
                    106,
                    0
                ],
                "title": "Transferable Deployment of Semantic Edge Inference Systems via\n  Unsupervised Domain Adaption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transferable Deployment of Semantic Edge Inference Systems via\n  Unsupervised Domain Adaption"
                },
                "summary": "This paper investigates deploying semantic edge inference systems for\nperforming a common image clarification task. In particular, each system\nconsists of multiple Internet of Things (IoT) devices that first locally encode\nthe sensing data into semantic features and then transmit them to an edge\nserver for subsequent data fusion and task inference. The inference accuracy is\ndetermined by efficient training of the feature encoder/decoder using labeled\ndata samples. Due to the difference in sensing data and communication channel\ndistributions, deploying the system in a new environment may induce high costs\nin annotating data labels and re-training the encoder/decoder models. To\nachieve cost-effective transferable system deployment, we propose an efficient\nDomain Adaptation method for Semantic Edge INference systems (DASEIN) that can\nmaintain high inference accuracy in a new environment without the need for\nlabeled samples. Specifically, DASEIN exploits the task-relevant data\ncorrelation between different deployment scenarios by leveraging the techniques\nof unsupervised domain adaptation and knowledge distillation. It devises an\nefficient two-step adaptation procedure that sequentially aligns the data\ndistributions and adapts to the channel variations. Numerical results show\nthat, under a substantial change in sensing data distributions, the proposed\nDASEIN outperforms the best-performing benchmark method by 7.09% and 21.33% in\ninference accuracy when the new environment has similar or 25 dB lower channel\nsignal to noise power ratios (SNRs), respectively. This verifies the\neffectiveness of the proposed method in adapting both data and channel\ndistributions in practical transfer deployment applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates deploying semantic edge inference systems for\nperforming a common image clarification task. In particular, each system\nconsists of multiple Internet of Things (IoT) devices that first locally encode\nthe sensing data into semantic features and then transmit them to an edge\nserver for subsequent data fusion and task inference. The inference accuracy is\ndetermined by efficient training of the feature encoder/decoder using labeled\ndata samples. Due to the difference in sensing data and communication channel\ndistributions, deploying the system in a new environment may induce high costs\nin annotating data labels and re-training the encoder/decoder models. To\nachieve cost-effective transferable system deployment, we propose an efficient\nDomain Adaptation method for Semantic Edge INference systems (DASEIN) that can\nmaintain high inference accuracy in a new environment without the need for\nlabeled samples. Specifically, DASEIN exploits the task-relevant data\ncorrelation between different deployment scenarios by leveraging the techniques\nof unsupervised domain adaptation and knowledge distillation. It devises an\nefficient two-step adaptation procedure that sequentially aligns the data\ndistributions and adapts to the channel variations. Numerical results show\nthat, under a substantial change in sensing data distributions, the proposed\nDASEIN outperforms the best-performing benchmark method by 7.09% and 21.33% in\ninference accuracy when the new environment has similar or 25 dB lower channel\nsignal to noise power ratios (SNRs), respectively. This verifies the\neffectiveness of the proposed method in adapting both data and channel\ndistributions in practical transfer deployment applications."
                },
                "authors": [
                    {
                        "name": "Weiqiang Jiao"
                    },
                    {
                        "name": "Suzhi Bi"
                    },
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Cheng Guo"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Zhi Quan"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Quan"
                },
                "author": "Zhi Quan",
                "arxiv_comment": "14 pages, 14 figures, the paper is submitted for potential journal\n  publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11860v1",
                "updated": "2025-04-16T08:30:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    8,
                    30,
                    43,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T08:30:43Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    8,
                    30,
                    43,
                    2,
                    106,
                    0
                ],
                "title": "From Data Behavior to Code Analysis: A Multimodal Study on Security and\n  Privacy Challenges in Blockchain-Based DApp",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Data Behavior to Code Analysis: A Multimodal Study on Security and\n  Privacy Challenges in Blockchain-Based DApp"
                },
                "summary": "The recent proliferation of blockchain-based decentralized applications\n(DApp) has catalyzed transformative advancements in distributed systems, with\nextensive deployments observed across financial, entertainment, media, and\ncybersecurity domains. These trustless architectures, characterized by their\ndecentralized nature and elimination of third-party intermediaries, have\ngarnered substantial institutional attention. Consequently, the escalating\nsecurity challenges confronting DApp demand rigorous scholarly investigation.\nThis study initiates with a systematic analysis of behavioral patterns derived\nfrom empirical DApp datasets, establishing foundational insights for subsequent\nmethodological developments. The principal security vulnerabilities in\nEthereum-based smart contracts developed via Solidity are then critically\nexamined. Specifically, reentrancy vulnerability attacks are addressed by\nformally representing contract logic using highly expressive code fragments.\nThis enables precise source code-level detection via bidirectional long\nshort-term memory networks with attention mechanisms (BLSTM-ATT). Regarding\nprivacy preservation challenges, contemporary solutions are evaluated through\ndual analytical lenses: identity privacy preservation and transaction anonymity\nenhancement, while proposing future research trajectories in cryptographic\nobfuscation techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent proliferation of blockchain-based decentralized applications\n(DApp) has catalyzed transformative advancements in distributed systems, with\nextensive deployments observed across financial, entertainment, media, and\ncybersecurity domains. These trustless architectures, characterized by their\ndecentralized nature and elimination of third-party intermediaries, have\ngarnered substantial institutional attention. Consequently, the escalating\nsecurity challenges confronting DApp demand rigorous scholarly investigation.\nThis study initiates with a systematic analysis of behavioral patterns derived\nfrom empirical DApp datasets, establishing foundational insights for subsequent\nmethodological developments. The principal security vulnerabilities in\nEthereum-based smart contracts developed via Solidity are then critically\nexamined. Specifically, reentrancy vulnerability attacks are addressed by\nformally representing contract logic using highly expressive code fragments.\nThis enables precise source code-level detection via bidirectional long\nshort-term memory networks with attention mechanisms (BLSTM-ATT). Regarding\nprivacy preservation challenges, contemporary solutions are evaluated through\ndual analytical lenses: identity privacy preservation and transaction anonymity\nenhancement, while proposing future research trajectories in cryptographic\nobfuscation techniques."
                },
                "authors": [
                    {
                        "name": "Haoyang Sun"
                    },
                    {
                        "name": "Yishun Wang"
                    },
                    {
                        "name": "Xiaoqi Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoqi Li"
                },
                "author": "Xiaoqi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11850v1",
                "updated": "2025-04-16T08:16:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    8,
                    16,
                    28,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T08:16:28Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    8,
                    16,
                    28,
                    2,
                    106,
                    0
                ],
                "title": "ACE: Attentional Concept Erasure in Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACE: Attentional Concept Erasure in Diffusion Models"
                },
                "summary": "Large text-to-image diffusion models have demonstrated remarkable image\nsynthesis capabilities, but their indiscriminate training on Internet-scale\ndata has led to learned concepts that enable harmful, copyrighted, or otherwise\nundesirable content generation. We address the task of concept erasure in\ndiffusion models, i.e., removing a specified concept from a pre-trained model\nsuch that prompting the concept (or related synonyms) no longer yields its\ndepiction, while preserving the model's ability to generate other content. We\npropose a novel method, Attentional Concept Erasure (ACE), that integrates a\nclosed-form attention manipulation with lightweight fine-tuning. Theoretically,\nwe formulate concept erasure as aligning the model's conditional distribution\non the target concept with a neutral distribution. Our approach identifies and\nnullifies concept-specific latent directions in the cross-attention modules via\na gated low-rank adaptation, followed by adversarially augmented fine-tuning to\nensure thorough erasure of the concept and its synonyms. Empirically, we\ndemonstrate on multiple benchmarks, including object classes, celebrity faces,\nexplicit content, and artistic styles, that ACE achieves state-of-the-art\nconcept removal efficacy and robustness. Compared to prior methods, ACE better\nbalances generality (erasing concept and related terms) and specificity\n(preserving unrelated content), scales to dozens of concepts, and is efficient,\nrequiring only a few seconds of adaptation per concept. We will release our\ncode to facilitate safer deployment of diffusion models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large text-to-image diffusion models have demonstrated remarkable image\nsynthesis capabilities, but their indiscriminate training on Internet-scale\ndata has led to learned concepts that enable harmful, copyrighted, or otherwise\nundesirable content generation. We address the task of concept erasure in\ndiffusion models, i.e., removing a specified concept from a pre-trained model\nsuch that prompting the concept (or related synonyms) no longer yields its\ndepiction, while preserving the model's ability to generate other content. We\npropose a novel method, Attentional Concept Erasure (ACE), that integrates a\nclosed-form attention manipulation with lightweight fine-tuning. Theoretically,\nwe formulate concept erasure as aligning the model's conditional distribution\non the target concept with a neutral distribution. Our approach identifies and\nnullifies concept-specific latent directions in the cross-attention modules via\na gated low-rank adaptation, followed by adversarially augmented fine-tuning to\nensure thorough erasure of the concept and its synonyms. Empirically, we\ndemonstrate on multiple benchmarks, including object classes, celebrity faces,\nexplicit content, and artistic styles, that ACE achieves state-of-the-art\nconcept removal efficacy and robustness. Compared to prior methods, ACE better\nbalances generality (erasing concept and related terms) and specificity\n(preserving unrelated content), scales to dozens of concepts, and is efficient,\nrequiring only a few seconds of adaptation per concept. We will release our\ncode to facilitate safer deployment of diffusion models."
                },
                "authors": [
                    {
                        "name": "Finn Carter"
                    }
                ],
                "author_detail": {
                    "name": "Finn Carter"
                },
                "author": "Finn Carter",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11844v1",
                "updated": "2025-04-16T08:07:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    8,
                    7,
                    8,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T08:07:08Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    8,
                    7,
                    8,
                    2,
                    106,
                    0
                ],
                "title": "Evaluating the Goal-Directedness of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Goal-Directedness of Large Language Models"
                },
                "summary": "To what extent do LLMs use their capabilities towards their given goal? We\ntake this as a measure of their goal-directedness. We evaluate\ngoal-directedness on tasks that require information gathering, cognitive\neffort, and plan execution, where we use subtasks to infer each model's\nrelevant capabilities. Our evaluations of LLMs from Google DeepMind, OpenAI,\nand Anthropic show that goal-directedness is relatively consistent across\ntasks, differs from task performance, and is only moderately sensitive to\nmotivational prompts. Notably, most models are not fully goal-directed. We hope\nour goal-directedness evaluations will enable better monitoring of LLM\nprogress, and enable more deliberate design choices of agentic properties in\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To what extent do LLMs use their capabilities towards their given goal? We\ntake this as a measure of their goal-directedness. We evaluate\ngoal-directedness on tasks that require information gathering, cognitive\neffort, and plan execution, where we use subtasks to infer each model's\nrelevant capabilities. Our evaluations of LLMs from Google DeepMind, OpenAI,\nand Anthropic show that goal-directedness is relatively consistent across\ntasks, differs from task performance, and is only moderately sensitive to\nmotivational prompts. Notably, most models are not fully goal-directed. We hope\nour goal-directedness evaluations will enable better monitoring of LLM\nprogress, and enable more deliberate design choices of agentic properties in\nLLMs."
                },
                "authors": [
                    {
                        "name": "Tom Everitt"
                    },
                    {
                        "name": "Cristina Garbacea"
                    },
                    {
                        "name": "Alexis Bellot"
                    },
                    {
                        "name": "Jonathan Richens"
                    },
                    {
                        "name": "Henry Papadatos"
                    },
                    {
                        "name": "Simon Campos"
                    },
                    {
                        "name": "Rohin Shah"
                    }
                ],
                "author_detail": {
                    "name": "Rohin Shah"
                },
                "author": "Rohin Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08754v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08754v3",
                "updated": "2025-04-16T07:59:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    59,
                    48,
                    2,
                    106,
                    0
                ],
                "published": "2025-03-28T15:49:52Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    49,
                    52,
                    4,
                    87,
                    0
                ],
                "title": "Towards Personalized Conversational Sales Agents : Contextual User\n  Profiling for Strategic Action",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Personalized Conversational Sales Agents : Contextual User\n  Profiling for Strategic Action"
                },
                "summary": "Conversational Recommender Systems (CRSs) aim to engage users in dialogue to\nprovide tailored recommendations. While traditional CRSs focus on eliciting\npreferences and retrieving items, real-world e-commerce interactions involve\nmore complex decision-making, where users consider multiple factors beyond\nsimple attributes. To bridge this gap, we introduce Conversational Sales\n(CSales), a novel task that unifies preference elicitation, recommendation, and\npersuasion to better support user decision-making. For a realistic evaluation\nof CSales, we present CSUser, an LLM-based user simulator constructed from\nreal-world data, modeling diverse user profiles with needs and personalities.\nAdditionally, we propose CSI, a conversational sales agent that proactively\ninfers contextual profiles through dialogue for personalized action planning.\nExtensive experiments demonstrate that CSUser effectively replicates real-world\nusers and emphasize the importance of contextual profiling for strategic action\nselection, ultimately driving successful purchases in e-commerce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Recommender Systems (CRSs) aim to engage users in dialogue to\nprovide tailored recommendations. While traditional CRSs focus on eliciting\npreferences and retrieving items, real-world e-commerce interactions involve\nmore complex decision-making, where users consider multiple factors beyond\nsimple attributes. To bridge this gap, we introduce Conversational Sales\n(CSales), a novel task that unifies preference elicitation, recommendation, and\npersuasion to better support user decision-making. For a realistic evaluation\nof CSales, we present CSUser, an LLM-based user simulator constructed from\nreal-world data, modeling diverse user profiles with needs and personalities.\nAdditionally, we propose CSI, a conversational sales agent that proactively\ninfers contextual profiles through dialogue for personalized action planning.\nExtensive experiments demonstrate that CSUser effectively replicates real-world\nusers and emphasize the importance of contextual profiling for strategic action\nselection, ultimately driving successful purchases in e-commerce."
                },
                "authors": [
                    {
                        "name": "Tongyoung Kim"
                    },
                    {
                        "name": "Jeongeun Lee"
                    },
                    {
                        "name": "Soojin Yoon"
                    },
                    {
                        "name": "Sunghwan Kim"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08754v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08754v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11839v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11839v1",
                "updated": "2025-04-16T07:56:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    56,
                    36,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T07:56:36Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    56,
                    36,
                    2,
                    106,
                    0
                ],
                "title": "\"Good\" and \"Bad\" Failures in Industrial CI/CD -- Balancing Cost and\n  Quality Assurance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Good\" and \"Bad\" Failures in Industrial CI/CD -- Balancing Cost and\n  Quality Assurance"
                },
                "summary": "Continuous Integration and Continuous Deployment (CI/CD) pipeline automates\nsoftware development to speed up and enhance the efficiency of engineering\nsoftware. These workflows consist of various jobs, such as code validation and\ntesting, which developers must wait to complete before receiving feedback. The\njobs can fail, which leads to unnecessary delays in build times, decreasing\nproductivity for developers, and increasing costs for companies. To explore how\ncompanies adopt CI/CD workflows and balance cost with quality assurance during\noptimization, we studied 4 companies, reporting industry experiences with CI/CD\npractices. Our findings reveal that organizations can confuse the distinction\nbetween CI and CD, whereas code merge and product release serve as more\neffective milestones for process optimization and risk control. While numerous\ntools and research efforts target the post-merge phase to enhance productivity,\nlimited attention has been given to the pre-merge phase, where early failure\nprevention brings more impacts and less risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous Integration and Continuous Deployment (CI/CD) pipeline automates\nsoftware development to speed up and enhance the efficiency of engineering\nsoftware. These workflows consist of various jobs, such as code validation and\ntesting, which developers must wait to complete before receiving feedback. The\njobs can fail, which leads to unnecessary delays in build times, decreasing\nproductivity for developers, and increasing costs for companies. To explore how\ncompanies adopt CI/CD workflows and balance cost with quality assurance during\noptimization, we studied 4 companies, reporting industry experiences with CI/CD\npractices. Our findings reveal that organizations can confuse the distinction\nbetween CI and CD, whereas code merge and product release serve as more\neffective milestones for process optimization and risk control. While numerous\ntools and research efforts target the post-merge phase to enhance productivity,\nlimited attention has been given to the pre-merge phase, where early failure\nprevention brings more impacts and less risks."
                },
                "authors": [
                    {
                        "name": "Simin Sun"
                    },
                    {
                        "name": "David Friberg"
                    },
                    {
                        "name": "Miroslaw Staron"
                    }
                ],
                "author_detail": {
                    "name": "Miroslaw Staron"
                },
                "author": "Miroslaw Staron",
                "arxiv_comment": "5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11839v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11839v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11837v1",
                "updated": "2025-04-16T07:52:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    52,
                    6,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T07:52:06Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    52,
                    6,
                    2,
                    106,
                    0
                ],
                "title": "FiSMiness: A Finite State Machine Based Paradigm for Emotional Support\n  Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiSMiness: A Finite State Machine Based Paradigm for Emotional Support\n  Conversations"
                },
                "summary": "Emotional support conversation (ESC) aims to alleviate the emotional distress\nof individuals through effective conversations. Although large language models\n(LLMs) have obtained remarkable progress on ESC, most of these studies might\nnot define the diagram from the state model perspective, therefore providing a\nsuboptimal solution for long-term satisfaction. To address such an issue, we\nleverage the Finite State Machine (FSM) on LLMs, and propose a framework called\nFiSMiness. Our framework allows a single LLM to bootstrap the planning during\nESC, and self-reason the seeker's emotion, support strategy and the final\nresponse upon each conversational turn. Substantial experiments on ESC datasets\nsuggest that FiSMiness outperforms many baselines, including direct inference,\nself-refine, chain of thought, finetuning, and external-assisted methods, even\nthose with many more parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotional support conversation (ESC) aims to alleviate the emotional distress\nof individuals through effective conversations. Although large language models\n(LLMs) have obtained remarkable progress on ESC, most of these studies might\nnot define the diagram from the state model perspective, therefore providing a\nsuboptimal solution for long-term satisfaction. To address such an issue, we\nleverage the Finite State Machine (FSM) on LLMs, and propose a framework called\nFiSMiness. Our framework allows a single LLM to bootstrap the planning during\nESC, and self-reason the seeker's emotion, support strategy and the final\nresponse upon each conversational turn. Substantial experiments on ESC datasets\nsuggest that FiSMiness outperforms many baselines, including direct inference,\nself-refine, chain of thought, finetuning, and external-assisted methods, even\nthose with many more parameters."
                },
                "authors": [
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "Qingqing Gu"
                    },
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Teng Chen"
                    },
                    {
                        "name": "Zhonglin Jiang"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Luo Ji"
                    }
                ],
                "author_detail": {
                    "name": "Luo Ji"
                },
                "author": "Luo Ji",
                "arxiv_comment": "accepted by CMCL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11833v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11833v1",
                "updated": "2025-04-16T07:45:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    45,
                    10,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T07:45:10Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    45,
                    10,
                    2,
                    106,
                    0
                ],
                "title": "Could Thinking Multilingually Empower LLM Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Could Thinking Multilingually Empower LLM Reasoning?"
                },
                "summary": "Previous work indicates that large language models exhibit a significant\n\"English bias\", i.e. they often perform better when tasks are presented in\nEnglish. Interestingly, we have observed that using certain other languages in\nreasoning tasks can yield better performance than English. However, this\nphenomenon remains under-explored. In this paper, we explore the upper bound of\nharnessing multilingualism in reasoning tasks, suggesting that multilingual\nreasoning promises significantly (by nearly 10 Acc@$k$ points) and robustly\n(tolerance for variations in translation quality and language choice) higher\nupper bounds than English-only reasoning. Besides analyzing the reason behind\nthe upper bound and challenges in reaching it, we also find that common answer\nselection methods cannot achieve this upper bound, due to their limitations and\nbiases. These insights could pave the way for future research aimed at fully\nharnessing the potential of multilingual reasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous work indicates that large language models exhibit a significant\n\"English bias\", i.e. they often perform better when tasks are presented in\nEnglish. Interestingly, we have observed that using certain other languages in\nreasoning tasks can yield better performance than English. However, this\nphenomenon remains under-explored. In this paper, we explore the upper bound of\nharnessing multilingualism in reasoning tasks, suggesting that multilingual\nreasoning promises significantly (by nearly 10 Acc@$k$ points) and robustly\n(tolerance for variations in translation quality and language choice) higher\nupper bounds than English-only reasoning. Besides analyzing the reason behind\nthe upper bound and challenges in reaching it, we also find that common answer\nselection methods cannot achieve this upper bound, due to their limitations and\nbiases. These insights could pave the way for future research aimed at fully\nharnessing the potential of multilingual reasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Changjiang Gao"
                    },
                    {
                        "name": "Xu Huang"
                    },
                    {
                        "name": "Wenhao Zhu"
                    },
                    {
                        "name": "Shujian Huang"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Fei Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Fei Yuan"
                },
                "author": "Fei Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11833v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11833v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11829v1",
                "updated": "2025-04-16T07:38:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    38,
                    19,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T07:38:19Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    38,
                    19,
                    2,
                    106,
                    0
                ],
                "title": "Dj Vu: Multilingual LLM Evaluation through the Lens of Machine\n  Translation Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dj Vu: Multilingual LLM Evaluation through the Lens of Machine\n  Translation Evaluation"
                },
                "summary": "Generation capabilities and language coverage of multilingual large language\nmodels (mLLMs) are advancing rapidly. However, evaluation practices for\ngenerative abilities of mLLMs are still lacking comprehensiveness, scientific\nrigor, and consistent adoption across research labs, which undermines their\npotential to meaningfully guide mLLM development. We draw parallels with\nmachine translation (MT) evaluation, a field that faced similar challenges and\nhas, over decades, developed transparent reporting standards and reliable\nevaluations for multilingual generative models. Through targeted experiments\nacross key stages of the generative evaluation pipeline, we demonstrate how\nbest practices from MT evaluation can deepen the understanding of quality\ndifferences between models. Additionally, we identify essential components for\nrobust meta-evaluation of mLLMs, ensuring the evaluation methods themselves are\nrigorously assessed. We distill these insights into a checklist of actionable\nrecommendations for mLLM research and development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generation capabilities and language coverage of multilingual large language\nmodels (mLLMs) are advancing rapidly. However, evaluation practices for\ngenerative abilities of mLLMs are still lacking comprehensiveness, scientific\nrigor, and consistent adoption across research labs, which undermines their\npotential to meaningfully guide mLLM development. We draw parallels with\nmachine translation (MT) evaluation, a field that faced similar challenges and\nhas, over decades, developed transparent reporting standards and reliable\nevaluations for multilingual generative models. Through targeted experiments\nacross key stages of the generative evaluation pipeline, we demonstrate how\nbest practices from MT evaluation can deepen the understanding of quality\ndifferences between models. Additionally, we identify essential components for\nrobust meta-evaluation of mLLMs, ensuring the evaluation methods themselves are\nrigorously assessed. We distill these insights into a checklist of actionable\nrecommendations for mLLM research and development."
                },
                "authors": [
                    {
                        "name": "Julia Kreutzer"
                    },
                    {
                        "name": "Eleftheria Briakou"
                    },
                    {
                        "name": "Sweta Agrawal"
                    },
                    {
                        "name": "Marzieh Fadaee"
                    },
                    {
                        "name": "Kocmi Tom"
                    }
                ],
                "author_detail": {
                    "name": "Kocmi Tom"
                },
                "author": "Kocmi Tom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04820v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04820v3",
                "updated": "2025-04-16T07:09:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    9,
                    59,
                    2,
                    106,
                    0
                ],
                "published": "2024-08-09T02:22:51Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    2,
                    22,
                    51,
                    4,
                    222,
                    0
                ],
                "title": "Natural Language Outlines for Code: Literate Programming in the LLM Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Outlines for Code: Literate Programming in the LLM Era"
                },
                "summary": "We propose using natural language outlines as a novel modality and\ninteraction surface for providing AI assistance to developers throughout the\nsoftware development process. An NL outline for a code function comprises\nmultiple statements written in concise prose, which partition the code and\nsummarize its main ideas in the style of literate programming. Crucially, we\nfind that modern LLMs can generate accurate and high-quality NL outlines in\npractice. Moreover, NL outlines enable a bidirectional sync between code and\nNL: a developer can change one and the LLM automatically updates the other. We\ndiscuss many use cases for NL outlines: they can accelerate understanding and\nnavigation of code and diffs, simplify code maintenance, augment code search,\nsteer code generation, and more. We then propose and compare multiple LLM\nprompting techniques for generating outlines and ask professional developers to\njudge outline quality. Finally, we present two case studies applying NL\noutlines toward code review and malware detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose using natural language outlines as a novel modality and\ninteraction surface for providing AI assistance to developers throughout the\nsoftware development process. An NL outline for a code function comprises\nmultiple statements written in concise prose, which partition the code and\nsummarize its main ideas in the style of literate programming. Crucially, we\nfind that modern LLMs can generate accurate and high-quality NL outlines in\npractice. Moreover, NL outlines enable a bidirectional sync between code and\nNL: a developer can change one and the LLM automatically updates the other. We\ndiscuss many use cases for NL outlines: they can accelerate understanding and\nnavigation of code and diffs, simplify code maintenance, augment code search,\nsteer code generation, and more. We then propose and compare multiple LLM\nprompting techniques for generating outlines and ask professional developers to\njudge outline quality. Finally, we present two case studies applying NL\noutlines toward code review and malware detection."
                },
                "authors": [
                    {
                        "name": "Kensen Shi"
                    },
                    {
                        "name": "Deniz Altnbken"
                    },
                    {
                        "name": "Saswat Anand"
                    },
                    {
                        "name": "Mihai Christodorescu"
                    },
                    {
                        "name": "Katja Grnwedel"
                    },
                    {
                        "name": "Alexa Koenings"
                    },
                    {
                        "name": "Sai Naidu"
                    },
                    {
                        "name": "Anurag Pathak"
                    },
                    {
                        "name": "Marc Rasi"
                    },
                    {
                        "name": "Fredde Ribeiro"
                    },
                    {
                        "name": "Brandon Ruffin"
                    },
                    {
                        "name": "Siddhant Sanyam"
                    },
                    {
                        "name": "Maxim Tabachnyk"
                    },
                    {
                        "name": "Sara Toth"
                    },
                    {
                        "name": "Roy Tu"
                    },
                    {
                        "name": "Tobias Welp"
                    },
                    {
                        "name": "Pengcheng Yin"
                    },
                    {
                        "name": "Manzil Zaheer"
                    },
                    {
                        "name": "Satish Chandra"
                    },
                    {
                        "name": "Charles Sutton"
                    }
                ],
                "author_detail": {
                    "name": "Charles Sutton"
                },
                "author": "Charles Sutton",
                "arxiv_comment": "Accepted to FSE'25 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04820v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04820v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21620v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21620v3",
                "updated": "2025-04-16T07:06:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    6,
                    40,
                    2,
                    106,
                    0
                ],
                "published": "2025-03-27T15:39:30Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    39,
                    30,
                    3,
                    86,
                    0
                ],
                "title": "UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement\n  Learning"
                },
                "summary": "The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities\nin LLMs through reinforcement learning (RL) with rule-based rewards. Despite\nits success in language models, its application in multi-modal domains,\nparticularly in graphic user interface (GUI) agent tasks, remains\nunder-explored. To address this issue, we propose UI-R1, the first framework to\nexplore how rule-based RL can enhance the reasoning capabilities of multimodal\nlarge language models (MLLMs) for GUI action prediction tasks. Specifically,\nUI-R1 introduces a novel rule-based action reward, enabling model optimization\nvia policy-based algorithms such as Group Relative Policy Optimization (GRPO).\nFor efficient training, we curate a small yet high-quality dataset of 136\nchallenging tasks, encompassing five common action types on mobile devices.\nExperimental results demonstrate that our proposed UI-R1-3B achieves\nsignificant improvements over the base model (i.e. Qwen2.5-VL-3B) on both\nin-domain (ID) and out-of-domain (OOD) tasks, with average accuracy gains of\n22.1% on ScreenSpot, 6.0% on ScreenSpot-Pro, and 12.7% on ANDROIDCONTROL.\nFurthermore, UI-R1-3B delivers competitive performance compared to larger\nmodels (e.g., OS-Atlas-7B) trained via supervised fine-tuning (SFT) on 76K\nsamples. These results underscore the potential of rule-based reinforcement\nlearning to advance GUI understanding and control, paving the way for future\nresearch in this domain. Code website: https://github.com/lll6gg/UI-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities\nin LLMs through reinforcement learning (RL) with rule-based rewards. Despite\nits success in language models, its application in multi-modal domains,\nparticularly in graphic user interface (GUI) agent tasks, remains\nunder-explored. To address this issue, we propose UI-R1, the first framework to\nexplore how rule-based RL can enhance the reasoning capabilities of multimodal\nlarge language models (MLLMs) for GUI action prediction tasks. Specifically,\nUI-R1 introduces a novel rule-based action reward, enabling model optimization\nvia policy-based algorithms such as Group Relative Policy Optimization (GRPO).\nFor efficient training, we curate a small yet high-quality dataset of 136\nchallenging tasks, encompassing five common action types on mobile devices.\nExperimental results demonstrate that our proposed UI-R1-3B achieves\nsignificant improvements over the base model (i.e. Qwen2.5-VL-3B) on both\nin-domain (ID) and out-of-domain (OOD) tasks, with average accuracy gains of\n22.1% on ScreenSpot, 6.0% on ScreenSpot-Pro, and 12.7% on ANDROIDCONTROL.\nFurthermore, UI-R1-3B delivers competitive performance compared to larger\nmodels (e.g., OS-Atlas-7B) trained via supervised fine-tuning (SFT) on 76K\nsamples. These results underscore the potential of rule-based reinforcement\nlearning to advance GUI understanding and control, paving the way for future\nresearch in this domain. Code website: https://github.com/lll6gg/UI-R1."
                },
                "authors": [
                    {
                        "name": "Zhengxi Lu"
                    },
                    {
                        "name": "Yuxiang Chai"
                    },
                    {
                        "name": "Yaxuan Guo"
                    },
                    {
                        "name": "Xi Yin"
                    },
                    {
                        "name": "Liang Liu"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Han Xiao"
                    },
                    {
                        "name": "Shuai Ren"
                    },
                    {
                        "name": "Guanjing Xiong"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21620v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21620v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11816v1",
                "updated": "2025-04-16T07:02:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    2,
                    38,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T07:02:38Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    2,
                    38,
                    2,
                    106,
                    0
                ],
                "title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading"
                },
                "summary": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads."
                },
                "authors": [
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Hyunsun Chung"
                    },
                    {
                        "name": "Myung-Hoon Cha"
                    },
                    {
                        "name": "Hong-Yeon Kim"
                    },
                    {
                        "name": "Youngjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Kim"
                },
                "author": "Youngjae Kim",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11809v1",
                "updated": "2025-04-16T06:46:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    6,
                    46,
                    15,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T06:46:15Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    6,
                    46,
                    15,
                    2,
                    106,
                    0
                ],
                "title": "Efficient and Adaptive Simultaneous Speech Translation with Fully\n  Unidirectional Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Adaptive Simultaneous Speech Translation with Fully\n  Unidirectional Architecture"
                },
                "summary": "Simultaneous speech translation (SimulST) produces translations incrementally\nwhile processing partial speech input. Although large language models (LLMs)\nhave showcased strong capabilities in offline translation tasks, applying them\nto SimulST poses notable challenges. Existing LLM-based SimulST approaches\neither incur significant computational overhead due to repeated encoding of\nbidirectional speech encoder, or they depend on a fixed read/write policy,\nlimiting the efficiency and performance. In this work, we introduce Efficient\nand Adaptive Simultaneous Speech Translation (EASiST) with fully unidirectional\narchitecture, including both speech encoder and LLM. EASiST includes a\nmulti-latency data curation strategy to generate semantically aligned SimulST\ntraining samples and redefines SimulST as an interleaved generation task with\nexplicit read/write tokens. To facilitate adaptive inference, we incorporate a\nlightweight policy head that dynamically predicts read/write actions.\nAdditionally, we employ a multi-stage training strategy to align speech-text\nmodalities and optimize both translation and policy behavior. Experiments on\nthe MuST-C En$\\rightarrow$De and En$\\rightarrow$Es datasets demonstrate that\nEASiST offers superior latency-quality trade-offs compared to several strong\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous speech translation (SimulST) produces translations incrementally\nwhile processing partial speech input. Although large language models (LLMs)\nhave showcased strong capabilities in offline translation tasks, applying them\nto SimulST poses notable challenges. Existing LLM-based SimulST approaches\neither incur significant computational overhead due to repeated encoding of\nbidirectional speech encoder, or they depend on a fixed read/write policy,\nlimiting the efficiency and performance. In this work, we introduce Efficient\nand Adaptive Simultaneous Speech Translation (EASiST) with fully unidirectional\narchitecture, including both speech encoder and LLM. EASiST includes a\nmulti-latency data curation strategy to generate semantically aligned SimulST\ntraining samples and redefines SimulST as an interleaved generation task with\nexplicit read/write tokens. To facilitate adaptive inference, we incorporate a\nlightweight policy head that dynamically predicts read/write actions.\nAdditionally, we employ a multi-stage training strategy to align speech-text\nmodalities and optimize both translation and policy behavior. Experiments on\nthe MuST-C En$\\rightarrow$De and En$\\rightarrow$Es datasets demonstrate that\nEASiST offers superior latency-quality trade-offs compared to several strong\nbaselines."
                },
                "authors": [
                    {
                        "name": "Biao Fu"
                    },
                    {
                        "name": "Donglei Yu"
                    },
                    {
                        "name": "Minpeng Liao"
                    },
                    {
                        "name": "Chengxi Li"
                    },
                    {
                        "name": "Yidong Chen"
                    },
                    {
                        "name": "Kai Fan"
                    },
                    {
                        "name": "Xiaodong Shi"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodong Shi"
                },
                "author": "Xiaodong Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02623v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02623v3",
                "updated": "2025-04-16T06:22:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    6,
                    22,
                    29,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-03T14:21:33Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    21,
                    33,
                    3,
                    93,
                    0
                ],
                "title": "Multi-Mission Tool Bench: Assessing the Robustness of LLM based Agents\n  through Related and Dynamic Missions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Mission Tool Bench: Assessing the Robustness of LLM based Agents\n  through Related and Dynamic Missions"
                },
                "summary": "Large language models (LLMs) demonstrate strong potential as agents for tool\ninvocation due to their advanced comprehension and planning capabilities. Users\nincreasingly rely on LLM-based agents to solve complex missions through\niterative interactions. However, existing benchmarks predominantly access\nagents in single-mission scenarios, failing to capture real-world complexity.\nTo bridge this gap, we propose the Multi-Mission Tool Bench. In the benchmark,\neach test case comprises multiple interrelated missions. This design requires\nagents to dynamically adapt to evolving demands. Moreover, the proposed\nbenchmark explores all possible mission-switching patterns within a fixed\nmission number. Specifically, we propose a multi-agent data generation\nframework to construct the benchmark. We also propose a novel method to\nevaluate the accuracy and efficiency of agent decisions with dynamic decision\ntrees. Experiments on diverse open-source and closed-source LLMs reveal\ncritical factors influencing agent robustness and provide actionable insights\nto the tool invocation society.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate strong potential as agents for tool\ninvocation due to their advanced comprehension and planning capabilities. Users\nincreasingly rely on LLM-based agents to solve complex missions through\niterative interactions. However, existing benchmarks predominantly access\nagents in single-mission scenarios, failing to capture real-world complexity.\nTo bridge this gap, we propose the Multi-Mission Tool Bench. In the benchmark,\neach test case comprises multiple interrelated missions. This design requires\nagents to dynamically adapt to evolving demands. Moreover, the proposed\nbenchmark explores all possible mission-switching patterns within a fixed\nmission number. Specifically, we propose a multi-agent data generation\nframework to construct the benchmark. We also propose a novel method to\nevaluate the accuracy and efficiency of agent decisions with dynamic decision\ntrees. Experiments on diverse open-source and closed-source LLMs reveal\ncritical factors influencing agent robustness and provide actionable insights\nto the tool invocation society."
                },
                "authors": [
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Jinjian Li"
                    },
                    {
                        "name": "Zelong Zhang"
                    },
                    {
                        "name": "Haorui Wang"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Feng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhang"
                },
                "author": "Feng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02623v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02623v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11793v2",
                "updated": "2025-04-17T06:24:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    6,
                    24,
                    14,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-16T05:59:29Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    59,
                    29,
                    2,
                    106,
                    0
                ],
                "title": "Selective Attention Federated Learning: Improving Privacy and Efficiency\n  for Clinical Text Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selective Attention Federated Learning: Improving Privacy and Efficiency\n  for Clinical Text Classification"
                },
                "summary": "Federated Learning (FL) faces major challenges regarding communication\noverhead and model privacy when training large language models (LLMs),\nespecially in healthcare applications. To address these, we introduce Selective\nAttention Federated Learning (SAFL), a novel approach that dynamically\nfine-tunes only those transformer layers identified as attention-critical. By\nemploying attention patterns to determine layer importance, SAFL significantly\nreduces communication bandwidth and enhances differential privacy resilience.\nEvaluations on clinical NLP benchmarks (i2b2 Clinical Concept Extraction and\nMIMIC-III discharge summaries) demonstrate that SAFL achieves competitive\nperformance with centralized models while substantially improving communication\nefficiency and privacy preservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) faces major challenges regarding communication\noverhead and model privacy when training large language models (LLMs),\nespecially in healthcare applications. To address these, we introduce Selective\nAttention Federated Learning (SAFL), a novel approach that dynamically\nfine-tunes only those transformer layers identified as attention-critical. By\nemploying attention patterns to determine layer importance, SAFL significantly\nreduces communication bandwidth and enhances differential privacy resilience.\nEvaluations on clinical NLP benchmarks (i2b2 Clinical Concept Extraction and\nMIMIC-III discharge summaries) demonstrate that SAFL achieves competitive\nperformance with centralized models while substantially improving communication\nefficiency and privacy preservation."
                },
                "authors": [
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Lihong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lihong Zhang"
                },
                "author": "Lihong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08738v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08738v2",
                "updated": "2025-04-16T05:59:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    59,
                    2,
                    2,
                    106,
                    0
                ],
                "published": "2025-03-20T18:56:22Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    18,
                    56,
                    22,
                    3,
                    79,
                    0
                ],
                "title": "AI-Driven Sentiment Analytics: Unlocking Business Value in the\n  E-Commerce Landscape_v1",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Driven Sentiment Analytics: Unlocking Business Value in the\n  E-Commerce Landscape_v1"
                },
                "summary": "The rapid growth of e-commerce has led to an overwhelming volume of customer\nfeedback, from product reviews to service interactions. Extracting meaningful\ninsights from this data is crucial for businesses aiming to improve customer\nsatisfaction and optimize decision-making. This paper presents an AI-driven\nsentiment analysis system designed specifically for e-commerce applications,\nbalancing accuracy with interpretability. Our approach integrates traditional\nmachine learning techniques with modern deep learning models, allowing for a\nmore nuanced understanding of customer sentiment while ensuring transparency in\ndecision-making. Experimental results show that our system outperforms standard\nsentiment analysis methods, achieving an accuracy of 89.7% on diverse,\nlarge-scale datasets. Beyond technical performance, real-world implementation\nacross multiple e-commerce platforms demonstrates tangible improvements in\ncustomer engagement and operational efficiency. This study highlights both the\npotential and the challenges of applying AI to sentiment analysis in a\ncommercial setting, offering insights into practical deployment strategies and\nareas for future refinement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of e-commerce has led to an overwhelming volume of customer\nfeedback, from product reviews to service interactions. Extracting meaningful\ninsights from this data is crucial for businesses aiming to improve customer\nsatisfaction and optimize decision-making. This paper presents an AI-driven\nsentiment analysis system designed specifically for e-commerce applications,\nbalancing accuracy with interpretability. Our approach integrates traditional\nmachine learning techniques with modern deep learning models, allowing for a\nmore nuanced understanding of customer sentiment while ensuring transparency in\ndecision-making. Experimental results show that our system outperforms standard\nsentiment analysis methods, achieving an accuracy of 89.7% on diverse,\nlarge-scale datasets. Beyond technical performance, real-world implementation\nacross multiple e-commerce platforms demonstrates tangible improvements in\ncustomer engagement and operational efficiency. This study highlights both the\npotential and the challenges of applying AI to sentiment analysis in a\ncommercial setting, offering insights into practical deployment strategies and\nareas for future refinement."
                },
                "authors": [
                    {
                        "name": "Qianye Wu"
                    },
                    {
                        "name": "Chengxuan Xia"
                    },
                    {
                        "name": "Sixuan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Sixuan Tian"
                },
                "author": "Sixuan Tian",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08738v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08738v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11792v1",
                "updated": "2025-04-16T05:52:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    52,
                    22,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T05:52:22Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    52,
                    22,
                    2,
                    106,
                    0
                ],
                "title": "Large Language Models for Drug Overdose Prediction from Longitudinal\n  Medical Records",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Drug Overdose Prediction from Longitudinal\n  Medical Records"
                },
                "summary": "The ability to predict drug overdose risk from a patient's medical records is\ncrucial for timely intervention and prevention. Traditional machine learning\nmodels have shown promise in analyzing longitudinal medical records for this\ntask. However, recent advancements in large language models (LLMs) offer an\nopportunity to enhance prediction performance by leveraging their ability to\nprocess long textual data and their inherent prior knowledge across diverse\ntasks. In this study, we assess the effectiveness of Open AI's GPT-4o LLM in\npredicting drug overdose events using patients' longitudinal insurance claims\nrecords. We evaluate its performance in both fine-tuned and zero-shot settings,\ncomparing them to strong traditional machine learning methods as baselines. Our\nresults show that LLMs not only outperform traditional models in certain\nsettings but can also predict overdose risk in a zero-shot setting without\ntask-specific training. These findings highlight the potential of LLMs in\nclinical decision support, particularly for drug overdose risk prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to predict drug overdose risk from a patient's medical records is\ncrucial for timely intervention and prevention. Traditional machine learning\nmodels have shown promise in analyzing longitudinal medical records for this\ntask. However, recent advancements in large language models (LLMs) offer an\nopportunity to enhance prediction performance by leveraging their ability to\nprocess long textual data and their inherent prior knowledge across diverse\ntasks. In this study, we assess the effectiveness of Open AI's GPT-4o LLM in\npredicting drug overdose events using patients' longitudinal insurance claims\nrecords. We evaluate its performance in both fine-tuned and zero-shot settings,\ncomparing them to strong traditional machine learning methods as baselines. Our\nresults show that LLMs not only outperform traditional models in certain\nsettings but can also predict overdose risk in a zero-shot setting without\ntask-specific training. These findings highlight the potential of LLMs in\nclinical decision support, particularly for drug overdose risk prediction."
                },
                "authors": [
                    {
                        "name": "Md Sultan Al Nahian"
                    },
                    {
                        "name": "Chris Delcher"
                    },
                    {
                        "name": "Daniel Harris"
                    },
                    {
                        "name": "Peter Akpunonu"
                    },
                    {
                        "name": "Ramakanth Kavuluru"
                    }
                ],
                "author_detail": {
                    "name": "Ramakanth Kavuluru"
                },
                "author": "Ramakanth Kavuluru",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11788v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11788v5",
                "updated": "2025-04-16T05:47:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    47,
                    53,
                    2,
                    106,
                    0
                ],
                "published": "2024-04-17T22:44:22Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    22,
                    44,
                    22,
                    2,
                    108,
                    0
                ],
                "title": "Understanding the Performance Horizon of the Latest ML Workloads with\n  NonGEMM Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Performance Horizon of the Latest ML Workloads with\n  NonGEMM Workloads"
                },
                "summary": "Among ML operators today, GEneralMatrix Multiplication (GEMM)-based operators\nare known to be key operators that build the main backbone of ML models. As\ntheir computational overhead dominates the overall execution time (e.g., 42.8%\n- 96.6% in our results), GEMM operators have been the prime optimization\ntargets for fast ML inference. This led to advanced GPUs and accelerators\navailable today, which provided significant boost in the GEMM performance\ncompared to CPUs, aligned with the lesson from Amdahl's law. However,\naccelerating GEMM has significantly shifted the Amdahl's law's landscape for ML\ninference; due to the decreased GEMM execution time, the relative execution\ntime of non-GEMM operators is now significant. Although the importance of\nnon-GEMM performance is increasing, we have little knowledge about the non-GEMM\nperformance horizon in the latest hardware platforms and models. Therefore, to\nguide non-GEMM-oriented optimizations, we conduct a thorough performance\nanalysis of 17 widely adopted ML models in Hugging Face and Torchvision on\nworkstation and data center platforms with/without GPUs. We discover that\nnon-GEMM performance bottleneck is a considerable issue across all the\nplatforms and models, accounting for 11.3% to 73.6% of total latency, on\naverage. The challenge significantly aggravates when we apply quantization,\nwhich is a common model compression technique, due to the boosted GEMM\nperformance and extra non-GEMM operators for dequantization and requantization.\nTo provide insights into non-GEMM optimization targets, we demystify the most\ndominant non-GEMM operators for each model and deployment software. We also\nshow that widely adopted optimizations such as operator fusion do not\ncompletely address the non-GEMM performance bottleneck, where non-GEMM\noperators still account for 15% to 48% of total latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Among ML operators today, GEneralMatrix Multiplication (GEMM)-based operators\nare known to be key operators that build the main backbone of ML models. As\ntheir computational overhead dominates the overall execution time (e.g., 42.8%\n- 96.6% in our results), GEMM operators have been the prime optimization\ntargets for fast ML inference. This led to advanced GPUs and accelerators\navailable today, which provided significant boost in the GEMM performance\ncompared to CPUs, aligned with the lesson from Amdahl's law. However,\naccelerating GEMM has significantly shifted the Amdahl's law's landscape for ML\ninference; due to the decreased GEMM execution time, the relative execution\ntime of non-GEMM operators is now significant. Although the importance of\nnon-GEMM performance is increasing, we have little knowledge about the non-GEMM\nperformance horizon in the latest hardware platforms and models. Therefore, to\nguide non-GEMM-oriented optimizations, we conduct a thorough performance\nanalysis of 17 widely adopted ML models in Hugging Face and Torchvision on\nworkstation and data center platforms with/without GPUs. We discover that\nnon-GEMM performance bottleneck is a considerable issue across all the\nplatforms and models, accounting for 11.3% to 73.6% of total latency, on\naverage. The challenge significantly aggravates when we apply quantization,\nwhich is a common model compression technique, due to the boosted GEMM\nperformance and extra non-GEMM operators for dequantization and requantization.\nTo provide insights into non-GEMM optimization targets, we demystify the most\ndominant non-GEMM operators for each model and deployment software. We also\nshow that widely adopted optimizations such as operator fusion do not\ncompletely address the non-GEMM performance bottleneck, where non-GEMM\noperators still account for 15% to 48% of total latency."
                },
                "authors": [
                    {
                        "name": "Rachid Karami"
                    },
                    {
                        "name": "Sheng-Chun Kao"
                    },
                    {
                        "name": "Hyoukjun Kwon"
                    }
                ],
                "author_detail": {
                    "name": "Hyoukjun Kwon"
                },
                "author": "Hyoukjun Kwon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11788v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11788v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09080v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09080v2",
                "updated": "2025-04-16T05:45:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    45,
                    31,
                    2,
                    106,
                    0
                ],
                "published": "2024-10-04T21:39:30Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    21,
                    39,
                    30,
                    4,
                    278,
                    0
                ],
                "title": "Leveraging Social Determinants of Health in Alzheimer's Research Using\n  LLM-Augmented Literature Mining and Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Social Determinants of Health in Alzheimer's Research Using\n  LLM-Augmented Literature Mining and Knowledge Graphs"
                },
                "summary": "Growing evidence suggests that social determinants of health (SDoH), a set of\nnonmedical factors, affect individuals' risks of developing Alzheimer's disease\n(AD) and related dementias. Nevertheless, the etiological mechanisms underlying\nsuch relationships remain largely unclear, mainly due to difficulties in\ncollecting relevant information. This study presents a novel, automated\nframework that leverages recent advancements of large language model (LLM) and\nnatural language processing techniques to mine SDoH knowledge from extensive\nliterature and integrate it with AD-related biological entities extracted from\nthe general-purpose knowledge graph PrimeKG. Utilizing graph neural networks,\nwe performed link prediction tasks to evaluate the resultant SDoH-augmented\nknowledge graph. Our framework shows promise for enhancing knowledge discovery\nin AD and can be generalized to other SDoH-related research areas, offering a\nnew tool for exploring the impact of social determinants on health outcomes.\nOur code is available at: https://github.com/hwq0726/SDoHenPKG",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Growing evidence suggests that social determinants of health (SDoH), a set of\nnonmedical factors, affect individuals' risks of developing Alzheimer's disease\n(AD) and related dementias. Nevertheless, the etiological mechanisms underlying\nsuch relationships remain largely unclear, mainly due to difficulties in\ncollecting relevant information. This study presents a novel, automated\nframework that leverages recent advancements of large language model (LLM) and\nnatural language processing techniques to mine SDoH knowledge from extensive\nliterature and integrate it with AD-related biological entities extracted from\nthe general-purpose knowledge graph PrimeKG. Utilizing graph neural networks,\nwe performed link prediction tasks to evaluate the resultant SDoH-augmented\nknowledge graph. Our framework shows promise for enhancing knowledge discovery\nin AD and can be generalized to other SDoH-related research areas, offering a\nnew tool for exploring the impact of social determinants on health outcomes.\nOur code is available at: https://github.com/hwq0726/SDoHenPKG"
                },
                "authors": [
                    {
                        "name": "Tianqi Shang"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Weiqing He"
                    },
                    {
                        "name": "Tianhua Zhai"
                    },
                    {
                        "name": "Dawei Li"
                    },
                    {
                        "name": "Bojian Hou"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Jason H. Moore"
                    },
                    {
                        "name": "Marylyn D. Ritchie"
                    },
                    {
                        "name": "Li Shen"
                    }
                ],
                "author_detail": {
                    "name": "Li Shen"
                },
                "author": "Li Shen",
                "arxiv_comment": "Accepted by AMIA-IS'25: AMIA Informatics Summit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09080v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09080v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11783v1",
                "updated": "2025-04-16T05:36:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    36,
                    28,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T05:36:28Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    36,
                    28,
                    2,
                    106,
                    0
                ],
                "title": "The Digital Cybersecurity Expert: How Far Have We Come?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Digital Cybersecurity Expert: How Far Have We Come?"
                },
                "summary": "The increasing deployment of large language models (LLMs) in the\ncybersecurity domain underscores the need for effective model selection and\nevaluation. However, traditional evaluation methods often overlook specific\ncybersecurity knowledge gaps that contribute to performance limitations. To\naddress this, we develop CSEBenchmark, a fine-grained cybersecurity evaluation\nframework based on 345 knowledge points expected of cybersecurity experts.\nDrawing from cognitive science, these points are categorized into factual,\nconceptual, and procedural types, enabling the design of 11,050 tailored\nmultiple-choice questions. We evaluate 12 popular LLMs on CSEBenchmark and find\nthat even the best-performing model achieves only 85.42% overall accuracy, with\nparticular knowledge gaps in the use of specialized tools and uncommon\ncommands. Different LLMs have unique knowledge gaps. Even large models from the\nsame family may perform poorly on knowledge points where smaller models excel.\nBy identifying and addressing specific knowledge gaps in each LLM, we achieve\nup to an 84% improvement in correcting previously incorrect predictions across\nthree existing benchmarks for two cybersecurity tasks. Furthermore, our\nassessment of each LLM's knowledge alignment with specific cybersecurity roles\nreveals that different models align better with different roles, such as GPT-4o\nfor the Google Senior Intelligence Analyst and Deepseek-V3 for the Amazon\nPrivacy Engineer. These findings underscore the importance of aligning LLM\nselection with the specific knowledge requirements of different cybersecurity\nroles for optimal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing deployment of large language models (LLMs) in the\ncybersecurity domain underscores the need for effective model selection and\nevaluation. However, traditional evaluation methods often overlook specific\ncybersecurity knowledge gaps that contribute to performance limitations. To\naddress this, we develop CSEBenchmark, a fine-grained cybersecurity evaluation\nframework based on 345 knowledge points expected of cybersecurity experts.\nDrawing from cognitive science, these points are categorized into factual,\nconceptual, and procedural types, enabling the design of 11,050 tailored\nmultiple-choice questions. We evaluate 12 popular LLMs on CSEBenchmark and find\nthat even the best-performing model achieves only 85.42% overall accuracy, with\nparticular knowledge gaps in the use of specialized tools and uncommon\ncommands. Different LLMs have unique knowledge gaps. Even large models from the\nsame family may perform poorly on knowledge points where smaller models excel.\nBy identifying and addressing specific knowledge gaps in each LLM, we achieve\nup to an 84% improvement in correcting previously incorrect predictions across\nthree existing benchmarks for two cybersecurity tasks. Furthermore, our\nassessment of each LLM's knowledge alignment with specific cybersecurity roles\nreveals that different models align better with different roles, such as GPT-4o\nfor the Google Senior Intelligence Analyst and Deepseek-V3 for the Amazon\nPrivacy Engineer. These findings underscore the importance of aligning LLM\nselection with the specific knowledge requirements of different cybersecurity\nroles for optimal performance."
                },
                "authors": [
                    {
                        "name": "Dawei Wang"
                    },
                    {
                        "name": "Geng Zhou"
                    },
                    {
                        "name": "Xianglong Li"
                    },
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Ting Qin"
                    },
                    {
                        "name": "Jian Sun"
                    },
                    {
                        "name": "Dan Li"
                    }
                ],
                "author_detail": {
                    "name": "Dan Li"
                },
                "author": "Dan Li",
                "arxiv_comment": "To appear in the IEEE Symposium on Security and Privacy (IEEE S&P)\n  2025, San Francisco, CA, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11781v1",
                "updated": "2025-04-16T05:33:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    33,
                    42,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T05:33:42Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    33,
                    42,
                    2,
                    106,
                    0
                ],
                "title": "ACMamba: Fast Unsupervised Anomaly Detection via An Asymmetrical\n  Consensus State Space Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACMamba: Fast Unsupervised Anomaly Detection via An Asymmetrical\n  Consensus State Space Model"
                },
                "summary": "Unsupervised anomaly detection in hyperspectral images (HSI), aiming to\ndetect unknown targets from backgrounds, is challenging for earth surface\nmonitoring. However, current studies are hindered by steep computational costs\ndue to the high-dimensional property of HSI and dense sampling-based training\nparadigm, constraining their rapid deployment. Our key observation is that,\nduring training, not all samples within the same homogeneous area are\nindispensable, whereas ingenious sampling can provide a powerful substitute for\nreducing costs. Motivated by this, we propose an Asymmetrical Consensus State\nSpace Model (ACMamba) to significantly reduce computational costs without\ncompromising accuracy. Specifically, we design an asymmetrical anomaly\ndetection paradigm that utilizes region-level instances as an efficient\nalternative to dense pixel-level samples. In this paradigm, a low-cost\nMamba-based module is introduced to discover global contextual attributes of\nregions that are essential for HSI reconstruction. Additionally, we develop a\nconsensus learning strategy from the optimization perspective to simultaneously\nfacilitate background reconstruction and anomaly compression, further\nalleviating the negative impact of anomaly reconstruction. Theoretical analysis\nand extensive experiments across eight benchmarks verify the superiority of\nACMamba, demonstrating a faster speed and stronger performance over the\nstate-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised anomaly detection in hyperspectral images (HSI), aiming to\ndetect unknown targets from backgrounds, is challenging for earth surface\nmonitoring. However, current studies are hindered by steep computational costs\ndue to the high-dimensional property of HSI and dense sampling-based training\nparadigm, constraining their rapid deployment. Our key observation is that,\nduring training, not all samples within the same homogeneous area are\nindispensable, whereas ingenious sampling can provide a powerful substitute for\nreducing costs. Motivated by this, we propose an Asymmetrical Consensus State\nSpace Model (ACMamba) to significantly reduce computational costs without\ncompromising accuracy. Specifically, we design an asymmetrical anomaly\ndetection paradigm that utilizes region-level instances as an efficient\nalternative to dense pixel-level samples. In this paradigm, a low-cost\nMamba-based module is introduced to discover global contextual attributes of\nregions that are essential for HSI reconstruction. Additionally, we develop a\nconsensus learning strategy from the optimization perspective to simultaneously\nfacilitate background reconstruction and anomaly compression, further\nalleviating the negative impact of anomaly reconstruction. Theoretical analysis\nand extensive experiments across eight benchmarks verify the superiority of\nACMamba, demonstrating a faster speed and stronger performance over the\nstate-of-the-art."
                },
                "authors": [
                    {
                        "name": "Guanchun Wang"
                    },
                    {
                        "name": "Xiangrong Zhang"
                    },
                    {
                        "name": "Yifei Zhang"
                    },
                    {
                        "name": "Zelin Peng"
                    },
                    {
                        "name": "Tianyang Zhang"
                    },
                    {
                        "name": "Xu Tang"
                    },
                    {
                        "name": "Licheng Jiao"
                    }
                ],
                "author_detail": {
                    "name": "Licheng Jiao"
                },
                "author": "Licheng Jiao",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11777v1",
                "updated": "2025-04-16T05:31:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    31,
                    18,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T05:31:18Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    31,
                    18,
                    2,
                    106,
                    0
                ],
                "title": "Bridging the Semantic Gaps: Improving Medical VQA Consistency with\n  LLM-Augmented Question Sets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Semantic Gaps: Improving Medical VQA Consistency with\n  LLM-Augmented Question Sets"
                },
                "summary": "Medical Visual Question Answering (MVQA) systems can interpret medical images\nin response to natural language queries. However, linguistic variability in\nquestion phrasing often undermines the consistency of these systems. To address\nthis challenge, we propose a Semantically Equivalent Question Augmentation\n(SEQA) framework, which leverages large language models (LLMs) to generate\ndiverse yet semantically equivalent rephrasings of questions. Specifically,\nthis approach enriches linguistic diversity while preserving semantic meaning.\nWe further introduce an evaluation metric, Total Agreement Rate with\nSemantically Equivalent Input and Correct Answer (TAR-SC), which assesses a\nmodel's capability to generate consistent and correct responses to semantically\nequivalent linguistic variations. In addition, we also propose three other\ndiversity metrics - average number of QA items per image (ANQI), average number\nof questions per image with the same answer (ANQA), and average number of\nopen-ended questions per image with the same semantics (ANQS). Using the SEQA\nframework, we augmented the benchmarked MVQA public datasets of SLAKE, VQA-RAD,\nand PathVQA. As a result, all three datasets achieved significant improvements\nby incorporating more semantically equivalent questions: ANQI increased by an\naverage of 86.1, ANQA by 85.1, and ANQS by 46. Subsequent experiments evaluate\nthree MVQA models (M2I2, MUMC, and BiomedGPT) under both zero-shot and\nfine-tuning settings on the enhanced datasets. Experimental results in MVQA\ndatasets show that fine-tuned models achieve an average accuracy improvement of\n19.35%, while our proposed TAR-SC metric shows an average improvement of 11.\n61%, indicating a substantial enhancement in model consistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Visual Question Answering (MVQA) systems can interpret medical images\nin response to natural language queries. However, linguistic variability in\nquestion phrasing often undermines the consistency of these systems. To address\nthis challenge, we propose a Semantically Equivalent Question Augmentation\n(SEQA) framework, which leverages large language models (LLMs) to generate\ndiverse yet semantically equivalent rephrasings of questions. Specifically,\nthis approach enriches linguistic diversity while preserving semantic meaning.\nWe further introduce an evaluation metric, Total Agreement Rate with\nSemantically Equivalent Input and Correct Answer (TAR-SC), which assesses a\nmodel's capability to generate consistent and correct responses to semantically\nequivalent linguistic variations. In addition, we also propose three other\ndiversity metrics - average number of QA items per image (ANQI), average number\nof questions per image with the same answer (ANQA), and average number of\nopen-ended questions per image with the same semantics (ANQS). Using the SEQA\nframework, we augmented the benchmarked MVQA public datasets of SLAKE, VQA-RAD,\nand PathVQA. As a result, all three datasets achieved significant improvements\nby incorporating more semantically equivalent questions: ANQI increased by an\naverage of 86.1, ANQA by 85.1, and ANQS by 46. Subsequent experiments evaluate\nthree MVQA models (M2I2, MUMC, and BiomedGPT) under both zero-shot and\nfine-tuning settings on the enhanced datasets. Experimental results in MVQA\ndatasets show that fine-tuned models achieve an average accuracy improvement of\n19.35%, while our proposed TAR-SC metric shows an average improvement of 11.\n61%, indicating a substantial enhancement in model consistency."
                },
                "authors": [
                    {
                        "name": "Yongpei Ma"
                    },
                    {
                        "name": "Pengyu Wang"
                    },
                    {
                        "name": "Adam Dunn"
                    },
                    {
                        "name": "Usman Naseem"
                    },
                    {
                        "name": "Jinman Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jinman Kim"
                },
                "author": "Jinman Kim",
                "arxiv_comment": "The first two listed authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01795v2",
                "updated": "2025-04-16T05:30:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    30,
                    34,
                    2,
                    106,
                    0
                ],
                "published": "2024-10-02T17:53:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    53,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "Knowledge-Driven Feature Selection and Engineering for Genotype Data\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Driven Feature Selection and Engineering for Genotype Data\n  with Large Language Models"
                },
                "summary": "Predicting phenotypes with complex genetic bases based on a small,\ninterpretable set of variant features remains a challenging task.\nConventionally, data-driven approaches are utilized for this task, yet the high\ndimensional nature of genotype data makes the analysis and prediction\ndifficult. Motivated by the extensive knowledge encoded in pre-trained LLMs and\ntheir success in processing complex biomedical concepts, we set to examine the\nability of LLMs in feature selection and engineering for tabular genotype data,\nwith a novel knowledge-driven framework. We develop FREEFORM, Free-flow\nReasoning and Ensembling for Enhanced Feature Output and Robust Modeling,\ndesigned with chain-of-thought and ensembling principles, to select and\nengineer features with the intrinsic knowledge of LLMs. Evaluated on two\ndistinct genotype-phenotype datasets, genetic ancestry and hereditary hearing\nloss, we find this framework outperforms several data-driven methods,\nparticularly on low-shot regimes. FREEFORM is available as open-source\nframework at GitHub: https://github.com/PennShenLab/FREEFORM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting phenotypes with complex genetic bases based on a small,\ninterpretable set of variant features remains a challenging task.\nConventionally, data-driven approaches are utilized for this task, yet the high\ndimensional nature of genotype data makes the analysis and prediction\ndifficult. Motivated by the extensive knowledge encoded in pre-trained LLMs and\ntheir success in processing complex biomedical concepts, we set to examine the\nability of LLMs in feature selection and engineering for tabular genotype data,\nwith a novel knowledge-driven framework. We develop FREEFORM, Free-flow\nReasoning and Ensembling for Enhanced Feature Output and Robust Modeling,\ndesigned with chain-of-thought and ensembling principles, to select and\nengineer features with the intrinsic knowledge of LLMs. Evaluated on two\ndistinct genotype-phenotype datasets, genetic ancestry and hereditary hearing\nloss, we find this framework outperforms several data-driven methods,\nparticularly on low-shot regimes. FREEFORM is available as open-source\nframework at GitHub: https://github.com/PennShenLab/FREEFORM."
                },
                "authors": [
                    {
                        "name": "Joseph Lee"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Jae Young Baik"
                    },
                    {
                        "name": "Xiaoxi Liu"
                    },
                    {
                        "name": "Zhen Tan"
                    },
                    {
                        "name": "Dawei Li"
                    },
                    {
                        "name": "Zixuan Wen"
                    },
                    {
                        "name": "Bojian Hou"
                    },
                    {
                        "name": "Duy Duong-Tran"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Li Shen"
                    }
                ],
                "author_detail": {
                    "name": "Li Shen"
                },
                "author": "Li Shen",
                "arxiv_comment": "accepted by AMIA-IS'25: AMIA Informatics Summit [Marco Ramoni\n  Distinguished Paper Award for Translational Bioinformatics]",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09566v2",
                "updated": "2025-04-16T05:02:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    2,
                    1,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-13T13:35:41Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    13,
                    35,
                    41,
                    6,
                    103,
                    0
                ],
                "title": "Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution"
                },
                "summary": "Chain-of-Thought (CoT) prompting enhances the reasoning of large language\nmodels (LLMs) by decomposing problems into sequential steps, mimicking human\nlogic and reducing errors. However, complex tasks with vast solution spaces and\nvague constraints often exceed the capacity of a single reasoning chain.\nInspired by Minimal Free Resolution (MFR) in commutative algebra and algebraic\ngeometry, we propose Syzygy of Thoughts (SoT)-a novel framework that extends\nCoT by introducing auxiliary, interrelated reasoning paths. SoT captures deeper\nlogical dependencies, enabling more robust and structured problem-solving. MFR\ndecomposes a module into a sequence of free modules with minimal rank,\nproviding a structured analytical approach to complex systems. This method\nintroduces the concepts of \"Module\", \"Betti numbers\",\"Freeness\", \"Mapping\",\n\"Exactness\" and \"Minimality\", enabling the systematic decomposition of the\noriginal complex problem into logically complete minimal subproblems while\npreserving key problem features and reducing reasoning length. We tested SoT\nacross diverse datasets (e.g., GSM8K, MATH) and models (e.g., GPT-4o-mini,\nQwen2.5), achieving inference accuracy that matches or surpasses mainstream\nCoTs standards. Additionally, by aligning the sampling process with algebraic\nconstraints, our approach enhances the scalability of inference time in LLMs,\nensuring both transparent reasoning and high performance. Our code will be\npublicly available at https://github.com/dlMARiA/Syzygy-of-thoughts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting enhances the reasoning of large language\nmodels (LLMs) by decomposing problems into sequential steps, mimicking human\nlogic and reducing errors. However, complex tasks with vast solution spaces and\nvague constraints often exceed the capacity of a single reasoning chain.\nInspired by Minimal Free Resolution (MFR) in commutative algebra and algebraic\ngeometry, we propose Syzygy of Thoughts (SoT)-a novel framework that extends\nCoT by introducing auxiliary, interrelated reasoning paths. SoT captures deeper\nlogical dependencies, enabling more robust and structured problem-solving. MFR\ndecomposes a module into a sequence of free modules with minimal rank,\nproviding a structured analytical approach to complex systems. This method\nintroduces the concepts of \"Module\", \"Betti numbers\",\"Freeness\", \"Mapping\",\n\"Exactness\" and \"Minimality\", enabling the systematic decomposition of the\noriginal complex problem into logically complete minimal subproblems while\npreserving key problem features and reducing reasoning length. We tested SoT\nacross diverse datasets (e.g., GSM8K, MATH) and models (e.g., GPT-4o-mini,\nQwen2.5), achieving inference accuracy that matches or surpasses mainstream\nCoTs standards. Additionally, by aligning the sampling process with algebraic\nconstraints, our approach enhances the scalability of inference time in LLMs,\nensuring both transparent reasoning and high performance. Our code will be\npublicly available at https://github.com/dlMARiA/Syzygy-of-thoughts."
                },
                "authors": [
                    {
                        "name": "Chenghao Li"
                    },
                    {
                        "name": "Chaoning Zhang"
                    },
                    {
                        "name": "Yi Lu"
                    },
                    {
                        "name": "Jiaquan Zhang"
                    },
                    {
                        "name": "Qigan Sun"
                    },
                    {
                        "name": "Xudong Wang"
                    },
                    {
                        "name": "Jiwei Wei"
                    },
                    {
                        "name": "Guoqing Wang"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Heng Tao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Heng Tao Shen"
                },
                "author": "Heng Tao Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11765v1",
                "updated": "2025-04-16T04:59:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    59,
                    18,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T04:59:18Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    59,
                    18,
                    2,
                    106,
                    0
                ],
                "title": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference\n  in RAG-Powered LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference\n  in RAG-Powered LLMs"
                },
                "summary": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration."
                },
                "authors": [
                    {
                        "name": "Hyungwoo Lee"
                    },
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Jungmin So"
                    },
                    {
                        "name": "Myung-Hoon Cha"
                    },
                    {
                        "name": "Hong-Yeon Kim"
                    },
                    {
                        "name": "James J. Kim"
                    },
                    {
                        "name": "Youngjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Kim"
                },
                "arxiv_affiliation": "Dept. of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea",
                "author": "Youngjae Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24235v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24235v2",
                "updated": "2025-04-16T04:32:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    32,
                    24,
                    2,
                    106,
                    0
                ],
                "published": "2025-03-31T15:46:15Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    46,
                    15,
                    0,
                    90,
                    0
                ],
                "title": "What, How, Where, and How Well? A Survey on Test-Time Scaling in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What, How, Where, and How Well? A Survey on Test-Time Scaling in Large\n  Language Models"
                },
                "summary": "As enthusiasm for scaling computation (data and parameters) in the\npretraining era gradually diminished, test-time scaling (TTS), also referred to\nas ``test-time computing'' has emerged as a prominent research focus. Recent\nstudies demonstrate that TTS can further elicit the problem-solving\ncapabilities of large language models (LLMs), enabling significant\nbreakthroughs not only in specialized reasoning tasks, such as mathematics and\ncoding, but also in general tasks like open-ended Q&A. However, despite the\nexplosion of recent efforts in this area, there remains an urgent need for a\ncomprehensive survey offering a systemic understanding. To fill this gap, we\npropose a unified, multidimensional framework structured along four core\ndimensions of TTS research: what to scale, how to scale, where to scale, and\nhow well to scale. Building upon this taxonomy, we conduct an extensive review\nof methods, application scenarios, and assessment aspects, and present an\norganized decomposition that highlights the unique functional roles of\nindividual techniques within the broader TTS landscape. From this analysis, we\ndistill the major developmental trajectories of TTS to date and offer hands-on\nguidelines for practical deployment. Furthermore, we identify several open\nchallenges and offer insights into promising future directions, including\nfurther scaling, clarifying the functional essence of techniques, generalizing\nto more tasks, and more attributions. Our repository is available on\nhttps://github.com/testtimescaling/testtimescaling.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As enthusiasm for scaling computation (data and parameters) in the\npretraining era gradually diminished, test-time scaling (TTS), also referred to\nas ``test-time computing'' has emerged as a prominent research focus. Recent\nstudies demonstrate that TTS can further elicit the problem-solving\ncapabilities of large language models (LLMs), enabling significant\nbreakthroughs not only in specialized reasoning tasks, such as mathematics and\ncoding, but also in general tasks like open-ended Q&A. However, despite the\nexplosion of recent efforts in this area, there remains an urgent need for a\ncomprehensive survey offering a systemic understanding. To fill this gap, we\npropose a unified, multidimensional framework structured along four core\ndimensions of TTS research: what to scale, how to scale, where to scale, and\nhow well to scale. Building upon this taxonomy, we conduct an extensive review\nof methods, application scenarios, and assessment aspects, and present an\norganized decomposition that highlights the unique functional roles of\nindividual techniques within the broader TTS landscape. From this analysis, we\ndistill the major developmental trajectories of TTS to date and offer hands-on\nguidelines for practical deployment. Furthermore, we identify several open\nchallenges and offer insights into promising future directions, including\nfurther scaling, clarifying the functional essence of techniques, generalizing\nto more tasks, and more attributions. Our repository is available on\nhttps://github.com/testtimescaling/testtimescaling.github.io/"
                },
                "authors": [
                    {
                        "name": "Qiyuan Zhang"
                    },
                    {
                        "name": "Fuyuan Lyu"
                    },
                    {
                        "name": "Zexu Sun"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Weixu Zhang"
                    },
                    {
                        "name": "Zhihan Guo"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Niklas Muennighoff"
                    },
                    {
                        "name": "Irwin King"
                    },
                    {
                        "name": "Xue Liu"
                    },
                    {
                        "name": "Chen Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ma"
                },
                "author": "Chen Ma",
                "arxiv_comment": "v2: Creating the GitHub repository, Citing some missed works,\n  Incorporating two new domains (agentic and evaluation) in where to scale,\n  Incorporating one direction (thoughtology research) in challenge and future\n  work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24235v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24235v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08626v2",
                "updated": "2025-04-16T04:31:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    31,
                    6,
                    2,
                    106,
                    0
                ],
                "published": "2024-07-11T16:05:56Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    16,
                    5,
                    56,
                    3,
                    193,
                    0
                ],
                "title": "RoboMorph: Evolving Robot Morphology using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboMorph: Evolving Robot Morphology using Large Language Models"
                },
                "summary": "We introduce RoboMorph, an automated approach for generating and optimizing\nmodular robot designs using large language models (LLMs) and evolutionary\nalgorithms. In this framework, we represent each robot design as a grammar and\nleverage the capabilities of LLMs to navigate the extensive robot design space,\nwhich is traditionally time-consuming and computationally demanding. By\nintroducing a best-shot prompting technique and a reinforcement learning-based\ncontrol algorithm, RoboMorph iteratively improves robot designs through\nfeedback loops. Experimental results demonstrate that RoboMorph successfully\ngenerates nontrivial robots optimized for different terrains while showcasing\nimprovements in robot morphology over successive evolutions. Our approach\nhighlights the potential of using LLMs for data-driven, modular robot design,\nproviding a promising methodology that can be extended to other domains with\nsimilar design frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RoboMorph, an automated approach for generating and optimizing\nmodular robot designs using large language models (LLMs) and evolutionary\nalgorithms. In this framework, we represent each robot design as a grammar and\nleverage the capabilities of LLMs to navigate the extensive robot design space,\nwhich is traditionally time-consuming and computationally demanding. By\nintroducing a best-shot prompting technique and a reinforcement learning-based\ncontrol algorithm, RoboMorph iteratively improves robot designs through\nfeedback loops. Experimental results demonstrate that RoboMorph successfully\ngenerates nontrivial robots optimized for different terrains while showcasing\nimprovements in robot morphology over successive evolutions. Our approach\nhighlights the potential of using LLMs for data-driven, modular robot design,\nproviding a promising methodology that can be extended to other domains with\nsimilar design frameworks."
                },
                "authors": [
                    {
                        "name": "Kevin Qiu"
                    },
                    {
                        "name": "Wadysaw Paucki"
                    },
                    {
                        "name": "Krzysztof Ciebiera"
                    },
                    {
                        "name": "Pawe Fijakowski"
                    },
                    {
                        "name": "Marek Cygan"
                    },
                    {
                        "name": "ukasz Kuciski"
                    }
                ],
                "author_detail": {
                    "name": "ukasz Kuciski"
                },
                "author": "ukasz Kuciski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11302v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11302v3",
                "updated": "2025-04-16T04:30:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    30,
                    56,
                    2,
                    106,
                    0
                ],
                "published": "2024-12-15T20:27:45Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    20,
                    27,
                    45,
                    6,
                    350,
                    0
                ],
                "title": "Sequence-Level Leakage Risk of Training Data in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence-Level Leakage Risk of Training Data in Large Language Models"
                },
                "summary": "This work quantifies the risk of training data leakage from LLMs (Large\nLanguage Models) using sequence-level probabilities. Computing extraction\nprobabilities for individual sequences provides finer-grained information than\nhas been studied in prior benchmarking work. We re-analyze the effects of\ndecoding schemes, model sizes, prefix lengths, partial sequence leakages, and\ntoken positions to uncover new insights that were not possible in previous\nworks due to their choice of metrics. We perform this study on two pre-trained\nmodels, Llama and OPT, trained on the Common Crawl and The Pile respectively.\nWe discover that 1) Extraction Rate, the predominant metric used in prior\nquantification work, underestimates the threat of leakage of training data in\nrandomized LLMs by as much as 2.14X. 2) Although on average, larger models and\nlonger prefixes can extract more data, this is not true for a substantial\nportion of individual sequences. 30.4-41.5% of our sequences are easier to\nextract with either shorter prefixes or smaller models. 3) Contrary to previous\nbeliefs, partial leakage in commonly used decoding schemes like top-k and top-p\nis not easier than leaking verbatim training data. The aim of this work is to\nencourage the adoption of this metric for future work on quantification of\ntraining data extraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work quantifies the risk of training data leakage from LLMs (Large\nLanguage Models) using sequence-level probabilities. Computing extraction\nprobabilities for individual sequences provides finer-grained information than\nhas been studied in prior benchmarking work. We re-analyze the effects of\ndecoding schemes, model sizes, prefix lengths, partial sequence leakages, and\ntoken positions to uncover new insights that were not possible in previous\nworks due to their choice of metrics. We perform this study on two pre-trained\nmodels, Llama and OPT, trained on the Common Crawl and The Pile respectively.\nWe discover that 1) Extraction Rate, the predominant metric used in prior\nquantification work, underestimates the threat of leakage of training data in\nrandomized LLMs by as much as 2.14X. 2) Although on average, larger models and\nlonger prefixes can extract more data, this is not true for a substantial\nportion of individual sequences. 30.4-41.5% of our sequences are easier to\nextract with either shorter prefixes or smaller models. 3) Contrary to previous\nbeliefs, partial leakage in commonly used decoding schemes like top-k and top-p\nis not easier than leaking verbatim training data. The aim of this work is to\nencourage the adoption of this metric for future work on quantification of\ntraining data extraction."
                },
                "authors": [
                    {
                        "name": "Trishita Tiwari"
                    },
                    {
                        "name": "G. Edward Suh"
                    }
                ],
                "author_detail": {
                    "name": "G. Edward Suh"
                },
                "author": "G. Edward Suh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11302v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11302v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11750v1",
                "updated": "2025-04-16T04:02:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    2,
                    39,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T04:02:39Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    2,
                    39,
                    2,
                    106,
                    0
                ],
                "title": "Characterizing and Optimizing LLM Inference Workloads on CPU-GPU Coupled\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing and Optimizing LLM Inference Workloads on CPU-GPU Coupled\n  Architectures"
                },
                "summary": "Large language model (LLM)-based inference workloads increasingly dominate\ndata center costs and resource utilization. Therefore, understanding the\ninference workload characteristics on evolving CPU-GPU coupled architectures is\ncrucial for optimization. This paper presents an in-depth analysis of LLM\ninference behavior on loosely-coupled (PCIe A100/H100) and closely-coupled\n(GH200) systems. We analyze performance dynamics using fine-grained\noperator-to-kernel trace analysis, facilitated by our novel profiler SKIP and\nmetrics like Total Kernel Launch and Queuing Time (TKLQT). Results show that\nclosely-coupled (CC) GH200 significantly outperforms loosely-coupled (LC)\nsystems at large batch sizes, achieving 1.9x-2.7x faster prefill latency for\nLlama 3.2-1B. However, our analysis also reveals that GH200 remains CPU-bound\nup to 4x larger batch sizes than LC systems. In this extended CPU-bound region,\nwe identify the performance characteristics of the Grace CPU as a key factor\ncontributing to higher inference latency at low batch sizes on GH200. We\ndemonstrate that TKLQT accurately identifies this CPU/GPU-bound transition\npoint. Based on this analysis, we further show that kernel fusion offers\nsignificant potential to mitigate GH200's low-batch latency bottleneck by\nreducing kernel launch overhead. This detailed kernel-level characterization\nprovides critical insights for optimizing diverse CPU-GPU coupling strategies.\nThis work is an initial effort, and we plan to explore other major AI/DL\nworkloads that demand different degrees of CPU-GPU heterogeneous architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based inference workloads increasingly dominate\ndata center costs and resource utilization. Therefore, understanding the\ninference workload characteristics on evolving CPU-GPU coupled architectures is\ncrucial for optimization. This paper presents an in-depth analysis of LLM\ninference behavior on loosely-coupled (PCIe A100/H100) and closely-coupled\n(GH200) systems. We analyze performance dynamics using fine-grained\noperator-to-kernel trace analysis, facilitated by our novel profiler SKIP and\nmetrics like Total Kernel Launch and Queuing Time (TKLQT). Results show that\nclosely-coupled (CC) GH200 significantly outperforms loosely-coupled (LC)\nsystems at large batch sizes, achieving 1.9x-2.7x faster prefill latency for\nLlama 3.2-1B. However, our analysis also reveals that GH200 remains CPU-bound\nup to 4x larger batch sizes than LC systems. In this extended CPU-bound region,\nwe identify the performance characteristics of the Grace CPU as a key factor\ncontributing to higher inference latency at low batch sizes on GH200. We\ndemonstrate that TKLQT accurately identifies this CPU/GPU-bound transition\npoint. Based on this analysis, we further show that kernel fusion offers\nsignificant potential to mitigate GH200's low-batch latency bottleneck by\nreducing kernel launch overhead. This detailed kernel-level characterization\nprovides critical insights for optimizing diverse CPU-GPU coupling strategies.\nThis work is an initial effort, and we plan to explore other major AI/DL\nworkloads that demand different degrees of CPU-GPU heterogeneous architectures."
                },
                "authors": [
                    {
                        "name": "Prabhu Vellaisamy"
                    },
                    {
                        "name": "Thomas Labonte"
                    },
                    {
                        "name": "Sourav Chakraborty"
                    },
                    {
                        "name": "Matt Turner"
                    },
                    {
                        "name": "Samantika Sury"
                    },
                    {
                        "name": "John Paul Shen"
                    }
                ],
                "author_detail": {
                    "name": "John Paul Shen"
                },
                "author": "John Paul Shen",
                "arxiv_comment": "Accepted for ISPASS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11741v1",
                "updated": "2025-04-16T03:39:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    39,
                    38,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T03:39:38Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    39,
                    38,
                    2,
                    106,
                    0
                ],
                "title": "Climbing the Ladder of Reasoning: What LLMs Can-and Still Can't-Solve\n  after SFT?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Climbing the Ladder of Reasoning: What LLMs Can-and Still Can't-Solve\n  after SFT?"
                },
                "summary": "Recent supervised fine-tuning (SFT) approaches have significantly improved\nlanguage models' performance on mathematical reasoning tasks, even when models\nare trained at a small scale. However, the specific capabilities enhanced\nthrough such fine-tuning remain poorly understood. In this paper, we conduct a\ndetailed analysis of model performance on the AIME24 dataset to understand how\nreasoning capabilities evolve. We discover a ladder-like structure in problem\ndifficulty, categorize questions into four tiers (Easy, Medium, Hard, and\nExtremely Hard (Exh)), and identify the specific requirements for advancing\nbetween tiers. We find that progression from Easy to Medium tier requires\nadopting an R1 reasoning style with minimal SFT (500-1K instances), while\nHard-level questions suffer from frequent model's errors at each step of the\nreasoning chain, with accuracy plateauing at around 65% despite logarithmic\nscaling. Exh-level questions present a fundamentally different challenge; they\nrequire unconventional problem-solving skills that current models uniformly\nstruggle with. Additional findings reveal that carefully curated small-scale\ndatasets offer limited advantage-scaling dataset size proves far more\neffective. Our analysis provides a clearer roadmap for advancing language model\ncapabilities in mathematical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent supervised fine-tuning (SFT) approaches have significantly improved\nlanguage models' performance on mathematical reasoning tasks, even when models\nare trained at a small scale. However, the specific capabilities enhanced\nthrough such fine-tuning remain poorly understood. In this paper, we conduct a\ndetailed analysis of model performance on the AIME24 dataset to understand how\nreasoning capabilities evolve. We discover a ladder-like structure in problem\ndifficulty, categorize questions into four tiers (Easy, Medium, Hard, and\nExtremely Hard (Exh)), and identify the specific requirements for advancing\nbetween tiers. We find that progression from Easy to Medium tier requires\nadopting an R1 reasoning style with minimal SFT (500-1K instances), while\nHard-level questions suffer from frequent model's errors at each step of the\nreasoning chain, with accuracy plateauing at around 65% despite logarithmic\nscaling. Exh-level questions present a fundamentally different challenge; they\nrequire unconventional problem-solving skills that current models uniformly\nstruggle with. Additional findings reveal that carefully curated small-scale\ndatasets offer limited advantage-scaling dataset size proves far more\neffective. Our analysis provides a clearer roadmap for advancing language model\ncapabilities in mathematical reasoning."
                },
                "authors": [
                    {
                        "name": "Yiyou Sun"
                    },
                    {
                        "name": "Georgia Zhou"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Dacheng Li"
                    },
                    {
                        "name": "Nouha Dziri"
                    },
                    {
                        "name": "Dawn Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawn Song"
                },
                "author": "Dawn Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11107v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11107v2",
                "updated": "2025-04-16T03:33:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    33,
                    29,
                    2,
                    106,
                    0
                ],
                "published": "2025-01-19T16:35:09Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    16,
                    35,
                    9,
                    6,
                    19,
                    0
                ],
                "title": "ChaosEater: Fully Automating Chaos Engineering with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChaosEater: Fully Automating Chaos Engineering with Large Language\n  Models"
                },
                "summary": "Chaos Engineering (CE) is an engineering technique aimed at improving the\nresiliency of distributed systems. It involves artificially injecting specific\nfailures into a distributed system and observing its behavior in response.\nBased on the observation, the system can be proactively improved to handle\nthose failures. Recent CE tools implement the automated execution of predefined\nCE experiments. However, defining these experiments and improving the system\nbased on the experimental results still remain manual. To reduce the costs of\nthe manual operations, we propose ChaosEater, a system for automating the\nentire CE operations with Large Language Models (LLMs). It predefines the\nagentic workflow according to a systematic CE cycle and assigns subdivided\noperations within the workflow to LLMs. ChaosEater targets CE for Kubernetes\nsystems, which are managed through code (i.e., Infrastructure as Code).\nTherefore, the LLMs in ChaosEater perform software engineering tasks to\ncomplete CE cycles, including requirement definition, code generation,\ndebugging, and testing. We evaluate ChaosEater through case studies on both\nsmall and large Kubernetes systems. The results demonstrate that it stably\ncompletes reasonable single CE cycles with significantly low time and monetary\ncosts. The CE cycles are also qualitatively validated by human engineers and\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chaos Engineering (CE) is an engineering technique aimed at improving the\nresiliency of distributed systems. It involves artificially injecting specific\nfailures into a distributed system and observing its behavior in response.\nBased on the observation, the system can be proactively improved to handle\nthose failures. Recent CE tools implement the automated execution of predefined\nCE experiments. However, defining these experiments and improving the system\nbased on the experimental results still remain manual. To reduce the costs of\nthe manual operations, we propose ChaosEater, a system for automating the\nentire CE operations with Large Language Models (LLMs). It predefines the\nagentic workflow according to a systematic CE cycle and assigns subdivided\noperations within the workflow to LLMs. ChaosEater targets CE for Kubernetes\nsystems, which are managed through code (i.e., Infrastructure as Code).\nTherefore, the LLMs in ChaosEater perform software engineering tasks to\ncomplete CE cycles, including requirement definition, code generation,\ndebugging, and testing. We evaluate ChaosEater through case studies on both\nsmall and large Kubernetes systems. The results demonstrate that it stably\ncompletes reasonable single CE cycles with significantly low time and monetary\ncosts. The CE cycles are also qualitatively validated by human engineers and\nLLMs."
                },
                "authors": [
                    {
                        "name": "Daisuke Kikuta"
                    },
                    {
                        "name": "Hiroki Ikeuchi"
                    },
                    {
                        "name": "Kengo Tajiri"
                    }
                ],
                "author_detail": {
                    "name": "Kengo Tajiri"
                },
                "author": "Kengo Tajiri",
                "arxiv_comment": "114 pages (7 main), 11 figures. Project page:\n  https://ntt-dkiku.github.io/chaos-eater",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11107v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11107v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11739v1",
                "updated": "2025-04-16T03:33:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    33,
                    25,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T03:33:25Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    33,
                    25,
                    2,
                    106,
                    0
                ],
                "title": "The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for\n  Text-to-Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for\n  Text-to-Video Generation"
                },
                "summary": "The evolution of Text-to-video (T2V) generative models, trained on\nlarge-scale datasets, has been marked by significant progress. However, the\nsensitivity of T2V generative models to input prompts highlights the critical\nrole of prompt design in influencing generative outcomes. Prior research has\npredominantly relied on Large Language Models (LLMs) to align user-provided\nprompts with the distribution of training prompts, albeit without tailored\nguidance encompassing prompt vocabulary and sentence structure nuances. To this\nend, we introduce \\textbf{RAPO}, a novel \\textbf{R}etrieval-\\textbf{A}ugmented\n\\textbf{P}rompt \\textbf{O}ptimization framework. In order to address potential\ninaccuracies and ambiguous details generated by LLM-generated prompts. RAPO\nrefines the naive prompts through dual optimization branches, selecting the\nsuperior prompt for T2V generation. The first branch augments user prompts with\ndiverse modifiers extracted from a learned relational graph, refining them to\nalign with the format of training prompts via a fine-tuned LLM. Conversely, the\nsecond branch rewrites the naive prompt using a pre-trained LLM following a\nwell-defined instruction set. Extensive experiments demonstrate that RAPO can\neffectively enhance both the static and dynamic dimensions of generated videos,\ndemonstrating the significance of prompt optimization for user-provided\nprompts. Project website:\n\\href{https://whynothaha.github.io/Prompt_optimizer/RAPO.html}{GitHub}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of Text-to-video (T2V) generative models, trained on\nlarge-scale datasets, has been marked by significant progress. However, the\nsensitivity of T2V generative models to input prompts highlights the critical\nrole of prompt design in influencing generative outcomes. Prior research has\npredominantly relied on Large Language Models (LLMs) to align user-provided\nprompts with the distribution of training prompts, albeit without tailored\nguidance encompassing prompt vocabulary and sentence structure nuances. To this\nend, we introduce \\textbf{RAPO}, a novel \\textbf{R}etrieval-\\textbf{A}ugmented\n\\textbf{P}rompt \\textbf{O}ptimization framework. In order to address potential\ninaccuracies and ambiguous details generated by LLM-generated prompts. RAPO\nrefines the naive prompts through dual optimization branches, selecting the\nsuperior prompt for T2V generation. The first branch augments user prompts with\ndiverse modifiers extracted from a learned relational graph, refining them to\nalign with the format of training prompts via a fine-tuned LLM. Conversely, the\nsecond branch rewrites the naive prompt using a pre-trained LLM following a\nwell-defined instruction set. Extensive experiments demonstrate that RAPO can\neffectively enhance both the static and dynamic dimensions of generated videos,\ndemonstrating the significance of prompt optimization for user-provided\nprompts. Project website:\n\\href{https://whynothaha.github.io/Prompt_optimizer/RAPO.html}{GitHub}."
                },
                "authors": [
                    {
                        "name": "Bingjie Gao"
                    },
                    {
                        "name": "Xinyu Gao"
                    },
                    {
                        "name": "Xiaoxue Wu"
                    },
                    {
                        "name": "Yujie Zhou"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Li Niu"
                    },
                    {
                        "name": "Xinyuan Chen"
                    },
                    {
                        "name": "Yaohui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yaohui Wang"
                },
                "author": "Yaohui Wang",
                "arxiv_comment": "accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11197v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11197v2",
                "updated": "2025-04-16T03:32:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    32,
                    23,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-15T13:53:08Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    53,
                    8,
                    1,
                    105,
                    0
                ],
                "title": "Efficient Distributed Retrieval-Augmented Generation for Enhancing\n  Language Model Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Distributed Retrieval-Augmented Generation for Enhancing\n  Language Model Performance"
                },
                "summary": "Small language models (SLMs) support efficient deployments on\nresource-constrained edge devices, but their limited capacity compromises\ninference performance. Retrieval-augmented generation (RAG) is a promising\nsolution to enhance model performance by integrating external databases,\nwithout requiring intensive on-device model retraining. However, large-scale\npublic databases and user-specific private contextual documents are typically\nlocated on the cloud and the device separately, while existing RAG\nimplementations are primarily centralized. To bridge this gap, we propose\nDRAGON, a distributed RAG framework to enhance on-device SLMs through both\ngeneral and personal knowledge without the risk of leaking document privacy.\nSpecifically, DRAGON decomposes multi-document RAG into multiple parallel token\ngeneration processes performed independently and locally on the cloud and the\ndevice, and employs a newly designed Speculative Aggregation, a dual-side\nspeculative algorithm to avoid frequent output synchronization between the\ncloud and device. A new scheduling algorithm is further introduced to identify\nthe optimal aggregation side based on real-time network conditions. Evaluations\non real-world hardware testbed demonstrate a significant performance\nimprovement of DRAGON-up to 1.9x greater gains over standalone SLM compared to\nthe centralized RAG, substantial reduction in per-token latency, and negligible\nTime to First Token (TTFT) overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small language models (SLMs) support efficient deployments on\nresource-constrained edge devices, but their limited capacity compromises\ninference performance. Retrieval-augmented generation (RAG) is a promising\nsolution to enhance model performance by integrating external databases,\nwithout requiring intensive on-device model retraining. However, large-scale\npublic databases and user-specific private contextual documents are typically\nlocated on the cloud and the device separately, while existing RAG\nimplementations are primarily centralized. To bridge this gap, we propose\nDRAGON, a distributed RAG framework to enhance on-device SLMs through both\ngeneral and personal knowledge without the risk of leaking document privacy.\nSpecifically, DRAGON decomposes multi-document RAG into multiple parallel token\ngeneration processes performed independently and locally on the cloud and the\ndevice, and employs a newly designed Speculative Aggregation, a dual-side\nspeculative algorithm to avoid frequent output synchronization between the\ncloud and device. A new scheduling algorithm is further introduced to identify\nthe optimal aggregation side based on real-time network conditions. Evaluations\non real-world hardware testbed demonstrate a significant performance\nimprovement of DRAGON-up to 1.9x greater gains over standalone SLM compared to\nthe centralized RAG, substantial reduction in per-token latency, and negligible\nTime to First Token (TTFT) overhead."
                },
                "authors": [
                    {
                        "name": "Shangyu Liu"
                    },
                    {
                        "name": "Zhenzhe Zheng"
                    },
                    {
                        "name": "Xiaoyao Huang"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Guihai Chen"
                    },
                    {
                        "name": "Jie Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Wu"
                },
                "author": "Jie Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11197v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11197v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11019v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11019v2",
                "updated": "2025-04-16T03:22:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    22,
                    41,
                    2,
                    106,
                    0
                ],
                "published": "2025-02-16T07:06:17Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    7,
                    6,
                    17,
                    6,
                    47,
                    0
                ],
                "title": "Unlocking the Power of Function Vectors for Characterizing and\n  Mitigating Catastrophic Forgetting in Continual Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking the Power of Function Vectors for Characterizing and\n  Mitigating Catastrophic Forgetting in Continual Instruction Tuning"
                },
                "summary": "Catastrophic forgetting (CF) poses a significant challenge in machine\nlearning, where a model forgets previously learned information upon learning\nnew tasks. Despite the advanced capabilities of Large Language Models (LLMs),\nthey continue to face challenges with CF during continual learning. The\nmajority of existing research focuses on analyzing forgetting patterns through\na singular training sequence, thereby overlooking the intricate effects that\ndiverse tasks have on model behavior. Our study explores CF across various\nsettings, discovering that model forgetting is influenced by both the specific\ntraining tasks and the models themselves. To this end, we interpret forgetting\nby examining the function vector (FV), a compact representation of functions in\nLLMs, offering a model-dependent indicator for the occurrence of CF. Through\ntheoretical and empirical analyses, we demonstrated that CF in LLMs primarily\nstems from biases in function activation rather than the overwriting of task\nprocessing functions. Leveraging these insights, we propose a novel function\nvector guided training methodology, incorporating a regularization technique to\nstabilize the FV and mitigate forgetting. Empirical tests on four benchmarks\nconfirm the effectiveness of our proposed training method, substantiating our\ntheoretical framework concerning CF and model function dynamics. We plan to\nmake our code publicly accessible in the near future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Catastrophic forgetting (CF) poses a significant challenge in machine\nlearning, where a model forgets previously learned information upon learning\nnew tasks. Despite the advanced capabilities of Large Language Models (LLMs),\nthey continue to face challenges with CF during continual learning. The\nmajority of existing research focuses on analyzing forgetting patterns through\na singular training sequence, thereby overlooking the intricate effects that\ndiverse tasks have on model behavior. Our study explores CF across various\nsettings, discovering that model forgetting is influenced by both the specific\ntraining tasks and the models themselves. To this end, we interpret forgetting\nby examining the function vector (FV), a compact representation of functions in\nLLMs, offering a model-dependent indicator for the occurrence of CF. Through\ntheoretical and empirical analyses, we demonstrated that CF in LLMs primarily\nstems from biases in function activation rather than the overwriting of task\nprocessing functions. Leveraging these insights, we propose a novel function\nvector guided training methodology, incorporating a regularization technique to\nstabilize the FV and mitigate forgetting. Empirical tests on four benchmarks\nconfirm the effectiveness of our proposed training method, substantiating our\ntheoretical framework concerning CF and model function dynamics. We plan to\nmake our code publicly accessible in the near future."
                },
                "authors": [
                    {
                        "name": "Gangwei Jiang"
                    },
                    {
                        "name": "Caigao Jiang"
                    },
                    {
                        "name": "Zhaoyi Li"
                    },
                    {
                        "name": "Siqiao Xue"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Linqi Song"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Ying Wei"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wei"
                },
                "author": "Ying Wei",
                "arxiv_comment": "10pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11019v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11019v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]