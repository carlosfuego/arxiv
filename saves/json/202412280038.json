[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.16187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16187v2",
                "updated": "2024-12-24T13:04:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    4,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-13T06:00:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    0,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing"
                },
                "summary": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks."
                },
                "authors": [
                    {
                        "name": "Minghui Liu"
                    },
                    {
                        "name": "Tahseen Rabbani"
                    },
                    {
                        "name": "Tony O'Halloran"
                    },
                    {
                        "name": "Ananth Sankaralingam"
                    },
                    {
                        "name": "Mary-Anne Hartley"
                    },
                    {
                        "name": "Brian Gravelle"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Cornelia FermÃ¼ller"
                    },
                    {
                        "name": "Yiannis Aloimonos"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Aloimonos"
                },
                "author": "Yiannis Aloimonos",
                "arxiv_comment": "10 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v2",
                "updated": "2024-12-24T00:46:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    0,
                    46,
                    0,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17747v1",
                "updated": "2024-12-23T18:02:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T18:02:25Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "title": "Deliberation in Latent Space via Differentiable Cache Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deliberation in Latent Space via Differentiable Cache Augmentation"
                },
                "summary": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks."
                },
                "authors": [
                    {
                        "name": "Luyang Liu"
                    },
                    {
                        "name": "Jonas Pfeiffer"
                    },
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Jun Xie"
                    },
                    {
                        "name": "Arthur Szlam"
                    }
                ],
                "author_detail": {
                    "name": "Arthur Szlam"
                },
                "author": "Arthur Szlam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17685v1",
                "updated": "2024-12-23T16:11:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T16:11:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment"
                },
                "summary": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application."
                },
                "authors": [
                    {
                        "name": "Edward J. Oughton"
                    },
                    {
                        "name": "Evan Alexander Peters"
                    },
                    {
                        "name": "Dennies Bor"
                    },
                    {
                        "name": "Noah Rivera"
                    },
                    {
                        "name": "C. Trevor Gaunt"
                    },
                    {
                        "name": "Robert Weigel"
                    }
                ],
                "author_detail": {
                    "name": "Robert Weigel"
                },
                "author": "Robert Weigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18919v2",
                "updated": "2024-12-23T14:40:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    40,
                    26,
                    0,
                    358,
                    0
                ],
                "published": "2024-05-29T09:22:25Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    9,
                    22,
                    25,
                    2,
                    150,
                    0
                ],
                "title": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN"
                },
                "summary": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Chenyu Wu"
                    },
                    {
                        "name": "Shuai Han"
                    },
                    {
                        "name": "Weixiao Meng"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "14 pages, 13 figures. This work has been accepted by IEEE Internet of\n  Things Journal. It is expanded on our previous research presented at the IEEE\n  Globecom 2024: Q. Chen, C. Wu, S. Han, W. Meng, and T. Q. Quek, \"Exploiting\n  Inter-Satellite Links for In-Flight Connectivity Scheme in Space-Air-Ground\n  Integrated Networks,\" in Proc. GLOBECOM 2024, Cape Town, South Africa, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03408v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03408v3",
                "updated": "2024-12-23T12:55:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    55,
                    21,
                    0,
                    358,
                    0
                ],
                "published": "2024-02-05T15:10:42Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    15,
                    10,
                    42,
                    0,
                    36,
                    0
                ],
                "title": "A Framework for Effective Invocation Methods of Various LLM Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Effective Invocation Methods of Various LLM Services"
                },
                "summary": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research."
                },
                "authors": [
                    {
                        "name": "Can Wang"
                    },
                    {
                        "name": "Dianbo Sui"
                    },
                    {
                        "name": "Bolin Zhang"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Jiabao Kang"
                    },
                    {
                        "name": "Zhidong Qiao"
                    },
                    {
                        "name": "Zhiying Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiying Tu"
                },
                "author": "Zhiying Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03408v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03408v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17464v1",
                "updated": "2024-12-23T10:41:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T10:41:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "CALLIC: Content Adaptive Learning for Lossless Image Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALLIC: Content Adaptive Learning for Lossless Image Compression"
                },
                "summary": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression."
                },
                "authors": [
                    {
                        "name": "Daxin Li"
                    },
                    {
                        "name": "Yuanchao Bai"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Junjun Jiang"
                    },
                    {
                        "name": "Xianming Liu"
                    },
                    {
                        "name": "Wen Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Gao"
                },
                "author": "Wen Gao",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17246v1",
                "updated": "2024-12-23T03:38:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T03:38:46Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "title": "Fast and Live Model Auto Scaling with O(1) Host Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Live Model Auto Scaling with O(1) Host Caching"
                },
                "summary": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling."
                },
                "authors": [
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05831v2",
                "updated": "2024-12-23T02:52:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    2,
                    52,
                    36,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-08T06:37:27Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "title": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval"
                },
                "summary": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval."
                },
                "authors": [
                    {
                        "name": "Shanti Stewart"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Lie Lu"
                    },
                    {
                        "name": "Andrea Fanelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Fanelli"
                },
                "author": "Andrea Fanelli",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17203v1",
                "updated": "2024-12-23T00:46:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T00:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "title": "Agile TLB Prefetching and Prediction Replacement Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agile TLB Prefetching and Prediction Replacement Policy"
                },
                "summary": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management."
                },
                "authors": [
                    {
                        "name": "Melkamu Mersha"
                    },
                    {
                        "name": "Tsion Abay"
                    },
                    {
                        "name": "Mingziem Bitewa"
                    },
                    {
                        "name": "Gedare Bloom"
                    }
                ],
                "author_detail": {
                    "name": "Gedare Bloom"
                },
                "author": "Gedare Bloom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16897v1",
                "updated": "2024-12-22T07:14:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "published": "2024-12-22T07:14:45Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "title": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context"
                },
                "summary": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC"
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Fangjian Liao"
                    },
                    {
                        "name": "Zeqi Ma"
                    },
                    {
                        "name": "Rongchen Zhang"
                    },
                    {
                        "name": "Dongmei Mo"
                    },
                    {
                        "name": "Waikeung Wong"
                    }
                ],
                "author_detail": {
                    "name": "Waikeung Wong"
                },
                "author": "Waikeung Wong",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17565v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17565v3",
                "updated": "2024-12-21T13:55:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    13,
                    55,
                    49,
                    5,
                    356,
                    0
                ],
                "published": "2024-06-25T14:02:08Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    14,
                    2,
                    8,
                    1,
                    177,
                    0
                ],
                "title": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool"
                },
                "summary": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time."
                },
                "authors": [
                    {
                        "name": "Cunchen Hu"
                    },
                    {
                        "name": "Heyang Huang"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Jiang Xu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Sa Wang"
                    },
                    {
                        "name": "Yungang Bao"
                    },
                    {
                        "name": "Ninghui Sun"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17565v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17565v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16585v1",
                "updated": "2024-12-21T11:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T11:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "title": "Parameterized Complexity of Caching in Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameterized Complexity of Caching in Networks"
                },
                "summary": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable."
                },
                "authors": [
                    {
                        "name": "Robert Ganian"
                    },
                    {
                        "name": "Fionn Mc Inerney"
                    },
                    {
                        "name": "Dimitra Tsigkari"
                    }
                ],
                "author_detail": {
                    "name": "Dimitra Tsigkari"
                },
                "author": "Dimitra Tsigkari",
                "arxiv_comment": "A shorter version of this paper will appear in the proceedings of\n  AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v4",
                "updated": "2024-12-21T02:36:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    2,
                    36,
                    3,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16434v1",
                "updated": "2024-12-21T01:48:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T01:48:52Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "title": "SYMPHONY: Improving Memory Management for LLM Inference Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SYMPHONY: Improving Memory Management for LLM Inference Workloads"
                },
                "summary": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile."
                },
                "authors": [
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Anyong Mao"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16001v1",
                "updated": "2024-12-20T15:51:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T15:51:42Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "title": "Multi-Strided Access Patterns to Boost Hardware Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Strided Access Patterns to Boost Hardware Prefetching"
                },
                "summary": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future."
                },
                "authors": [
                    {
                        "name": "Miguel O. Blom"
                    },
                    {
                        "name": "Kristian F. D. Rietveld"
                    },
                    {
                        "name": "Rob V. van Nieuwpoort"
                    }
                ],
                "author_detail": {
                    "name": "Rob V. van Nieuwpoort"
                },
                "author": "Rob V. van Nieuwpoort",
                "arxiv_comment": "12 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14485v2",
                "updated": "2024-12-20T15:18:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    18,
                    44,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-19T03:11:33Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    3,
                    11,
                    33,
                    3,
                    354,
                    0
                ],
                "title": "Towards Projected and Incremental Pseudo-Boolean Model Counting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Projected and Incremental Pseudo-Boolean Model Counting"
                },
                "summary": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting."
                },
                "authors": [
                    {
                        "name": "Suwei Yang"
                    },
                    {
                        "name": "Kuldeep S. Meel"
                    }
                ],
                "author_detail": {
                    "name": "Kuldeep S. Meel"
                },
                "author": "Kuldeep S. Meel",
                "arxiv_comment": "To appear in AAAI25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15605v1",
                "updated": "2024-12-20T06:58:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T06:58:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v3",
                "updated": "2024-12-19T23:52:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    23,
                    52,
                    16,
                    3,
                    354,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v2",
                "updated": "2024-12-19T22:34:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    22,
                    34,
                    37,
                    3,
                    354,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. JimÃ©nez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v1",
                "updated": "2024-12-19T13:28:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v3",
                "updated": "2024-12-19T12:38:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    38,
                    23,
                    3,
                    354,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "In this version, we achieved a nearly lossless acceleration of 1.51\n  times for ToCa on FLUX in the appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14392v1",
                "updated": "2024-12-18T22:52:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:52:12Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "title": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems"
                },
                "summary": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies."
                },
                "authors": [
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14363v1",
                "updated": "2024-12-18T22:01:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:01:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals"
                },
                "summary": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Sayeh Sharify"
                    },
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "14 pages, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v1",
                "updated": "2024-12-18T21:09:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v4",
                "updated": "2024-12-18T17:36:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    36,
                    36,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13779v1",
                "updated": "2024-12-18T12:16:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:16:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization"
                },
                "summary": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Tianzhe Xiao"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Yining Qi"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13771v1",
                "updated": "2024-12-18T12:07:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:07:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization"
                },
                "summary": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems."
                },
                "authors": [
                    {
                        "name": "Guanghan Li"
                    },
                    {
                        "name": "Xun Zhang"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Yifan Yin"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "arxiv_comment": "7 pages, 3 figures, AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v2",
                "updated": "2024-12-18T09:47:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    47,
                    25,
                    2,
                    353,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13649v1",
                "updated": "2024-12-18T09:27:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T09:27:33Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation"
                },
                "summary": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods."
                },
                "authors": [
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Yilong Lai"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08584v2",
                "updated": "2024-12-18T07:45:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    7,
                    45,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-11T07:24:21Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification"
                },
                "summary": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs."
                },
                "authors": [
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13509v1",
                "updated": "2024-12-18T05:16:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T05:16:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation"
                },
                "summary": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization."
                },
                "authors": [
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Kaiyuan Hou"
                    },
                    {
                        "name": "Heming Fu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12486v2",
                "updated": "2024-12-18T05:08:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    8,
                    39,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-17T02:43:54Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    43,
                    54,
                    1,
                    352,
                    0
                ],
                "title": "Boosting Long-Context Management via Query-Guided Activation Refilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Long-Context Management via Query-Guided Activation Refilling"
                },
                "summary": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Defu Lian"
                    }
                ],
                "author_detail": {
                    "name": "Defu Lian"
                },
                "author": "Defu Lian",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v2",
                "updated": "2024-12-17T20:41:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    20,
                    41,
                    59,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v3",
                "updated": "2024-12-17T14:45:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    45,
                    12,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Code is available at https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12953v1",
                "updated": "2024-12-17T14:34:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T14:34:51Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "title": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning"
                },
                "summary": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/."
                },
                "authors": [
                    {
                        "name": "Moritz Reuss"
                    },
                    {
                        "name": "Jyothish Pari"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Rudolf Lioutikov"
                    }
                ],
                "author_detail": {
                    "name": "Rudolf Lioutikov"
                },
                "author": "Rudolf Lioutikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12798v1",
                "updated": "2024-12-17T11:00:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T11:00:56Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "title": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation"
                },
                "summary": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI."
                },
                "authors": [
                    {
                        "name": "Shiqi Huang"
                    },
                    {
                        "name": "Shuting He"
                    },
                    {
                        "name": "Bihan Wen"
                    }
                ],
                "author_detail": {
                    "name": "Bihan Wen"
                },
                "author": "Bihan Wen",
                "arxiv_comment": "AAAI 2025, code see https://github.com/HuangShiqi128/ZoRI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12706v1",
                "updated": "2024-12-17T09:20:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:20:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression"
                },
                "summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Chuqiao Kuang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "13pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v2",
                "updated": "2024-12-17T09:11:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    11,
                    47,
                    1,
                    352,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08585v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08585v3",
                "updated": "2024-12-17T05:40:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    40,
                    9,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-11T18:03:05Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    3,
                    5,
                    2,
                    346,
                    0
                ],
                "title": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs"
                },
                "summary": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "James Hensman"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Victor Ruhle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08585v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08585v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12543v1",
                "updated": "2024-12-17T05:09:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T05:09:45Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "title": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks"
                },
                "summary": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks."
                },
                "authors": [
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Tan Li"
                    },
                    {
                        "name": "Hai Liu"
                    },
                    {
                        "name": "Tse-Tin Chan"
                    }
                ],
                "author_detail": {
                    "name": "Tse-Tin Chan"
                },
                "author": "Tse-Tin Chan",
                "arxiv_comment": "8 pages, 8 figures, WiOpt 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12488v1",
                "updated": "2024-12-17T02:44:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T02:44:43Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "title": "A System for Microserving of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A System for Microserving of LLMs"
                },
                "summary": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies."
                },
                "authors": [
                    {
                        "name": "Hongyi Jin"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Charlie F. Ruan"
                    },
                    {
                        "name": "Yingcheng Wang"
                    },
                    {
                        "name": "Todd C. Mowry"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Tianqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianqi Chen"
                },
                "author": "Tianqi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12444v1",
                "updated": "2024-12-17T01:12:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T01:12:35Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "title": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers"
                },
                "summary": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency."
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yanyu Li"
                    },
                    {
                        "name": "Yifan Gong"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Zhihao Shu"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11828v1",
                "updated": "2024-12-16T14:49:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T14:49:32Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "title": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey"
                },
                "summary": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field."
                },
                "authors": [
                    {
                        "name": "Sergey Zinchenko"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11741v1",
                "updated": "2024-12-16T13:01:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:01:53Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "title": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation"
                },
                "summary": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments."
                },
                "authors": [
                    {
                        "name": "Hongxuan Zhang"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v1",
                "updated": "2024-12-16T12:28:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11685v1",
                "updated": "2024-12-16T11:55:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T11:55:26Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "title": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning"
                },
                "summary": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU."
                },
                "authors": [
                    {
                        "name": "Xingchi Chen"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    },
                    {
                        "name": "Xuerui Li"
                    },
                    {
                        "name": "Yuying Chen"
                    },
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Wenqi Ren"
                    }
                ],
                "author_detail": {
                    "name": "Wenqi Ren"
                },
                "author": "Wenqi Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14201v1",
                "updated": "2024-12-15T21:02:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T21:02:16Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "title": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models"
                },
                "summary": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint."
                },
                "authors": [
                    {
                        "name": "Boris Ruf"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "arxiv_comment": "Presented at the 18th IEEE International Workshop on Multimedia\n  Technologies for E-Learning (MTEL), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.02388v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.02388v3",
                "updated": "2024-12-15T03:29:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    3,
                    29,
                    54,
                    6,
                    350,
                    0
                ],
                "published": "2023-05-03T19:07:06Z",
                "published_parsed": [
                    2023,
                    5,
                    3,
                    19,
                    7,
                    6,
                    2,
                    123,
                    0
                ],
                "title": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)"
                },
                "summary": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone."
                },
                "authors": [
                    {
                        "name": "Yupeng Tang"
                    },
                    {
                        "name": "Seung-seob Lee"
                    },
                    {
                        "name": "Abhishek Bhattacharjee"
                    },
                    {
                        "name": "Anurag Khandelwal"
                    }
                ],
                "author_detail": {
                    "name": "Anurag Khandelwal"
                },
                "author": "Anurag Khandelwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.02388v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.02388v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11021v1",
                "updated": "2024-12-15T02:30:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T02:30:09Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "title": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array"
                },
                "summary": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works."
                },
                "authors": [
                    {
                        "name": "Xiaobing Ni"
                    },
                    {
                        "name": "Mengke Ge"
                    },
                    {
                        "name": "Jiaheng Ruan"
                    },
                    {
                        "name": "Song Chen"
                    },
                    {
                        "name": "Yi Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Kang"
                },
                "author": "Yi Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15246v1",
                "updated": "2024-12-14T06:47:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    14,
                    6,
                    47,
                    56,
                    5,
                    349,
                    0
                ],
                "published": "2024-12-14T06:47:56Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    6,
                    47,
                    56,
                    5,
                    349,
                    0
                ],
                "title": "Accelerating Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Retrieval-Augmented Generation"
                },
                "summary": "An evolving solution to address hallucination and enhance accuracy in large\nlanguage models (LLMs) is Retrieval-Augmented Generation (RAG), which involves\naugmenting LLMs with information retrieved from an external knowledge source,\nsuch as the web. This paper profiles several RAG execution pipelines and\ndemystifies the complex interplay between their retrieval and generation\nphases. We demonstrate that while exact retrieval schemes are expensive, they\ncan reduce inference time compared to approximate retrieval variants because an\nexact retrieval model can send a smaller but more accurate list of documents to\nthe generative model while maintaining the same end-to-end accuracy. This\nobservation motivates the acceleration of the exact nearest neighbor search for\nRAG.\n  In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL\ndevice that implements a scale-out near-memory acceleration architecture with a\nnovel cache-coherent interface between the host CPU and near-memory\naccelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a\n512GB vector database compared with executing the search on Intel Sapphire\nRapids CPUs. This higher search performance translates to 1.7-26.3x lower\nend-to-end inference time for representative RAG applications. IKS is\ninherently a memory expander; its internal DRAM can be disaggregated and used\nfor other applications running on the server to prevent DRAM, which is the most\nexpensive component in today's servers, from being stranded.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An evolving solution to address hallucination and enhance accuracy in large\nlanguage models (LLMs) is Retrieval-Augmented Generation (RAG), which involves\naugmenting LLMs with information retrieved from an external knowledge source,\nsuch as the web. This paper profiles several RAG execution pipelines and\ndemystifies the complex interplay between their retrieval and generation\nphases. We demonstrate that while exact retrieval schemes are expensive, they\ncan reduce inference time compared to approximate retrieval variants because an\nexact retrieval model can send a smaller but more accurate list of documents to\nthe generative model while maintaining the same end-to-end accuracy. This\nobservation motivates the acceleration of the exact nearest neighbor search for\nRAG.\n  In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL\ndevice that implements a scale-out near-memory acceleration architecture with a\nnovel cache-coherent interface between the host CPU and near-memory\naccelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a\n512GB vector database compared with executing the search on Intel Sapphire\nRapids CPUs. This higher search performance translates to 1.7-26.3x lower\nend-to-end inference time for representative RAG applications. IKS is\ninherently a memory expander; its internal DRAM can be disaggregated and used\nfor other applications running on the server to prevent DRAM, which is the most\nexpensive component in today's servers, from being stranded."
                },
                "authors": [
                    {
                        "name": "Derrick Quinn"
                    },
                    {
                        "name": "Mohammad Nouri"
                    },
                    {
                        "name": "Neel Patel"
                    },
                    {
                        "name": "John Salihu"
                    },
                    {
                        "name": "Alireza Salemi"
                    },
                    {
                        "name": "Sukhan Lee"
                    },
                    {
                        "name": "Hamed Zamani"
                    },
                    {
                        "name": "Mohammad Alian"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Alian"
                },
                "author": "Mohammad Alian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10685v1",
                "updated": "2024-12-14T05:20:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "published": "2024-12-14T05:20:50Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "title": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs"
                },
                "summary": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm."
                },
                "authors": [
                    {
                        "name": "Baljinder Singh Heera"
                    },
                    {
                        "name": "Shrinivas Petale"
                    },
                    {
                        "name": "Yatindra Nath Singh"
                    },
                    {
                        "name": "Suresh Subramaniam"
                    }
                ],
                "author_detail": {
                    "name": "Suresh Subramaniam"
                },
                "author": "Suresh Subramaniam",
                "arxiv_comment": "The preliminary work was presented at ONDM 2023 conference.\n  https://doi.org/10.23919/ONDM57372.2023.10144866",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v1",
                "updated": "2024-12-13T17:59:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10302v1",
                "updated": "2024-12-13T17:37:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:37:48Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding"
                },
                "summary": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2."
                },
                "authors": [
                    {
                        "name": "Zhiyu Wu"
                    },
                    {
                        "name": "Xiaokang Chen"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Xingchao Liu"
                    },
                    {
                        "name": "Wen Liu"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Huazuo Gao"
                    },
                    {
                        "name": "Yiyang Ma"
                    },
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Bingxuan Wang"
                    },
                    {
                        "name": "Zhenda Xie"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Yaofeng Sun"
                    },
                    {
                        "name": "Yukun Li"
                    },
                    {
                        "name": "Yishi Piao"
                    },
                    {
                        "name": "Kang Guan"
                    },
                    {
                        "name": "Aixin Liu"
                    },
                    {
                        "name": "Xin Xie"
                    },
                    {
                        "name": "Yuxiang You"
                    },
                    {
                        "name": "Kai Dong"
                    },
                    {
                        "name": "Xingkai Yu"
                    },
                    {
                        "name": "Haowei Zhang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Yisong Wang"
                    },
                    {
                        "name": "Chong Ruan"
                    }
                ],
                "author_detail": {
                    "name": "Chong Ruan"
                },
                "author": "Chong Ruan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18566v2",
                "updated": "2024-12-13T16:13:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    13,
                    39,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-27T18:09:29Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "title": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software"
                },
                "summary": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software."
                },
                "authors": [
                    {
                        "name": "Oliver Maximilian Zobel"
                    },
                    {
                        "name": "Johannes Maierhofer"
                    },
                    {
                        "name": "Andreas KÃ¶stler"
                    },
                    {
                        "name": "Daniel J. Rixen"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Rixen"
                },
                "author": "Daniel J. Rixen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10153v1",
                "updated": "2024-12-13T14:11:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T14:11:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector"
                },
                "summary": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Shuzhao Xie"
                    },
                    {
                        "name": "Chengwei Ren"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shijia Ge"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12021v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12021v2",
                "updated": "2024-12-13T14:08:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    8,
                    55,
                    4,
                    348,
                    0
                ],
                "published": "2024-09-18T14:31:33Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "title": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues"
                },
                "summary": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized)."
                },
                "authors": [
                    {
                        "name": "Thore ThieÃen"
                    },
                    {
                        "name": "Jan Vahrenhold"
                    }
                ],
                "author_detail": {
                    "name": "Jan Vahrenhold"
                },
                "author": "Jan Vahrenhold",
                "arxiv_doi": "10.4230/LIPIcs.ISAAC.2024.55",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ISAAC.2024.55",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12021v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12021v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, full version of the paper in ISAAC 2024; minor changes",
                "arxiv_journal_ref": "Thore Thie{\\ss}en and Jan Vahrenhold. Optimal offline ORAM with\n  perfect security via simple oblivious priority queues. In 35th International\n  Symposium on Algorithms and Computation (ISAAC 2024), 18 pages. 2024",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12178v1",
                "updated": "2024-12-13T02:26:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T02:26:54Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "title": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models"
                },
                "summary": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize compression\nrate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN)\ncomponents, which typically comprise a large proportion of parameters (around\n3/2), ensure that our FFN optimizations would have a better chance of achieving\neffective compression. Moreover, our findings are beneficial to general LLMs\nand are not restricted to ReLU-based models. This work systematically\ninvestigates the tradeoff between enforcing activation sparsity and perplexity\n(accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that\nwe can obtain around 50% of main memory and computing reductions for critical\nFFN components with negligible accuracy degradation. This extra 50% sparsity\ndoes not naturally exist in the current LLMs, which require tuning LLMs'\nactivation outputs by injecting zero-enforcing thresholds. To obtain the\nbenefits of activation sparsity, we provide a guideline for the system\narchitect for LLM prediction and prefetching. The success prediction allows the\nsystem to prefetch the necessary weights while omitting the inactive ones and\ntheir successors, therefore lowering cache and memory pollution and reducing\nLLM execution time on resource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize compression\nrate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN)\ncomponents, which typically comprise a large proportion of parameters (around\n3/2), ensure that our FFN optimizations would have a better chance of achieving\neffective compression. Moreover, our findings are beneficial to general LLMs\nand are not restricted to ReLU-based models. This work systematically\ninvestigates the tradeoff between enforcing activation sparsity and perplexity\n(accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that\nwe can obtain around 50% of main memory and computing reductions for critical\nFFN components with negligible accuracy degradation. This extra 50% sparsity\ndoes not naturally exist in the current LLMs, which require tuning LLMs'\nactivation outputs by injecting zero-enforcing thresholds. To obtain the\nbenefits of activation sparsity, we provide a guideline for the system\narchitect for LLM prediction and prefetching. The success prediction allows the\nsystem to prefetch the necessary weights while omitting the inactive ones and\ntheir successors, therefore lowering cache and memory pollution and reducing\nLLM execution time on resource-constrained edge devices."
                },
                "authors": [
                    {
                        "name": "Nobel Dhar"
                    },
                    {
                        "name": "Bobin Deng"
                    },
                    {
                        "name": "Md Romyull Islam"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Kun Suo"
                    }
                ],
                "author_detail": {
                    "name": "Kun Suo"
                },
                "author": "Kun Suo",
                "arxiv_comment": "Conference submission for IPCCC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09474v1",
                "updated": "2024-12-12T17:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T17:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "title": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance"
                },
                "summary": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments."
                },
                "authors": [
                    {
                        "name": "Md Nurul Absur"
                    },
                    {
                        "name": "Sourya Saha"
                    },
                    {
                        "name": "Sifat Nawrin Nova"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Md Rahat Ul Nasib"
                    }
                ],
                "author_detail": {
                    "name": "Md Rahat Ul Nasib"
                },
                "author": "Md Rahat Ul Nasib",
                "arxiv_comment": "6 Pages, 10 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09416v1",
                "updated": "2024-12-12T16:24:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T16:24:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors"
                },
                "summary": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v3",
                "updated": "2024-12-12T15:39:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    39,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08760v2",
                "updated": "2024-12-12T14:43:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    14,
                    43,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-10-11T12:19:18Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "title": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation"
                },
                "summary": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL."
                },
                "authors": [
                    {
                        "name": "Konstantin Burlachenko"
                    },
                    {
                        "name": "Peter RichtÃ¡rik"
                    }
                ],
                "author_detail": {
                    "name": "Peter RichtÃ¡rik"
                },
                "author": "Peter RichtÃ¡rik",
                "arxiv_comment": "55 pages, 12 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4; C.3; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06282v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06282v3",
                "updated": "2024-12-12T12:24:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    24,
                    18,
                    3,
                    347,
                    0
                ],
                "published": "2024-06-10T14:01:21Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    14,
                    1,
                    21,
                    0,
                    162,
                    0
                ],
                "title": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone"
                },
                "summary": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation."
                },
                "authors": [
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06282v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06282v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v3",
                "updated": "2024-12-12T12:03:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    3,
                    19,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01415v2",
                "updated": "2024-12-12T10:07:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    7,
                    17,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-02T11:57:03Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    57,
                    3,
                    0,
                    337,
                    0
                ],
                "title": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure"
                },
                "summary": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW."
                },
                "authors": [
                    {
                        "name": "A. B. Batrakov"
                    },
                    {
                        "name": "S. Yu. Karelin"
                    },
                    {
                        "name": "O. M. Lebedenko"
                    },
                    {
                        "name": "V. S. Mukhin"
                    },
                    {
                        "name": "I. N. Onishchenko"
                    },
                    {
                        "name": "O. L. Rak"
                    },
                    {
                        "name": "V. G. Sinitsin"
                    },
                    {
                        "name": "M. V. Volovenko"
                    }
                ],
                "author_detail": {
                    "name": "M. V. Volovenko"
                },
                "author": "M. V. Volovenko",
                "arxiv_comment": "4 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09057v1",
                "updated": "2024-12-12T08:33:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T08:33:39Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "title": "PhishIntel: Toward Practical Deployment of Reference-based Phishing\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhishIntel: Toward Practical Deployment of Reference-based Phishing\n  Detection"
                },
                "summary": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) achieve high precision by analyzing\nbrand-domain consistency, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) achieve high precision by analyzing\nbrand-domain consistency, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility."
                },
                "authors": [
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Hiok Kuek Tan"
                    },
                    {
                        "name": "Qiaoran Meng"
                    },
                    {
                        "name": "Mei Lin Lock"
                    },
                    {
                        "name": "Tri Cao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Nay Oo"
                    },
                    {
                        "name": "Hoon Wei Lim"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09036v1",
                "updated": "2024-12-12T07:52:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    56,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T07:52:56Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    56,
                    3,
                    347,
                    0
                ],
                "title": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty"
                },
                "summary": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance."
                },
                "authors": [
                    {
                        "name": "Meizhi Zhong"
                    },
                    {
                        "name": "Xikai Liu"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yikun Lei"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v3",
                "updated": "2024-12-12T03:21:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    21,
                    13,
                    3,
                    347,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Forecasting GPU Performance for Deep Learning Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting GPU Performance for Deep Learning Training and Inference"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 121.4% and 30.8% to 2.3% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior work, where both GPT3 and H100 were not used\nto train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 121.4% and 30.8% to 2.3% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior work, where both GPT3 and H100 were not used\nto train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "arxiv_doi": "10.1145/3669940.3707265",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3669940.3707265",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.13853v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS), 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08890v1",
                "updated": "2024-12-12T03:00:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    0,
                    29,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T03:00:29Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    0,
                    29,
                    3,
                    347,
                    0
                ],
                "title": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries"
                },
                "summary": "We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy."
                },
                "authors": [
                    {
                        "name": "Junhyuck Kim"
                    },
                    {
                        "name": "Jongho Park"
                    },
                    {
                        "name": "Jaewoong Cho"
                    },
                    {
                        "name": "Dimitris Papailiopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Papailiopoulos"
                },
                "author": "Dimitris Papailiopoulos",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08521v1",
                "updated": "2024-12-11T16:35:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T16:35:13Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance"
                },
                "summary": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task."
                },
                "authors": [
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Xinzhu Ma"
                    },
                    {
                        "name": "Zihan Geng"
                    },
                    {
                        "name": "Shutao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v3",
                "updated": "2024-12-11T12:03:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    12,
                    3,
                    40,
                    2,
                    346,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Pushing the Limits of In-Network Caching for Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing the Limits of In-Network Caching for Key-Value Stores"
                },
                "summary": "We present OrbitCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, OrbitCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement an OrbitCache prototype on an Intel Tofino\nswitch. Our experimental results show that OrbitCache can balance highly skewed\nworkloads and is robust to various system conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OrbitCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, OrbitCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement an OrbitCache prototype on an Intel Tofino\nswitch. Our experimental results show that OrbitCache can balance highly skewed\nworkloads and is robust to various system conditions."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "arxiv_comment": "To be appeared in USENIX NSDI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08176v1",
                "updated": "2024-12-11T08:07:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    7,
                    12,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T08:07:12Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    7,
                    12,
                    2,
                    346,
                    0
                ],
                "title": "TextRefiner: Internal Visual Feature as Efficient Refiner for\n  Vision-Language Models Prompt Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextRefiner: Internal Visual Feature as Efficient Refiner for\n  Vision-Language Models Prompt Tuning"
                },
                "summary": "Despite the efficiency of prompt learning in transferring vision-language\nmodels (VLMs) to downstream tasks, existing methods mainly learn the prompts in\na coarse-grained manner where the learned prompt vectors are shared across all\ncategories. Consequently, the tailored prompts often fail to discern\nclass-specific visual concepts, thereby hindering the transferred performance\nfor classes that share similar or complex visual attributes. Recent advances\nmitigate this challenge by leveraging external knowledge from Large Language\nModels (LLMs) to furnish class descriptions, yet incurring notable inference\ncosts. In this paper, we introduce TextRefiner, a plug-and-play method to\nrefine the text prompts of existing methods by leveraging the internal\nknowledge of VLMs. Particularly, TextRefiner builds a novel local cache module\nto encapsulate fine-grained visual concepts derivedfrom local tokens within the\nimage branch. By aggregating and aligning the cached visual descriptions with\nthe original output of the text branch, TextRefiner can efficiently refine and\nenrich the learned prompts from existing methods without relying on any\nexternal expertise. For example, it improves the performance of CoOp from 71.66\n% to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise\nfeatures for text prompts. Equipped with TextRefiner, PromptKD achieves\nstate-of-the-art performance and is efficient in inference. Our code is relesed\nat https://github.com/xjjxmu/TextRefiner",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the efficiency of prompt learning in transferring vision-language\nmodels (VLMs) to downstream tasks, existing methods mainly learn the prompts in\na coarse-grained manner where the learned prompt vectors are shared across all\ncategories. Consequently, the tailored prompts often fail to discern\nclass-specific visual concepts, thereby hindering the transferred performance\nfor classes that share similar or complex visual attributes. Recent advances\nmitigate this challenge by leveraging external knowledge from Large Language\nModels (LLMs) to furnish class descriptions, yet incurring notable inference\ncosts. In this paper, we introduce TextRefiner, a plug-and-play method to\nrefine the text prompts of existing methods by leveraging the internal\nknowledge of VLMs. Particularly, TextRefiner builds a novel local cache module\nto encapsulate fine-grained visual concepts derivedfrom local tokens within the\nimage branch. By aggregating and aligning the cached visual descriptions with\nthe original output of the text branch, TextRefiner can efficiently refine and\nenrich the learned prompts from existing methods without relying on any\nexternal expertise. For example, it improves the performance of CoOp from 71.66\n% to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise\nfeatures for text prompts. Equipped with TextRefiner, PromptKD achieves\nstate-of-the-art performance and is efficient in inference. Our code is relesed\nat https://github.com/xjjxmu/TextRefiner"
                },
                "authors": [
                    {
                        "name": "Jingjing Xie"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Jun Peng"
                    },
                    {
                        "name": "Zhaohong Huang"
                    },
                    {
                        "name": "Liujuan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Liujuan Cao"
                },
                "author": "Liujuan Cao",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08063v1",
                "updated": "2024-12-11T03:15:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    15,
                    49,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T03:15:49Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    15,
                    49,
                    2,
                    346,
                    0
                ],
                "title": "ContextModule: Improving Code Completion via Repository-level Contextual\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextModule: Improving Code Completion via Repository-level Contextual\n  Information"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode completion tasks, where they assist developers by predicting and\ngenerating new code in real-time. However, existing LLM-based code completion\nsystems primarily rely on the immediate context of the file being edited, often\nmissing valuable repository-level information, user behaviour and edit history\nthat could improve suggestion accuracy. Additionally, challenges such as\nefficiently retrieving relevant code snippets from large repositories,\nincorporating user behavior, and balancing accuracy with low-latency\nrequirements in production environments remain unresolved. In this paper, we\npropose ContextModule, a framework designed to enhance LLM-based code\ncompletion by retrieving and integrating three types of contextual information\nfrom the repository: user behavior-based code, similar code snippets, and\ncritical symbol definitions. By capturing user interactions across files and\nleveraging repository-wide static analysis, ContextModule improves the\nrelevance and precision of generated code. We implement performance\noptimizations, such as index caching, to ensure the system meets the latency\nconstraints of real-world coding environments. Experimental results and\nindustrial practise demonstrate that ContextModule significantly improves code\ncompletion accuracy and user acceptance rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode completion tasks, where they assist developers by predicting and\ngenerating new code in real-time. However, existing LLM-based code completion\nsystems primarily rely on the immediate context of the file being edited, often\nmissing valuable repository-level information, user behaviour and edit history\nthat could improve suggestion accuracy. Additionally, challenges such as\nefficiently retrieving relevant code snippets from large repositories,\nincorporating user behavior, and balancing accuracy with low-latency\nrequirements in production environments remain unresolved. In this paper, we\npropose ContextModule, a framework designed to enhance LLM-based code\ncompletion by retrieving and integrating three types of contextual information\nfrom the repository: user behavior-based code, similar code snippets, and\ncritical symbol definitions. By capturing user interactions across files and\nleveraging repository-wide static analysis, ContextModule improves the\nrelevance and precision of generated code. We implement performance\noptimizations, such as index caching, to ensure the system meets the latency\nconstraints of real-world coding environments. Experimental results and\nindustrial practise demonstrate that ContextModule significantly improves code\ncompletion accuracy and user acceptance rates."
                },
                "authors": [
                    {
                        "name": "Zhanming Guan"
                    },
                    {
                        "name": "Junlin Liu"
                    },
                    {
                        "name": "Jierui Liu"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Dexin Liu"
                    },
                    {
                        "name": "Ningyuan Sun"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Wenchao Li"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Hang Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Zhu"
                },
                "author": "Hang Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12952v2",
                "updated": "2024-12-10T22:53:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    22,
                    53,
                    16,
                    1,
                    345,
                    0
                ],
                "published": "2024-03-19T17:54:34Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    17,
                    54,
                    34,
                    1,
                    79,
                    0
                ],
                "title": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization\n  with Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization\n  with Vision-Language Models"
                },
                "summary": "Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 image classification datasets involving natural distribution shifts\nand cross-dataset generalization, as well as in context-dependent visual\nreasoning, demonstrate TPS's superior performance, achieving state-of-the-art\nresults while reducing resource requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 image classification datasets involving natural distribution shifts\nand cross-dataset generalization, as well as in context-dependent visual\nreasoning, demonstrate TPS's superior performance, achieving state-of-the-art\nresults while reducing resource requirements."
                },
                "authors": [
                    {
                        "name": "Elaine Sui"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Serena Yeung-Levy"
                    }
                ],
                "author_detail": {
                    "name": "Serena Yeung-Levy"
                },
                "author": "Serena Yeung-Levy",
                "arxiv_comment": "Accepted at WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v1",
                "updated": "2024-12-10T18:59:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Causal Video Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Causal Video Generators"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to a causal\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nsupports fast streaming generation of high quality videos at 9.4 FPS on a\nsingle GPU thanks to KV caching. Our approach also enables streaming\nvideo-to-video translation, image-to-video, and dynamic prompting in a\nzero-shot manner. We will release the code based on an open-source model in the\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to a causal\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nsupports fast streaming generation of high quality videos at 9.4 FPS on a\nsingle GPU thanks to KV caching. Our approach also enables streaming\nvideo-to-video translation, image-to-video, and dynamic prompting in a\nzero-shot manner. We will release the code based on an open-source model in the\nfuture."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v1",
                "updated": "2024-12-10T18:50:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}"
                },
                "authors": [
                    {
                        "name": "Korbinian PÃ¶ppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07720v1",
                "updated": "2024-12-10T18:13:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:13:20Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer"
                },
                "summary": "The recent surge of interest in comprehensive multimodal models has\nnecessitated the unification of diverse modalities. However, the unification\nsuffers from disparate methodologies. Continuous visual generation necessitates\nthe full-sequence diffusion-based approach, despite its divergence from the\nautoregressive modeling in the text domain. We posit that autoregressive\nmodeling, i.e., predicting the future based on past deterministic experience,\nremains crucial in developing both a visual generation model and a potential\nunified multimodal model. In this paper, we explore an interpolation between\nthe autoregressive modeling and full-parameters diffusion to model visual\ninformation. At its core, we present ACDiT, an Autoregressive blockwise\nConditional Diffusion Transformer, where the block size of diffusion, i.e., the\nsize of autoregressive units, can be flexibly adjusted to interpolate between\ntoken-wise autoregression and full-sequence diffusion. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) during\ntraining. During inference, the process iterates between diffusion denoising\nand autoregressive decoding that can make full use of KV-Cache. We verify the\neffectiveness of ACDiT on image and video generation tasks. We also demonstrate\nthat benefitted from autoregressive modeling, ACDiT can be seamlessly used in\nvisual understanding tasks despite being trained on the diffusion objective.\nThe analysis of the trade-off between autoregressive modeling and diffusion\ndemonstrates the potential of ACDiT to be used in long-horizon visual\ngeneration tasks. These strengths make it promising as the backbone of future\nunified models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent surge of interest in comprehensive multimodal models has\nnecessitated the unification of diverse modalities. However, the unification\nsuffers from disparate methodologies. Continuous visual generation necessitates\nthe full-sequence diffusion-based approach, despite its divergence from the\nautoregressive modeling in the text domain. We posit that autoregressive\nmodeling, i.e., predicting the future based on past deterministic experience,\nremains crucial in developing both a visual generation model and a potential\nunified multimodal model. In this paper, we explore an interpolation between\nthe autoregressive modeling and full-parameters diffusion to model visual\ninformation. At its core, we present ACDiT, an Autoregressive blockwise\nConditional Diffusion Transformer, where the block size of diffusion, i.e., the\nsize of autoregressive units, can be flexibly adjusted to interpolate between\ntoken-wise autoregression and full-sequence diffusion. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) during\ntraining. During inference, the process iterates between diffusion denoising\nand autoregressive decoding that can make full use of KV-Cache. We verify the\neffectiveness of ACDiT on image and video generation tasks. We also demonstrate\nthat benefitted from autoregressive modeling, ACDiT can be seamlessly used in\nvisual understanding tasks despite being trained on the diffusion objective.\nThe analysis of the trade-off between autoregressive modeling and diffusion\ndemonstrates the potential of ACDiT to be used in long-horizon visual\ngeneration tasks. These strengths make it promising as the backbone of future\nunified models."
                },
                "authors": [
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14485v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14485v4",
                "updated": "2024-12-10T12:45:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    12,
                    45,
                    31,
                    1,
                    345,
                    0
                ],
                "published": "2024-09-22T15:13:31Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    15,
                    13,
                    31,
                    6,
                    266,
                    0
                ],
                "title": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding"
                },
                "summary": "Long video understanding poses a significant challenge for current\nMulti-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained\nby their limited context lengths and the substantial costs while processing\nlong videos. Although several existing methods attempt to reduce visual tokens,\ntheir strategies encounter severe bottleneck, restricting MLLMs' ability to\nperceive fine-grained visual details. In this work, we propose Video-XL, a\nnovel approach that leverages MLLMs' inherent key-value (KV) sparsification\ncapacity to condense the visual input. Specifically, we introduce a new special\ntoken, the Visual Summarization Token (VST), for each interval of the video,\nwhich summarizes the visual information within the interval as its associated\nKV. The VST module is trained by instruction fine-tuning, where two optimizing\nstrategies are offered. 1.Curriculum learning, where VST learns to make small\n(easy) and large compression (hard) progressively. 2. Composite data curation,\nwhich integrates single-image, multi-image, and synthetic data to overcome the\nscarcity of long-video instruction data. The compression quality is further\nimproved by dynamic compression, which customizes compression granularity based\non the information density of different video intervals. Video-XL's\neffectiveness is verified from three aspects. First, it achieves a superior\nlong-video understanding capability, outperforming state-of-the-art models of\ncomparable sizes across multiple popular benchmarks. Second, it effectively\npreserves video information, with minimal compression loss even at 16x\ncompression ratio. Third, it realizes outstanding cost-effectiveness, enabling\nhigh-quality processing of thousands of frames on a single A100 GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long video understanding poses a significant challenge for current\nMulti-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained\nby their limited context lengths and the substantial costs while processing\nlong videos. Although several existing methods attempt to reduce visual tokens,\ntheir strategies encounter severe bottleneck, restricting MLLMs' ability to\nperceive fine-grained visual details. In this work, we propose Video-XL, a\nnovel approach that leverages MLLMs' inherent key-value (KV) sparsification\ncapacity to condense the visual input. Specifically, we introduce a new special\ntoken, the Visual Summarization Token (VST), for each interval of the video,\nwhich summarizes the visual information within the interval as its associated\nKV. The VST module is trained by instruction fine-tuning, where two optimizing\nstrategies are offered. 1.Curriculum learning, where VST learns to make small\n(easy) and large compression (hard) progressively. 2. Composite data curation,\nwhich integrates single-image, multi-image, and synthetic data to overcome the\nscarcity of long-video instruction data. The compression quality is further\nimproved by dynamic compression, which customizes compression granularity based\non the information density of different video intervals. Video-XL's\neffectiveness is verified from three aspects. First, it achieves a superior\nlong-video understanding capability, outperforming state-of-the-art models of\ncomparable sizes across multiple popular benchmarks. Second, it effectively\npreserves video information, with minimal compression loss even at 16x\ncompression ratio. Third, it realizes outstanding cost-effectiveness, enabling\nhigh-quality processing of thousands of frames on a single A100 GPU."
                },
                "authors": [
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Junjie Zhou"
                    },
                    {
                        "name": "Zhengyang Liang"
                    },
                    {
                        "name": "Tiejun Huang"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14485v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14485v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v3",
                "updated": "2024-12-09T01:44:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    44,
                    10,
                    0,
                    344,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01844v3",
                "updated": "2024-12-09T01:39:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    39,
                    15,
                    0,
                    344,
                    0
                ],
                "published": "2024-05-03T04:27:32Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    4,
                    27,
                    32,
                    4,
                    124,
                    0
                ],
                "title": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges"
                },
                "summary": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Shazia Riaz"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "arxiv_doi": "10.1145/3706630",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706630",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.01844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05896v1",
                "updated": "2024-12-08T11:32:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    8,
                    11,
                    32,
                    8,
                    6,
                    343,
                    0
                ],
                "published": "2024-12-08T11:32:08Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    11,
                    32,
                    8,
                    6,
                    343,
                    0
                ],
                "title": "XKV: Personalized KV Cache Memory Reduction for Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XKV: Personalized KV Cache Memory Reduction for Long-Context LLM\n  Inference"
                },
                "summary": "Recently the generative Large Language Model (LLM) has achieved remarkable\nsuccess in numerous applications. Notably its inference generates output tokens\none-by-one, leading to many redundant computations. The widely-used KV-Cache\nframework makes a compromise between time and space complexities. However,\ncaching data generates the increasingly growing memory demand, that can quickly\nexhaust the limited memory capacity of the modern accelerator like GPUs,\nparticularly in long-context inference tasks. Existing studies reduce memory\nconsumption by evicting some of cached data that have less important impact on\ninference accuracy. But the benefit in practice is far from ideal due to the\nstatic cache allocation across different LLM network layers. This paper\nobserves that the layer-specific cached data have very different impacts on\naccuracy. We quantify this difference, and give experimental and theoretical\nvalidation. We accordingly make a formal analysis and shows that customizing\nthe cache size for each layer in a personalized manner can yield a significant\nmemory reduction, while still providing comparable accuracy. We simulate the\ncache allocation as a combinatorial optimization problem and give a global\noptimal solution. In particular, we devise a mini- and sampling-based inference\nover a lightweight variant of the LLM model, so as to quickly capture the\ndifference and then feed it into the personalized algorithms. Extensive\nexperiments on real-world datasets demonstrate that our proposals can reduce KV\ncache memory consumption by 61.6% on average, improve computational efficiency\nby 2.1x and then increase the throughput by up to 5.5x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently the generative Large Language Model (LLM) has achieved remarkable\nsuccess in numerous applications. Notably its inference generates output tokens\none-by-one, leading to many redundant computations. The widely-used KV-Cache\nframework makes a compromise between time and space complexities. However,\ncaching data generates the increasingly growing memory demand, that can quickly\nexhaust the limited memory capacity of the modern accelerator like GPUs,\nparticularly in long-context inference tasks. Existing studies reduce memory\nconsumption by evicting some of cached data that have less important impact on\ninference accuracy. But the benefit in practice is far from ideal due to the\nstatic cache allocation across different LLM network layers. This paper\nobserves that the layer-specific cached data have very different impacts on\naccuracy. We quantify this difference, and give experimental and theoretical\nvalidation. We accordingly make a formal analysis and shows that customizing\nthe cache size for each layer in a personalized manner can yield a significant\nmemory reduction, while still providing comparable accuracy. We simulate the\ncache allocation as a combinatorial optimization problem and give a global\noptimal solution. In particular, we devise a mini- and sampling-based inference\nover a lightweight variant of the LLM model, so as to quickly capture the\ndifference and then feed it into the personalized algorithms. Extensive\nexperiments on real-world datasets demonstrate that our proposals can reduce KV\ncache memory consumption by 61.6% on average, improve computational efficiency\nby 2.1x and then increase the throughput by up to 5.5x."
                },
                "authors": [
                    {
                        "name": "Weizhuo Li"
                    },
                    {
                        "name": "Zhigang Wang"
                    },
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Ge Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ge Yu"
                },
                "author": "Ge Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05704v1",
                "updated": "2024-12-07T17:22:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    17,
                    22,
                    14,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-07T17:22:14Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    17,
                    22,
                    14,
                    5,
                    342,
                    0
                ],
                "title": "Ultrafast lattice and electron dynamics induced in a PbSe crystal by an\n  intense terahertz pulse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultrafast lattice and electron dynamics induced in a PbSe crystal by an\n  intense terahertz pulse"
                },
                "summary": "We have studied the ultrafast optical response of a PbSe crystal to an\nintense picosecond terahertz pulse with a peak electric field strength of up to\n$\\sim$ 500 kV/cm. The reflectivity anisotropy signal contains oscillations at\nthe fundamental frequency of the resonant infrared-active phonon mode as well\nas its second, third, and fourth harmonics. The effect is ascribed to coherent\nanharmonic phonons resonantly excited by the strong terahertz field. Pump\nterahertz pulses also induce an almost instantaneous Kerr effect and a\nlong-lived optical anisotropy of the crystal with a characteristic decay time\nof $\\gtrsim$ 100 ps. We consider lattice distortion and phonon-assisted side\nvalley population as possible origins of this metastable state.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We have studied the ultrafast optical response of a PbSe crystal to an\nintense picosecond terahertz pulse with a peak electric field strength of up to\n$\\sim$ 500 kV/cm. The reflectivity anisotropy signal contains oscillations at\nthe fundamental frequency of the resonant infrared-active phonon mode as well\nas its second, third, and fourth harmonics. The effect is ascribed to coherent\nanharmonic phonons resonantly excited by the strong terahertz field. Pump\nterahertz pulses also induce an almost instantaneous Kerr effect and a\nlong-lived optical anisotropy of the crystal with a characteristic decay time\nof $\\gtrsim$ 100 ps. We consider lattice distortion and phonon-assisted side\nvalley population as possible origins of this metastable state."
                },
                "authors": [
                    {
                        "name": "A. A. Melnikov"
                    },
                    {
                        "name": "Yu. G. Selivanov"
                    },
                    {
                        "name": "D. G. Poydashev"
                    },
                    {
                        "name": "S. V. Chekalin"
                    }
                ],
                "author_detail": {
                    "name": "S. V. Chekalin"
                },
                "author": "S. V. Chekalin",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v1",
                "updated": "2024-12-07T16:41:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06567v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06567v2",
                "updated": "2024-12-07T13:23:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    13,
                    23,
                    39,
                    5,
                    342,
                    0
                ],
                "published": "2024-06-03T13:28:43Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    13,
                    28,
                    43,
                    0,
                    155,
                    0
                ],
                "title": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via\n  Adaptive Heads Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via\n  Adaptive Heads Fusion"
                },
                "summary": "Large language models (LLMs) with billions of parameters demonstrate\nimpressive performance. However, the widely used Multi-Head Attention (MHA) in\nLLMs incurs substantial computational and memory costs during inference. While\nsome efforts have optimized attention mechanisms by pruning heads or sharing\nparameters among heads, these methods often lead to performance degradation or\nnecessitate substantial continued pre-training costs to restore performance.\nBased on the analysis of attention redundancy, we design a Decoupled-Head\nAttention (DHA) mechanism. DHA adaptively configures group sharing for key\nheads and value heads across various layers, achieving a better balance between\nperformance and efficiency. Inspired by the observation of clustering similar\nheads, we propose to progressively transform the MHA checkpoint into the DHA\nmodel through linear fusion of similar head parameters step by step, retaining\nthe parametric knowledge of the MHA checkpoint. We construct DHA models by\ntransforming various scales of MHA checkpoints given target head budgets. Our\nexperiments show that DHA remarkably requires a mere 0.25\\% of the original\nmodel's pre-training budgets to achieve 97.6\\% of performance while saving 75\\%\nof KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5$\\times$\ntraining acceleration, a maximum of 13.93\\% performance improvement under\n0.01\\% pre-training budget, and 4\\% relative improvement under 0.05\\%\npre-training budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with billions of parameters demonstrate\nimpressive performance. However, the widely used Multi-Head Attention (MHA) in\nLLMs incurs substantial computational and memory costs during inference. While\nsome efforts have optimized attention mechanisms by pruning heads or sharing\nparameters among heads, these methods often lead to performance degradation or\nnecessitate substantial continued pre-training costs to restore performance.\nBased on the analysis of attention redundancy, we design a Decoupled-Head\nAttention (DHA) mechanism. DHA adaptively configures group sharing for key\nheads and value heads across various layers, achieving a better balance between\nperformance and efficiency. Inspired by the observation of clustering similar\nheads, we propose to progressively transform the MHA checkpoint into the DHA\nmodel through linear fusion of similar head parameters step by step, retaining\nthe parametric knowledge of the MHA checkpoint. We construct DHA models by\ntransforming various scales of MHA checkpoints given target head budgets. Our\nexperiments show that DHA remarkably requires a mere 0.25\\% of the original\nmodel's pre-training budgets to achieve 97.6\\% of performance while saving 75\\%\nof KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5$\\times$\ntraining acceleration, a maximum of 13.93\\% performance improvement under\n0.01\\% pre-training budget, and 4\\% relative improvement under 0.05\\%\npre-training budget."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Linhao Zhang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yu Sun"
                },
                "author": "Yu Sun",
                "arxiv_comment": "Accepted at NeurIPS 2024 10 pages, 9 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06567v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06567v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v2",
                "updated": "2024-12-07T13:23:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    13,
                    23,
                    39,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v2",
                "updated": "2024-12-07T04:08:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    4,
                    8,
                    56,
                    5,
                    342,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05392v1",
                "updated": "2024-12-06T19:35:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    19,
                    35,
                    52,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T19:35:52Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    19,
                    35,
                    52,
                    4,
                    341,
                    0
                ],
                "title": "Effect of electric field on excitons in wide quantum wells",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effect of electric field on excitons in wide quantum wells"
                },
                "summary": "A microscopic model of a heterostructure with a quantum well (QW) is proposed\nto study the exciton behavior in an external electric field. The effect of an\nelectric field ranging from 0 to 6 kV/cm applied to the GaAs/AlGaAs QW\nstructure in the growth direction is studied for several QWs of various widths\nup to 100 nm. The three-dimensional Schr\\\"odinger equation (SE) of exciton is\nnumerically solved using the finite difference method. Wave functions and\nenergies for several states of the heavy-hole and light-hole excitons are\ncalculated. Dependencies of the exciton state energy, the binding energy, the\nradiative broadening, and the static dipole moment on the applied electric\nfields are determined. The threshold of exciton dissociation for the 100-nm QW\nis also determined. In addition, we found the electric-field-induced shift of\nthe center of mass of the heavy-hole and light-hole exciton in the QWs.\nFinally, we have modeled reflection spectra of heterostructures with the\nGaAs/AlGaAs QWs in the electric field using the calculated energies and\nradiative broadenings of excitons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A microscopic model of a heterostructure with a quantum well (QW) is proposed\nto study the exciton behavior in an external electric field. The effect of an\nelectric field ranging from 0 to 6 kV/cm applied to the GaAs/AlGaAs QW\nstructure in the growth direction is studied for several QWs of various widths\nup to 100 nm. The three-dimensional Schr\\\"odinger equation (SE) of exciton is\nnumerically solved using the finite difference method. Wave functions and\nenergies for several states of the heavy-hole and light-hole excitons are\ncalculated. Dependencies of the exciton state energy, the binding energy, the\nradiative broadening, and the static dipole moment on the applied electric\nfields are determined. The threshold of exciton dissociation for the 100-nm QW\nis also determined. In addition, we found the electric-field-induced shift of\nthe center of mass of the heavy-hole and light-hole exciton in the QWs.\nFinally, we have modeled reflection spectra of heterostructures with the\nGaAs/AlGaAs QWs in the electric field using the calculated energies and\nradiative broadenings of excitons."
                },
                "authors": [
                    {
                        "name": "Shiming Zheng"
                    },
                    {
                        "name": "E. S. Khramtsov"
                    },
                    {
                        "name": "I. V. Ignatiev"
                    }
                ],
                "author_detail": {
                    "name": "I. V. Ignatiev"
                },
                "author": "I. V. Ignatiev",
                "arxiv_comment": "12 pages, 8 figures, to be published in Physical Review B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05228v1",
                "updated": "2024-12-06T17:58:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "title": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips"
                },
                "summary": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate."
                },
                "authors": [
                    {
                        "name": "Ismet Dagli"
                    },
                    {
                        "name": "James Crea"
                    },
                    {
                        "name": "Soner Seckiner"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "SelÃ§uk KÃ¶se"
                    },
                    {
                        "name": "Mehmet E. Belviranli"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet E. Belviranli"
                },
                "author": "Mehmet E. Belviranli",
                "arxiv_comment": "This paper is accepted to 2025 Design, Automation Test in Europe\n  Conference Exhibition (DATE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02031v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02031v2",
                "updated": "2024-12-06T11:47:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    47,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-07-02T07:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    7,
                    59,
                    8,
                    1,
                    184,
                    0
                ],
                "title": "SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules"
                },
                "summary": "Text-to-image (T2I) generation using diffusion models has become a\nblockbuster service in today's AI cloud. A production T2I service typically\ninvolves a serving workflow where a base diffusion model is augmented with\nvarious \"add-on\" modules, notably ControlNet and LoRA, to enhance image\ngeneration control. Compared to serving the base model alone, these add-on\nmodules introduce significant loading and computational overhead, resulting in\nincreased latency. In this paper, we present SwiftDiffusion, a system that\nefficiently serves a T2I workflow through a holistic approach. SwiftDiffusion\ndecouples ControNet from the base model and deploys it as a separate,\nindependently scaled service on dedicated GPUs, enabling ControlNet caching,\nparallelization, and sharing. To mitigate the high loading overhead of LoRA\nserving, SwiftDiffusion employs a bounded asynchronous LoRA loading (BAL)\ntechnique, allowing LoRA loading to overlap with the initial base model\nexecution by up to k steps without compromising image quality. Furthermore,\nSwiftDiffusion optimizes base model execution with a novel latent parallelism\ntechnique. Collectively, these designs enable SwiftDiffusion to outperform the\nstate-of-the-art T2I serving systems, achieving up to 7.8x latency reduction\nand 1.6x throughput improvement in serving SDXL models on H800 GPUs, without\nsacrificing image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) generation using diffusion models has become a\nblockbuster service in today's AI cloud. A production T2I service typically\ninvolves a serving workflow where a base diffusion model is augmented with\nvarious \"add-on\" modules, notably ControlNet and LoRA, to enhance image\ngeneration control. Compared to serving the base model alone, these add-on\nmodules introduce significant loading and computational overhead, resulting in\nincreased latency. In this paper, we present SwiftDiffusion, a system that\nefficiently serves a T2I workflow through a holistic approach. SwiftDiffusion\ndecouples ControNet from the base model and deploys it as a separate,\nindependently scaled service on dedicated GPUs, enabling ControlNet caching,\nparallelization, and sharing. To mitigate the high loading overhead of LoRA\nserving, SwiftDiffusion employs a bounded asynchronous LoRA loading (BAL)\ntechnique, allowing LoRA loading to overlap with the initial base model\nexecution by up to k steps without compromising image quality. Furthermore,\nSwiftDiffusion optimizes base model execution with a novel latent parallelism\ntechnique. Collectively, these designs enable SwiftDiffusion to outperform the\nstate-of-the-art T2I serving systems, achieving up to 7.8x latency reduction\nand 1.6x throughput improvement in serving SDXL models on H800 GPUs, without\nsacrificing image quality."
                },
                "authors": [
                    {
                        "name": "Suyi Li"
                    },
                    {
                        "name": "Lingyun Yang"
                    },
                    {
                        "name": "Xiaoxiao Jiang"
                    },
                    {
                        "name": "Hanfeng Lu"
                    },
                    {
                        "name": "Dakai An"
                    },
                    {
                        "name": "Zhipeng Di"
                    },
                    {
                        "name": "Weiyi Lu"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Kan Liu"
                    },
                    {
                        "name": "Yinghao Yu"
                    },
                    {
                        "name": "Tao Lan"
                    },
                    {
                        "name": "Guodong Yang"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Liping Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02031v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04757v1",
                "updated": "2024-12-06T03:46:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    46,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T03:46:06Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    46,
                    6,
                    4,
                    341,
                    0
                ],
                "title": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern"
                },
                "summary": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference."
                },
                "authors": [
                    {
                        "name": "Hongyin Tang"
                    },
                    {
                        "name": "Di Xiu"
                    },
                    {
                        "name": "Lanrui Wang"
                    },
                    {
                        "name": "Xiurui Geng"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xunliang Cai"
                },
                "author": "Xunliang Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04698v1",
                "updated": "2024-12-06T01:20:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    47,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T01:20:47Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    47,
                    4,
                    341,
                    0
                ],
                "title": "One-Hop Sub-Query Result Caches for Graph Database Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Hop Sub-Query Result Caches for Graph Database Systems"
                },
                "summary": "This paper introduces a novel one-hop sub-query result cache for processing\ngraph read transactions, gR-Txs, in a graph database system. The one-hop\nnavigation is from a vertex using either its in-coming or out-going edges with\nselection predicates that filter edges and vertices. Its cache entry identifies\na unique one-hop sub-query (key) and its result set consisting of immutable\nvertex ids (value). When processing a gR-Tx, the query processor identifies its\nsequence of individual one-hop sub-queries and looks up their results in the\ncache. A cache hit fetches less data from the storage manager and eliminates\nthe requirement to process the one-hop sub-query. A cache miss populates the\ncache asynchronously and in a transactional manner, maintaining the separation\nof read and write paths of our transactional storage manager. A graph read and\nwrite transaction, gRW-Tx, identifies the impacted cache entries and either\ndeletes or updates them. Our implementation of the cache is inside the graph\nquery processing engine and transparent to a user application. We evaluate the\ncache using our eCommerce production workload and with rules that re-write\ngraph queries to maximize the performance enhancements observed with the cache.\nObtained results show the cache enhances 95th and 99th percentile of query\nresponse times by at least 2x and 1.63x, respectively. When combined with query\nre-writing, the enhancements are at least 2.33x and 4.48x, respectively. An\ninteresting result is the significant performance enhancement observed by the\nindirect beneficiaries of the cache, gRW-Txs and gR-Txs that do not reference\none-hop sub-queries. The cache frees system resources to expedite their\nprocessing significantly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel one-hop sub-query result cache for processing\ngraph read transactions, gR-Txs, in a graph database system. The one-hop\nnavigation is from a vertex using either its in-coming or out-going edges with\nselection predicates that filter edges and vertices. Its cache entry identifies\na unique one-hop sub-query (key) and its result set consisting of immutable\nvertex ids (value). When processing a gR-Tx, the query processor identifies its\nsequence of individual one-hop sub-queries and looks up their results in the\ncache. A cache hit fetches less data from the storage manager and eliminates\nthe requirement to process the one-hop sub-query. A cache miss populates the\ncache asynchronously and in a transactional manner, maintaining the separation\nof read and write paths of our transactional storage manager. A graph read and\nwrite transaction, gRW-Tx, identifies the impacted cache entries and either\ndeletes or updates them. Our implementation of the cache is inside the graph\nquery processing engine and transparent to a user application. We evaluate the\ncache using our eCommerce production workload and with rules that re-write\ngraph queries to maximize the performance enhancements observed with the cache.\nObtained results show the cache enhances 95th and 99th percentile of query\nresponse times by at least 2x and 1.63x, respectively. When combined with query\nre-writing, the enhancements are at least 2.33x and 4.48x, respectively. An\ninteresting result is the significant performance enhancement observed by the\nindirect beneficiaries of the cache, gRW-Txs and gR-Txs that do not reference\none-hop sub-queries. The cache frees system resources to expedite their\nprocessing significantly."
                },
                "authors": [
                    {
                        "name": "Hieu Nguyen"
                    },
                    {
                        "name": "Jun Li"
                    },
                    {
                        "name": "Shahram Ghandeharizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Shahram Ghandeharizadeh"
                },
                "author": "Shahram Ghandeharizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04652v1",
                "updated": "2024-12-05T22:47:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    47,
                    17,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T22:47:17Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    47,
                    17,
                    3,
                    340,
                    0
                ],
                "title": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference"
                },
                "summary": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP"
                },
                "authors": [
                    {
                        "name": "Xiaohuan Pei"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04634v1",
                "updated": "2024-12-05T22:06:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    6,
                    23,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T22:06:23Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    6,
                    23,
                    3,
                    340,
                    0
                ],
                "title": "Neural Two-Level Monte Carlo Real-Time Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Two-Level Monte Carlo Real-Time Rendering"
                },
                "summary": "We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques."
                },
                "authors": [
                    {
                        "name": "Mikhail Dereviannykh"
                    },
                    {
                        "name": "Dmitrii Klepikov"
                    },
                    {
                        "name": "Johannes Hanika"
                    },
                    {
                        "name": "Carsten Dachsbacher"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Dachsbacher"
                },
                "author": "Carsten Dachsbacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04449v1",
                "updated": "2024-12-05T18:58:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:58:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay"
                },
                "summary": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training."
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Desen Meng"
                    },
                    {
                        "name": "Ji Qi"
                    },
                    {
                        "name": "Zhenpeng Huang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "Technical Report; Code released at https://github.com/MCG-NJU/p-MoD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v2",
                "updated": "2024-12-05T14:56:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    56,
                    56,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19574v2",
                "updated": "2024-12-05T12:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    19,
                    38,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-29T09:42:38Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    42,
                    38,
                    4,
                    334,
                    0
                ],
                "title": "KV Shifting Attention Enhances Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Shifting Attention Enhances Language Modeling"
                },
                "summary": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters."
                },
                "authors": [
                    {
                        "name": "Mingyu Xu"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01485v2",
                "updated": "2024-12-05T06:52:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    6,
                    52,
                    42,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-02T12:35:53Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts"
                },
                "summary": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.01516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.01516v2",
                "updated": "2024-12-05T01:50:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    1,
                    50,
                    27,
                    3,
                    340,
                    0
                ],
                "published": "2023-05-02T15:27:16Z",
                "published_parsed": [
                    2023,
                    5,
                    2,
                    15,
                    27,
                    16,
                    1,
                    122,
                    0
                ],
                "title": "F2: Designing a Key-Value Store for Large Skewed Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F2: Designing a Key-Value Store for Large Skewed Workloads"
                },
                "summary": "Many real-world workloads present a challenging set of requirements: point\noperations requiring high throughput, working sets much larger than main\nmemory, and natural skew in key access patterns for both reads and writes. We\nfind that modern key-value designs are either optimized for memory-efficiency,\nsacrificing high-performance (LSM-tree designs), or achieve high-performance,\nsaturating modern NVMe SSD bandwidth, at the cost of substantial memory\nresources or high disk wear (CPU-optimized designs). Unfortunately these\ndesigns are not able to handle meet the challenging demands of such\nlarger-than-memory, skewed workloads.\n  To this end, we present F2, a new key-value store that bridges this gap by\ncombining the strengths of both approaches. F2 adopts a tiered, record-oriented\narchitecture inspired by LSM-trees to effectively separate hot from cold\nrecords, while incorporating concurrent latch-free mechanisms from\nCPU-optimized engines to maximize performance on modern NVMe SSDs. To realize\nthis design, we tackle key challenges and introduce several innovations,\nincluding new latch-free algorithms for multi-threaded log compaction and user\noperations (e.g., RMWs), as well as new components: a two-level hash index to\nreduce indexing overhead for cold records and a read-cache for serving read-hot\ndata.\n  Detailed experimental results show that F2 matches or outperforms existing\nsolutions, achieving on average better throughput on memory-constrained\nenvironments compared to state-of-the-art systems like RocksDB (11.75x),\nSplinterDB (4.52x), KVell (10.56x), LeanStore (2.04x), and FASTER (2.38x). F2\nalso maintains its high performance across varying workload skewness levels and\nmemory budgets, while achieving low disk write amplification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many real-world workloads present a challenging set of requirements: point\noperations requiring high throughput, working sets much larger than main\nmemory, and natural skew in key access patterns for both reads and writes. We\nfind that modern key-value designs are either optimized for memory-efficiency,\nsacrificing high-performance (LSM-tree designs), or achieve high-performance,\nsaturating modern NVMe SSD bandwidth, at the cost of substantial memory\nresources or high disk wear (CPU-optimized designs). Unfortunately these\ndesigns are not able to handle meet the challenging demands of such\nlarger-than-memory, skewed workloads.\n  To this end, we present F2, a new key-value store that bridges this gap by\ncombining the strengths of both approaches. F2 adopts a tiered, record-oriented\narchitecture inspired by LSM-trees to effectively separate hot from cold\nrecords, while incorporating concurrent latch-free mechanisms from\nCPU-optimized engines to maximize performance on modern NVMe SSDs. To realize\nthis design, we tackle key challenges and introduce several innovations,\nincluding new latch-free algorithms for multi-threaded log compaction and user\noperations (e.g., RMWs), as well as new components: a two-level hash index to\nreduce indexing overhead for cold records and a read-cache for serving read-hot\ndata.\n  Detailed experimental results show that F2 matches or outperforms existing\nsolutions, achieving on average better throughput on memory-constrained\nenvironments compared to state-of-the-art systems like RocksDB (11.75x),\nSplinterDB (4.52x), KVell (10.56x), LeanStore (2.04x), and FASTER (2.38x). F2\nalso maintains its high performance across varying workload skewness levels and\nmemory budgets, while achieving low disk write amplification."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Badrish Chandramouli"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.01516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.01516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v2",
                "updated": "2024-12-04T18:40:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    40,
                    24,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03361v1",
                "updated": "2024-12-04T14:47:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    47,
                    42,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T14:47:42Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    47,
                    42,
                    2,
                    339,
                    0
                ],
                "title": "Measurement of electron beam induced sample heating in SEM experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement of electron beam induced sample heating in SEM experiments"
                },
                "summary": "Scanning Electron Microscopy (SEM) experiments provide detailed insights into\nmaterial microstructures, enabling high-resolution imaging as well as\ncrystallographic analysis through advanced techniques like Electron Backscatter\nDiffraction (EBSD). However, the interaction of the high-energy electron beam\nwith the material can lead to localized heating, which may significantly impact\nspecimen integrity, especially in applications requiring prolonged beam\nexposure, for instance when mapping the crystal structure using EBSD. This\nstudy examines electron-beam-induced heating effects on a model metal sample\n(iron), directly measuring the locally deposited electron beam energy with a\nMEMS-based heating device and validating these measurements through\nsimulations, including Monte Carlo and Finite Element methods. The analysis\nfocuses on the effects of various experimental parameters such as acceleration\nvoltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time\n(from 1$\\mu$s to 1ms) and sample tilt (0{\\deg} to 70{\\deg}). The findings\nreveal that local sample temperatures can increase by up to 70 {\\deg}C during\nEBSD experiments, primarily affected by the choice in beam current and\nacceleration voltage, with beam current having the most significant impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scanning Electron Microscopy (SEM) experiments provide detailed insights into\nmaterial microstructures, enabling high-resolution imaging as well as\ncrystallographic analysis through advanced techniques like Electron Backscatter\nDiffraction (EBSD). However, the interaction of the high-energy electron beam\nwith the material can lead to localized heating, which may significantly impact\nspecimen integrity, especially in applications requiring prolonged beam\nexposure, for instance when mapping the crystal structure using EBSD. This\nstudy examines electron-beam-induced heating effects on a model metal sample\n(iron), directly measuring the locally deposited electron beam energy with a\nMEMS-based heating device and validating these measurements through\nsimulations, including Monte Carlo and Finite Element methods. The analysis\nfocuses on the effects of various experimental parameters such as acceleration\nvoltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time\n(from 1$\\mu$s to 1ms) and sample tilt (0{\\deg} to 70{\\deg}). The findings\nreveal that local sample temperatures can increase by up to 70 {\\deg}C during\nEBSD experiments, primarily affected by the choice in beam current and\nacceleration voltage, with beam current having the most significant impact."
                },
                "authors": [
                    {
                        "name": "Christina Koenig"
                    },
                    {
                        "name": "Alice Bastos da Silva Fanta"
                    },
                    {
                        "name": "Joerg R. Jinschek"
                    }
                ],
                "author_detail": {
                    "name": "Joerg R. Jinschek"
                },
                "author": "Joerg R. Jinschek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.18603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18603v1",
                "updated": "2024-12-24T18:56:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    18,
                    56,
                    46,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T18:56:46Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    18,
                    56,
                    46,
                    1,
                    359,
                    0
                ],
                "title": "Long-Form Speech Generation with Spoken Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Form Speech Generation with Spoken Language Models"
                },
                "summary": "We consider the generative modeling of speech over multiple minutes, a\nrequirement for long-form multimedia generation and audio-native voice\nassistants. However, current spoken language models struggle to generate\nplausible speech past tens of seconds, from high temporal resolution of speech\ntokens causing loss of coherence, to architectural issues with long-sequence\ntraining or extrapolation, to memory costs at inference time. With these\nconsiderations we propose SpeechSSM, the first speech language model to learn\nfrom and sample long-form spoken audio (e.g., 16 minutes of read or\nextemporaneous speech) in a single decoding session without text intermediates,\nbased on recent advances in linear-time sequence modeling. Furthermore, to\naddress growing challenges in spoken language evaluation, especially in this\nnew long-form setting, we propose: new embedding-based and LLM-judged metrics;\nquality measurements over length and time; and a new benchmark for long-form\nspeech processing and generation, LibriSpeech-Long. Speech samples and the\ndataset are released at\nhttps://google.github.io/tacotron/publications/speechssm/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the generative modeling of speech over multiple minutes, a\nrequirement for long-form multimedia generation and audio-native voice\nassistants. However, current spoken language models struggle to generate\nplausible speech past tens of seconds, from high temporal resolution of speech\ntokens causing loss of coherence, to architectural issues with long-sequence\ntraining or extrapolation, to memory costs at inference time. With these\nconsiderations we propose SpeechSSM, the first speech language model to learn\nfrom and sample long-form spoken audio (e.g., 16 minutes of read or\nextemporaneous speech) in a single decoding session without text intermediates,\nbased on recent advances in linear-time sequence modeling. Furthermore, to\naddress growing challenges in spoken language evaluation, especially in this\nnew long-form setting, we propose: new embedding-based and LLM-judged metrics;\nquality measurements over length and time; and a new benchmark for long-form\nspeech processing and generation, LibriSpeech-Long. Speech samples and the\ndataset are released at\nhttps://google.github.io/tacotron/publications/speechssm/"
                },
                "authors": [
                    {
                        "name": "Se Jin Park"
                    },
                    {
                        "name": "Julian Salazar"
                    },
                    {
                        "name": "Aren Jansen"
                    },
                    {
                        "name": "Keisuke Kinoshita"
                    },
                    {
                        "name": "Yong Man Ro"
                    },
                    {
                        "name": "RJ Skerry-Ryan"
                    }
                ],
                "author_detail": {
                    "name": "RJ Skerry-Ryan"
                },
                "author": "RJ Skerry-Ryan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18601v1",
                "updated": "2024-12-24T18:56:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    18,
                    56,
                    0,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T18:56:00Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    18,
                    56,
                    0,
                    1,
                    359,
                    0
                ],
                "title": "Decentralized Intelligence in GameFi: Embodied AI Agents and the\n  Convergence of DeFi and Virtual Ecosystems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Intelligence in GameFi: Embodied AI Agents and the\n  Convergence of DeFi and Virtual Ecosystems"
                },
                "summary": "In the rapidly evolving landscape of GameFi, a fusion of gaming and\ndecentralized finance (DeFi), there exists a critical need to enhance player\nengagement and economic interaction within gaming ecosystems. Our GameFi\necosystem aims to fundamentally transform this landscape by integrating\nadvanced embodied AI agents into GameFi platforms. These AI agents, developed\nusing cutting-edge large language models (LLMs), such as GPT-4 and Claude AI,\nare capable of proactive, adaptive, and contextually rich interactions with\nplayers. By going beyond traditional scripted responses, these agents become\nintegral participants in the game's narrative and economic systems, directly\ninfluencing player strategies and in-game economies. We address the limitations\nof current GameFi platforms, which often lack immersive AI interactions and\nmechanisms for community engagement or creator monetization. Through the deep\nintegration of AI agents with blockchain technology, we establish a\nconsensus-driven, decentralized GameFi ecosystem. This ecosystem empowers\ncreators to monetize their contributions and fosters democratic collaboration\namong players and creators. Furthermore, by embedding DeFi mechanisms into the\ngaming experience, we enhance economic participation and provide new\nopportunities for financial interactions within the game. Our approach enhances\nplayer immersion and retention and advances the GameFi ecosystem by bridging\ntraditional gaming with Web3 technologies. By integrating sophisticated AI and\nDeFi elements, we contribute to the development of more engaging, economically\nrobust, and community-centric gaming environments. This project represents a\nsignificant advancement in the state-of-the-art in GameFi, offering insights\nand methodologies that can be applied throughout the gaming industry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving landscape of GameFi, a fusion of gaming and\ndecentralized finance (DeFi), there exists a critical need to enhance player\nengagement and economic interaction within gaming ecosystems. Our GameFi\necosystem aims to fundamentally transform this landscape by integrating\nadvanced embodied AI agents into GameFi platforms. These AI agents, developed\nusing cutting-edge large language models (LLMs), such as GPT-4 and Claude AI,\nare capable of proactive, adaptive, and contextually rich interactions with\nplayers. By going beyond traditional scripted responses, these agents become\nintegral participants in the game's narrative and economic systems, directly\ninfluencing player strategies and in-game economies. We address the limitations\nof current GameFi platforms, which often lack immersive AI interactions and\nmechanisms for community engagement or creator monetization. Through the deep\nintegration of AI agents with blockchain technology, we establish a\nconsensus-driven, decentralized GameFi ecosystem. This ecosystem empowers\ncreators to monetize their contributions and fosters democratic collaboration\namong players and creators. Furthermore, by embedding DeFi mechanisms into the\ngaming experience, we enhance economic participation and provide new\nopportunities for financial interactions within the game. Our approach enhances\nplayer immersion and retention and advances the GameFi ecosystem by bridging\ntraditional gaming with Web3 technologies. By integrating sophisticated AI and\nDeFi elements, we contribute to the development of more engaging, economically\nrobust, and community-centric gaming environments. This project represents a\nsignificant advancement in the state-of-the-art in GameFi, offering insights\nand methodologies that can be applied throughout the gaming industry."
                },
                "authors": [
                    {
                        "name": "Fernando Jia"
                    },
                    {
                        "name": "Jade Zheng"
                    },
                    {
                        "name": "Florence Li"
                    }
                ],
                "author_detail": {
                    "name": "Florence Li"
                },
                "author": "Florence Li",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18596v1",
                "updated": "2024-12-24T18:51:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    18,
                    51,
                    11,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T18:51:11Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    18,
                    51,
                    11,
                    1,
                    359,
                    0
                ],
                "title": "LatentCRF: Continuous CRF for Efficient Latent Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LatentCRF: Continuous CRF for Efficient Latent Diffusion"
                },
                "summary": "Latent Diffusion Models (LDMs) produce high-quality, photo-realistic images,\nhowever, the latency incurred by multiple costly inference iterations can\nrestrict their applicability. We introduce LatentCRF, a continuous Conditional\nRandom Field (CRF) model, implemented as a neural network layer, that models\nthe spatial and semantic relationships among the latent vectors in the LDM. By\nreplacing some of the computationally-intensive LDM inference iterations with\nour lightweight LatentCRF, we achieve a superior balance between quality, speed\nand diversity. We increase inference efficiency by 33% with no loss in image\nquality or diversity compared to the full LDM. LatentCRF is an easy add-on,\nwhich does not require modifying the LDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Diffusion Models (LDMs) produce high-quality, photo-realistic images,\nhowever, the latency incurred by multiple costly inference iterations can\nrestrict their applicability. We introduce LatentCRF, a continuous Conditional\nRandom Field (CRF) model, implemented as a neural network layer, that models\nthe spatial and semantic relationships among the latent vectors in the LDM. By\nreplacing some of the computationally-intensive LDM inference iterations with\nour lightweight LatentCRF, we achieve a superior balance between quality, speed\nand diversity. We increase inference efficiency by 33% with no loss in image\nquality or diversity compared to the full LDM. LatentCRF is an easy add-on,\nwhich does not require modifying the LDM."
                },
                "authors": [
                    {
                        "name": "Kanchana Ranasinghe"
                    },
                    {
                        "name": "Sadeep Jayasumana"
                    },
                    {
                        "name": "Andreas Veit"
                    },
                    {
                        "name": "Ayan Chakrabarti"
                    },
                    {
                        "name": "Daniel Glasner"
                    },
                    {
                        "name": "Michael S Ryoo"
                    },
                    {
                        "name": "Srikumar Ramalingam"
                    },
                    {
                        "name": "Sanjiv Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Sanjiv Kumar"
                },
                "author": "Sanjiv Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18588v1",
                "updated": "2024-12-24T18:41:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    18,
                    41,
                    15,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T18:41:15Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    18,
                    41,
                    15,
                    1,
                    359,
                    0
                ],
                "title": "A Paragraph is All It Takes: Rich Robot Behaviors from Interacting,\n  Trusted LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Paragraph is All It Takes: Rich Robot Behaviors from Interacting,\n  Trusted LLMs"
                },
                "summary": "Large Language Models (LLMs) are compact representations of all public\nknowledge of our physical environment and animal and human behaviors. The\napplication of LLMs to robotics may offer a path to highly capable robots that\nperform well across most human tasks with limited or even zero tuning. Aside\nfrom increasingly sophisticated reasoning and task planning, networks of\n(suitably designed) LLMs offer ease of upgrading capabilities and allow humans\nto directly observe the robot's thinking. Here we explore the advantages,\nlimitations, and particularities of using LLMs to control physical robots. The\nbasic system consists of four LLMs communicating via a human language data bus\nimplemented via web sockets and ROS2 message passing. Surprisingly, rich robot\nbehaviors and good performance across different tasks could be achieved despite\nthe robot's data fusion cycle running at only 1Hz and the central data bus\nrunning at the extremely limited rates of the human brain, of around 40 bits/s.\nThe use of natural language for inter-LLM communication allowed the robot's\nreasoning and decision making to be directly observed by humans and made it\ntrivial to bias the system's behavior with sets of rules written in plain\nEnglish. These rules were immutably written into Ethereum, a global, public,\nand censorship resistant Turing-complete computer. We suggest that by using\nnatural language as the data bus among interacting AIs, and immutable public\nledgers to store behavior constraints, it is possible to build robots that\ncombine unexpectedly rich performance, upgradability, and durable alignment\nwith humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are compact representations of all public\nknowledge of our physical environment and animal and human behaviors. The\napplication of LLMs to robotics may offer a path to highly capable robots that\nperform well across most human tasks with limited or even zero tuning. Aside\nfrom increasingly sophisticated reasoning and task planning, networks of\n(suitably designed) LLMs offer ease of upgrading capabilities and allow humans\nto directly observe the robot's thinking. Here we explore the advantages,\nlimitations, and particularities of using LLMs to control physical robots. The\nbasic system consists of four LLMs communicating via a human language data bus\nimplemented via web sockets and ROS2 message passing. Surprisingly, rich robot\nbehaviors and good performance across different tasks could be achieved despite\nthe robot's data fusion cycle running at only 1Hz and the central data bus\nrunning at the extremely limited rates of the human brain, of around 40 bits/s.\nThe use of natural language for inter-LLM communication allowed the robot's\nreasoning and decision making to be directly observed by humans and made it\ntrivial to bias the system's behavior with sets of rules written in plain\nEnglish. These rules were immutably written into Ethereum, a global, public,\nand censorship resistant Turing-complete computer. We suggest that by using\nnatural language as the data bus among interacting AIs, and immutable public\nledgers to store behavior constraints, it is possible to build robots that\ncombine unexpectedly rich performance, upgradability, and durable alignment\nwith humans."
                },
                "authors": [
                    {
                        "name": "OpenMind"
                    },
                    {
                        "name": "Shaohong Zhong"
                    },
                    {
                        "name": "Adam Zhou"
                    },
                    {
                        "name": "Boyuan Chen"
                    },
                    {
                        "name": "Homin Luo"
                    },
                    {
                        "name": "Jan Liphardt"
                    }
                ],
                "author_detail": {
                    "name": "Jan Liphardt"
                },
                "author": "Jan Liphardt",
                "arxiv_comment": "10 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10924v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10924v3",
                "updated": "2024-12-24T17:56:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    17,
                    56,
                    50,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-14T18:18:52Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    18,
                    18,
                    52,
                    5,
                    349,
                    0
                ],
                "title": "Tokens, the oft-overlooked appetizer: Large language models, the\n  distributional hypothesis, and meaning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokens, the oft-overlooked appetizer: Large language models, the\n  distributional hypothesis, and meaning"
                },
                "summary": "Tokenization is a necessary component within the current architecture of many\nlanguage models, including the transformer-based large language models (LLMs)\nof Generative AI, yet its impact on the model's cognition is often overlooked.\nWe argue that LLMs demonstrate that the Distributional Hypothesis (DH) is\nsufficient for reasonably human-like language performance, and that the\nemergence of human-meaningful linguistic units among tokens motivates\nlinguistically-informed interventions in existing, linguistically-agnostic\ntokenization techniques, particularly with respect to their roles as (1)\nsemantic primitives and as (2) vehicles for conveying salient distributional\npatterns from human language to the model. We explore tokenizations from a BPE\ntokenizer; extant model vocabularies obtained from Hugging Face and tiktoken;\nand the information in exemplar token vectors as they move through the layers\nof a RoBERTa (large) model. Besides creating sub-optimal semantic building\nblocks and obscuring the model's access to the necessary distributional\npatterns, we describe how tokenization pretraining can be a backdoor for bias\nand other unwanted content, which current alignment practices may not\nremediate. Additionally, we relay evidence that the tokenization algorithm's\nobjective function impacts the LLM's cognition, despite being meaningfully\ninsulated from the main system intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization is a necessary component within the current architecture of many\nlanguage models, including the transformer-based large language models (LLMs)\nof Generative AI, yet its impact on the model's cognition is often overlooked.\nWe argue that LLMs demonstrate that the Distributional Hypothesis (DH) is\nsufficient for reasonably human-like language performance, and that the\nemergence of human-meaningful linguistic units among tokens motivates\nlinguistically-informed interventions in existing, linguistically-agnostic\ntokenization techniques, particularly with respect to their roles as (1)\nsemantic primitives and as (2) vehicles for conveying salient distributional\npatterns from human language to the model. We explore tokenizations from a BPE\ntokenizer; extant model vocabularies obtained from Hugging Face and tiktoken;\nand the information in exemplar token vectors as they move through the layers\nof a RoBERTa (large) model. Besides creating sub-optimal semantic building\nblocks and obscuring the model's access to the necessary distributional\npatterns, we describe how tokenization pretraining can be a backdoor for bias\nand other unwanted content, which current alignment practices may not\nremediate. Additionally, we relay evidence that the tokenization algorithm's\nobjective function impacts the LLM's cognition, despite being meaningfully\ninsulated from the main system intelligence."
                },
                "authors": [
                    {
                        "name": "Julia Witte Zimmerman"
                    },
                    {
                        "name": "Denis Hudon"
                    },
                    {
                        "name": "Kathryn Cramer"
                    },
                    {
                        "name": "Alejandro J. Ruiz"
                    },
                    {
                        "name": "Calla Beauregard"
                    },
                    {
                        "name": "Ashley Fehr"
                    },
                    {
                        "name": "Mikaela Irene Fudolig"
                    },
                    {
                        "name": "Bradford Demarest"
                    },
                    {
                        "name": "Yoshi Meke Bird"
                    },
                    {
                        "name": "Milo Z. Trujillo"
                    },
                    {
                        "name": "Christopher M. Danforth"
                    },
                    {
                        "name": "Peter Sheridan Dodds"
                    }
                ],
                "author_detail": {
                    "name": "Peter Sheridan Dodds"
                },
                "author": "Peter Sheridan Dodds",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10924v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10924v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18574v1",
                "updated": "2024-12-24T17:56:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    17,
                    56,
                    35,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T17:56:35Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    17,
                    56,
                    35,
                    1,
                    359,
                    0
                ],
                "title": "Inferring intermediate states by leveraging the many-body Arrhenius law",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring intermediate states by leveraging the many-body Arrhenius law"
                },
                "summary": "Metastable states appear as long-lived intermediate states in various natural\ntransport phenomena which are governed by energy landscapes. Moreover, they\ndominate a system's evolution in deciding the selective outcome or shedding\nlight on the preferred mechanism on how a system explores the energy landscape.\nIt is thus crucial to develop techniques to quantify these metastabilities\nhence uncovering key details of the energy landscape. Here, we propose a\npowerful method by leveraging a many-body Arrhenius law that detects the\nmetastabilites in an escape problem, involving interacting particles with\nexcluded volume confined to a complex energy landscape. Observing transport in\ncolloidal systems or translocation of macromolecules through biological pores\ncan be an ideal test bed to verify our results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metastable states appear as long-lived intermediate states in various natural\ntransport phenomena which are governed by energy landscapes. Moreover, they\ndominate a system's evolution in deciding the selective outcome or shedding\nlight on the preferred mechanism on how a system explores the energy landscape.\nIt is thus crucial to develop techniques to quantify these metastabilities\nhence uncovering key details of the energy landscape. Here, we propose a\npowerful method by leveraging a many-body Arrhenius law that detects the\nmetastabilites in an escape problem, involving interacting particles with\nexcluded volume confined to a complex energy landscape. Observing transport in\ncolloidal systems or translocation of macromolecules through biological pores\ncan be an ideal test bed to verify our results."
                },
                "authors": [
                    {
                        "name": "Vishwajeet Kumar"
                    },
                    {
                        "name": "Arnab Pal"
                    },
                    {
                        "name": "Ohad Shpielberg"
                    }
                ],
                "author_detail": {
                    "name": "Ohad Shpielberg"
                },
                "author": "Ohad Shpielberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18573v1",
                "updated": "2024-12-24T17:56:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    17,
                    56,
                    8,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T17:56:08Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    17,
                    56,
                    8,
                    1,
                    359,
                    0
                ],
                "title": "How Well Do LLMs Generate Code for Different Application Domains?\n  Benchmark and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Well Do LLMs Generate Code for Different Application Domains?\n  Benchmark and Evaluation"
                },
                "summary": "Recently, an increasing number of AI-driven programming assistants powered by\ncode LLMs have been integrated into various real-world software development\nenvironments, significantly boosting developer productivity. However, existing\ncode generation benchmarks primarily focus on general-purpose scenarios,\nleaving the code generation performance of LLMs for specific application\ndomains largely unknown. In this paper, we introduce a new benchmark,\nMultiCodeBench, to fill this gap. MultiCodeBench comprises 2,400 programming\ntasks, covering 12 popular software development domains and 15 programming\nlanguages. Specifically, we perform in-depth research to identify these 12\napplication domains. Given that each domain may involve multiple technical\nframeworks, and that different frameworks present distinct challenges in the\ncoding process, we categorize the commonly used frameworks and platforms within\neach domain. We then sample programming problems from GitHub repositories\nrelated to these subdomains. To ensure the quality of the tasks and mitigate\ndata leakage issues, we invite annotators to rewrite the docstrings for each\ntask in MultiCodeBench. Additionally, we build a static analysis-based\ndependency parsing tool to extract the dependencies in the ground truth for\neach task, enabling deeper performance analysis. Through extensive experiments\non MultiCodeBench with eleven representative mainstream LLMs, we reveal the\ncode generation performance of the LLMs across different application domains,\nproviding practical insights for developers in downstream fields when selecting\nLLMs. Furthermore, we analyze the reasons behind the models' failures in\ncompleting software application development tasks, offering guidance for model\ndevelopers to enhance domain-specific code generation capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, an increasing number of AI-driven programming assistants powered by\ncode LLMs have been integrated into various real-world software development\nenvironments, significantly boosting developer productivity. However, existing\ncode generation benchmarks primarily focus on general-purpose scenarios,\nleaving the code generation performance of LLMs for specific application\ndomains largely unknown. In this paper, we introduce a new benchmark,\nMultiCodeBench, to fill this gap. MultiCodeBench comprises 2,400 programming\ntasks, covering 12 popular software development domains and 15 programming\nlanguages. Specifically, we perform in-depth research to identify these 12\napplication domains. Given that each domain may involve multiple technical\nframeworks, and that different frameworks present distinct challenges in the\ncoding process, we categorize the commonly used frameworks and platforms within\neach domain. We then sample programming problems from GitHub repositories\nrelated to these subdomains. To ensure the quality of the tasks and mitigate\ndata leakage issues, we invite annotators to rewrite the docstrings for each\ntask in MultiCodeBench. Additionally, we build a static analysis-based\ndependency parsing tool to extract the dependencies in the ground truth for\neach task, enabling deeper performance analysis. Through extensive experiments\non MultiCodeBench with eleven representative mainstream LLMs, we reveal the\ncode generation performance of the LLMs across different application domains,\nproviding practical insights for developers in downstream fields when selecting\nLLMs. Furthermore, we analyze the reasons behind the models' failures in\ncompleting software application development tasks, offering guidance for model\ndevelopers to enhance domain-specific code generation capabilities."
                },
                "authors": [
                    {
                        "name": "Dewu Zheng"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Ensheng Shi"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16135v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16135v2",
                "updated": "2024-12-24T17:50:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    17,
                    50,
                    1,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-20T18:31:24Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    31,
                    24,
                    4,
                    355,
                    0
                ],
                "title": "Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models\n  into Assembly Code Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models\n  into Assembly Code Obfuscation"
                },
                "summary": "Malware authors often employ code obfuscations to make their malware harder\nto detect. Existing tools for generating obfuscated code often require access\nto the original source code (e.g., C++ or Java), and adding new obfuscations is\na non-trivial, labor-intensive process. In this study, we ask the following\nquestion: Can Large Language Models (LLMs) potentially generate a new\nobfuscated assembly code? If so, this poses a risk to anti-virus engines and\npotentially increases the flexibility of attackers to create new obfuscation\npatterns. We answer this in the affirmative by developing the MetamorphASM\nbenchmark comprising MetamorphASM Dataset (MAD) along with three code\nobfuscation techniques: dead code, register substitution, and control flow\nchange. The MetamorphASM systematically evaluates the ability of LLMs to\ngenerate and analyze obfuscated code using MAD, which contains 328,200\nobfuscated assembly code samples. We release this dataset and analyze the\nsuccess rate of various LLMs (e.g., GPT-3.5/4, GPT-4o-mini, Starcoder,\nCodeGemma, CodeLlama, CodeT5, and LLaMA 3.1) in generating obfuscated assembly\ncode. The evaluation was performed using established information-theoretic\nmetrics and manual human review to ensure correctness and provide the\nfoundation for researchers to study and develop remediations to this risk. The\nsource code can be found at the following GitHub link:\nhttps://github.com/mohammadi-ali/MetamorphASM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Malware authors often employ code obfuscations to make their malware harder\nto detect. Existing tools for generating obfuscated code often require access\nto the original source code (e.g., C++ or Java), and adding new obfuscations is\na non-trivial, labor-intensive process. In this study, we ask the following\nquestion: Can Large Language Models (LLMs) potentially generate a new\nobfuscated assembly code? If so, this poses a risk to anti-virus engines and\npotentially increases the flexibility of attackers to create new obfuscation\npatterns. We answer this in the affirmative by developing the MetamorphASM\nbenchmark comprising MetamorphASM Dataset (MAD) along with three code\nobfuscation techniques: dead code, register substitution, and control flow\nchange. The MetamorphASM systematically evaluates the ability of LLMs to\ngenerate and analyze obfuscated code using MAD, which contains 328,200\nobfuscated assembly code samples. We release this dataset and analyze the\nsuccess rate of various LLMs (e.g., GPT-3.5/4, GPT-4o-mini, Starcoder,\nCodeGemma, CodeLlama, CodeT5, and LLaMA 3.1) in generating obfuscated assembly\ncode. The evaluation was performed using established information-theoretic\nmetrics and manual human review to ensure correctness and provide the\nfoundation for researchers to study and develop remediations to this risk. The\nsource code can be found at the following GitHub link:\nhttps://github.com/mohammadi-ali/MetamorphASM."
                },
                "authors": [
                    {
                        "name": "Seyedreza Mohseni"
                    },
                    {
                        "name": "Seyedali Mohammadi"
                    },
                    {
                        "name": "Deepa Tilwani"
                    },
                    {
                        "name": "Yash Saxena"
                    },
                    {
                        "name": "Gerald Ndawula"
                    },
                    {
                        "name": "Sriram Vema"
                    },
                    {
                        "name": "Edward Raff"
                    },
                    {
                        "name": "Manas Gaur"
                    }
                ],
                "author_detail": {
                    "name": "Manas Gaur"
                },
                "author": "Manas Gaur",
                "arxiv_comment": "To appear in AAAI 2025, Main Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16135v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16135v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18568v1",
                "updated": "2024-12-24T17:41:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    17,
                    41,
                    41,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T17:41:41Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    17,
                    41,
                    41,
                    1,
                    359,
                    0
                ],
                "title": "HNCI: High-Dimensional Network Causal Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HNCI: High-Dimensional Network Causal Inference"
                },
                "summary": "The problem of evaluating the effectiveness of a treatment or policy commonly\nappears in causal inference applications under network interference. In this\npaper, we suggest the new method of high-dimensional network causal inference\n(HNCI) that provides both valid confidence interval on the average direct\ntreatment effect on the treated (ADET) and valid confidence set for the\nneighborhood size for interference effect. We exploit the model setting in\nBelloni et al. (2022) and allow certain type of heterogeneity in node\ninterference neighborhood sizes. We propose a linear regression formulation of\npotential outcomes, where the regression coefficients correspond to the\nunderlying true interference function values of nodes and exhibit a latent\nhomogeneous structure. Such a formulation allows us to leverage existing\nliterature from linear regression and homogeneity pursuit to conduct valid\nstatistical inferences with theoretical guarantees. The resulting confidence\nintervals for the ADET are formally justified through asymptotic normalities\nwith estimable variances. We further provide the confidence set for the\nneighborhood size with theoretical guarantees exploiting the repro samples\napproach. The practical utilities of the newly suggested methods are\ndemonstrated through simulation and real data examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The problem of evaluating the effectiveness of a treatment or policy commonly\nappears in causal inference applications under network interference. In this\npaper, we suggest the new method of high-dimensional network causal inference\n(HNCI) that provides both valid confidence interval on the average direct\ntreatment effect on the treated (ADET) and valid confidence set for the\nneighborhood size for interference effect. We exploit the model setting in\nBelloni et al. (2022) and allow certain type of heterogeneity in node\ninterference neighborhood sizes. We propose a linear regression formulation of\npotential outcomes, where the regression coefficients correspond to the\nunderlying true interference function values of nodes and exhibit a latent\nhomogeneous structure. Such a formulation allows us to leverage existing\nliterature from linear regression and homogeneity pursuit to conduct valid\nstatistical inferences with theoretical guarantees. The resulting confidence\nintervals for the ADET are formally justified through asymptotic normalities\nwith estimable variances. We further provide the confidence set for the\nneighborhood size with theoretical guarantees exploiting the repro samples\napproach. The practical utilities of the newly suggested methods are\ndemonstrated through simulation and real data examples."
                },
                "authors": [
                    {
                        "name": "Wenqin Du"
                    },
                    {
                        "name": "Rundong Ding"
                    },
                    {
                        "name": "Yingying Fan"
                    },
                    {
                        "name": "Jinchi Lv"
                    }
                ],
                "author_detail": {
                    "name": "Jinchi Lv"
                },
                "author": "Jinchi Lv",
                "arxiv_comment": "89 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18566v1",
                "updated": "2024-12-24T17:37:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    17,
                    37,
                    11,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T17:37:11Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    17,
                    37,
                    11,
                    1,
                    359,
                    0
                ],
                "title": "Zero-resource Speech Translation and Recognition with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-resource Speech Translation and Recognition with LLMs"
                },
                "summary": "Despite recent advancements in speech processing, zero-resource speech\ntranslation (ST) and automatic speech recognition (ASR) remain challenging\nproblems. In this work, we propose to leverage a multilingual Large Language\nModel (LLM) to perform ST and ASR in languages for which the model has never\nseen paired audio-text data. We achieve this by using a pre-trained\nmultilingual speech encoder, a multilingual LLM, and a lightweight adaptation\nmodule that maps the audio representations to the token embedding space of the\nLLM. We perform several experiments both in ST and ASR to understand how to\nbest train the model and what data has the most impact on performance in\npreviously unseen languages. In ST, our best model is capable to achieve BLEU\nscores over 23 in CoVoST2 for two previously unseen languages, while in ASR, we\nachieve WERs of up to 28.2\\%. We finally show that the performance of our\nsystem is bounded by the ability of the LLM to output text in the desired\nlanguage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advancements in speech processing, zero-resource speech\ntranslation (ST) and automatic speech recognition (ASR) remain challenging\nproblems. In this work, we propose to leverage a multilingual Large Language\nModel (LLM) to perform ST and ASR in languages for which the model has never\nseen paired audio-text data. We achieve this by using a pre-trained\nmultilingual speech encoder, a multilingual LLM, and a lightweight adaptation\nmodule that maps the audio representations to the token embedding space of the\nLLM. We perform several experiments both in ST and ASR to understand how to\nbest train the model and what data has the most impact on performance in\npreviously unseen languages. In ST, our best model is capable to achieve BLEU\nscores over 23 in CoVoST2 for two previously unseen languages, while in ASR, we\nachieve WERs of up to 28.2\\%. We finally show that the performance of our\nsystem is bounded by the ability of the LLM to output text in the desired\nlanguage."
                },
                "authors": [
                    {
                        "name": "Karel Mundnich"
                    },
                    {
                        "name": "Xing Niu"
                    },
                    {
                        "name": "Prashant Mathur"
                    },
                    {
                        "name": "Srikanth Ronanki"
                    },
                    {
                        "name": "Brady Houston"
                    },
                    {
                        "name": "Veera Raghavendra Elluru"
                    },
                    {
                        "name": "Nilaksh Das"
                    },
                    {
                        "name": "Zejiang Hou"
                    },
                    {
                        "name": "Goeric Huybrechts"
                    },
                    {
                        "name": "Anshu Bhatia"
                    },
                    {
                        "name": "Daniel Garcia-Romero"
                    },
                    {
                        "name": "Kyu J. Han"
                    },
                    {
                        "name": "Katrin Kirchhoff"
                    }
                ],
                "author_detail": {
                    "name": "Katrin Kirchhoff"
                },
                "author": "Katrin Kirchhoff",
                "arxiv_comment": "ICASSP 2025, 5 pages, 2 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18552v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18552v1",
                "updated": "2024-12-24T17:05:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    17,
                    5,
                    26,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T17:05:26Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    17,
                    5,
                    26,
                    1,
                    359,
                    0
                ],
                "title": "Distilling Fine-grained Sentiment Understanding from Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Fine-grained Sentiment Understanding from Large Language\n  Models"
                },
                "summary": "Fine-grained sentiment analysis (FSA) aims to extract and summarize user\nopinions from vast opinionated text. Recent studies demonstrate that large\nlanguage models (LLMs) possess exceptional sentiment understanding\ncapabilities. However, directly deploying LLMs for FSA applications incurs high\ninference costs. Therefore, this paper investigates the distillation of\nfine-grained sentiment understanding from LLMs into small language models\n(SLMs). We prompt LLMs to examine and interpret the sentiments of given reviews\nand then utilize the generated content to pretrain SLMs. Additionally, we\ndevelop a comprehensive FSA benchmark to evaluate both SLMs and LLMs. Extensive\nexperiments on this benchmark reveal that: (1) distillation significantly\nenhances the performance of SLMs in FSA tasks, achieving a 6.00\\% improvement\nin $F_1$-score, and the distilled model can outperform Llama-2-7b with only\n220M parameters; (2) distillation equips SLMs with excellent zero-shot\nsentiment classification capabilities, enabling them to match or even exceed\ntheir teacher models. These results suggest that distillation from LLMs is a\nhighly promising direction for FSA. We will release our code, data, and\npretrained model weights at\n\\url{https://github.com/HITSZ-HLT/FSA-Distillation}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained sentiment analysis (FSA) aims to extract and summarize user\nopinions from vast opinionated text. Recent studies demonstrate that large\nlanguage models (LLMs) possess exceptional sentiment understanding\ncapabilities. However, directly deploying LLMs for FSA applications incurs high\ninference costs. Therefore, this paper investigates the distillation of\nfine-grained sentiment understanding from LLMs into small language models\n(SLMs). We prompt LLMs to examine and interpret the sentiments of given reviews\nand then utilize the generated content to pretrain SLMs. Additionally, we\ndevelop a comprehensive FSA benchmark to evaluate both SLMs and LLMs. Extensive\nexperiments on this benchmark reveal that: (1) distillation significantly\nenhances the performance of SLMs in FSA tasks, achieving a 6.00\\% improvement\nin $F_1$-score, and the distilled model can outperform Llama-2-7b with only\n220M parameters; (2) distillation equips SLMs with excellent zero-shot\nsentiment classification capabilities, enabling them to match or even exceed\ntheir teacher models. These results suggest that distillation from LLMs is a\nhighly promising direction for FSA. We will release our code, data, and\npretrained model weights at\n\\url{https://github.com/HITSZ-HLT/FSA-Distillation}."
                },
                "authors": [
                    {
                        "name": "Yice Zhang"
                    },
                    {
                        "name": "Guangyu Xie"
                    },
                    {
                        "name": "Hongling Xu"
                    },
                    {
                        "name": "Kaiheng Hou"
                    },
                    {
                        "name": "Jianzhu Bao"
                    },
                    {
                        "name": "Qianlong Wang"
                    },
                    {
                        "name": "Shiwei Chen"
                    },
                    {
                        "name": "Ruifeng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruifeng Xu"
                },
                "author": "Ruifeng Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18552v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18552v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18551v1",
                "updated": "2024-12-24T17:03:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    17,
                    3,
                    44,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T17:03:44Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    17,
                    3,
                    44,
                    1,
                    359,
                    0
                ],
                "title": "Libra-Leaderboard: Towards Responsible AI through a Balanced Leaderboard\n  of Safety and Capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Libra-Leaderboard: Towards Responsible AI through a Balanced Leaderboard\n  of Safety and Capability"
                },
                "summary": "To address this gap, we introduce Libra-Leaderboard, a comprehensive\nframework designed to rank LLMs through a balanced evaluation of performance\nand safety. Combining a dynamic leaderboard with an interactive LLM arena,\nLibra-Leaderboard encourages the joint optimization of capability and safety.\nUnlike traditional approaches that average performance and safety metrics,\nLibra-Leaderboard uses a distance-to-optimal-score method to calculate the\noverall rankings. This approach incentivizes models to achieve a balance rather\nthan excelling in one dimension at the expense of some other ones. In the first\nrelease, Libra-Leaderboard evaluates 26 mainstream LLMs from 14 leading\norganizations, identifying critical safety challenges even in state-of-the-art\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To address this gap, we introduce Libra-Leaderboard, a comprehensive\nframework designed to rank LLMs through a balanced evaluation of performance\nand safety. Combining a dynamic leaderboard with an interactive LLM arena,\nLibra-Leaderboard encourages the joint optimization of capability and safety.\nUnlike traditional approaches that average performance and safety metrics,\nLibra-Leaderboard uses a distance-to-optimal-score method to calculate the\noverall rankings. This approach incentivizes models to achieve a balance rather\nthan excelling in one dimension at the expense of some other ones. In the first\nrelease, Libra-Leaderboard evaluates 26 mainstream LLMs from 14 leading\norganizations, identifying critical safety challenges even in state-of-the-art\nmodels."
                },
                "authors": [
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Xudong Han"
                    },
                    {
                        "name": "Zenan Zhai"
                    },
                    {
                        "name": "Honglin Mu"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Zhenxuan Zhang"
                    },
                    {
                        "name": "Yilin Geng"
                    },
                    {
                        "name": "Shom Lin"
                    },
                    {
                        "name": "Renxi Wang"
                    },
                    {
                        "name": "Artem Shelmanov"
                    },
                    {
                        "name": "Xiangyu Qi"
                    },
                    {
                        "name": "Yuxia Wang"
                    },
                    {
                        "name": "Donghai Hong"
                    },
                    {
                        "name": "Youliang Yuan"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Haoqin Tu"
                    },
                    {
                        "name": "Fajri Koto"
                    },
                    {
                        "name": "Tatsuki Kuribayashi"
                    },
                    {
                        "name": "Cong Zeng"
                    },
                    {
                        "name": "Rishabh Bhardwaj"
                    },
                    {
                        "name": "Bingchen Zhao"
                    },
                    {
                        "name": "Yawen Duan"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Emad A. Alghamdi"
                    },
                    {
                        "name": "Yaodong Yang"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Soujanya Poria"
                    },
                    {
                        "name": "Pengfei Liu"
                    },
                    {
                        "name": "Zhengzhong Liu"
                    },
                    {
                        "name": "Xuguang Ren"
                    },
                    {
                        "name": "Eduard Hovy"
                    },
                    {
                        "name": "Iryna Gurevych"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Monojit Choudhury"
                    },
                    {
                        "name": "Timothy Baldwin"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Baldwin"
                },
                "author": "Timothy Baldwin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18547v1",
                "updated": "2024-12-24T16:55:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    55,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T16:55:45Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    55,
                    45,
                    1,
                    359,
                    0
                ],
                "title": "Token-Budget-Aware LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-Budget-Aware LLM Reasoning"
                },
                "summary": "Reasoning is critical for large language models (LLMs) to excel in a wide\nrange of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM\nperformance by decomposing problems into intermediate steps, they also incur\nsignificant overhead in token usage, leading to increased costs. We find that\nthe reasoning process of current LLMs is unnecessarily lengthy and it can be\ncompressed by including a reasonable token budget in the prompt, but the choice\nof token budget plays a crucial role in the actual compression effectiveness.\nWe then propose a token-budget-aware LLM reasoning framework, which dynamically\nestimates token budgets for different problems based on reasoning complexity\nand uses the estimated token budgets to guide the reasoning process.\nExperiments show that our method effectively reduces token costs in CoT\nreasoning with only a slight performance reduction, offering a practical\nsolution to balance efficiency and accuracy in LLM reasoning. Code:\nhttps://github.com/GeniusHTX/TALE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning is critical for large language models (LLMs) to excel in a wide\nrange of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM\nperformance by decomposing problems into intermediate steps, they also incur\nsignificant overhead in token usage, leading to increased costs. We find that\nthe reasoning process of current LLMs is unnecessarily lengthy and it can be\ncompressed by including a reasonable token budget in the prompt, but the choice\nof token budget plays a crucial role in the actual compression effectiveness.\nWe then propose a token-budget-aware LLM reasoning framework, which dynamically\nestimates token budgets for different problems based on reasoning complexity\nand uses the estimated token budgets to guide the reasoning process.\nExperiments show that our method effectively reduces token costs in CoT\nreasoning with only a slight performance reduction, offering a practical\nsolution to balance efficiency and accuracy in LLM reasoning. Code:\nhttps://github.com/GeniusHTX/TALE."
                },
                "authors": [
                    {
                        "name": "Tingxu Han"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Shiyu Zhao"
                    },
                    {
                        "name": "Shiqing Ma"
                    },
                    {
                        "name": "Zhenyu Chen"
                    },
                    {
                        "name": "Zhenting Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhenting Wang"
                },
                "author": "Zhenting Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18544v1",
                "updated": "2024-12-24T16:51:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    51,
                    35,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T16:51:35Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    51,
                    35,
                    1,
                    359,
                    0
                ],
                "title": "Consistency Checks for Language Model Forecasters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consistency Checks for Language Model Forecasters"
                },
                "summary": "Forecasting is a task that is difficult to evaluate: the ground truth can\nonly be known in the future. Recent work showing LLM forecasters rapidly\napproaching human-level performance begs the question: how can we benchmark and\nevaluate these forecasters instantaneously? Following the consistency check\nframework, we measure the performance of forecasters in terms of the\nconsistency of their predictions on different logically-related questions. We\npropose a new, general consistency metric based on arbitrage: for example, if a\nforecasting AI illogically predicts that both the Democratic and Republican\nparties have 60% probability of winning the 2024 US presidential election, an\narbitrageur can trade against the forecaster's predictions and make a profit.\nWe build an automated evaluation system that generates a set of base questions,\ninstantiates consistency checks from these questions, elicits the predictions\nof the forecaster, and measures the consistency of the predictions. We then\nbuild a standard, proper-scoring-rule forecasting benchmark, and show that our\n(instantaneous) consistency metrics correlate with LLM forecasters' ground\ntruth Brier scores (which are only known in the future). We also release a\nconsistency benchmark that resolves in 2028, providing a long-term evaluation\ntool for forecasting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting is a task that is difficult to evaluate: the ground truth can\nonly be known in the future. Recent work showing LLM forecasters rapidly\napproaching human-level performance begs the question: how can we benchmark and\nevaluate these forecasters instantaneously? Following the consistency check\nframework, we measure the performance of forecasters in terms of the\nconsistency of their predictions on different logically-related questions. We\npropose a new, general consistency metric based on arbitrage: for example, if a\nforecasting AI illogically predicts that both the Democratic and Republican\nparties have 60% probability of winning the 2024 US presidential election, an\narbitrageur can trade against the forecaster's predictions and make a profit.\nWe build an automated evaluation system that generates a set of base questions,\ninstantiates consistency checks from these questions, elicits the predictions\nof the forecaster, and measures the consistency of the predictions. We then\nbuild a standard, proper-scoring-rule forecasting benchmark, and show that our\n(instantaneous) consistency metrics correlate with LLM forecasters' ground\ntruth Brier scores (which are only known in the future). We also release a\nconsistency benchmark that resolves in 2028, providing a long-term evaluation\ntool for forecasting."
                },
                "authors": [
                    {
                        "name": "Daniel Paleka"
                    },
                    {
                        "name": "Abhimanyu Pallavi Sudhir"
                    },
                    {
                        "name": "Alejandro Alvarez"
                    },
                    {
                        "name": "Vineeth Bhat"
                    },
                    {
                        "name": "Adam Shen"
                    },
                    {
                        "name": "Evan Wang"
                    },
                    {
                        "name": "Florian TramÃ¨r"
                    }
                ],
                "author_detail": {
                    "name": "Florian TramÃ¨r"
                },
                "author": "Florian TramÃ¨r",
                "arxiv_comment": "56 pages, 25 figures. Submitted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18539v1",
                "updated": "2024-12-24T16:42:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    42,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T16:42:45Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    42,
                    45,
                    1,
                    359,
                    0
                ],
                "title": "Convergence of Statistical Estimators via Mutual Information Bounds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convergence of Statistical Estimators via Mutual Information Bounds"
                },
                "summary": "Recent advances in statistical learning theory have revealed profound\nconnections between mutual information (MI) bounds, PAC-Bayesian theory, and\nBayesian nonparametrics. This work introduces a novel mutual information bound\nfor statistical models. The derived bound has wide-ranging applications in\nstatistical inference. It yields improved contraction rates for fractional\nposteriors in Bayesian nonparametrics. It can also be used to study a wide\nrange of estimation methods, such as variational inference or Maximum\nLikelihood Estimation (MLE). By bridging these diverse areas, this work\nadvances our understanding of the fundamental limits of statistical inference\nand the role of information in learning from data. We hope that these results\nwill not only clarify connections between statistical inference and information\ntheory but also help to develop a new toolbox to study a wide range of\nestimators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in statistical learning theory have revealed profound\nconnections between mutual information (MI) bounds, PAC-Bayesian theory, and\nBayesian nonparametrics. This work introduces a novel mutual information bound\nfor statistical models. The derived bound has wide-ranging applications in\nstatistical inference. It yields improved contraction rates for fractional\nposteriors in Bayesian nonparametrics. It can also be used to study a wide\nrange of estimation methods, such as variational inference or Maximum\nLikelihood Estimation (MLE). By bridging these diverse areas, this work\nadvances our understanding of the fundamental limits of statistical inference\nand the role of information in learning from data. We hope that these results\nwill not only clarify connections between statistical inference and information\ntheory but also help to develop a new toolbox to study a wide range of\nestimators."
                },
                "authors": [
                    {
                        "name": "El Mahdi Khribch"
                    },
                    {
                        "name": "Pierre Alquier"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Alquier"
                },
                "author": "Pierre Alquier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12564v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12564v2",
                "updated": "2024-12-24T16:41:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    41,
                    40,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-17T05:48:48Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    48,
                    48,
                    1,
                    352,
                    0
                ],
                "title": "Evaluating Zero-Shot Multilingual Aspect-Based Sentiment Analysis with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Zero-Shot Multilingual Aspect-Based Sentiment Analysis with\n  Large Language Models"
                },
                "summary": "Aspect-based sentiment analysis (ABSA), a sequence labeling task, has\nattracted increasing attention in multilingual contexts. While previous\nresearch has focused largely on fine-tuning or training models specifically for\nABSA, we evaluate large language models (LLMs) under zero-shot conditions to\nexplore their potential to tackle this challenge with minimal task-specific\nadaptation. We conduct a comprehensive empirical evaluation of a series of LLMs\non multilingual ABSA tasks, investigating various prompting strategies,\nincluding vanilla zero-shot, chain-of-thought (CoT), self-improvement,\nself-debate, and self-consistency, across nine different models. Results\nindicate that while LLMs show promise in handling multilingual ABSA, they\ngenerally fall short of fine-tuned, task-specific models. Notably, simpler\nzero-shot prompts often outperform more complex strategies, especially in\nhigh-resource languages like English. These findings underscore the need for\nfurther refinement of LLM-based approaches to effectively address ABSA task\nacross diverse languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-based sentiment analysis (ABSA), a sequence labeling task, has\nattracted increasing attention in multilingual contexts. While previous\nresearch has focused largely on fine-tuning or training models specifically for\nABSA, we evaluate large language models (LLMs) under zero-shot conditions to\nexplore their potential to tackle this challenge with minimal task-specific\nadaptation. We conduct a comprehensive empirical evaluation of a series of LLMs\non multilingual ABSA tasks, investigating various prompting strategies,\nincluding vanilla zero-shot, chain-of-thought (CoT), self-improvement,\nself-debate, and self-consistency, across nine different models. Results\nindicate that while LLMs show promise in handling multilingual ABSA, they\ngenerally fall short of fine-tuned, task-specific models. Notably, simpler\nzero-shot prompts often outperform more complex strategies, especially in\nhigh-resource languages like English. These findings underscore the need for\nfurther refinement of LLM-based approaches to effectively address ABSA task\nacross diverse languages."
                },
                "authors": [
                    {
                        "name": "Chengyan Wu"
                    },
                    {
                        "name": "Bolei Ma"
                    },
                    {
                        "name": "Zheyu Zhang"
                    },
                    {
                        "name": "Ningyuan Deng"
                    },
                    {
                        "name": "Yanqing He"
                    },
                    {
                        "name": "Yun Xue"
                    }
                ],
                "author_detail": {
                    "name": "Yun Xue"
                },
                "author": "Yun Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12564v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12564v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18537v1",
                "updated": "2024-12-24T16:38:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    38,
                    4,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T16:38:04Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    38,
                    4,
                    1,
                    359,
                    0
                ],
                "title": "Harnessing Large Language Models for Knowledge Graph Question Answering\n  via Adaptive Multi-Aspect Retrieval-Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Large Language Models for Knowledge Graph Question Answering\n  via Adaptive Multi-Aspect Retrieval-Augmentation"
                },
                "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities, yet\nstruggle with hallucination and outdated knowledge when tasked with complex\nknowledge reasoning, resulting in factually incorrect outputs. Previous studies\nhave attempted to mitigate it by retrieving factual knowledge from large-scale\nknowledge graphs (KGs) to assist LLMs in logical reasoning and prediction of\nanswers. However, this kind of approach often introduces noise and irrelevant\ndata, especially in situations with extensive context from multiple knowledge\naspects. In this way, LLM attention can be potentially mislead from question\nand relevant information. In our study, we introduce an Adaptive Multi-Aspect\nRetrieval-augmented over KGs (Amar) framework. This method retrieves knowledge\nincluding entities, relations, and subgraphs, and converts each piece of\nretrieved text into prompt embeddings. The Amar framework comprises two key\nsub-components: 1) a self-alignment module that aligns commonalities among\nentities, relations, and subgraphs to enhance retrieved text, thereby reducing\nnoise interference; 2) a relevance gating module that employs a soft gate to\nlearn the relevance score between question and multi-aspect retrieved data, to\ndetermine which information should be used to enhance LLMs' output, or even\nfiltered altogether. Our method has achieved state-of-the-art performance on\ntwo common datasets, WebQSP and CWQ, showing a 1.9\\% improvement in accuracy\nover its best competitor and a 6.6\\% improvement in logical form generation\nover a method that directly uses retrieved text as context prompts. These\nresults demonstrate the effectiveness of Amar in improving the reasoning of\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate remarkable capabilities, yet\nstruggle with hallucination and outdated knowledge when tasked with complex\nknowledge reasoning, resulting in factually incorrect outputs. Previous studies\nhave attempted to mitigate it by retrieving factual knowledge from large-scale\nknowledge graphs (KGs) to assist LLMs in logical reasoning and prediction of\nanswers. However, this kind of approach often introduces noise and irrelevant\ndata, especially in situations with extensive context from multiple knowledge\naspects. In this way, LLM attention can be potentially mislead from question\nand relevant information. In our study, we introduce an Adaptive Multi-Aspect\nRetrieval-augmented over KGs (Amar) framework. This method retrieves knowledge\nincluding entities, relations, and subgraphs, and converts each piece of\nretrieved text into prompt embeddings. The Amar framework comprises two key\nsub-components: 1) a self-alignment module that aligns commonalities among\nentities, relations, and subgraphs to enhance retrieved text, thereby reducing\nnoise interference; 2) a relevance gating module that employs a soft gate to\nlearn the relevance score between question and multi-aspect retrieved data, to\ndetermine which information should be used to enhance LLMs' output, or even\nfiltered altogether. Our method has achieved state-of-the-art performance on\ntwo common datasets, WebQSP and CWQ, showing a 1.9\\% improvement in accuracy\nover its best competitor and a 6.6\\% improvement in logical form generation\nover a method that directly uses retrieved text as context prompts. These\nresults demonstrate the effectiveness of Amar in improving the reasoning of\nLLMs."
                },
                "authors": [
                    {
                        "name": "Derong Xu Xinhang Li"
                    },
                    {
                        "name": "Ziheng Zhang"
                    },
                    {
                        "name": "Zhenxi Lin"
                    },
                    {
                        "name": "Zhihong Zhu"
                    },
                    {
                        "name": "Zhi Zheng"
                    },
                    {
                        "name": "Xian Wu"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "arxiv_comment": "Accepted by AAAI'2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14909v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14909v2",
                "updated": "2024-12-24T16:25:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    25,
                    27,
                    1,
                    359,
                    0
                ],
                "published": "2024-08-27T09:35:49Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    35,
                    49,
                    1,
                    240,
                    0
                ],
                "title": "SpikingSSMs: Learning Long Sequences with Sparse and Parallel Spiking\n  State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpikingSSMs: Learning Long Sequences with Sparse and Parallel Spiking\n  State Space Models"
                },
                "summary": "Known as low energy consumption networks, spiking neural networks (SNNs) have\ngained a lot of attention within the past decades. While SNNs are increasing\ncompetitive with artificial neural networks (ANNs) for vision tasks, they are\nrarely used for long sequence tasks, despite their intrinsic temporal dynamics.\nIn this work, we develop spiking state space models (SpikingSSMs) for long\nsequence learning by leveraging on the sequence learning abilities of state\nspace models (SSMs). Inspired by dendritic neuron structure, we hierarchically\nintegrate neuronal dynamics with the original SSM block, meanwhile realizing\nsparse synaptic computation. Furthermore, to solve the conflict of event-driven\nneuronal dynamics with parallel computing, we propose a light-weight surrogate\ndynamic network which accurately predicts the after-reset membrane potential\nand compatible to learnable thresholds, enabling orders of acceleration in\ntraining speed compared with conventional iterative methods. On the long range\narena benchmark task, SpikingSSM achieves competitive performance to\nstate-of-the-art SSMs meanwhile realizing on average 90\\% of network sparsity.\nOn language modeling, our network significantly surpasses existing spiking\nlarge language models (spikingLLMs) on the WikiText-103 dataset with only a\nthird of the model size, demonstrating its potential as backbone architecture\nfor low computation cost LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Known as low energy consumption networks, spiking neural networks (SNNs) have\ngained a lot of attention within the past decades. While SNNs are increasing\ncompetitive with artificial neural networks (ANNs) for vision tasks, they are\nrarely used for long sequence tasks, despite their intrinsic temporal dynamics.\nIn this work, we develop spiking state space models (SpikingSSMs) for long\nsequence learning by leveraging on the sequence learning abilities of state\nspace models (SSMs). Inspired by dendritic neuron structure, we hierarchically\nintegrate neuronal dynamics with the original SSM block, meanwhile realizing\nsparse synaptic computation. Furthermore, to solve the conflict of event-driven\nneuronal dynamics with parallel computing, we propose a light-weight surrogate\ndynamic network which accurately predicts the after-reset membrane potential\nand compatible to learnable thresholds, enabling orders of acceleration in\ntraining speed compared with conventional iterative methods. On the long range\narena benchmark task, SpikingSSM achieves competitive performance to\nstate-of-the-art SSMs meanwhile realizing on average 90\\% of network sparsity.\nOn language modeling, our network significantly surpasses existing spiking\nlarge language models (spikingLLMs) on the WikiText-103 dataset with only a\nthird of the model size, demonstrating its potential as backbone architecture\nfor low computation cost LLMs."
                },
                "authors": [
                    {
                        "name": "Shuaijie Shen"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Renzhuo Huang"
                    },
                    {
                        "name": "Yan Zhong"
                    },
                    {
                        "name": "Qinghai Guo"
                    },
                    {
                        "name": "Zhichao Lu"
                    },
                    {
                        "name": "Jianguo Zhang"
                    },
                    {
                        "name": "Luziwei Leng"
                    }
                ],
                "author_detail": {
                    "name": "Luziwei Leng"
                },
                "author": "Luziwei Leng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14909v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18531v1",
                "updated": "2024-12-24T16:24:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    24,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T16:24:45Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    24,
                    45,
                    1,
                    359,
                    0
                ],
                "title": "Automated Code Review In Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Code Review In Practice"
                },
                "summary": "Code review is a widespread practice to improve software quality and transfer\nknowledge. It is often seen as time-consuming due to the need for manual effort\nand potential delays. Several AI-assisted tools, such as Qodo, GitHub Copilot,\nand Coderabbit, provide automated reviews using large language models (LLMs).\nThe effects of such tools in the industry are yet to be examined.\n  This study examines the impact of LLM-based automated code review tools in an\nindustrial setting. The study was conducted within a software development\nenvironment that adopted an AI-assisted review tool (based on open-source Qodo\nPR Agent). Around 238 practitioners across ten projects had access to the tool.\nWe focused on three projects with 4,335 pull requests, 1,568 of which underwent\nautomated reviews. Data collection comprised three sources: (1) a quantitative\nanalysis of pull request data, including comment labels indicating whether\ndevelopers acted on the automated comments, (2) surveys sent to developers\nregarding their experience with reviews on individual pull requests, and (3) a\nbroader survey of 22 practitioners capturing their general opinions on\nautomated reviews.\n  73.8% of automated comments were resolved. However, the average pull request\nclosure duration increased from five hours 52 minutes to eight hours 20\nminutes, with varying trends across projects. Most practitioners reported a\nminor improvement in code quality due to automated reviews.\n  The LLM-based tool proved useful in software development, enhancing bug\ndetection, increasing awareness of code quality, and promoting best practices.\nHowever, it also led to longer pull request closure times and introduced\ndrawbacks like faulty reviews, unnecessary corrections, and irrelevant\ncomments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code review is a widespread practice to improve software quality and transfer\nknowledge. It is often seen as time-consuming due to the need for manual effort\nand potential delays. Several AI-assisted tools, such as Qodo, GitHub Copilot,\nand Coderabbit, provide automated reviews using large language models (LLMs).\nThe effects of such tools in the industry are yet to be examined.\n  This study examines the impact of LLM-based automated code review tools in an\nindustrial setting. The study was conducted within a software development\nenvironment that adopted an AI-assisted review tool (based on open-source Qodo\nPR Agent). Around 238 practitioners across ten projects had access to the tool.\nWe focused on three projects with 4,335 pull requests, 1,568 of which underwent\nautomated reviews. Data collection comprised three sources: (1) a quantitative\nanalysis of pull request data, including comment labels indicating whether\ndevelopers acted on the automated comments, (2) surveys sent to developers\nregarding their experience with reviews on individual pull requests, and (3) a\nbroader survey of 22 practitioners capturing their general opinions on\nautomated reviews.\n  73.8% of automated comments were resolved. However, the average pull request\nclosure duration increased from five hours 52 minutes to eight hours 20\nminutes, with varying trends across projects. Most practitioners reported a\nminor improvement in code quality due to automated reviews.\n  The LLM-based tool proved useful in software development, enhancing bug\ndetection, increasing awareness of code quality, and promoting best practices.\nHowever, it also led to longer pull request closure times and introduced\ndrawbacks like faulty reviews, unnecessary corrections, and irrelevant\ncomments."
                },
                "authors": [
                    {
                        "name": "Umut Cihan"
                    },
                    {
                        "name": "Vahid Haratian"
                    },
                    {
                        "name": "Arda Ä°Ã§Ã¶z"
                    },
                    {
                        "name": "Mert Kaan GÃ¼l"
                    },
                    {
                        "name": "Ãmercan Devran"
                    },
                    {
                        "name": "Emircan Furkan Bayendur"
                    },
                    {
                        "name": "Baykal Mehmet UÃ§ar"
                    },
                    {
                        "name": "Eray TÃ¼zÃ¼n"
                    }
                ],
                "author_detail": {
                    "name": "Eray TÃ¼zÃ¼n"
                },
                "author": "Eray TÃ¼zÃ¼n",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17743v2",
                "updated": "2024-12-24T16:07:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    7,
                    47,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-23T17:47:53Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    47,
                    53,
                    0,
                    358,
                    0
                ],
                "title": "YuLan-Mini: An Open Data-efficient Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "YuLan-Mini: An Open Data-efficient Language Model"
                },
                "summary": "Effective pre-training of large language models (LLMs) has been challenging\ndue to the immense resource demands and the complexity of the technical\nprocesses involved. This paper presents a detailed technical report on\nYuLan-Mini, a highly capable base model with 2.42B parameters that achieves\ntop-tier performance among models of similar parameter scale. Our pre-training\napproach focuses on enhancing training efficacy through three key technical\ncontributions: an elaborate data pipeline combines data cleaning with data\nschedule strategies, a robust optimization method to mitigate training\ninstability, and an effective annealing approach that incorporates targeted\ndata selection and long context training. Remarkably, YuLan-Mini, trained on\n1.08T tokens, achieves performance comparable to industry-leading models that\nrequire significantly more data. To facilitate reproduction, we release the\nfull details of the data composition for each training phase. Project details\ncan be accessed at the following link: https://github.com/RUC-GSAI/YuLan-Mini.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective pre-training of large language models (LLMs) has been challenging\ndue to the immense resource demands and the complexity of the technical\nprocesses involved. This paper presents a detailed technical report on\nYuLan-Mini, a highly capable base model with 2.42B parameters that achieves\ntop-tier performance among models of similar parameter scale. Our pre-training\napproach focuses on enhancing training efficacy through three key technical\ncontributions: an elaborate data pipeline combines data cleaning with data\nschedule strategies, a robust optimization method to mitigate training\ninstability, and an effective annealing approach that incorporates targeted\ndata selection and long context training. Remarkably, YuLan-Mini, trained on\n1.08T tokens, achieves performance comparable to industry-leading models that\nrequire significantly more data. To facilitate reproduction, we release the\nfull details of the data composition for each training phase. Project details\ncan be accessed at the following link: https://github.com/RUC-GSAI/YuLan-Mini."
                },
                "authors": [
                    {
                        "name": "Yiwen Hu"
                    },
                    {
                        "name": "Huatong Song"
                    },
                    {
                        "name": "Jia Deng"
                    },
                    {
                        "name": "Jiapeng Wang"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Kun Zhou"
                    },
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Zican Dong"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18511v1",
                "updated": "2024-12-24T15:50:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    15,
                    50,
                    10,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T15:50:10Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    15,
                    50,
                    10,
                    1,
                    359,
                    0
                ],
                "title": "Large Language Model guided Deep Reinforcement Learning for Decision\n  Making in Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model guided Deep Reinforcement Learning for Decision\n  Making in Autonomous Driving"
                },
                "summary": "Deep reinforcement learning (DRL) shows promising potential for autonomous\ndriving decision-making. However, DRL demands extensive computational resources\nto achieve a qualified policy in complex driving scenarios due to its low\nlearning efficiency. Moreover, leveraging expert guidance from human to enhance\nDRL performance incurs prohibitively high labor costs, which limits its\npractical application. In this study, we propose a novel large language model\n(LLM) guided deep reinforcement learning (LGDRL) framework for addressing the\ndecision-making problem of autonomous vehicles. Within this framework, an\nLLM-based driving expert is integrated into the DRL to provide intelligent\nguidance for the learning process of DRL. Subsequently, in order to efficiently\nutilize the guidance of the LLM expert to enhance the performance of DRL\ndecision-making policies, the learning and interaction process of DRL is\nenhanced through an innovative expert policy constrained algorithm and a novel\nLLM-intervened interaction mechanism. Experimental results demonstrate that our\nmethod not only achieves superior driving performance with a 90\\% task success\nrate but also significantly improves the learning efficiency and expert\nguidance utilization efficiency compared to state-of-the-art baseline\nalgorithms. Moreover, the proposed method enables the DRL agent to maintain\nconsistent and reliable performance in the absence of LLM expert guidance. The\ncode and supplementary videos are available at\nhttps://bitmobility.github.io/LGDRL/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep reinforcement learning (DRL) shows promising potential for autonomous\ndriving decision-making. However, DRL demands extensive computational resources\nto achieve a qualified policy in complex driving scenarios due to its low\nlearning efficiency. Moreover, leveraging expert guidance from human to enhance\nDRL performance incurs prohibitively high labor costs, which limits its\npractical application. In this study, we propose a novel large language model\n(LLM) guided deep reinforcement learning (LGDRL) framework for addressing the\ndecision-making problem of autonomous vehicles. Within this framework, an\nLLM-based driving expert is integrated into the DRL to provide intelligent\nguidance for the learning process of DRL. Subsequently, in order to efficiently\nutilize the guidance of the LLM expert to enhance the performance of DRL\ndecision-making policies, the learning and interaction process of DRL is\nenhanced through an innovative expert policy constrained algorithm and a novel\nLLM-intervened interaction mechanism. Experimental results demonstrate that our\nmethod not only achieves superior driving performance with a 90\\% task success\nrate but also significantly improves the learning efficiency and expert\nguidance utilization efficiency compared to state-of-the-art baseline\nalgorithms. Moreover, the proposed method enables the DRL agent to maintain\nconsistent and reliable performance in the absence of LLM expert guidance. The\ncode and supplementary videos are available at\nhttps://bitmobility.github.io/LGDRL/."
                },
                "authors": [
                    {
                        "name": "Hao Pang"
                    },
                    {
                        "name": "Zhenpo Wang"
                    },
                    {
                        "name": "Guoqiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Guoqiang Li"
                },
                "author": "Guoqiang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18497v1",
                "updated": "2024-12-24T15:28:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    15,
                    28,
                    56,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T15:28:56Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    15,
                    28,
                    56,
                    1,
                    359,
                    0
                ],
                "title": "Think or Remember? Detecting and Directing LLMs Towards Memorization or\n  Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think or Remember? Detecting and Directing LLMs Towards Memorization or\n  Generalization"
                },
                "summary": "In this paper, we explore the foundational mechanisms of memorization and\ngeneralization in Large Language Models (LLMs), inspired by the functional\nspecialization observed in the human brain. Our investigation serves as a case\nstudy leveraging specially designed datasets and experimental-scale LLMs to lay\nthe groundwork for understanding these behaviors. Specifically, we aim to first\nenable LLMs to exhibit both memorization and generalization by training with\nthe designed dataset, then (a) examine whether LLMs exhibit neuron-level\nspatial differentiation for memorization and generalization, (b) predict these\nbehaviors using model internal representations, and (c) steer the behaviors\nthrough inference-time interventions. Our findings reveal that neuron-wise\ndifferentiation of memorization and generalization is observable in LLMs, and\ntargeted interventions can successfully direct their behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we explore the foundational mechanisms of memorization and\ngeneralization in Large Language Models (LLMs), inspired by the functional\nspecialization observed in the human brain. Our investigation serves as a case\nstudy leveraging specially designed datasets and experimental-scale LLMs to lay\nthe groundwork for understanding these behaviors. Specifically, we aim to first\nenable LLMs to exhibit both memorization and generalization by training with\nthe designed dataset, then (a) examine whether LLMs exhibit neuron-level\nspatial differentiation for memorization and generalization, (b) predict these\nbehaviors using model internal representations, and (c) steer the behaviors\nthrough inference-time interventions. Our findings reveal that neuron-wise\ndifferentiation of memorization and generalization is observable in LLMs, and\ntargeted interventions can successfully direct their behavior."
                },
                "authors": [
                    {
                        "name": "Yi-Fu Fu"
                    },
                    {
                        "name": "Yu-Chieh Tu"
                    },
                    {
                        "name": "Tzu-Ling Cheng"
                    },
                    {
                        "name": "Cheng-Yu Lin"
                    },
                    {
                        "name": "Yi-Ting Yang"
                    },
                    {
                        "name": "Heng-Yi Liu"
                    },
                    {
                        "name": "Keng-Te Liao"
                    },
                    {
                        "name": "Da-Cheng Juan"
                    },
                    {
                        "name": "Shou-De Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shou-De Lin"
                },
                "author": "Shou-De Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.19224v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.19224v2",
                "updated": "2024-12-24T15:28:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    15,
                    28,
                    50,
                    1,
                    359,
                    0
                ],
                "published": "2024-04-30T02:53:14Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    2,
                    53,
                    14,
                    1,
                    121,
                    0
                ],
                "title": "Computationally efficient variational-like approximations of\n  possibilistic inferential models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computationally efficient variational-like approximations of\n  possibilistic inferential models"
                },
                "summary": "Inferential models (IMs) offer provably reliable, data-driven, possibilistic\nstatistical inference. But despite IMs' theoretical and foundational\nadvantages, efficient computation is often a challenge. This paper presents a\nsimple and powerful numerical strategy for approximating the IM's possibility\ncontour, or at least its $\\alpha$-cut for a specified $\\alpha \\in (0,1)$. Our\nproposal starts with the specification a parametric family that, in a certain\nsense, approximately covers the credal set associated with the IM's possibility\nmeasure. Then the parameters of that parametric family are tuned in such a way\nthat the family's $100(1-\\alpha)\\%$ credible set roughly matches the IM\ncontour's $\\alpha$-cut. This is reminiscent of the variational approximations\nnow widely used in Bayesian statistics, hence the name variational-like IM\napproximation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferential models (IMs) offer provably reliable, data-driven, possibilistic\nstatistical inference. But despite IMs' theoretical and foundational\nadvantages, efficient computation is often a challenge. This paper presents a\nsimple and powerful numerical strategy for approximating the IM's possibility\ncontour, or at least its $\\alpha$-cut for a specified $\\alpha \\in (0,1)$. Our\nproposal starts with the specification a parametric family that, in a certain\nsense, approximately covers the credal set associated with the IM's possibility\nmeasure. Then the parameters of that parametric family are tuned in such a way\nthat the family's $100(1-\\alpha)\\%$ credible set roughly matches the IM\ncontour's $\\alpha$-cut. This is reminiscent of the variational approximations\nnow widely used in Bayesian statistics, hence the name variational-like IM\napproximation."
                },
                "authors": [
                    {
                        "name": "Leonardo Cella"
                    },
                    {
                        "name": "Ryan Martin"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Martin"
                },
                "author": "Ryan Martin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.19224v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.19224v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17737v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17737v2",
                "updated": "2024-12-24T15:24:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    15,
                    24,
                    32,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-23T17:36:51Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    36,
                    51,
                    0,
                    358,
                    0
                ],
                "title": "Contextual Backpropagation Loops: Amplifying Deep Reasoning with\n  Iterative Top-Down Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual Backpropagation Loops: Amplifying Deep Reasoning with\n  Iterative Top-Down Feedback"
                },
                "summary": "Deep neural networks typically rely on a single forward pass for inference,\nwhich can limit their capacity to resolve ambiguous inputs. We introduce\nContextual Backpropagation Loops (CBLs) as an iterative mechanism that\nincorporates top-down feedback to refine intermediate representations, thereby\nimproving accuracy and robustness. This repeated process mirrors how humans\ncontinuously re-interpret sensory information in daily life-by checking and\nre-checking our perceptions using contextual cues. Our results suggest that\nCBLs can offer a straightforward yet powerful way to incorporate such\ncontextual reasoning in modern deep learning architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks typically rely on a single forward pass for inference,\nwhich can limit their capacity to resolve ambiguous inputs. We introduce\nContextual Backpropagation Loops (CBLs) as an iterative mechanism that\nincorporates top-down feedback to refine intermediate representations, thereby\nimproving accuracy and robustness. This repeated process mirrors how humans\ncontinuously re-interpret sensory information in daily life-by checking and\nre-checking our perceptions using contextual cues. Our results suggest that\nCBLs can offer a straightforward yet powerful way to incorporate such\ncontextual reasoning in modern deep learning architectures."
                },
                "authors": [
                    {
                        "name": "Jacob Fein-Ashley"
                    }
                ],
                "author_detail": {
                    "name": "Jacob Fein-Ashley"
                },
                "author": "Jacob Fein-Ashley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17737v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17737v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18484v1",
                "updated": "2024-12-24T15:16:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    15,
                    16,
                    35,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T15:16:35Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    15,
                    16,
                    35,
                    1,
                    359,
                    0
                ],
                "title": "PrettiSmart: Visual Interpretation of Smart Contracts via Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrettiSmart: Visual Interpretation of Smart Contracts via Simulation"
                },
                "summary": "Smart contracts are the fundamental components of blockchain technology. They\nare programs to determine cryptocurrency transactions, and are irreversible\nonce deployed, making it crucial for cryptocurrency investors to understand the\ncryptocurrency transaction behaviors of smart contracts comprehensively.\nHowever, it is a challenging (if not impossible) task for investors, as they do\nnot necessarily have a programming background to check the complex source code.\nEven for investors with certain programming skills, inferring all the potential\nbehaviors from the code alone is still difficult, since the actual behaviors\ncan be different when different investors are involved. To address this\nchallenge, we propose PrettiSmart, a novel visualization approach via execution\nsimulation to achieve intuitive and reliable visual interpretation of smart\ncontracts. Specifically, we develop a simulator to comprehensively capture most\nof the possible real-world smart contract behaviors, involving multiple\ninvestors and various smart contract functions. Then, we present PrettiSmart to\nintuitively visualize the simulation results of a smart contract, which\nconsists of two modules: The Simulation Overview Module is a barcode-based\ndesign, providing a visual summary for each simulation, and the Simulation\nDetail Module is an augmented sequential design to display the cryptocurrency\ntransaction details in each simulation, such as function call sequences,\ncryptocurrency flows, and state variable changes. It can allow investors to\nintuitively inspect and understand how a smart contract will work. We evaluate\nPrettiSmart through two case studies and in-depth user interviews with 12\ninvestors. The results demonstrate the effectiveness and usability of\nPrettiSmart in facilitating an easy interpretation of smart contracts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart contracts are the fundamental components of blockchain technology. They\nare programs to determine cryptocurrency transactions, and are irreversible\nonce deployed, making it crucial for cryptocurrency investors to understand the\ncryptocurrency transaction behaviors of smart contracts comprehensively.\nHowever, it is a challenging (if not impossible) task for investors, as they do\nnot necessarily have a programming background to check the complex source code.\nEven for investors with certain programming skills, inferring all the potential\nbehaviors from the code alone is still difficult, since the actual behaviors\ncan be different when different investors are involved. To address this\nchallenge, we propose PrettiSmart, a novel visualization approach via execution\nsimulation to achieve intuitive and reliable visual interpretation of smart\ncontracts. Specifically, we develop a simulator to comprehensively capture most\nof the possible real-world smart contract behaviors, involving multiple\ninvestors and various smart contract functions. Then, we present PrettiSmart to\nintuitively visualize the simulation results of a smart contract, which\nconsists of two modules: The Simulation Overview Module is a barcode-based\ndesign, providing a visual summary for each simulation, and the Simulation\nDetail Module is an augmented sequential design to display the cryptocurrency\ntransaction details in each simulation, such as function call sequences,\ncryptocurrency flows, and state variable changes. It can allow investors to\nintuitively inspect and understand how a smart contract will work. We evaluate\nPrettiSmart through two case studies and in-depth user interviews with 12\ninvestors. The results demonstrate the effectiveness and usability of\nPrettiSmart in facilitating an easy interpretation of smart contracts."
                },
                "authors": [
                    {
                        "name": "Xiaolin Wen"
                    },
                    {
                        "name": "Tai D. Nguyen"
                    },
                    {
                        "name": "Lun Zhang"
                    },
                    {
                        "name": "Jun Sun"
                    },
                    {
                        "name": "Yong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wang"
                },
                "author": "Yong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04220v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04220v4",
                "updated": "2024-12-24T15:08:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    15,
                    8,
                    40,
                    1,
                    359,
                    0
                ],
                "published": "2024-06-06T16:18:30Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    16,
                    18,
                    30,
                    3,
                    158,
                    0
                ],
                "title": "BEADs: Bias Evaluation Across Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEADs: Bias Evaluation Across Domains"
                },
                "summary": "Recent advancements in large language models (LLMs) have greatly enhanced\nnatural language processing (NLP) applications. Nevertheless, these models\noften inherit biases from their training data. Despite the availability of\nvarious datasets for bias detection, most are limited to one or two NLP tasks\n(typically classification or evaluation) and lack comprehensive evaluations\nacross a broader range of NLP tasks. To address this gap, we introduce the Bias\nEvaluations Across Domains BEADs dataset, designed to support a wide array of\nNLP tasks, including text classification, token classification, bias\nquantification, and benign language generation. A key focus of this paper is\nthe gold label dataset that is annotated by GPT4 for scalabilty and verified by\nexperts to ensure high reliability. BEADs provides data for both fine-tuning,\nincluding classification and language generation tasks, and for evaluating\nLLMs. Our findings indicate that BEADs effectively identifies numerous biases\nwhen fine-tuned on this dataset. It also reduces biases when used for\nfine-tuning language generation task, while preserving language quality. The\nresults also reveal some prevalent demographic biases in LLMs when BEADs is\nused for evaluation in demographic task. We provide the BEADs dataset for\ndetecting biases in various domains, and this dataset is readily usable for\nresponsible AI development and application. The dataset can be accessed at\nhttps://huggingface.co/datasets/shainar/BEAD .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have greatly enhanced\nnatural language processing (NLP) applications. Nevertheless, these models\noften inherit biases from their training data. Despite the availability of\nvarious datasets for bias detection, most are limited to one or two NLP tasks\n(typically classification or evaluation) and lack comprehensive evaluations\nacross a broader range of NLP tasks. To address this gap, we introduce the Bias\nEvaluations Across Domains BEADs dataset, designed to support a wide array of\nNLP tasks, including text classification, token classification, bias\nquantification, and benign language generation. A key focus of this paper is\nthe gold label dataset that is annotated by GPT4 for scalabilty and verified by\nexperts to ensure high reliability. BEADs provides data for both fine-tuning,\nincluding classification and language generation tasks, and for evaluating\nLLMs. Our findings indicate that BEADs effectively identifies numerous biases\nwhen fine-tuned on this dataset. It also reduces biases when used for\nfine-tuning language generation task, while preserving language quality. The\nresults also reveal some prevalent demographic biases in LLMs when BEADs is\nused for evaluation in demographic task. We provide the BEADs dataset for\ndetecting biases in various domains, and this dataset is readily usable for\nresponsible AI development and application. The dataset can be accessed at\nhttps://huggingface.co/datasets/shainar/BEAD ."
                },
                "authors": [
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Mizanur Rahman"
                    },
                    {
                        "name": "Michael R. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Zhang"
                },
                "author": "Michael R. Zhang",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04220v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04220v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11006v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11006v3",
                "updated": "2024-12-24T15:04:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    15,
                    4,
                    50,
                    1,
                    359,
                    0
                ],
                "published": "2024-08-20T17:00:04Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    0,
                    4,
                    1,
                    233,
                    0
                ],
                "title": "Security Attacks on LLM-based Code Completion Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security Attacks on LLM-based Code Completion Tools"
                },
                "summary": "The rapid development of large language models (LLMs) has significantly\nadvanced code completion capabilities, giving rise to a new generation of\nLLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, these\ntools possess unique workflows, integrating multiple information sources as\ninput and prioritizing code suggestions over natural language interaction,\nwhich introduces distinct security challenges. Additionally, LCCTs often rely\non proprietary code datasets for training, raising concerns about the potential\nexposure of sensitive data. This paper exploits these distinct characteristics\nof LCCTs to develop targeted attack methodologies on two critical security\nrisks: jailbreaking and training data extraction attacks. Our experimental\nresults expose significant vulnerabilities within LCCTs, including a 99.4%\nsuccess rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rate\non Amazon Q. Furthermore, We successfully extracted sensitive user data from\nGitHub Copilot, including 54 real email addresses and 314 physical addresses\nassociated with GitHub usernames. Our study also demonstrates that these\ncode-based attack methods are effective against general-purpose LLMs, such as\nthe GPT series, highlighting a broader security misalignment in the handling of\ncode by modern LLMs. These findings underscore critical security challenges\nassociated with LCCTs and suggest essential directions for strengthening their\nsecurity frameworks. The example code and attack samples from our research are\nprovided at https://github.com/Sensente/Security-Attacks-on-LCCTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) has significantly\nadvanced code completion capabilities, giving rise to a new generation of\nLLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, these\ntools possess unique workflows, integrating multiple information sources as\ninput and prioritizing code suggestions over natural language interaction,\nwhich introduces distinct security challenges. Additionally, LCCTs often rely\non proprietary code datasets for training, raising concerns about the potential\nexposure of sensitive data. This paper exploits these distinct characteristics\nof LCCTs to develop targeted attack methodologies on two critical security\nrisks: jailbreaking and training data extraction attacks. Our experimental\nresults expose significant vulnerabilities within LCCTs, including a 99.4%\nsuccess rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rate\non Amazon Q. Furthermore, We successfully extracted sensitive user data from\nGitHub Copilot, including 54 real email addresses and 314 physical addresses\nassociated with GitHub usernames. Our study also demonstrates that these\ncode-based attack methods are effective against general-purpose LLMs, such as\nthe GPT series, highlighting a broader security misalignment in the handling of\ncode by modern LLMs. These findings underscore critical security challenges\nassociated with LCCTs and suggest essential directions for strengthening their\nsecurity frameworks. The example code and attack samples from our research are\nprovided at https://github.com/Sensente/Security-Attacks-on-LCCTs."
                },
                "authors": [
                    {
                        "name": "Wen Cheng"
                    },
                    {
                        "name": "Ke Sun"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "arxiv_comment": "Paper accepted at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11006v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11006v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06736v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06736v3",
                "updated": "2024-12-24T14:44:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    14,
                    44,
                    32,
                    1,
                    359,
                    0
                ],
                "published": "2024-11-11T06:04:53Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    6,
                    4,
                    53,
                    0,
                    316,
                    0
                ],
                "title": "MrSteve: Instruction-Following Agents in Minecraft with What-Where-When\n  Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MrSteve: Instruction-Following Agents in Minecraft with What-Where-When\n  Memory"
                },
                "summary": "Significant advances have been made in developing general-purpose embodied AI\nin environments like Minecraft through the adoption of LLM-augmented\nhierarchical approaches. While these approaches, which combine high-level\nplanners with low-level controllers, show promise, low-level controllers\nfrequently become performance bottlenecks due to repeated failures. In this\npaper, we argue that the primary cause of failure in many low-level controllers\nis the absence of an episodic memory system. To address this, we introduce\nMrSteve (Memory Recall Steve-1), a novel low-level controller equipped with\nPlace Event Memory (PEM), a form of episodic memory that captures what, where,\nand when information from episodes. This directly addresses the main limitation\nof the popular low-level controller, Steve-1. Unlike previous models that rely\non short-term memory, PEM organizes spatial and event-based data, enabling\nefficient recall and navigation in long-horizon tasks. Additionally, we propose\nan Exploration Strategy and a Memory-Augmented Task Solving Framework, allowing\nagents to alternate between exploration and task-solving based on recalled\nevents. Our approach significantly improves task-solving and exploration\nefficiency compared to existing methods. We will release our code and demos on\nthe project page: https://sites.google.com/view/mr-steve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant advances have been made in developing general-purpose embodied AI\nin environments like Minecraft through the adoption of LLM-augmented\nhierarchical approaches. While these approaches, which combine high-level\nplanners with low-level controllers, show promise, low-level controllers\nfrequently become performance bottlenecks due to repeated failures. In this\npaper, we argue that the primary cause of failure in many low-level controllers\nis the absence of an episodic memory system. To address this, we introduce\nMrSteve (Memory Recall Steve-1), a novel low-level controller equipped with\nPlace Event Memory (PEM), a form of episodic memory that captures what, where,\nand when information from episodes. This directly addresses the main limitation\nof the popular low-level controller, Steve-1. Unlike previous models that rely\non short-term memory, PEM organizes spatial and event-based data, enabling\nefficient recall and navigation in long-horizon tasks. Additionally, we propose\nan Exploration Strategy and a Memory-Augmented Task Solving Framework, allowing\nagents to alternate between exploration and task-solving based on recalled\nevents. Our approach significantly improves task-solving and exploration\nefficiency compared to existing methods. We will release our code and demos on\nthe project page: https://sites.google.com/view/mr-steve."
                },
                "authors": [
                    {
                        "name": "Junyeong Park"
                    },
                    {
                        "name": "Junmo Cho"
                    },
                    {
                        "name": "Sungjin Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Sungjin Ahn"
                },
                "author": "Sungjin Ahn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06736v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06736v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18118v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18118v4",
                "updated": "2024-12-24T14:26:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    14,
                    26,
                    36,
                    1,
                    359,
                    0
                ],
                "published": "2024-06-26T07:15:44Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    7,
                    15,
                    44,
                    2,
                    178,
                    0
                ],
                "title": "SafeAligner: Safety Alignment against Jailbreak Attacks via Response\n  Disparity Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeAligner: Safety Alignment against Jailbreak Attacks via Response\n  Disparity Guidance"
                },
                "summary": "As the development of large language models (LLMs) rapidly advances, securing\nthese models effectively without compromising their utility has become a\npivotal area of research. However, current defense strategies against jailbreak\nattacks (i.e., efforts to bypass security protocols) often suffer from limited\nadaptability, restricted general capability, and high cost. To address these\nchallenges, we introduce SafeAligner, a methodology implemented at the decoding\nstage to fortify defenses against jailbreak attacks. We begin by developing two\nspecialized models: the Sentinel Model, which is trained to foster safety, and\nthe Intruder Model, designed to generate riskier responses. SafeAligner\nleverages the disparity in security levels between the responses from these\nmodels to differentiate between harmful and beneficial tokens, effectively\nguiding the safety alignment by altering the output token distribution of the\ntarget model. Extensive experiments show that SafeAligner can increase the\nlikelihood of beneficial tokens, while reducing the occurrence of harmful ones,\nthereby ensuring secure alignment with minimal loss to generality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the development of large language models (LLMs) rapidly advances, securing\nthese models effectively without compromising their utility has become a\npivotal area of research. However, current defense strategies against jailbreak\nattacks (i.e., efforts to bypass security protocols) often suffer from limited\nadaptability, restricted general capability, and high cost. To address these\nchallenges, we introduce SafeAligner, a methodology implemented at the decoding\nstage to fortify defenses against jailbreak attacks. We begin by developing two\nspecialized models: the Sentinel Model, which is trained to foster safety, and\nthe Intruder Model, designed to generate riskier responses. SafeAligner\nleverages the disparity in security levels between the responses from these\nmodels to differentiate between harmful and beneficial tokens, effectively\nguiding the safety alignment by altering the output token distribution of the\ntarget model. Extensive experiments show that SafeAligner can increase the\nlikelihood of beneficial tokens, while reducing the occurrence of harmful ones,\nthereby ensuring secure alignment with minimal loss to generality."
                },
                "authors": [
                    {
                        "name": "Caishuang Huang"
                    },
                    {
                        "name": "Wanxu Zhao"
                    },
                    {
                        "name": "Rui Zheng"
                    },
                    {
                        "name": "Huijie Lv"
                    },
                    {
                        "name": "Wenyu Zhan"
                    },
                    {
                        "name": "Shihan Dou"
                    },
                    {
                        "name": "Sixian Li"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Enyu Zhou"
                    },
                    {
                        "name": "Junjie Ye"
                    },
                    {
                        "name": "Yuming Yang"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18118v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18118v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18450v1",
                "updated": "2024-12-24T14:21:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    14,
                    21,
                    58,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T14:21:58Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    14,
                    21,
                    58,
                    1,
                    359,
                    0
                ],
                "title": "3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D\n  Scene Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D\n  Scene Understanding"
                },
                "summary": "A 3D scene graph represents a compact scene model, storing information about\nthe objects and the semantic relationships between them, making its use\npromising for robotic tasks. When interacting with a user, an embodied\nintelligent agent should be capable of responding to various queries about the\nscene formulated in natural language. Large Language Models (LLMs) are\nbeneficial solutions for user-robot interaction due to their natural language\nunderstanding and reasoning abilities. Recent methods for creating learnable\nrepresentations of 3D scenes have demonstrated the potential to improve the\nquality of LLMs responses by adapting to the 3D world. However, the existing\nmethods do not explicitly utilize information about the semantic relationships\nbetween objects, limiting themselves to information about their coordinates. In\nthis work, we propose a method 3DGraphLLM for constructing a learnable\nrepresentation of a 3D scene graph. The learnable representation is used as\ninput for LLMs to perform 3D vision-language tasks. In our experiments on\npopular ScanRefer, RIORefer, Multi3DRefer, ScanQA, Sqa3D, and Scan2cap\ndatasets, we demonstrate the advantage of this approach over baseline methods\nthat do not use information about the semantic relationships between objects.\nThe code is publicly available at\nhttps://github.com/CognitiveAISystems/3DGraphLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A 3D scene graph represents a compact scene model, storing information about\nthe objects and the semantic relationships between them, making its use\npromising for robotic tasks. When interacting with a user, an embodied\nintelligent agent should be capable of responding to various queries about the\nscene formulated in natural language. Large Language Models (LLMs) are\nbeneficial solutions for user-robot interaction due to their natural language\nunderstanding and reasoning abilities. Recent methods for creating learnable\nrepresentations of 3D scenes have demonstrated the potential to improve the\nquality of LLMs responses by adapting to the 3D world. However, the existing\nmethods do not explicitly utilize information about the semantic relationships\nbetween objects, limiting themselves to information about their coordinates. In\nthis work, we propose a method 3DGraphLLM for constructing a learnable\nrepresentation of a 3D scene graph. The learnable representation is used as\ninput for LLMs to perform 3D vision-language tasks. In our experiments on\npopular ScanRefer, RIORefer, Multi3DRefer, ScanQA, Sqa3D, and Scan2cap\ndatasets, we demonstrate the advantage of this approach over baseline methods\nthat do not use information about the semantic relationships between objects.\nThe code is publicly available at\nhttps://github.com/CognitiveAISystems/3DGraphLLM."
                },
                "authors": [
                    {
                        "name": "Tatiana Zemskova"
                    },
                    {
                        "name": "Dmitry Yudin"
                    }
                ],
                "author_detail": {
                    "name": "Dmitry Yudin"
                },
                "author": "Dmitry Yudin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18443v1",
                "updated": "2024-12-24T14:03:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    14,
                    3,
                    7,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T14:03:07Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    14,
                    3,
                    7,
                    1,
                    359,
                    0
                ],
                "title": "Is Large Language Model Good at Triple Set Prediction? An Empirical\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Large Language Model Good at Triple Set Prediction? An Empirical\n  Study"
                },
                "summary": "The core of the Knowledge Graph Completion (KGC) task is to predict and\ncomplete the missing relations or nodes in a KG. Common KGC tasks are mostly\nabout inferring unknown elements with one or two elements being known in a\ntriple. In comparison, the Triple Set Prediction (TSP) task is a more realistic\nknowledge graph completion task. It aims to predict all elements of unknown\ntriples based on the information from known triples. In recent years, large\nlanguage models (LLMs) have exhibited significant advancements in language\ncomprehension, demonstrating considerable potential for KGC tasks. However, the\npotential of LLM on the TSP task has not yet to be investigated. Thus in this\npaper we proposed a new framework to explore the strengths and limitations of\nLLM in the TSP task. Specifically, the framework consists of LLM-based rule\nmining and LLM-based triple set prediction. The relation list of KG embedded\nwithin rich semantic information is first leveraged to prompt LLM in the\ngeneration of rules. This process is both efficient and independent of\nstatistical information, making it easier to mine effective and realistic\nrules. For each subgraph, the specified rule is applied in conjunction with the\nrelevant triples within that subgraph to guide the LLM in predicting the\nmissing triples. Subsequently, the predictions from all subgraphs are\nconsolidated to derive the complete set of predicted triples on KG. Finally,\nthe method is evaluated on the relatively complete CFamily dataset. The\nexperimental results indicate that when LLMs are required to adhere to a large\namount of factual knowledge to predict missing triples, significant\nhallucinations occurs, leading to a noticeable decline in performance. To\nfurther explore the causes of this phenomenon, this paper presents a\ncomprehensive analysis supported by a detailed case study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The core of the Knowledge Graph Completion (KGC) task is to predict and\ncomplete the missing relations or nodes in a KG. Common KGC tasks are mostly\nabout inferring unknown elements with one or two elements being known in a\ntriple. In comparison, the Triple Set Prediction (TSP) task is a more realistic\nknowledge graph completion task. It aims to predict all elements of unknown\ntriples based on the information from known triples. In recent years, large\nlanguage models (LLMs) have exhibited significant advancements in language\ncomprehension, demonstrating considerable potential for KGC tasks. However, the\npotential of LLM on the TSP task has not yet to be investigated. Thus in this\npaper we proposed a new framework to explore the strengths and limitations of\nLLM in the TSP task. Specifically, the framework consists of LLM-based rule\nmining and LLM-based triple set prediction. The relation list of KG embedded\nwithin rich semantic information is first leveraged to prompt LLM in the\ngeneration of rules. This process is both efficient and independent of\nstatistical information, making it easier to mine effective and realistic\nrules. For each subgraph, the specified rule is applied in conjunction with the\nrelevant triples within that subgraph to guide the LLM in predicting the\nmissing triples. Subsequently, the predictions from all subgraphs are\nconsolidated to derive the complete set of predicted triples on KG. Finally,\nthe method is evaluated on the relatively complete CFamily dataset. The\nexperimental results indicate that when LLMs are required to adhere to a large\namount of factual knowledge to predict missing triples, significant\nhallucinations occurs, leading to a noticeable decline in performance. To\nfurther explore the causes of this phenomenon, this paper presents a\ncomprehensive analysis supported by a detailed case study."
                },
                "authors": [
                    {
                        "name": "Yuan Yuan"
                    },
                    {
                        "name": "Yajing Xu"
                    },
                    {
                        "name": "Wen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wen Zhang"
                },
                "author": "Wen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18428v1",
                "updated": "2024-12-24T13:42:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    42,
                    44,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T13:42:44Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    42,
                    44,
                    1,
                    359,
                    0
                ],
                "title": "Explainable Multi-Modal Data Exploration in Natural Language via LLM\n  Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable Multi-Modal Data Exploration in Natural Language via LLM\n  Agent"
                },
                "summary": "International enterprises, organizations, or hospitals collect large amounts\nof multi-modal data stored in databases, text documents, images, and videos.\nWhile there has been recent progress in the separate fields of multi-modal data\nexploration as well as in database systems that automatically translate natural\nlanguage questions to database query languages, the research challenge of\nquerying database systems combined with other unstructured modalities such as\nimages in natural language is widely unexplored.\n  In this paper, we propose XMODE - a system that enables explainable,\nmulti-modal data exploration in natural language. Our approach is based on the\nfollowing research contributions: (1) Our system is inspired by a real-world\nuse case that enables users to explore multi-modal information systems. (2)\nXMODE leverages a LLM-based agentic AI framework to decompose a natural\nlanguage question into subtasks such as text-to-SQL generation and image\nanalysis. (3) Experimental results on multi-modal datasets over relational data\nand images demonstrate that our system outperforms state-of-the-art multi-modal\nexploration systems, excelling not only in accuracy but also in various\nperformance metrics such as query latency, API costs, planning efficiency, and\nexplanation quality, thanks to the more effective utilization of the reasoning\ncapabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "International enterprises, organizations, or hospitals collect large amounts\nof multi-modal data stored in databases, text documents, images, and videos.\nWhile there has been recent progress in the separate fields of multi-modal data\nexploration as well as in database systems that automatically translate natural\nlanguage questions to database query languages, the research challenge of\nquerying database systems combined with other unstructured modalities such as\nimages in natural language is widely unexplored.\n  In this paper, we propose XMODE - a system that enables explainable,\nmulti-modal data exploration in natural language. Our approach is based on the\nfollowing research contributions: (1) Our system is inspired by a real-world\nuse case that enables users to explore multi-modal information systems. (2)\nXMODE leverages a LLM-based agentic AI framework to decompose a natural\nlanguage question into subtasks such as text-to-SQL generation and image\nanalysis. (3) Experimental results on multi-modal datasets over relational data\nand images demonstrate that our system outperforms state-of-the-art multi-modal\nexploration systems, excelling not only in accuracy but also in various\nperformance metrics such as query latency, API costs, planning efficiency, and\nexplanation quality, thanks to the more effective utilization of the reasoning\ncapabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Farhad Nooralahzadeh"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Jonathan Furst"
                    },
                    {
                        "name": "Kurt Stockinger"
                    }
                ],
                "author_detail": {
                    "name": "Kurt Stockinger"
                },
                "author": "Kurt Stockinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06504v2",
                "updated": "2024-12-24T13:41:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    41,
                    8,
                    1,
                    359,
                    0
                ],
                "published": "2024-03-11T08:25:53Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    8,
                    25,
                    53,
                    0,
                    71,
                    0
                ],
                "title": "LoHan: Low-Cost High-Performance Framework to Fine-Tune 100B Model on a\n  Consumer GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoHan: Low-Cost High-Performance Framework to Fine-Tune 100B Model on a\n  Consumer GPU"
                },
                "summary": "Nowadays, AI researchers become more and more interested in fine-tuning a\npre-trained LLM, whose size has grown to up to over 100B parameters, for their\ndownstream tasks. One approach to fine-tune such huge models is to aggregate\ndevice memory from many GPUs. However, this approach introduces prohibitive\ncosts for most data scientists with a limited budget for high-end GPU servers.\nIn this paper, we focus on LLM fine-tuning on a single consumer-grade GPU in a\ncommodity server with limited main memory capacity, which is accessible to most\nAI researchers. In such a scenario, existing offloading-based methods fail to\nfine-tune an LLM efficiently due to a lack of holistic intra-server tensor\nmovement management. To this end, we present LoHan, a low-cost,\nhigh-performance deep learning training framework that enables efficient\n100B-scale model fine-tuning on a commodity server with a consumer-grade GPU\nand limited main memory capacity. The key idea is to add holistic offloading\ntraffic as an optimization dimension for 1)active gradient offloading, and\n2)holistic traffic-aware activation swapping mechanism. The experimental\nresults show that 1)LoHan is the first to fine-tune a 175B model on an RTX 4090\nand 256 GB main memory, 2)LoHan achieves 2.32x throughput than the\nstate-of-the-art baselines when fine-tuning a small 13B model, and 3)LoHan\nenables a cheap low-end consumer GPU to have higher cost-effectiveness than a\nDGX-A100 cluster when fine-tuning a 175B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, AI researchers become more and more interested in fine-tuning a\npre-trained LLM, whose size has grown to up to over 100B parameters, for their\ndownstream tasks. One approach to fine-tune such huge models is to aggregate\ndevice memory from many GPUs. However, this approach introduces prohibitive\ncosts for most data scientists with a limited budget for high-end GPU servers.\nIn this paper, we focus on LLM fine-tuning on a single consumer-grade GPU in a\ncommodity server with limited main memory capacity, which is accessible to most\nAI researchers. In such a scenario, existing offloading-based methods fail to\nfine-tune an LLM efficiently due to a lack of holistic intra-server tensor\nmovement management. To this end, we present LoHan, a low-cost,\nhigh-performance deep learning training framework that enables efficient\n100B-scale model fine-tuning on a commodity server with a consumer-grade GPU\nand limited main memory capacity. The key idea is to add holistic offloading\ntraffic as an optimization dimension for 1)active gradient offloading, and\n2)holistic traffic-aware activation swapping mechanism. The experimental\nresults show that 1)LoHan is the first to fine-tune a 175B model on an RTX 4090\nand 256 GB main memory, 2)LoHan achieves 2.32x throughput than the\nstate-of-the-art baselines when fine-tuning a small 13B model, and 3)LoHan\nenables a cheap low-end consumer GPU to have higher cost-effectiveness than a\nDGX-A100 cluster when fine-tuning a 175B model."
                },
                "authors": [
                    {
                        "name": "Changyue Liao"
                    },
                    {
                        "name": "Mo Sun"
                    },
                    {
                        "name": "Zihan Yang"
                    },
                    {
                        "name": "Jun Xie"
                    },
                    {
                        "name": "Kaiqi Chen"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Fei Wu"
                    },
                    {
                        "name": "Zeke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zeke Wang"
                },
                "author": "Zeke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.08309v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.08309v3",
                "updated": "2024-12-24T13:37:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    37,
                    49,
                    1,
                    359,
                    0
                ],
                "published": "2024-02-13T09:12:55Z",
                "published_parsed": [
                    2024,
                    2,
                    13,
                    9,
                    12,
                    55,
                    1,
                    44,
                    0
                ],
                "title": "Prompted Contextual Vectors for Spear-Phishing Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompted Contextual Vectors for Spear-Phishing Detection"
                },
                "summary": "Spear-phishing attacks present a significant security challenge, with large\nlanguage models (LLMs) escalating the threat by generating convincing emails\nand facilitating target reconnaissance. To address this, we propose a detection\napproach based on a novel document vectorization method that utilizes an\nensemble of LLMs to create representation vectors. By prompting LLMs to reason\nand respond to human-crafted questions, we quantify the presence of common\npersuasion principles in the email's content, producing prompted contextual\ndocument vectors for a downstream supervised machine learning model. We\nevaluate our method using a unique dataset generated by a proprietary system\nthat automates target reconnaissance and spear-phishing email creation. Our\nmethod achieves a 91\\% F1 score in identifying LLM-generated spear-phishing\nemails, with the training set comprising only traditional phishing and benign\nemails. Key contributions include a novel document vectorization method\nutilizing LLM reasoning, a publicly available dataset of high-quality\nspear-phishing emails, and the demonstrated effectiveness of our method in\ndetecting such emails. This methodology can be utilized for various document\nclassification tasks, particularly in adversarial problem domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spear-phishing attacks present a significant security challenge, with large\nlanguage models (LLMs) escalating the threat by generating convincing emails\nand facilitating target reconnaissance. To address this, we propose a detection\napproach based on a novel document vectorization method that utilizes an\nensemble of LLMs to create representation vectors. By prompting LLMs to reason\nand respond to human-crafted questions, we quantify the presence of common\npersuasion principles in the email's content, producing prompted contextual\ndocument vectors for a downstream supervised machine learning model. We\nevaluate our method using a unique dataset generated by a proprietary system\nthat automates target reconnaissance and spear-phishing email creation. Our\nmethod achieves a 91\\% F1 score in identifying LLM-generated spear-phishing\nemails, with the training set comprising only traditional phishing and benign\nemails. Key contributions include a novel document vectorization method\nutilizing LLM reasoning, a publicly available dataset of high-quality\nspear-phishing emails, and the demonstrated effectiveness of our method in\ndetecting such emails. This methodology can be utilized for various document\nclassification tasks, particularly in adversarial problem domains."
                },
                "authors": [
                    {
                        "name": "Daniel Nahmias"
                    },
                    {
                        "name": "Gal Engelberg"
                    },
                    {
                        "name": "Dan Klein"
                    },
                    {
                        "name": "Asaf Shabtai"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Shabtai"
                },
                "author": "Asaf Shabtai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.08309v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.08309v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18419v1",
                "updated": "2024-12-24T13:24:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    24,
                    1,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T13:24:01Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    24,
                    1,
                    1,
                    359,
                    0
                ],
                "title": "Research on the Proximity Relationships of Psychosomatic Disease\n  Knowledge Graph Modules Extracted by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on the Proximity Relationships of Psychosomatic Disease\n  Knowledge Graph Modules Extracted by Large Language Models"
                },
                "summary": "As social changes accelerate, the incidence of psychosomatic disorders has\nsignificantly increased, becoming a major challenge in global health issues.\nThis necessitates an innovative knowledge system and analytical methods to aid\nin diagnosis and treatment. Here, we establish the ontology model and entity\ntypes, using the BERT model and LoRA-tuned LLM for named entity recognition,\nconstructing the knowledge graph with 9668 triples. Next, by analyzing the\nnetwork distances between disease, symptom, and drug modules, it was found that\ncloser network distances among diseases can predict greater similarities in\ntheir clinical manifestations, treatment approaches, and psychological\nmechanisms, and closer distances between symptoms indicate that they are more\nlikely to co-occur. Lastly, by comparing the proximity d and proximity z score,\nit was shown that symptom-disease pairs in primary diagnostic relationships\nhave a stronger association and are of higher referential value than those in\ndiagnostic relationships. The research results revealed the potential\nconnections between diseases, co-occurring symptoms, and similarities in\ntreatment strategies, providing new perspectives for the diagnosis and\ntreatment of psychosomatic disorders and valuable information for future mental\nhealth research and practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As social changes accelerate, the incidence of psychosomatic disorders has\nsignificantly increased, becoming a major challenge in global health issues.\nThis necessitates an innovative knowledge system and analytical methods to aid\nin diagnosis and treatment. Here, we establish the ontology model and entity\ntypes, using the BERT model and LoRA-tuned LLM for named entity recognition,\nconstructing the knowledge graph with 9668 triples. Next, by analyzing the\nnetwork distances between disease, symptom, and drug modules, it was found that\ncloser network distances among diseases can predict greater similarities in\ntheir clinical manifestations, treatment approaches, and psychological\nmechanisms, and closer distances between symptoms indicate that they are more\nlikely to co-occur. Lastly, by comparing the proximity d and proximity z score,\nit was shown that symptom-disease pairs in primary diagnostic relationships\nhave a stronger association and are of higher referential value than those in\ndiagnostic relationships. The research results revealed the potential\nconnections between diseases, co-occurring symptoms, and similarities in\ntreatment strategies, providing new perspectives for the diagnosis and\ntreatment of psychosomatic disorders and valuable information for future mental\nhealth research and practice."
                },
                "authors": [
                    {
                        "name": "Zihan Zhou"
                    },
                    {
                        "name": "Ziyi Zeng"
                    },
                    {
                        "name": "Wenhao Jiang"
                    },
                    {
                        "name": "Yihui Zhu"
                    },
                    {
                        "name": "Jiaxin Mao"
                    },
                    {
                        "name": "Yonggui Yuan"
                    },
                    {
                        "name": "Min Xia"
                    },
                    {
                        "name": "Shubin Zhao"
                    },
                    {
                        "name": "Mengyu Yao"
                    },
                    {
                        "name": "Yunqian Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yunqian Chen"
                },
                "author": "Yunqian Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18416v1",
                "updated": "2024-12-24T13:08:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    8,
                    34,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T13:08:34Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    8,
                    34,
                    1,
                    359,
                    0
                ],
                "title": "Muse: A Multimodal Conversational Recommendation Dataset with\n  Scenario-Grounded User Profiles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Muse: A Multimodal Conversational Recommendation Dataset with\n  Scenario-Grounded User Profiles"
                },
                "summary": "Current conversational recommendation systems focus predominantly on text.\nHowever, real-world recommendation settings are generally multimodal, causing a\nsignificant gap between existing research and practical applications. To\naddress this issue, we propose Muse, the first multimodal conversational\nrecommendation dataset. Muse comprises 83,148 utterances from 7,000\nconversations centered around the Clothing domain. Each conversation contains\ncomprehensive multimodal interactions, rich elements, and natural dialogues.\nData in Muse are automatically synthesized by a multi-agent framework powered\nby multimodal large language models (MLLMs). It innovatively derives user\nprofiles from real-world scenarios rather than depending on manual design and\nhistory data for better scalability, and then it fulfills conversation\nsimulation and optimization. Both human and LLM evaluations demonstrate the\nhigh quality of conversations in Muse. Additionally, fine-tuning experiments on\nthree MLLMs demonstrate Muse's learnable patterns for recommendations and\nresponses, confirming its value for multimodal conversational recommendation.\nOur dataset and codes are available at\n\\url{https://anonymous.4open.science/r/Muse-0086}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current conversational recommendation systems focus predominantly on text.\nHowever, real-world recommendation settings are generally multimodal, causing a\nsignificant gap between existing research and practical applications. To\naddress this issue, we propose Muse, the first multimodal conversational\nrecommendation dataset. Muse comprises 83,148 utterances from 7,000\nconversations centered around the Clothing domain. Each conversation contains\ncomprehensive multimodal interactions, rich elements, and natural dialogues.\nData in Muse are automatically synthesized by a multi-agent framework powered\nby multimodal large language models (MLLMs). It innovatively derives user\nprofiles from real-world scenarios rather than depending on manual design and\nhistory data for better scalability, and then it fulfills conversation\nsimulation and optimization. Both human and LLM evaluations demonstrate the\nhigh quality of conversations in Muse. Additionally, fine-tuning experiments on\nthree MLLMs demonstrate Muse's learnable patterns for recommendations and\nresponses, confirming its value for multimodal conversational recommendation.\nOur dataset and codes are available at\n\\url{https://anonymous.4open.science/r/Muse-0086}."
                },
                "authors": [
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Xiaocui Yang"
                    },
                    {
                        "name": "Yongkang Liu"
                    },
                    {
                        "name": "Shi Feng"
                    },
                    {
                        "name": "Daling Wang"
                    },
                    {
                        "name": "Yifei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yifei Zhang"
                },
                "author": "Yifei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18415v1",
                "updated": "2024-12-24T13:07:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    7,
                    29,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T13:07:29Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    7,
                    29,
                    1,
                    359,
                    0
                ],
                "title": "Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi\n  and English",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi\n  and English"
                },
                "summary": "Large Language Models (LLMs) excel in linguistic tasks but struggle with\nmathematical reasoning, particularly in non English languages like Hindi. This\nresearch aims to enhance the mathematical reasoning skills of smaller, resource\nefficient open-source LLMs in both Hindi and English. We evaluate models like\nOpenHathi 7B, LLaMA-2 7B, WizardMath 7B, Mistral 7B, LLeMMa 7B, MAmmoTH 7B,\nGemini Pro, and GPT-4 using zero-shot, few-shot chain-of-thought (CoT) methods,\nand supervised fine-tuning. Our approach incorporates curriculum learning,\nprogressively training models on increasingly difficult problems, a novel\nDecomposition Strategy to simplify complex arithmetic operations, and a\nStructured Solution Design that divides solutions into phases. Our experiments\nresult in notable performance enhancements. WizardMath 7B exceeds Gemini's\naccuracy on English datasets by +6% and matches Gemini's performance on Hindi\ndatasets. Adopting a bilingual approach that combines English and Hindi samples\nachieves results comparable to individual language models, demonstrating the\ncapability to learn mathematical reasoning in both languages. This research\nhighlights the potential for improving mathematical reasoning in open-source\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in linguistic tasks but struggle with\nmathematical reasoning, particularly in non English languages like Hindi. This\nresearch aims to enhance the mathematical reasoning skills of smaller, resource\nefficient open-source LLMs in both Hindi and English. We evaluate models like\nOpenHathi 7B, LLaMA-2 7B, WizardMath 7B, Mistral 7B, LLeMMa 7B, MAmmoTH 7B,\nGemini Pro, and GPT-4 using zero-shot, few-shot chain-of-thought (CoT) methods,\nand supervised fine-tuning. Our approach incorporates curriculum learning,\nprogressively training models on increasingly difficult problems, a novel\nDecomposition Strategy to simplify complex arithmetic operations, and a\nStructured Solution Design that divides solutions into phases. Our experiments\nresult in notable performance enhancements. WizardMath 7B exceeds Gemini's\naccuracy on English datasets by +6% and matches Gemini's performance on Hindi\ndatasets. Adopting a bilingual approach that combines English and Hindi samples\nachieves results comparable to individual language models, demonstrating the\ncapability to learn mathematical reasoning in both languages. This research\nhighlights the potential for improving mathematical reasoning in open-source\nLLMs."
                },
                "authors": [
                    {
                        "name": "Avinash Anand"
                    },
                    {
                        "name": "Kritarth Prasad"
                    },
                    {
                        "name": "Chhavi Kirtani"
                    },
                    {
                        "name": "Ashwin R Nair"
                    },
                    {
                        "name": "Manvendra Kumar Nema"
                    },
                    {
                        "name": "Raj Jaiswal"
                    },
                    {
                        "name": "Rajiv Ratn Shah"
                    }
                ],
                "author_detail": {
                    "name": "Rajiv Ratn Shah"
                },
                "author": "Rajiv Ratn Shah",
                "arxiv_comment": "Accepted at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16187v2",
                "updated": "2024-12-24T13:04:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    4,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-13T06:00:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    0,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing"
                },
                "summary": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks."
                },
                "authors": [
                    {
                        "name": "Minghui Liu"
                    },
                    {
                        "name": "Tahseen Rabbani"
                    },
                    {
                        "name": "Tony O'Halloran"
                    },
                    {
                        "name": "Ananth Sankaralingam"
                    },
                    {
                        "name": "Mary-Anne Hartley"
                    },
                    {
                        "name": "Brian Gravelle"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Cornelia FermÃ¼ller"
                    },
                    {
                        "name": "Yiannis Aloimonos"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Aloimonos"
                },
                "author": "Yiannis Aloimonos",
                "arxiv_comment": "10 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18407v1",
                "updated": "2024-12-24T12:54:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    12,
                    54,
                    19,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T12:54:19Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    12,
                    54,
                    19,
                    1,
                    359,
                    0
                ],
                "title": "A Statistical Framework for Ranking LLM-Based Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Statistical Framework for Ranking LLM-Based Chatbots"
                },
                "summary": "Large language models (LLMs) have transformed natural language processing,\nwith frameworks like Chatbot Arena providing pioneering platforms for\nevaluating these models. By facilitating millions of pairwise comparisons based\non human judgments, Chatbot Arena has become a cornerstone in LLM evaluation,\noffering rich datasets for ranking models in open-ended conversational tasks.\nBuilding upon this foundation, we propose a statistical framework that\nincorporates key advancements to address specific challenges in pairwise\ncomparison analysis. First, we introduce a factored tie model that enhances the\nability to handle ties -- an integral aspect of human-judged comparisons --\nsignificantly improving the model's fit to observed data. Second, we extend the\nframework to model covariance between competitors, enabling deeper insights\ninto performance relationships and facilitating intuitive groupings into\nperformance tiers. Third, we resolve optimization challenges arising from\nparameter non-uniqueness by introducing novel constraints, ensuring stable and\ninterpretable parameter estimation. Through rigorous evaluation and extensive\nexperimentation, our framework demonstrates substantial improvements over\nexisting methods in modeling pairwise comparison data. To support\nreproducibility and practical adoption, we release leaderbot, an open-source\nPython package implementing our models and analyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have transformed natural language processing,\nwith frameworks like Chatbot Arena providing pioneering platforms for\nevaluating these models. By facilitating millions of pairwise comparisons based\non human judgments, Chatbot Arena has become a cornerstone in LLM evaluation,\noffering rich datasets for ranking models in open-ended conversational tasks.\nBuilding upon this foundation, we propose a statistical framework that\nincorporates key advancements to address specific challenges in pairwise\ncomparison analysis. First, we introduce a factored tie model that enhances the\nability to handle ties -- an integral aspect of human-judged comparisons --\nsignificantly improving the model's fit to observed data. Second, we extend the\nframework to model covariance between competitors, enabling deeper insights\ninto performance relationships and facilitating intuitive groupings into\nperformance tiers. Third, we resolve optimization challenges arising from\nparameter non-uniqueness by introducing novel constraints, ensuring stable and\ninterpretable parameter estimation. Through rigorous evaluation and extensive\nexperimentation, our framework demonstrates substantial improvements over\nexisting methods in modeling pairwise comparison data. To support\nreproducibility and practical adoption, we release leaderbot, an open-source\nPython package implementing our models and analyses."
                },
                "authors": [
                    {
                        "name": "Siavash Ameli"
                    },
                    {
                        "name": "Siyuan Zhuang"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    }
                ],
                "author_detail": {
                    "name": "Michael W. Mahoney"
                },
                "author": "Michael W. Mahoney",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18390v1",
                "updated": "2024-12-24T12:28:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    12,
                    28,
                    19,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T12:28:19Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    12,
                    28,
                    19,
                    1,
                    359,
                    0
                ],
                "title": "RDPM: Solve Diffusion Probabilistic Models via Recurrent Token\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RDPM: Solve Diffusion Probabilistic Models via Recurrent Token\n  Prediction"
                },
                "summary": "Diffusion Probabilistic Models (DPMs) have emerged as the de facto approach\nfor high-fidelity image synthesis, operating diffusion processes on continuous\nVAE latent, which significantly differ from the text generation methods\nemployed by Large Language Models (LLMs). In this paper, we introduce a novel\ngenerative framework, the Recurrent Diffusion Probabilistic Model (RDPM), which\nenhances the diffusion process through a recurrent token prediction mechanism,\nthereby pioneering the field of Discrete Diffusion. By progressively\nintroducing Gaussian noise into the latent representations of images and\nencoding them into vector-quantized tokens in a recurrent manner, RDPM\nfacilitates a unique diffusion process on discrete-value domains. This process\niteratively predicts the token codes for subsequent timesteps, transforming the\ninitial standard Gaussian noise into the source data distribution, aligning\nwith GPT-style models in terms of the loss function. RDPM demonstrates superior\nperformance while benefiting from the speed advantage of requiring only a few\ninference steps. This model not only leverages the diffusion process to ensure\nhigh-quality generation but also converts continuous signals into a series of\nhigh-fidelity discrete tokens, thereby maintaining a unified optimization\nstrategy with other discrete tokens, such as text. We anticipate that this work\nwill contribute to the development of a unified model for multimodal\ngeneration, specifically by integrating continuous signal domains such as\nimages, videos, and audio with text. We will release the code and model weights\nto the open-source community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Probabilistic Models (DPMs) have emerged as the de facto approach\nfor high-fidelity image synthesis, operating diffusion processes on continuous\nVAE latent, which significantly differ from the text generation methods\nemployed by Large Language Models (LLMs). In this paper, we introduce a novel\ngenerative framework, the Recurrent Diffusion Probabilistic Model (RDPM), which\nenhances the diffusion process through a recurrent token prediction mechanism,\nthereby pioneering the field of Discrete Diffusion. By progressively\nintroducing Gaussian noise into the latent representations of images and\nencoding them into vector-quantized tokens in a recurrent manner, RDPM\nfacilitates a unique diffusion process on discrete-value domains. This process\niteratively predicts the token codes for subsequent timesteps, transforming the\ninitial standard Gaussian noise into the source data distribution, aligning\nwith GPT-style models in terms of the loss function. RDPM demonstrates superior\nperformance while benefiting from the speed advantage of requiring only a few\ninference steps. This model not only leverages the diffusion process to ensure\nhigh-quality generation but also converts continuous signals into a series of\nhigh-fidelity discrete tokens, thereby maintaining a unified optimization\nstrategy with other discrete tokens, such as text. We anticipate that this work\nwill contribute to the development of a unified model for multimodal\ngeneration, specifically by integrating continuous signal domains such as\nimages, videos, and audio with text. We will release the code and model weights\nto the open-source community."
                },
                "authors": [
                    {
                        "name": "Wu Xiaoping"
                    },
                    {
                        "name": "Hu Jie"
                    },
                    {
                        "name": "Wei Xiaoming"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xiaoming"
                },
                "author": "Wei Xiaoming",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18377v1",
                "updated": "2024-12-24T12:03:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    12,
                    3,
                    36,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T12:03:36Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    12,
                    3,
                    36,
                    1,
                    359,
                    0
                ],
                "title": "ChaI-TeA: A Benchmark for Evaluating Autocompletion of Interactions with\n  LLM-based Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChaI-TeA: A Benchmark for Evaluating Autocompletion of Interactions with\n  LLM-based Chatbots"
                },
                "summary": "The rise of LLMs has deflected a growing portion of human-computer\ninteractions towards LLM-based chatbots. The remarkable abilities of these\nmodels allow users to interact using long, diverse natural language text\ncovering a wide range of topics and styles. Phrasing these messages is a time\nand effort consuming task, calling for an autocomplete solution to assist\nusers. We introduce the task of chatbot interaction autocomplete. We present\nChaI-TeA: CHat InTEraction Autocomplete; An autcomplete evaluation framework\nfor LLM-based chatbot interactions. The framework includes a formal definition\nof the task, coupled with suitable datasets and metrics. We use the framework\nto evaluate After formally defining the task along with suitable datasets and\nmetrics, we test 9 models on the defined auto completion task, finding that\nwhile current off-the-shelf models perform fairly, there is still much room for\nimprovement, mainly in ranking of the generated suggestions. We provide\ninsights for practitioners working on this task and open new research\ndirections for researchers in the field. We release our framework to serve as a\nfoundation for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of LLMs has deflected a growing portion of human-computer\ninteractions towards LLM-based chatbots. The remarkable abilities of these\nmodels allow users to interact using long, diverse natural language text\ncovering a wide range of topics and styles. Phrasing these messages is a time\nand effort consuming task, calling for an autocomplete solution to assist\nusers. We introduce the task of chatbot interaction autocomplete. We present\nChaI-TeA: CHat InTEraction Autocomplete; An autcomplete evaluation framework\nfor LLM-based chatbot interactions. The framework includes a formal definition\nof the task, coupled with suitable datasets and metrics. We use the framework\nto evaluate After formally defining the task along with suitable datasets and\nmetrics, we test 9 models on the defined auto completion task, finding that\nwhile current off-the-shelf models perform fairly, there is still much room for\nimprovement, mainly in ranking of the generated suggestions. We provide\ninsights for practitioners working on this task and open new research\ndirections for researchers in the field. We release our framework to serve as a\nfoundation for future research."
                },
                "authors": [
                    {
                        "name": "Shani Goren"
                    },
                    {
                        "name": "Oren Kalinsky"
                    },
                    {
                        "name": "Tomer Stav"
                    },
                    {
                        "name": "Yuri Rapoport"
                    },
                    {
                        "name": "Yaron Fairstein"
                    },
                    {
                        "name": "Ram Yazdy"
                    },
                    {
                        "name": "Nachshon Cohen"
                    },
                    {
                        "name": "Alexander Libov"
                    },
                    {
                        "name": "Guy Kushilevitz"
                    }
                ],
                "author_detail": {
                    "name": "Guy Kushilevitz"
                },
                "author": "Guy Kushilevitz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18371v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18371v1",
                "updated": "2024-12-24T11:54:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    11,
                    54,
                    14,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T11:54:14Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    11,
                    54,
                    14,
                    1,
                    359,
                    0
                ],
                "title": "Defining and Detecting the Defects of the Large Language Model-based\n  Autonomous Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defining and Detecting the Defects of the Large Language Model-based\n  Autonomous Agents"
                },
                "summary": "AI agents are systems capable of perceiving their environment, autonomously\nplanning and executing tasks. Recent advancements in LLM have introduced a\ntransformative paradigm for AI agents, enabling them to interact with external\nresources and tools through prompts. In such agents, the workflow integrates\ndeveloper-written code, which manages framework construction and logic control,\nwith LLM-generated natural language that enhances dynamic decision-making and\ninteraction. However, discrepancies between developer-implemented logic and the\ndynamically generated content of LLMs in terms of behavior and expected\noutcomes can lead to defects, such as tool invocation failures and task\nexecution errors. These issues introduce specific risks, leading to various\ndefects in LLM-based AI Agents, such as service interruptions. Despite the\nimportance of these issues, there is a lack of systematic work that focuses on\nanalyzing LLM-based AI Agents to uncover defects in their code. In this paper,\nwe present the first study focused on identifying and detecting defects in LLM\nAgents. We collected and analyzed 6,854 relevant posts from StackOverflow to\ndefine 8 types of agent defects. For each type, we provided detailed\ndescriptions with an example. Then, we designed a static analysis tool, named\nAgentable, to detect the defects. Agentable leverages Code Property Graphs and\nLLMs to analyze Agent workflows by efficiently identifying specific code\npatterns and analyzing natural language descriptions. To evaluate Agentable, we\nconstructed two datasets: AgentSet, consists of 84 real-world Agents, and\nAgentTest, which contains 78 Agents specifically designed to include various\ntypes of defects. Our results show that Agentable achieved an overall accuracy\nof 88.79% and a recall rate of 91.03%. Furthermore, our analysis reveals the\n889 defects of the AgentSet, highlighting the prevalence of these defects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents are systems capable of perceiving their environment, autonomously\nplanning and executing tasks. Recent advancements in LLM have introduced a\ntransformative paradigm for AI agents, enabling them to interact with external\nresources and tools through prompts. In such agents, the workflow integrates\ndeveloper-written code, which manages framework construction and logic control,\nwith LLM-generated natural language that enhances dynamic decision-making and\ninteraction. However, discrepancies between developer-implemented logic and the\ndynamically generated content of LLMs in terms of behavior and expected\noutcomes can lead to defects, such as tool invocation failures and task\nexecution errors. These issues introduce specific risks, leading to various\ndefects in LLM-based AI Agents, such as service interruptions. Despite the\nimportance of these issues, there is a lack of systematic work that focuses on\nanalyzing LLM-based AI Agents to uncover defects in their code. In this paper,\nwe present the first study focused on identifying and detecting defects in LLM\nAgents. We collected and analyzed 6,854 relevant posts from StackOverflow to\ndefine 8 types of agent defects. For each type, we provided detailed\ndescriptions with an example. Then, we designed a static analysis tool, named\nAgentable, to detect the defects. Agentable leverages Code Property Graphs and\nLLMs to analyze Agent workflows by efficiently identifying specific code\npatterns and analyzing natural language descriptions. To evaluate Agentable, we\nconstructed two datasets: AgentSet, consists of 84 real-world Agents, and\nAgentTest, which contains 78 Agents specifically designed to include various\ntypes of defects. Our results show that Agentable achieved an overall accuracy\nof 88.79% and a recall rate of 91.03%. Furthermore, our analysis reveals the\n889 defects of the AgentSet, highlighting the prevalence of these defects."
                },
                "authors": [
                    {
                        "name": "Kaiwen Ning"
                    },
                    {
                        "name": "Jiachi Chen"
                    },
                    {
                        "name": "Jingwen Zhang"
                    },
                    {
                        "name": "Wei Lia"
                    },
                    {
                        "name": "Zexu Wang"
                    },
                    {
                        "name": "Yuming Feng"
                    },
                    {
                        "name": "Weizhe Zhang"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18371v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18371v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18367v1",
                "updated": "2024-12-24T11:50:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    11,
                    50,
                    18,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T11:50:18Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    11,
                    50,
                    18,
                    1,
                    359,
                    0
                ],
                "title": "Towards Global AI Inclusivity: A Large-Scale Multilingual Terminology\n  Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Global AI Inclusivity: A Large-Scale Multilingual Terminology\n  Dataset"
                },
                "summary": "The field of machine translation has achieved significant advancements, yet\ndomain-specific terminology translation, particularly in AI, remains\nchallenging. We introduced GIST, a large-scale multilingual AI terminology\ndataset containing 5K terms extracted from top AI conference papers spanning\n2000 to 2023. The terms were translated into Arabic, Chinese, French, Japanese,\nand Russian using a hybrid framework that combines LLMs for extraction with\nhuman expertise for translation. The dataset's quality was benchmarked against\nexisting resources, demonstrating superior translation accuracy through\ncrowdsourced evaluation. GIST was integrated into translation workflows using\npost-translation refinement methods that required no retraining, where LLM\nprompting consistently improved BLEU and COMET scores. A web demonstration on\nthe ACL Anthology platform highlights its practical application, showcasing\nimproved accessibility for non-English speakers. This work aims to address\ncritical gaps in AI terminology resources and fosters global inclusivity and\ncollaboration in AI research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of machine translation has achieved significant advancements, yet\ndomain-specific terminology translation, particularly in AI, remains\nchallenging. We introduced GIST, a large-scale multilingual AI terminology\ndataset containing 5K terms extracted from top AI conference papers spanning\n2000 to 2023. The terms were translated into Arabic, Chinese, French, Japanese,\nand Russian using a hybrid framework that combines LLMs for extraction with\nhuman expertise for translation. The dataset's quality was benchmarked against\nexisting resources, demonstrating superior translation accuracy through\ncrowdsourced evaluation. GIST was integrated into translation workflows using\npost-translation refinement methods that required no retraining, where LLM\nprompting consistently improved BLEU and COMET scores. A web demonstration on\nthe ACL Anthology platform highlights its practical application, showcasing\nimproved accessibility for non-English speakers. This work aims to address\ncritical gaps in AI terminology resources and fosters global inclusivity and\ncollaboration in AI research."
                },
                "authors": [
                    {
                        "name": "Jiarui Liu"
                    },
                    {
                        "name": "Iman Ouzzani"
                    },
                    {
                        "name": "Wenkai Li"
                    },
                    {
                        "name": "Lechen Zhang"
                    },
                    {
                        "name": "Tianyue Ou"
                    },
                    {
                        "name": "Houda Bouamor"
                    },
                    {
                        "name": "Zhijing Jin"
                    },
                    {
                        "name": "Mona Diab"
                    }
                ],
                "author_detail": {
                    "name": "Mona Diab"
                },
                "author": "Mona Diab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16771v2",
                "updated": "2024-12-24T11:46:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    11,
                    46,
                    23,
                    1,
                    359,
                    0
                ],
                "published": "2024-05-27T02:42:33Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    2,
                    42,
                    33,
                    0,
                    148,
                    0
                ],
                "title": "ARC: A Generalist Graph Anomaly Detector with In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARC: A Generalist Graph Anomaly Detector with In-Context Learning"
                },
                "summary": "Graph anomaly detection (GAD), which aims to identify abnormal nodes that\ndiffer from the majority within a graph, has garnered significant attention.\nHowever, current GAD methods necessitate training specific to each dataset,\nresulting in high training costs, substantial data requirements, and limited\ngeneralizability when being applied to new datasets and domains. To address\nthese limitations, this paper proposes ARC, a generalist GAD approach that\nenables a ``one-for-all'' GAD model to detect anomalies across various graph\ndatasets on-the-fly. Equipped with in-context learning, ARC can directly\nextract dataset-specific patterns from the target dataset using few-shot normal\nsamples at the inference stage, without the need for retraining or fine-tuning\non the target dataset. ARC comprises three components that are well-crafted for\ncapturing universal graph anomaly patterns: 1) smoothness-based feature\nAlignment module that unifies the features of different datasets into a common\nand anomaly-sensitive space; 2) ego-neighbor Residual graph encoder that learns\nabnormality-related node embeddings; and 3) cross-attentive in-Context anomaly\nscoring module that predicts node abnormality by leveraging few-shot normal\nsamples. Extensive experiments on multiple benchmark datasets from various\ndomains demonstrate the superior anomaly detection performance, efficiency, and\ngeneralizability of ARC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph anomaly detection (GAD), which aims to identify abnormal nodes that\ndiffer from the majority within a graph, has garnered significant attention.\nHowever, current GAD methods necessitate training specific to each dataset,\nresulting in high training costs, substantial data requirements, and limited\ngeneralizability when being applied to new datasets and domains. To address\nthese limitations, this paper proposes ARC, a generalist GAD approach that\nenables a ``one-for-all'' GAD model to detect anomalies across various graph\ndatasets on-the-fly. Equipped with in-context learning, ARC can directly\nextract dataset-specific patterns from the target dataset using few-shot normal\nsamples at the inference stage, without the need for retraining or fine-tuning\non the target dataset. ARC comprises three components that are well-crafted for\ncapturing universal graph anomaly patterns: 1) smoothness-based feature\nAlignment module that unifies the features of different datasets into a common\nand anomaly-sensitive space; 2) ego-neighbor Residual graph encoder that learns\nabnormality-related node embeddings; and 3) cross-attentive in-Context anomaly\nscoring module that predicts node abnormality by leveraging few-shot normal\nsamples. Extensive experiments on multiple benchmark datasets from various\ndomains demonstrate the superior anomaly detection performance, efficiency, and\ngeneralizability of ARC."
                },
                "authors": [
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Shiyuan Li"
                    },
                    {
                        "name": "Yu Zheng"
                    },
                    {
                        "name": "Qingfeng Chen"
                    },
                    {
                        "name": "Chengqi Zhang"
                    },
                    {
                        "name": "Shirui Pan"
                    }
                ],
                "author_detail": {
                    "name": "Shirui Pan"
                },
                "author": "Shirui Pan",
                "arxiv_comment": "25 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16964v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16964v2",
                "updated": "2024-12-24T11:43:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    11,
                    43,
                    25,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-22T10:49:27Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    10,
                    49,
                    27,
                    6,
                    357,
                    0
                ],
                "title": "System-2 Mathematical Reasoning via Enriched Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "System-2 Mathematical Reasoning via Enriched Instruction Tuning"
                },
                "summary": "Solving complex mathematical problems via system-2 reasoning is a natural\nhuman skill, yet it remains a significant challenge for current large language\nmodels (LLMs). We identify the scarcity of deliberate multi-step reasoning data\nas a primary limiting factor. To this end, we introduce Enriched Instruction\nTuning (EIT), a method that enriches existing human-annotated mathematical\ndatasets by synergizing human and AI feedback to create fine-grained reasoning\ntrajectories. These datasets are then used to fine-tune open-source LLMs,\nenhancing their mathematical reasoning abilities without reliance on any\nsymbolic verification program. Concretely, EIT is composed of two critical\nsteps: Enriching with Reasoning Plan (ERP) and Enriching with Reasoning Step\n(ERS). The former generates a high-level plan that breaks down complex\ninstructions into a sequence of simpler objectives, while ERS fills in\nreasoning contexts often overlooked by human annotators, creating a smoother\nreasoning trajectory for LLM fine-tuning. Unlike existing CoT prompting methods\nthat generate reasoning chains only depending on LLM's internal knowledge, our\nmethod leverages human-annotated initial answers as ``meta-knowledge'' to help\nLLMs generate more detailed and precise reasoning processes, leading to a more\ntrustworthy LLM expert for complex mathematical problems. In experiments, EIT\nachieves an accuracy of 84.1% on GSM8K and 32.5% on MATH, surpassing\nstate-of-the-art fine-tuning and prompting methods, and even matching the\nperformance of tool-augmented methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving complex mathematical problems via system-2 reasoning is a natural\nhuman skill, yet it remains a significant challenge for current large language\nmodels (LLMs). We identify the scarcity of deliberate multi-step reasoning data\nas a primary limiting factor. To this end, we introduce Enriched Instruction\nTuning (EIT), a method that enriches existing human-annotated mathematical\ndatasets by synergizing human and AI feedback to create fine-grained reasoning\ntrajectories. These datasets are then used to fine-tune open-source LLMs,\nenhancing their mathematical reasoning abilities without reliance on any\nsymbolic verification program. Concretely, EIT is composed of two critical\nsteps: Enriching with Reasoning Plan (ERP) and Enriching with Reasoning Step\n(ERS). The former generates a high-level plan that breaks down complex\ninstructions into a sequence of simpler objectives, while ERS fills in\nreasoning contexts often overlooked by human annotators, creating a smoother\nreasoning trajectory for LLM fine-tuning. Unlike existing CoT prompting methods\nthat generate reasoning chains only depending on LLM's internal knowledge, our\nmethod leverages human-annotated initial answers as ``meta-knowledge'' to help\nLLMs generate more detailed and precise reasoning processes, leading to a more\ntrustworthy LLM expert for complex mathematical problems. In experiments, EIT\nachieves an accuracy of 84.1% on GSM8K and 32.5% on MATH, surpassing\nstate-of-the-art fine-tuning and prompting methods, and even matching the\nperformance of tool-augmented methods."
                },
                "authors": [
                    {
                        "name": "Huanqia Cai"
                    },
                    {
                        "name": "Yijun Yang"
                    },
                    {
                        "name": "Zhifeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhifeng Li"
                },
                "author": "Zhifeng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16964v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16964v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18351v1",
                "updated": "2024-12-24T11:24:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    11,
                    24,
                    56,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T11:24:56Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    11,
                    24,
                    56,
                    1,
                    359,
                    0
                ],
                "title": "Multi-Agents Based on Large Language Models for Knowledge-based Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agents Based on Large Language Models for Knowledge-based Visual\n  Question Answering"
                },
                "summary": "Large Language Models (LLMs) have achieved impressive results in\nknowledge-based Visual Question Answering (VQA). However existing methods still\nhave challenges: the inability to use external tools autonomously, and the\ninability to work in teams. Humans tend to know whether they need to use\nexternal tools when they encounter a new question, e.g., they tend to be able\nto give a direct answer to a familiar question, whereas they tend to use tools\nsuch as search engines when they encounter an unfamiliar question. In addition,\nhumans also tend to collaborate and discuss with others to get better answers.\nInspired by this, we propose the multi-agent voting framework. We design three\nLLM-based agents that simulate different levels of staff in a team, and assign\nthe available tools according to the levels. Each agent provides the\ncorresponding answer, and finally all the answers provided by the agents are\nvoted to get the final answer. Experiments on OK-VQA and A-OKVQA show that our\napproach outperforms other baselines by 2.2 and 1.0, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved impressive results in\nknowledge-based Visual Question Answering (VQA). However existing methods still\nhave challenges: the inability to use external tools autonomously, and the\ninability to work in teams. Humans tend to know whether they need to use\nexternal tools when they encounter a new question, e.g., they tend to be able\nto give a direct answer to a familiar question, whereas they tend to use tools\nsuch as search engines when they encounter an unfamiliar question. In addition,\nhumans also tend to collaborate and discuss with others to get better answers.\nInspired by this, we propose the multi-agent voting framework. We design three\nLLM-based agents that simulate different levels of staff in a team, and assign\nthe available tools according to the levels. Each agent provides the\ncorresponding answer, and finally all the answers provided by the agents are\nvoted to get the final answer. Experiments on OK-VQA and A-OKVQA show that our\napproach outperforms other baselines by 2.2 and 1.0, respectively."
                },
                "authors": [
                    {
                        "name": "Zhongjian Hu"
                    },
                    {
                        "name": "Peng Yang"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Zhenqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhenqi Wang"
                },
                "author": "Zhenqi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17690v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17690v2",
                "updated": "2024-12-24T11:03:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    11,
                    3,
                    42,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-23T16:16:30Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    16,
                    30,
                    0,
                    358,
                    0
                ],
                "title": "RAGONITE: Iterative Retrieval on Induced Databases and Verbalized RDF\n  for Conversational QA over KGs with RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGONITE: Iterative Retrieval on Induced Databases and Verbalized RDF\n  for Conversational QA over KGs with RAG"
                },
                "summary": "Conversational question answering (ConvQA) is a convenient means of searching\nover RDF knowledge graphs (KGs), where a prevalent approach is to translate\nnatural language questions to SPARQL queries. However, SPARQL has certain\nshortcomings: (i) it is brittle for complex intents and conversational\nquestions, and (ii) it is not suitable for more abstract needs. Instead, we\npropose a novel two-pronged system where we fuse: (i) SQL-query results over a\ndatabase automatically derived from the KG, and (ii) text-search results over\nverbalizations of KG facts. Our pipeline supports iterative retrieval: when the\nresults of any branch are found to be unsatisfactory, the system can\nautomatically opt for further rounds. We put everything together in a retrieval\naugmented generation (RAG) setup, where an LLM generates a coherent response\nfrom accumulated search results. We demonstrate the superiority of our proposed\nsystem over several baselines on a knowledge graph of BMW automobiles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational question answering (ConvQA) is a convenient means of searching\nover RDF knowledge graphs (KGs), where a prevalent approach is to translate\nnatural language questions to SPARQL queries. However, SPARQL has certain\nshortcomings: (i) it is brittle for complex intents and conversational\nquestions, and (ii) it is not suitable for more abstract needs. Instead, we\npropose a novel two-pronged system where we fuse: (i) SQL-query results over a\ndatabase automatically derived from the KG, and (ii) text-search results over\nverbalizations of KG facts. Our pipeline supports iterative retrieval: when the\nresults of any branch are found to be unsatisfactory, the system can\nautomatically opt for further rounds. We put everything together in a retrieval\naugmented generation (RAG) setup, where an LLM generates a coherent response\nfrom accumulated search results. We demonstrate the superiority of our proposed\nsystem over several baselines on a knowledge graph of BMW automobiles."
                },
                "authors": [
                    {
                        "name": "Rishiraj Saha Roy"
                    },
                    {
                        "name": "Chris Hinze"
                    },
                    {
                        "name": "Joel Schlotthauer"
                    },
                    {
                        "name": "Farzad Naderi"
                    },
                    {
                        "name": "Viktor Hangya"
                    },
                    {
                        "name": "Andreas Foltyn"
                    },
                    {
                        "name": "Luzian Hahn"
                    },
                    {
                        "name": "Fabian Kuech"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Kuech"
                },
                "author": "Fabian Kuech",
                "arxiv_comment": "Accepted at BTW 2025, 10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17690v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17690v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2208.02627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2208.02627v2",
                "updated": "2024-12-24T10:35:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    10,
                    35,
                    9,
                    1,
                    359,
                    0
                ],
                "published": "2022-07-29T14:54:53Z",
                "published_parsed": [
                    2022,
                    7,
                    29,
                    14,
                    54,
                    53,
                    4,
                    210,
                    0
                ],
                "title": "Modelling multivariate extreme value distributions via Markov trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling multivariate extreme value distributions via Markov trees"
                },
                "summary": "Multivariate extreme value distributions are a common choice for modelling\nmultivariate extremes. In high dimensions, however, the construction of\nflexible and parsimonious models is challenging. We propose to combine\nbivariate max-stable distributions into a Markov random field with respect to a\ntree. Although in general not max-stable itself, this Markov tree is attracted\nby a multivariate max-stable distribution. The latter serves as a tree-based\napproximation to an unknown max-stable distribution with the given bivariate\ndistributions as margins. Given data, we learn an appropriate tree structure by\nPrim's algorithm with estimated pairwise upper tail dependence coefficients as\nedge weights. The distributions of pairs of connected variables can be fitted\nin various ways. The resulting tree-structured max-stable distribution allows\nfor inference on rare event probabilities, as illustrated on river discharge\ndata from the upper Danube basin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate extreme value distributions are a common choice for modelling\nmultivariate extremes. In high dimensions, however, the construction of\nflexible and parsimonious models is challenging. We propose to combine\nbivariate max-stable distributions into a Markov random field with respect to a\ntree. Although in general not max-stable itself, this Markov tree is attracted\nby a multivariate max-stable distribution. The latter serves as a tree-based\napproximation to an unknown max-stable distribution with the given bivariate\ndistributions as margins. Given data, we learn an appropriate tree structure by\nPrim's algorithm with estimated pairwise upper tail dependence coefficients as\nedge weights. The distributions of pairs of connected variables can be fitted\nin various ways. The resulting tree-structured max-stable distribution allows\nfor inference on rare event probabilities, as illustrated on river discharge\ndata from the upper Danube basin."
                },
                "authors": [
                    {
                        "name": "Shuang Hu"
                    },
                    {
                        "name": "Zuoxiang Peng"
                    },
                    {
                        "name": "Johan Segers"
                    }
                ],
                "author_detail": {
                    "name": "Johan Segers"
                },
                "author": "Johan Segers",
                "arxiv_doi": "10.1111/sjos.12698",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1111/sjos.12698",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2208.02627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2208.02627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "49 pages, 6 figures, 3 tables",
                "arxiv_journal_ref": "Scandinavian Journal of Statistics (2024), volume 51, pages\n  760-800",
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62G32, 62H22",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15529v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15529v2",
                "updated": "2024-12-24T10:32:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    10,
                    32,
                    13,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-20T03:37:07Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    3,
                    37,
                    7,
                    4,
                    355,
                    0
                ],
                "title": "XRAG: eXamining the Core -- Benchmarking Foundational Components in\n  Advanced Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XRAG: eXamining the Core -- Benchmarking Foundational Components in\n  Advanced Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) synergizes the retrieval of pertinent\ndata with the generative capabilities of Large Language Models (LLMs), ensuring\nthat the generated output is not only contextually relevant but also accurate\nand current. We introduce XRAG, an open-source, modular codebase that\nfacilitates exhaustive evaluation of the performance of foundational components\nof advanced RAG modules. These components are systematically categorized into\nfour core phases: pre-retrieval, retrieval, post-retrieval, and generation. We\nsystematically analyse them across reconfigured datasets, providing a\ncomprehensive benchmark for their effectiveness. As the complexity of RAG\nsystems continues to escalate, we underscore the critical need to identify\npotential failure points in RAG systems. We formulate a suite of experimental\nmethodologies and diagnostic testing protocols to dissect the failure points\ninherent in RAG engineering. Subsequently, we proffer bespoke solutions aimed\nat bolstering the overall performance of these modules. Our work thoroughly\nevaluates the performance of advanced core components in RAG systems, providing\ninsights into optimizations for prevalent failure points.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) synergizes the retrieval of pertinent\ndata with the generative capabilities of Large Language Models (LLMs), ensuring\nthat the generated output is not only contextually relevant but also accurate\nand current. We introduce XRAG, an open-source, modular codebase that\nfacilitates exhaustive evaluation of the performance of foundational components\nof advanced RAG modules. These components are systematically categorized into\nfour core phases: pre-retrieval, retrieval, post-retrieval, and generation. We\nsystematically analyse them across reconfigured datasets, providing a\ncomprehensive benchmark for their effectiveness. As the complexity of RAG\nsystems continues to escalate, we underscore the critical need to identify\npotential failure points in RAG systems. We formulate a suite of experimental\nmethodologies and diagnostic testing protocols to dissect the failure points\ninherent in RAG engineering. Subsequently, we proffer bespoke solutions aimed\nat bolstering the overall performance of these modules. Our work thoroughly\nevaluates the performance of advanced core components in RAG systems, providing\ninsights into optimizations for prevalent failure points."
                },
                "authors": [
                    {
                        "name": "Qianren Mao"
                    },
                    {
                        "name": "Yangyifei Luo"
                    },
                    {
                        "name": "Jinlong Zhang"
                    },
                    {
                        "name": "Hanwen Hao"
                    },
                    {
                        "name": "Zhilong Cao"
                    },
                    {
                        "name": "Xiaolong Wang"
                    },
                    {
                        "name": "Xiao Guan"
                    },
                    {
                        "name": "Zhenting Huang"
                    },
                    {
                        "name": "Weifeng Jiang"
                    },
                    {
                        "name": "Shuyu Guo"
                    },
                    {
                        "name": "Zhentao Han"
                    },
                    {
                        "name": "Qili Zhang"
                    },
                    {
                        "name": "Siyuan Tao"
                    },
                    {
                        "name": "Yujie Liu"
                    },
                    {
                        "name": "Junnan Liu"
                    },
                    {
                        "name": "Zhixing Tan"
                    },
                    {
                        "name": "Jie Sun"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xudong Liu"
                    },
                    {
                        "name": "Richong Zhang"
                    },
                    {
                        "name": "Jianxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianxin Li"
                },
                "author": "Jianxin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15529v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15529v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18318v1",
                "updated": "2024-12-24T10:07:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    10,
                    7,
                    10,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T10:07:10Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    10,
                    7,
                    10,
                    1,
                    359,
                    0
                ],
                "title": "Search for a gravitational wave background from primordial black hole\n  binaries using data from the first three LIGO-Virgo-KAGRA observing runs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search for a gravitational wave background from primordial black hole\n  binaries using data from the first three LIGO-Virgo-KAGRA observing runs"
                },
                "summary": "Using the cross-correlation data from the first three observing runs of the\nLIGO-Virgo-KAGRA Collaboration, we search for a gravitational-wave background\n(GWB) from primordial black holes, arising from the superposition of compact\nbinary coalescence events. We consider both early and late binary formation\nmechanisms, and perform Bayesian parameter inference, investigating different\nprior distributions of the model parameters. From the non-detection of the GWB,\nwe provide constraints on the fraction of primordial black holes contributing\nto the present dark matter energy density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using the cross-correlation data from the first three observing runs of the\nLIGO-Virgo-KAGRA Collaboration, we search for a gravitational-wave background\n(GWB) from primordial black holes, arising from the superposition of compact\nbinary coalescence events. We consider both early and late binary formation\nmechanisms, and perform Bayesian parameter inference, investigating different\nprior distributions of the model parameters. From the non-detection of the GWB,\nwe provide constraints on the fraction of primordial black holes contributing\nto the present dark matter energy density."
                },
                "authors": [
                    {
                        "name": "Tore Boybeyi"
                    },
                    {
                        "name": "Sebastien Clesse"
                    },
                    {
                        "name": "Sachiko Kuroyanagi"
                    },
                    {
                        "name": "Mairi Sakellariadou"
                    }
                ],
                "author_detail": {
                    "name": "Mairi Sakellariadou"
                },
                "author": "Mairi Sakellariadou",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17915v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17915v4",
                "updated": "2024-12-24T09:35:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    9,
                    35,
                    5,
                    1,
                    359,
                    0
                ],
                "published": "2024-07-25T10:09:21Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    10,
                    9,
                    21,
                    3,
                    207,
                    0
                ],
                "title": "The Dark Side of Function Calling: Pathways to Jailbreaking Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dark Side of Function Calling: Pathways to Jailbreaking Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities, but\ntheir power comes with significant security considerations. While extensive\nresearch has been conducted on the safety of LLMs in chat mode, the security\nimplications of their function calling feature have been largely overlooked.\nThis paper uncovers a critical vulnerability in the function calling process of\nLLMs, introducing a novel \"jailbreak function\" attack method that exploits\nalignment discrepancies, user coercion, and the absence of rigorous safety\nfilters. Our empirical study, conducted on six state-of-the-art LLMs including\nGPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-pro, reveals an alarming average\nsuccess rate of over 90\\% for this attack. We provide a comprehensive analysis\nof why function calls are susceptible to such attacks and propose defensive\nstrategies, including the use of defensive prompts. Our findings highlight the\nurgent need for enhanced security measures in the function calling capabilities\nof LLMs, contributing to the field of AI safety by identifying a previously\nunexplored risk, designing an effective attack method, and suggesting practical\ndefensive measures. Our code is available at\nhttps://github.com/wooozihui/jailbreakfunction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities, but\ntheir power comes with significant security considerations. While extensive\nresearch has been conducted on the safety of LLMs in chat mode, the security\nimplications of their function calling feature have been largely overlooked.\nThis paper uncovers a critical vulnerability in the function calling process of\nLLMs, introducing a novel \"jailbreak function\" attack method that exploits\nalignment discrepancies, user coercion, and the absence of rigorous safety\nfilters. Our empirical study, conducted on six state-of-the-art LLMs including\nGPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-pro, reveals an alarming average\nsuccess rate of over 90\\% for this attack. We provide a comprehensive analysis\nof why function calls are susceptible to such attacks and propose defensive\nstrategies, including the use of defensive prompts. Our findings highlight the\nurgent need for enhanced security measures in the function calling capabilities\nof LLMs, contributing to the field of AI safety by identifying a previously\nunexplored risk, designing an effective attack method, and suggesting practical\ndefensive measures. Our code is available at\nhttps://github.com/wooozihui/jailbreakfunction."
                },
                "authors": [
                    {
                        "name": "Zihui Wu"
                    },
                    {
                        "name": "Haichang Gao"
                    },
                    {
                        "name": "Jianping He"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17915v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17915v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18303v1",
                "updated": "2024-12-24T09:15:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    9,
                    15,
                    0,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T09:15:00Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    9,
                    15,
                    0,
                    1,
                    359,
                    0
                ],
                "title": "Efficient and Context-Aware Label Propagation for Zero-/Few-Shot\n  Training-Free Adaptation of Vision-Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Context-Aware Label Propagation for Zero-/Few-Shot\n  Training-Free Adaptation of Vision-Language Model"
                },
                "summary": "Vision-language models (VLMs) have revolutionized machine learning by\nleveraging large pre-trained models to tackle various downstream tasks. Despite\nimprovements in label, training, and data efficiency, many state-of-the-art\nVLMs still require task-specific hyperparameter tuning and fail to fully\nexploit test samples. To overcome these challenges, we propose a graph-based\napproach for label-efficient adaptation and inference. Our method dynamically\nconstructs a graph over text prompts, few-shot examples, and test samples,\nusing label propagation for inference without task-specific tuning. Unlike\nexisting zero-shot label propagation techniques, our approach requires no\nadditional unlabeled support set and effectively leverages the test sample\nmanifold through dynamic graph expansion. We further introduce a context-aware\nfeature re-weighting mechanism to improve task adaptation accuracy.\nAdditionally, our method supports efficient graph expansion, enabling real-time\ninductive inference. Extensive evaluations on downstream tasks, such as\nfine-grained categorization and out-of-distribution generalization, demonstrate\nthe effectiveness of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) have revolutionized machine learning by\nleveraging large pre-trained models to tackle various downstream tasks. Despite\nimprovements in label, training, and data efficiency, many state-of-the-art\nVLMs still require task-specific hyperparameter tuning and fail to fully\nexploit test samples. To overcome these challenges, we propose a graph-based\napproach for label-efficient adaptation and inference. Our method dynamically\nconstructs a graph over text prompts, few-shot examples, and test samples,\nusing label propagation for inference without task-specific tuning. Unlike\nexisting zero-shot label propagation techniques, our approach requires no\nadditional unlabeled support set and effectively leverages the test sample\nmanifold through dynamic graph expansion. We further introduce a context-aware\nfeature re-weighting mechanism to improve task adaptation accuracy.\nAdditionally, our method supports efficient graph expansion, enabling real-time\ninductive inference. Extensive evaluations on downstream tasks, such as\nfine-grained categorization and out-of-distribution generalization, demonstrate\nthe effectiveness of our approach."
                },
                "authors": [
                    {
                        "name": "Yushu Li"
                    },
                    {
                        "name": "Yongyi Su"
                    },
                    {
                        "name": "Adam Goodge"
                    },
                    {
                        "name": "Kui Jia"
                    },
                    {
                        "name": "Xun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xun Xu"
                },
                "author": "Xun Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16956v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16956v2",
                "updated": "2024-12-24T09:07:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    9,
                    7,
                    26,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-22T10:28:52Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    10,
                    28,
                    52,
                    6,
                    357,
                    0
                ],
                "title": "Semantic Hierarchical Prompt Tuning for Parameter-Efficient Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Hierarchical Prompt Tuning for Parameter-Efficient Fine-Tuning"
                },
                "summary": "As the scale of vision models continues to grow, Visual Prompt Tuning (VPT)\nhas emerged as a parameter-efficient transfer learning technique, noted for its\nsuperior performance compared to full fine-tuning. However, indiscriminately\napplying prompts to every layer without considering their inherent\ncorrelations, can cause significant disturbances, leading to suboptimal\ntransferability. Additionally, VPT disrupts the original self-attention\nstructure, affecting the aggregation of visual features, and lacks a mechanism\nfor explicitly mining discriminative visual features, which are crucial for\nclassification. To address these issues, we propose a Semantic Hierarchical\nPrompt (SHIP) fine-tuning strategy. We adaptively construct semantic\nhierarchies and use semantic-independent and semantic-shared prompts to learn\nhierarchical representations. We also integrate attribute prompts and a prompt\nmatching loss to enhance feature discrimination and employ decoupled attention\nfor robustness and reduced inference costs. SHIP significantly improves\nperformance, achieving a 4.9% gain in accuracy over VPT with a ViT-B/16\nbackbone on VTAB-1k tasks. Our code is available at\nhttps://github.com/haoweiz23/SHIP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the scale of vision models continues to grow, Visual Prompt Tuning (VPT)\nhas emerged as a parameter-efficient transfer learning technique, noted for its\nsuperior performance compared to full fine-tuning. However, indiscriminately\napplying prompts to every layer without considering their inherent\ncorrelations, can cause significant disturbances, leading to suboptimal\ntransferability. Additionally, VPT disrupts the original self-attention\nstructure, affecting the aggregation of visual features, and lacks a mechanism\nfor explicitly mining discriminative visual features, which are crucial for\nclassification. To address these issues, we propose a Semantic Hierarchical\nPrompt (SHIP) fine-tuning strategy. We adaptively construct semantic\nhierarchies and use semantic-independent and semantic-shared prompts to learn\nhierarchical representations. We also integrate attribute prompts and a prompt\nmatching loss to enhance feature discrimination and employ decoupled attention\nfor robustness and reduced inference costs. SHIP significantly improves\nperformance, achieving a 4.9% gain in accuracy over VPT with a ViT-B/16\nbackbone on VTAB-1k tasks. Our code is available at\nhttps://github.com/haoweiz23/SHIP."
                },
                "authors": [
                    {
                        "name": "Haowei Zhu"
                    },
                    {
                        "name": "Fangyuan Zhang"
                    },
                    {
                        "name": "Rui Qin"
                    },
                    {
                        "name": "Tianxiang Pan"
                    },
                    {
                        "name": "Junhai Yong"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16956v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18299v1",
                "updated": "2024-12-24T09:06:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    9,
                    6,
                    58,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T09:06:58Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    9,
                    6,
                    58,
                    1,
                    359,
                    0
                ],
                "title": "M-Ped: Multi-Prompt Ensemble Decoding for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M-Ped: Multi-Prompt Ensemble Decoding for Large Language Models"
                },
                "summary": "With the widespread application of Large Language Models (LLMs) in the field\nof Natural Language Processing (NLP), enhancing their performance has become a\nresearch hotspot. This paper presents a novel multi-prompt ensemble decoding\napproach designed to bolster the generation quality of LLMs by leveraging the\naggregation of outcomes from multiple prompts. Given a unique input $X$, we\nsubmit $n$ variations of prompts with $X$ to LLMs in batch mode to decode and\nderive probability distributions. For each token prediction, we calculate the\nensemble probability by averaging the $n$ probability distributions within the\nbatch, utilizing this aggregated probability to generate the token. This\ntechnique is dubbed Inner-Batch Ensemble. To facilitate efficient batch\ninference, we implement a Left-Padding strategy to maintain uniform input\nlengths across the n prompts. Through extensive experimentation on diverse NLP\ntasks, including machine translation, code generation, and text simplification,\nwe demonstrate the efficacy of our method in enhancing LLM performance. The\nresults show substantial improvements in BLEU scores, pass@$k$ rates, and LENS\nmetrics over conventional methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread application of Large Language Models (LLMs) in the field\nof Natural Language Processing (NLP), enhancing their performance has become a\nresearch hotspot. This paper presents a novel multi-prompt ensemble decoding\napproach designed to bolster the generation quality of LLMs by leveraging the\naggregation of outcomes from multiple prompts. Given a unique input $X$, we\nsubmit $n$ variations of prompts with $X$ to LLMs in batch mode to decode and\nderive probability distributions. For each token prediction, we calculate the\nensemble probability by averaging the $n$ probability distributions within the\nbatch, utilizing this aggregated probability to generate the token. This\ntechnique is dubbed Inner-Batch Ensemble. To facilitate efficient batch\ninference, we implement a Left-Padding strategy to maintain uniform input\nlengths across the n prompts. Through extensive experimentation on diverse NLP\ntasks, including machine translation, code generation, and text simplification,\nwe demonstrate the efficacy of our method in enhancing LLM performance. The\nresults show substantial improvements in BLEU scores, pass@$k$ rates, and LENS\nmetrics over conventional methods."
                },
                "authors": [
                    {
                        "name": "Jiaxin Guo"
                    },
                    {
                        "name": "Daimeng Wei"
                    },
                    {
                        "name": "Yuanchang Luo"
                    },
                    {
                        "name": "Shimin Tao"
                    },
                    {
                        "name": "Hengchao Shang"
                    },
                    {
                        "name": "Zongyao Li"
                    },
                    {
                        "name": "Shaojun Li"
                    },
                    {
                        "name": "Jinlong Yang"
                    },
                    {
                        "name": "Zhanglin Wu"
                    },
                    {
                        "name": "Zhiqiang Rao"
                    },
                    {
                        "name": "Hao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Yang"
                },
                "author": "Hao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.07189v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.07189v4",
                "updated": "2024-12-24T09:06:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    9,
                    6,
                    15,
                    1,
                    359,
                    0
                ],
                "published": "2023-03-13T15:30:28Z",
                "published_parsed": [
                    2023,
                    3,
                    13,
                    15,
                    30,
                    28,
                    0,
                    72,
                    0
                ],
                "title": "Optimizing Convolutional Neural Networks for Chronic Obstructive\n  Pulmonary Disease Detection in Clinical Computed Tomography Imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Convolutional Neural Networks for Chronic Obstructive\n  Pulmonary Disease Detection in Clinical Computed Tomography Imaging"
                },
                "summary": "We aim to optimize the binary detection of Chronic Obstructive Pulmonary\nDisease (COPD) based on emphysema presence in the lung with convolutional\nneural networks (CNN) by exploring manually adjusted versus automated\nwindow-setting optimization (WSO) on computed tomography (CT) images. 7,194 CT\nimages (3,597 with COPD; 3,597 healthy controls) from 78 subjects were selected\nretrospectively (10.2018-12.2021) and preprocessed. For each image, intensity\nvalues were manually clipped to the emphysema window setting and a baseline\n'full-range' window setting. Class-balanced train, validation, and test sets\ncontained 3,392, 1,114, and 2,688 images. The network backbone was optimized by\ncomparing various CNN architectures. Furthermore, automated WSO was implemented\nby adding a customized layer to the model. The image-level area under the\nReceiver Operating Characteristics curve (AUC) [lower, upper limit 95%\nconfidence] was utilized to compare model variations. Repeated inference (n=7)\non the test set showed that the DenseNet was the most efficient backbone and\nachieved a mean AUC of 0.80 [0.76, 0.85] without WSO. Comparably, with input\nimages manually adjusted to the emphysema window, the DenseNet model predicted\nCOPD with a mean AUC of 0.86 [0.82, 0.89]. By adding a customized WSO layer to\nthe DenseNet, an optimal window in the proximity of the emphysema window\nsetting was learned automatically, and a mean AUC of 0.82 [0.78, 0.86] was\nachieved. Detection of COPD with DenseNet models was improved by WSO of CT data\nto the emphysema window setting range.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We aim to optimize the binary detection of Chronic Obstructive Pulmonary\nDisease (COPD) based on emphysema presence in the lung with convolutional\nneural networks (CNN) by exploring manually adjusted versus automated\nwindow-setting optimization (WSO) on computed tomography (CT) images. 7,194 CT\nimages (3,597 with COPD; 3,597 healthy controls) from 78 subjects were selected\nretrospectively (10.2018-12.2021) and preprocessed. For each image, intensity\nvalues were manually clipped to the emphysema window setting and a baseline\n'full-range' window setting. Class-balanced train, validation, and test sets\ncontained 3,392, 1,114, and 2,688 images. The network backbone was optimized by\ncomparing various CNN architectures. Furthermore, automated WSO was implemented\nby adding a customized layer to the model. The image-level area under the\nReceiver Operating Characteristics curve (AUC) [lower, upper limit 95%\nconfidence] was utilized to compare model variations. Repeated inference (n=7)\non the test set showed that the DenseNet was the most efficient backbone and\nachieved a mean AUC of 0.80 [0.76, 0.85] without WSO. Comparably, with input\nimages manually adjusted to the emphysema window, the DenseNet model predicted\nCOPD with a mean AUC of 0.86 [0.82, 0.89]. By adding a customized WSO layer to\nthe DenseNet, an optimal window in the proximity of the emphysema window\nsetting was learned automatically, and a mean AUC of 0.82 [0.78, 0.86] was\nachieved. Detection of COPD with DenseNet models was improved by WSO of CT data\nto the emphysema window setting range."
                },
                "authors": [
                    {
                        "name": "Tina Dorosti"
                    },
                    {
                        "name": "Manuel Schultheiss"
                    },
                    {
                        "name": "Felix Hofmann"
                    },
                    {
                        "name": "Johannes Thalhammer"
                    },
                    {
                        "name": "Luisa Kirchner"
                    },
                    {
                        "name": "Theresa Urban"
                    },
                    {
                        "name": "Franz Pfeiffer"
                    },
                    {
                        "name": "Florian Schaff"
                    },
                    {
                        "name": "Tobias Lasser"
                    },
                    {
                        "name": "Daniela Pfeiffer"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Pfeiffer"
                },
                "author": "Daniela Pfeiffer",
                "arxiv_doi": "10.1016/j.compbiomed.2024.109533",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.compbiomed.2024.109533",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2303.07189v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.07189v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Comput Biol Med 185, 109533 (2025)",
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18298v1",
                "updated": "2024-12-24T09:05:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    9,
                    5,
                    37,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T09:05:37Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    9,
                    5,
                    37,
                    1,
                    359,
                    0
                ],
                "title": "Quo Vadis, Anomaly Detection? LLMs and VLMs in the Spotlight",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quo Vadis, Anomaly Detection? LLMs and VLMs in the Spotlight"
                },
                "summary": "Video anomaly detection (VAD) has witnessed significant advancements through\nthe integration of large language models (LLMs) and vision-language models\n(VLMs), addressing critical challenges such as interpretability, temporal\nreasoning, and generalization in dynamic, open-world scenarios. This paper\npresents an in-depth review of cutting-edge LLM-/VLM-based methods in 2024,\nfocusing on four key aspects: (i) enhancing interpretability through semantic\ninsights and textual explanations, making visual anomalies more understandable;\n(ii) capturing intricate temporal relationships to detect and localize dynamic\nanomalies across video frames; (iii) enabling few-shot and zero-shot detection\nto minimize reliance on large, annotated datasets; and (iv) addressing\nopen-world and class-agnostic anomalies by using semantic understanding and\nmotion features for spatiotemporal coherence. We highlight their potential to\nredefine the landscape of VAD. Additionally, we explore the synergy between\nvisual and textual modalities offered by LLMs and VLMs, highlighting their\ncombined strengths and proposing future directions to fully exploit the\npotential in enhancing video anomaly detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video anomaly detection (VAD) has witnessed significant advancements through\nthe integration of large language models (LLMs) and vision-language models\n(VLMs), addressing critical challenges such as interpretability, temporal\nreasoning, and generalization in dynamic, open-world scenarios. This paper\npresents an in-depth review of cutting-edge LLM-/VLM-based methods in 2024,\nfocusing on four key aspects: (i) enhancing interpretability through semantic\ninsights and textual explanations, making visual anomalies more understandable;\n(ii) capturing intricate temporal relationships to detect and localize dynamic\nanomalies across video frames; (iii) enabling few-shot and zero-shot detection\nto minimize reliance on large, annotated datasets; and (iv) addressing\nopen-world and class-agnostic anomalies by using semantic understanding and\nmotion features for spatiotemporal coherence. We highlight their potential to\nredefine the landscape of VAD. Additionally, we explore the synergy between\nvisual and textual modalities offered by LLMs and VLMs, highlighting their\ncombined strengths and proposing future directions to fully exploit the\npotential in enhancing video anomaly detection."
                },
                "authors": [
                    {
                        "name": "Xi Ding"
                    },
                    {
                        "name": "Lei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Wang"
                },
                "author": "Lei Wang",
                "arxiv_comment": "Research report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18295v1",
                "updated": "2024-12-24T09:03:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    9,
                    3,
                    57,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T09:03:57Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    9,
                    3,
                    57,
                    1,
                    359,
                    0
                ],
                "title": "Pirates of the RAG: Adaptively Attacking LLMs to Leak Knowledge Bases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pirates of the RAG: Adaptively Attacking LLMs to Leak Knowledge Bases"
                },
                "summary": "The growing ubiquity of Retrieval-Augmented Generation (RAG) systems in\nseveral real-world services triggers severe concerns about their security. A\nRAG system improves the generative capabilities of a Large Language Models\n(LLM) by a retrieval mechanism which operates on a private knowledge base,\nwhose unintended exposure could lead to severe consequences, including breaches\nof private and sensitive information. This paper presents a black-box attack to\nforce a RAG system to leak its private knowledge base which, differently from\nexisting approaches, is adaptive and automatic. A relevance-based mechanism and\nan attacker-side open-source LLM favor the generation of effective queries to\nleak most of the (hidden) knowledge base. Extensive experimentation proves the\nquality of the proposed algorithm in different RAG pipelines and domains,\ncomparing to very recent related approaches, which turn out to be either not\nfully black-box, not adaptive, or not based on open-source models. The findings\nfrom our study remark the urgent need for more robust privacy safeguards in the\ndesign and deployment of RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing ubiquity of Retrieval-Augmented Generation (RAG) systems in\nseveral real-world services triggers severe concerns about their security. A\nRAG system improves the generative capabilities of a Large Language Models\n(LLM) by a retrieval mechanism which operates on a private knowledge base,\nwhose unintended exposure could lead to severe consequences, including breaches\nof private and sensitive information. This paper presents a black-box attack to\nforce a RAG system to leak its private knowledge base which, differently from\nexisting approaches, is adaptive and automatic. A relevance-based mechanism and\nan attacker-side open-source LLM favor the generation of effective queries to\nleak most of the (hidden) knowledge base. Extensive experimentation proves the\nquality of the proposed algorithm in different RAG pipelines and domains,\ncomparing to very recent related approaches, which turn out to be either not\nfully black-box, not adaptive, or not based on open-source models. The findings\nfrom our study remark the urgent need for more robust privacy safeguards in the\ndesign and deployment of RAG systems."
                },
                "authors": [
                    {
                        "name": "Christian Di Maio"
                    },
                    {
                        "name": "Cristian Cosci"
                    },
                    {
                        "name": "Marco Maggini"
                    },
                    {
                        "name": "Valentina Poggioni"
                    },
                    {
                        "name": "Stefano Melacci"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Melacci"
                },
                "author": "Stefano Melacci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11465v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11465v3",
                "updated": "2024-12-24T09:03:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    9,
                    3,
                    2,
                    1,
                    359,
                    0
                ],
                "published": "2024-11-18T10:58:46Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    10,
                    58,
                    46,
                    0,
                    323,
                    0
                ],
                "title": "Re-examining learning linear functions in context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-examining learning linear functions in context"
                },
                "summary": "In-context learning (ICL) has emerged as a powerful paradigm for easily\nadapting Large Language Models (LLMs) to various tasks. However, our\nunderstanding of how ICL works remains limited. We explore a simple model of\nICL in a controlled setup with synthetic training data to investigate ICL of\nunivariate linear functions. We experiment with a range of GPT-2-like\ntransformer models trained from scratch. Our findings challenge the prevailing\nnarrative that transformers adopt algorithmic approaches like linear regression\nto learn a linear function in-context. These models fail to generalize beyond\ntheir training distribution, highlighting fundamental limitations in their\ncapacity to infer abstract task structures. Our experiments lead us to propose\na mathematically precise hypothesis of what the model might be learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) has emerged as a powerful paradigm for easily\nadapting Large Language Models (LLMs) to various tasks. However, our\nunderstanding of how ICL works remains limited. We explore a simple model of\nICL in a controlled setup with synthetic training data to investigate ICL of\nunivariate linear functions. We experiment with a range of GPT-2-like\ntransformer models trained from scratch. Our findings challenge the prevailing\nnarrative that transformers adopt algorithmic approaches like linear regression\nto learn a linear function in-context. These models fail to generalize beyond\ntheir training distribution, highlighting fundamental limitations in their\ncapacity to infer abstract task structures. Our experiments lead us to propose\na mathematically precise hypothesis of what the model might be learning."
                },
                "authors": [
                    {
                        "name": "Omar Naim"
                    },
                    {
                        "name": "Guilhem FouilhÃ©"
                    },
                    {
                        "name": "Nicholas Asher"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Asher"
                },
                "author": "Nicholas Asher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11465v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11465v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18293v1",
                "updated": "2024-12-24T09:01:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    9,
                    1,
                    43,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T09:01:43Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    9,
                    1,
                    43,
                    1,
                    359,
                    0
                ],
                "title": "MinsStudio: A Streamlined Package for Minecraft AI Agent Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MinsStudio: A Streamlined Package for Minecraft AI Agent Development"
                },
                "summary": "Minecraft has emerged as a valuable testbed for embodied intelligence and\nsequential decision-making research, yet the development and validation of\nnovel agents remains hindered by significant engineering challenges. This paper\npresents MineStudio, an open-source software package designed to streamline\nembodied policy development in Minecraft. MineStudio represents the first\ncomprehensive integration of seven critical engineering components: simulator,\ndata, model, offline pretraining, online finetuning, inference, and benchmark,\nthereby allowing users to concentrate their efforts on algorithm innovation. We\nprovide a user-friendly API design accompanied by comprehensive documentation\nand tutorials. The complete codebase is publicly available at\nhttps://github.com/CraftJarvis/MineStudio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minecraft has emerged as a valuable testbed for embodied intelligence and\nsequential decision-making research, yet the development and validation of\nnovel agents remains hindered by significant engineering challenges. This paper\npresents MineStudio, an open-source software package designed to streamline\nembodied policy development in Minecraft. MineStudio represents the first\ncomprehensive integration of seven critical engineering components: simulator,\ndata, model, offline pretraining, online finetuning, inference, and benchmark,\nthereby allowing users to concentrate their efforts on algorithm innovation. We\nprovide a user-friendly API design accompanied by comprehensive documentation\nand tutorials. The complete codebase is publicly available at\nhttps://github.com/CraftJarvis/MineStudio."
                },
                "authors": [
                    {
                        "name": "Shaofei Cai"
                    },
                    {
                        "name": "Zhancun Mu"
                    },
                    {
                        "name": "Kaichen He"
                    },
                    {
                        "name": "Bowei Zhang"
                    },
                    {
                        "name": "Xinyue Zheng"
                    },
                    {
                        "name": "Anji Liu"
                    },
                    {
                        "name": "Yitao Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yitao Liang"
                },
                "author": "Yitao Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18291v1",
                "updated": "2024-12-24T08:53:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    8,
                    53,
                    54,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T08:53:54Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    8,
                    53,
                    54,
                    1,
                    359,
                    0
                ],
                "title": "DeepCRCEval: Revisiting the Evaluation of Code Review Comment Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepCRCEval: Revisiting the Evaluation of Code Review Comment Generation"
                },
                "summary": "Code review is a vital but demanding aspect of software development,\ngenerating significant interest in automating review comments. Traditional\nevaluation methods for these comments, primarily based on text similarity, face\ntwo major challenges: inconsistent reliability of human-authored comments in\nopen-source projects and the weak correlation of text similarity with\nobjectives like enhancing code quality and detecting defects.\n  This study empirically analyzes benchmark comments using a novel set of\ncriteria informed by prior research and developer interviews. We then similarly\nrevisit the evaluation of existing methodologies. Our evaluation framework,\nDeepCRCEval, integrates human evaluators and Large Language Models (LLMs) for a\ncomprehensive reassessment of current techniques based on the criteria set.\nBesides, we also introduce an innovative and efficient baseline, LLM-Reviewer,\nleveraging the few-shot learning capabilities of LLMs for a target-oriented\ncomparison.\n  Our research highlights the limitations of text similarity metrics, finding\nthat less than 10% of benchmark comments are high quality for automation. In\ncontrast, DeepCRCEval effectively distinguishes between high and low-quality\ncomments, proving to be a more reliable evaluation mechanism. Incorporating LLM\nevaluators into DeepCRCEval significantly boosts efficiency, reducing time and\ncost by 88.78% and 90.32%, respectively. Furthermore, LLM-Reviewer demonstrates\nsignificant potential of focusing task real targets in comment generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code review is a vital but demanding aspect of software development,\ngenerating significant interest in automating review comments. Traditional\nevaluation methods for these comments, primarily based on text similarity, face\ntwo major challenges: inconsistent reliability of human-authored comments in\nopen-source projects and the weak correlation of text similarity with\nobjectives like enhancing code quality and detecting defects.\n  This study empirically analyzes benchmark comments using a novel set of\ncriteria informed by prior research and developer interviews. We then similarly\nrevisit the evaluation of existing methodologies. Our evaluation framework,\nDeepCRCEval, integrates human evaluators and Large Language Models (LLMs) for a\ncomprehensive reassessment of current techniques based on the criteria set.\nBesides, we also introduce an innovative and efficient baseline, LLM-Reviewer,\nleveraging the few-shot learning capabilities of LLMs for a target-oriented\ncomparison.\n  Our research highlights the limitations of text similarity metrics, finding\nthat less than 10% of benchmark comments are high quality for automation. In\ncontrast, DeepCRCEval effectively distinguishes between high and low-quality\ncomments, proving to be a more reliable evaluation mechanism. Incorporating LLM\nevaluators into DeepCRCEval significantly boosts efficiency, reducing time and\ncost by 88.78% and 90.32%, respectively. Furthermore, LLM-Reviewer demonstrates\nsignificant potential of focusing task real targets in comment generation."
                },
                "authors": [
                    {
                        "name": "Junyi Lu"
                    },
                    {
                        "name": "Xiaojia Li"
                    },
                    {
                        "name": "Zihan Hua"
                    },
                    {
                        "name": "Lei Yu"
                    },
                    {
                        "name": "Shiqi Cheng"
                    },
                    {
                        "name": "Li Yang"
                    },
                    {
                        "name": "Fengjun Zhang"
                    },
                    {
                        "name": "Chun Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Chun Zuo"
                },
                "author": "Chun Zuo",
                "arxiv_comment": "Accepted to the 28th International Conference on Fundamental\n  Approaches to Software Engineering (FASE 2025), part of the 28th European\n  Joint Conferences on Theory and Practice of Software (ETAPS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18279v1",
                "updated": "2024-12-24T08:39:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    8,
                    39,
                    35,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T08:39:35Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    8,
                    39,
                    35,
                    1,
                    359,
                    0
                ],
                "title": "Improving Multi-Step Reasoning Abilities of Large Language Models with\n  Direct Advantage Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Multi-Step Reasoning Abilities of Large Language Models with\n  Direct Advantage Policy Optimization"
                },
                "summary": "The role of reinforcement learning (RL) in enhancing the reasoning of large\nlanguage models (LLMs) is becoming increasingly significant. Despite the\nsuccess of RL in many scenarios, there are still many challenges in improving\nthe reasoning of LLMs. One challenge is the sparse reward, which makes\noptimization difficult for RL and necessitates a large amount of data samples.\nAnother challenge stems from the inherent instability of RL, particularly when\nusing Actor-Critic (AC) methods to derive optimal policies, which often leads\nto unstable training processes. To address these issues, we introduce Direct\nAdvantage Policy Optimization (DAPO), an novel step-level offline RL algorithm.\nUnlike standard alignment that rely solely outcome rewards to optimize policies\n(such as DPO), DAPO employs a critic function to predict the reasoning accuracy\nat each step, thereby generating dense signals to refine the generation\nstrategy. Additionally, the Actor and Critic components in DAPO are trained\nindependently, avoiding the co-training instability observed in standard AC\nalgorithms like PPO. We train DAPO on mathematical and code query datasets and\nthen evaluate its performance on multiple benchmarks. Our results show that\nDAPO can effectively enhance the mathematical and code capabilities on both SFT\nmodels and RL models, demonstrating the effectiveness of DAPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The role of reinforcement learning (RL) in enhancing the reasoning of large\nlanguage models (LLMs) is becoming increasingly significant. Despite the\nsuccess of RL in many scenarios, there are still many challenges in improving\nthe reasoning of LLMs. One challenge is the sparse reward, which makes\noptimization difficult for RL and necessitates a large amount of data samples.\nAnother challenge stems from the inherent instability of RL, particularly when\nusing Actor-Critic (AC) methods to derive optimal policies, which often leads\nto unstable training processes. To address these issues, we introduce Direct\nAdvantage Policy Optimization (DAPO), an novel step-level offline RL algorithm.\nUnlike standard alignment that rely solely outcome rewards to optimize policies\n(such as DPO), DAPO employs a critic function to predict the reasoning accuracy\nat each step, thereby generating dense signals to refine the generation\nstrategy. Additionally, the Actor and Critic components in DAPO are trained\nindependently, avoiding the co-training instability observed in standard AC\nalgorithms like PPO. We train DAPO on mathematical and code query datasets and\nthen evaluate its performance on multiple benchmarks. Our results show that\nDAPO can effectively enhance the mathematical and code capabilities on both SFT\nmodels and RL models, demonstrating the effectiveness of DAPO."
                },
                "authors": [
                    {
                        "name": "Jiacai Liu"
                    },
                    {
                        "name": "Chaojie Wang"
                    },
                    {
                        "name": "Chris Yuhao Liu"
                    },
                    {
                        "name": "Liang Zeng"
                    },
                    {
                        "name": "Rui Yan"
                    },
                    {
                        "name": "Yiwen Sun"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yahui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yahui Zhou"
                },
                "author": "Yahui Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18274v1",
                "updated": "2024-12-24T08:33:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    8,
                    33,
                    44,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T08:33:44Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    8,
                    33,
                    44,
                    1,
                    359,
                    0
                ],
                "title": "GenAI Content Detection Task 2: AI vs. Human -- Academic Essay\n  Authenticity Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenAI Content Detection Task 2: AI vs. Human -- Academic Essay\n  Authenticity Challenge"
                },
                "summary": "This paper presents a comprehensive overview of the first edition of the\nAcademic Essay Authenticity Challenge, organized as part of the GenAI Content\nDetection shared tasks collocated with COLING 2025. This challenge focuses on\ndetecting machine-generated vs. human-authored essays for academic purposes.\nThe task is defined as follows: \"Given an essay, identify whether it is\ngenerated by a machine or authored by a human.'' The challenge involves two\nlanguages: English and Arabic. During the evaluation phase, 25 teams submitted\nsystems for English and 21 teams for Arabic, reflecting substantial interest in\nthe task. Finally, seven teams submitted system description papers. The\nmajority of submissions utilized fine-tuned transformer-based models, with one\nteam employing Large Language Models (LLMs) such as Llama 2 and Llama 3. This\npaper outlines the task formulation, details the dataset construction process,\nand explains the evaluation framework. Additionally, we present a summary of\nthe approaches adopted by participating teams. Nearly all submitted systems\noutperformed the n-gram-based baseline, with the top-performing systems\nachieving F1 scores exceeding 0.98 for both languages, indicating significant\nprogress in the detection of machine-generated text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive overview of the first edition of the\nAcademic Essay Authenticity Challenge, organized as part of the GenAI Content\nDetection shared tasks collocated with COLING 2025. This challenge focuses on\ndetecting machine-generated vs. human-authored essays for academic purposes.\nThe task is defined as follows: \"Given an essay, identify whether it is\ngenerated by a machine or authored by a human.'' The challenge involves two\nlanguages: English and Arabic. During the evaluation phase, 25 teams submitted\nsystems for English and 21 teams for Arabic, reflecting substantial interest in\nthe task. Finally, seven teams submitted system description papers. The\nmajority of submissions utilized fine-tuned transformer-based models, with one\nteam employing Large Language Models (LLMs) such as Llama 2 and Llama 3. This\npaper outlines the task formulation, details the dataset construction process,\nand explains the evaluation framework. Additionally, we present a summary of\nthe approaches adopted by participating teams. Nearly all submitted systems\noutperformed the n-gram-based baseline, with the top-performing systems\nachieving F1 scores exceeding 0.98 for both languages, indicating significant\nprogress in the detection of machine-generated text."
                },
                "authors": [
                    {
                        "name": "Shammur Absar Chowdhury"
                    },
                    {
                        "name": "Hind Almerekhi"
                    },
                    {
                        "name": "Mucahid Kutlu"
                    },
                    {
                        "name": "Kaan Efe Keles"
                    },
                    {
                        "name": "Fatema Ahmad"
                    },
                    {
                        "name": "Tasnim Mohiuddin"
                    },
                    {
                        "name": "George Mikros"
                    },
                    {
                        "name": "Firoj Alam"
                    }
                ],
                "author_detail": {
                    "name": "Firoj Alam"
                },
                "author": "Firoj Alam",
                "arxiv_comment": "AI Generated Content, Academic Essay, LLMs, Arabic, English",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18270v1",
                "updated": "2024-12-24T08:29:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    8,
                    29,
                    0,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T08:29:00Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    8,
                    29,
                    0,
                    1,
                    359,
                    0
                ],
                "title": "Annotating References to Mythological Entities in French Literature",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Annotating References to Mythological Entities in French Literature"
                },
                "summary": "In this paper, we explore the relevance of large language models (LLMs) for\nannotating references to Roman and Greek mythological entities in modern and\ncontemporary French literature. We present an annotation scheme and demonstrate\nthat recent LLMs can be directly applied to follow this scheme effectively,\nalthough not without occasionally making significant analytical errors.\nAdditionally, we show that LLMs (and, more specifically, ChatGPT) are capable\nof offering interpretative insights into the use of mythological references by\nliterary authors. However, we also find that LLMs struggle to accurately\nidentify relevant passages in novels (when used as an information retrieval\nengine), often hallucinating and generating fabricated examples-an issue that\nraises significant ethical concerns. Nonetheless, when used carefully, LLMs\nremain valuable tools for performing annotations with high accuracy, especially\nfor tasks that would be difficult to annotate comprehensively on a large scale\nthrough manual methods alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we explore the relevance of large language models (LLMs) for\nannotating references to Roman and Greek mythological entities in modern and\ncontemporary French literature. We present an annotation scheme and demonstrate\nthat recent LLMs can be directly applied to follow this scheme effectively,\nalthough not without occasionally making significant analytical errors.\nAdditionally, we show that LLMs (and, more specifically, ChatGPT) are capable\nof offering interpretative insights into the use of mythological references by\nliterary authors. However, we also find that LLMs struggle to accurately\nidentify relevant passages in novels (when used as an information retrieval\nengine), often hallucinating and generating fabricated examples-an issue that\nraises significant ethical concerns. Nonetheless, when used carefully, LLMs\nremain valuable tools for performing annotations with high accuracy, especially\nfor tasks that would be difficult to annotate comprehensively on a large scale\nthrough manual methods alone."
                },
                "authors": [
                    {
                        "name": "Thierry Poibeau"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Poibeau"
                },
                "arxiv_affiliation": "Lattice",
                "author": "Thierry Poibeau",
                "arxiv_journal_ref": "CHR (Computational Humanities Research) -- Digital Methods for\n  Mythological Research Workshop, Dec 2024, Aarhus (Danemark), Denmark",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08719v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08719v2",
                "updated": "2024-12-24T08:24:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    8,
                    24,
                    5,
                    1,
                    359,
                    0
                ],
                "published": "2024-01-16T06:54:44Z",
                "published_parsed": [
                    2024,
                    1,
                    16,
                    6,
                    54,
                    44,
                    1,
                    16,
                    0
                ],
                "title": "CodeComplex: Dataset for Worst-Case Time Complexity Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeComplex: Dataset for Worst-Case Time Complexity Prediction"
                },
                "summary": "Reasoning ability of Large Language Models (LLMs) is a crucial ability,\nespecially in complex decision-making tasks. One significant task to show LLMs'\nreasoning capability is code time complexity prediction, which involves various\nintricate factors such as the input range of variables and conditional loops.\nCurrent benchmarks fall short of providing a rigorous assessment due to limited\ndata, language constraints, and insufficient labeling. They do not consider\ntime complexity based on input representation and merely evaluate whether\npredictions fall into the same class, lacking a measure of how close incorrect\npredictions are to the correct ones. To address these dependencies, we\nintroduce CodeComplex, the first robust and extensive dataset designed to\nevaluate LLMs' reasoning abilities in predicting code time complexity.\nCodeComplex comprises 4,900 Java codes and an equivalent number of Python\ncodes, overcoming language and labeling constraints, carefully annotated with\ncomplexity labels based on input characteristics by a panel of algorithmic\nexperts. Additionally, we propose specialized evaluation metrics for the\nreasoning of complexity prediction tasks, offering a more precise and reliable\nassessment of LLMs' reasoning capabilities. We release our dataset\n(https://github.com/sybaik1/CodeComplex-Data) and baseline models\n(https://github.com/sybaik1/CodeComplex-Models) publicly to encourage the\nrelevant (NLP, SE, and PL) communities to utilize and participate in this\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning ability of Large Language Models (LLMs) is a crucial ability,\nespecially in complex decision-making tasks. One significant task to show LLMs'\nreasoning capability is code time complexity prediction, which involves various\nintricate factors such as the input range of variables and conditional loops.\nCurrent benchmarks fall short of providing a rigorous assessment due to limited\ndata, language constraints, and insufficient labeling. They do not consider\ntime complexity based on input representation and merely evaluate whether\npredictions fall into the same class, lacking a measure of how close incorrect\npredictions are to the correct ones. To address these dependencies, we\nintroduce CodeComplex, the first robust and extensive dataset designed to\nevaluate LLMs' reasoning abilities in predicting code time complexity.\nCodeComplex comprises 4,900 Java codes and an equivalent number of Python\ncodes, overcoming language and labeling constraints, carefully annotated with\ncomplexity labels based on input characteristics by a panel of algorithmic\nexperts. Additionally, we propose specialized evaluation metrics for the\nreasoning of complexity prediction tasks, offering a more precise and reliable\nassessment of LLMs' reasoning capabilities. We release our dataset\n(https://github.com/sybaik1/CodeComplex-Data) and baseline models\n(https://github.com/sybaik1/CodeComplex-Models) publicly to encourage the\nrelevant (NLP, SE, and PL) communities to utilize and participate in this\nresearch."
                },
                "authors": [
                    {
                        "name": "Seung-Yeop Baik"
                    },
                    {
                        "name": "Joonghyuk Hahn"
                    },
                    {
                        "name": "Jungin Kim"
                    },
                    {
                        "name": "Mingi Jeon"
                    },
                    {
                        "name": "Aditi"
                    },
                    {
                        "name": "Yo-Sub Han"
                    },
                    {
                        "name": "Sang-Ki Ko"
                    }
                ],
                "author_detail": {
                    "name": "Sang-Ki Ko"
                },
                "author": "Sang-Ki Ko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08719v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08719v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18260v1",
                "updated": "2024-12-24T08:20:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    8,
                    20,
                    29,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T08:20:29Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    8,
                    20,
                    29,
                    1,
                    359,
                    0
                ],
                "title": "Investigating Large Language Models for Code Vulnerability Detection: An\n  Experimental Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Large Language Models for Code Vulnerability Detection: An\n  Experimental Study"
                },
                "summary": "Code vulnerability detection (CVD) is essential for addressing and preventing\nsystem security issues, playing a crucial role in ensuring software security.\nPrevious learning-based vulnerability detection methods rely on either\nfine-tuning medium-size sequence models or training smaller neural networks\nfrom scratch. Recent advancements in large pre-trained language models (LLMs)\nhave showcased remarkable capabilities in various code intelligence tasks\nincluding code understanding and generation. However, the effectiveness of LLMs\nin detecting code vulnerabilities is largely under-explored. This work aims to\ninvestigate the gap by fine-tuning LLMs for the CVD task, involving four\nwidely-used open-source LLMs. We also implement other five previous graph-based\nor medium-size sequence models for comparison. Experiments are conducted on\nfive commonly-used CVD datasets, including both the part of short samples and\nlong samples. In addition, we conduct quantitative experiments to investigate\nthe class imbalance issue and the model's performance on samples of different\nlengths, which are rarely studied in previous works. To better facilitate\ncommunities, we open-source all codes and resources of this study in\nhttps://github.com/SakiRinn/LLM4CVD and\nhttps://huggingface.co/datasets/xuefen/VulResource.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code vulnerability detection (CVD) is essential for addressing and preventing\nsystem security issues, playing a crucial role in ensuring software security.\nPrevious learning-based vulnerability detection methods rely on either\nfine-tuning medium-size sequence models or training smaller neural networks\nfrom scratch. Recent advancements in large pre-trained language models (LLMs)\nhave showcased remarkable capabilities in various code intelligence tasks\nincluding code understanding and generation. However, the effectiveness of LLMs\nin detecting code vulnerabilities is largely under-explored. This work aims to\ninvestigate the gap by fine-tuning LLMs for the CVD task, involving four\nwidely-used open-source LLMs. We also implement other five previous graph-based\nor medium-size sequence models for comparison. Experiments are conducted on\nfive commonly-used CVD datasets, including both the part of short samples and\nlong samples. In addition, we conduct quantitative experiments to investigate\nthe class imbalance issue and the model's performance on samples of different\nlengths, which are rarely studied in previous works. To better facilitate\ncommunities, we open-source all codes and resources of this study in\nhttps://github.com/SakiRinn/LLM4CVD and\nhttps://huggingface.co/datasets/xuefen/VulResource."
                },
                "authors": [
                    {
                        "name": "Xuefeng Jiang"
                    },
                    {
                        "name": "Lvhua Wu"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Jingjing Xue"
                    },
                    {
                        "name": "Yuwei Wang"
                    },
                    {
                        "name": "Tingting Wu"
                    },
                    {
                        "name": "Min Liu"
                    }
                ],
                "author_detail": {
                    "name": "Min Liu"
                },
                "author": "Min Liu",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16597v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16597v2",
                "updated": "2024-12-24T08:14:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    8,
                    14,
                    36,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-21T12:03:31Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    12,
                    3,
                    31,
                    5,
                    356,
                    0
                ],
                "title": "LLMs Enable Context-Aware Augmented Reality in Surgical Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Enable Context-Aware Augmented Reality in Surgical Navigation"
                },
                "summary": "Wearable Augmented Reality (AR) technologies are gaining recognition for\ntheir potential to transform surgical navigation systems. As these technologies\nevolve, selecting the right interaction method to control the system becomes\ncrucial. Our work introduces a voice-controlled user interface (VCUI) for\nsurgical AR assistance systems (ARAS), designed for pancreatic surgery, that\nintegrates Large Language Models (LLMs). Employing a mixed-method research\napproach, we assessed the usability of our LLM-based design in both simulated\nsurgical tasks and during pancreatic surgeries, comparing its performance\nagainst conventional VCUI for surgical ARAS using speech commands. Our findings\ndemonstrated the usability of our proposed LLM-based VCUI, yielding a\nsignificantly lower task completion time and cognitive workload compared to\nspeech commands. Additionally, qualitative insights from interviews with\nsurgeons aligned with the quantitative data, revealing a strong preference for\nthe LLM-based VCUI. Surgeons emphasized its intuitiveness and highlighted the\npotential of LLM-based VCUI in expediting decision-making in surgical\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wearable Augmented Reality (AR) technologies are gaining recognition for\ntheir potential to transform surgical navigation systems. As these technologies\nevolve, selecting the right interaction method to control the system becomes\ncrucial. Our work introduces a voice-controlled user interface (VCUI) for\nsurgical AR assistance systems (ARAS), designed for pancreatic surgery, that\nintegrates Large Language Models (LLMs). Employing a mixed-method research\napproach, we assessed the usability of our LLM-based design in both simulated\nsurgical tasks and during pancreatic surgeries, comparing its performance\nagainst conventional VCUI for surgical ARAS using speech commands. Our findings\ndemonstrated the usability of our proposed LLM-based VCUI, yielding a\nsignificantly lower task completion time and cognitive workload compared to\nspeech commands. Additionally, qualitative insights from interviews with\nsurgeons aligned with the quantitative data, revealing a strong preference for\nthe LLM-based VCUI. Surgeons emphasized its intuitiveness and highlighted the\npotential of LLM-based VCUI in expediting decision-making in surgical\nenvironments."
                },
                "authors": [
                    {
                        "name": "Hamraz Javaheri"
                    },
                    {
                        "name": "Omid Ghamarnejad"
                    },
                    {
                        "name": "Paul Lukowicz"
                    },
                    {
                        "name": "Gregor Alexander Stavrou"
                    },
                    {
                        "name": "Jakob Karolus"
                    }
                ],
                "author_detail": {
                    "name": "Jakob Karolus"
                },
                "author": "Jakob Karolus",
                "arxiv_comment": "32 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16597v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16597v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15342v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15342v2",
                "updated": "2024-12-24T08:13:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    8,
                    13,
                    43,
                    1,
                    359,
                    0
                ],
                "published": "2024-10-20T09:32:03Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    9,
                    32,
                    3,
                    6,
                    294,
                    0
                ],
                "title": "ConSinger: Efficient High-Fidelity Singing Voice Generation with Minimal\n  Steps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConSinger: Efficient High-Fidelity Singing Voice Generation with Minimal\n  Steps"
                },
                "summary": "Singing voice synthesis (SVS) system is expected to generate high-fidelity\nsinging voice from given music scores (lyrics, duration and pitch). Recently,\ndiffusion models have performed well in this field. However, sacrificing\ninference speed to exchange with high-quality sample generation limits its\napplication scenarios. In order to obtain high quality synthetic singing voice\nmore efficiently, we propose a singing voice synthesis method based on the\nconsistency model, ConSinger, to achieve high-fidelity singing voice synthesis\nwith minimal steps. The model is trained by applying consistency constraint and\nthe generation quality is greatly improved at the expense of a small amount of\ninference speed. Our experiments show that ConSinger is highly competitive with\nthe baseline model in terms of generation speed and quality. Audio samples are\navailable at https://keylxiao.github.io/consinger.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Singing voice synthesis (SVS) system is expected to generate high-fidelity\nsinging voice from given music scores (lyrics, duration and pitch). Recently,\ndiffusion models have performed well in this field. However, sacrificing\ninference speed to exchange with high-quality sample generation limits its\napplication scenarios. In order to obtain high quality synthetic singing voice\nmore efficiently, we propose a singing voice synthesis method based on the\nconsistency model, ConSinger, to achieve high-fidelity singing voice synthesis\nwith minimal steps. The model is trained by applying consistency constraint and\nthe generation quality is greatly improved at the expense of a small amount of\ninference speed. Our experiments show that ConSinger is highly competitive with\nthe baseline model in terms of generation speed and quality. Audio samples are\navailable at https://keylxiao.github.io/consinger."
                },
                "authors": [
                    {
                        "name": "Yulin Song"
                    },
                    {
                        "name": "Guorui Sang"
                    },
                    {
                        "name": "Jing Yu"
                    },
                    {
                        "name": "Chuangbai Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Chuangbai Xiao"
                },
                "author": "Chuangbai Xiao",
                "arxiv_comment": "Singing voice synthesis, Consistency models, Shallow Diffusion\n  Mechanism; Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15342v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15342v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11887v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11887v2",
                "updated": "2024-12-24T08:03:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    8,
                    3,
                    47,
                    1,
                    359,
                    0
                ],
                "published": "2024-10-12T04:03:36Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    4,
                    3,
                    36,
                    5,
                    286,
                    0
                ],
                "title": "Thermal Comfort in Sight: Thermal Affordance and its Visual Assessment\n  for Sustainable Streetscape Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thermal Comfort in Sight: Thermal Affordance and its Visual Assessment\n  for Sustainable Streetscape Design"
                },
                "summary": "In response to climate change and urban heat island effects, enhancing human\nthermal comfort in cities is crucial for sustainable urban development.\nTraditional methods for investigating the urban thermal environment and\ncorresponding human thermal comfort level are often resource intensive,\ninefficient, and limited in scope. To address these challenges, we (1)\nintroduce a new concept named thermal affordance, which formalizes the\nintegrated inherent capacity of a streetscape to influence human thermal\ncomfort based on its visual and physical features; and (2) an efficient method\nto evaluate it (visual assessment of thermal affordance -- VATA), which\ncombines street view imagery (SVI), online and in-field surveys, and\nstatistical learning algorithms. VATA extracts five categories of image\nfeatures from SVI data and establishes 19 visual-perceptual indicators for\nstreetscape visual assessment. Using a multi-task neural network and elastic\nnet regression, we model their chained relationship to predict and comprehend\nthermal affordance for Singapore. VATA predictions are validated with\nfield-investigated OTC data, providing a cost-effective, scalable, and\ntransferable method to assess the thermal comfort potential of urban\nstreetscape. Moreover, we demonstrate its utility by generating a geospatially\nexplicit mapping of thermal affordance, outlining a model update workflow for\nlong-term urban-scale analysis, and implementing a two-stage prediction and\ninference approach (IF-VPI-VATA) to guide future streetscape improvements. This\nframework can inform streetscape design to support sustainable, liveable, and\nresilient urban environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In response to climate change and urban heat island effects, enhancing human\nthermal comfort in cities is crucial for sustainable urban development.\nTraditional methods for investigating the urban thermal environment and\ncorresponding human thermal comfort level are often resource intensive,\ninefficient, and limited in scope. To address these challenges, we (1)\nintroduce a new concept named thermal affordance, which formalizes the\nintegrated inherent capacity of a streetscape to influence human thermal\ncomfort based on its visual and physical features; and (2) an efficient method\nto evaluate it (visual assessment of thermal affordance -- VATA), which\ncombines street view imagery (SVI), online and in-field surveys, and\nstatistical learning algorithms. VATA extracts five categories of image\nfeatures from SVI data and establishes 19 visual-perceptual indicators for\nstreetscape visual assessment. Using a multi-task neural network and elastic\nnet regression, we model their chained relationship to predict and comprehend\nthermal affordance for Singapore. VATA predictions are validated with\nfield-investigated OTC data, providing a cost-effective, scalable, and\ntransferable method to assess the thermal comfort potential of urban\nstreetscape. Moreover, we demonstrate its utility by generating a geospatially\nexplicit mapping of thermal affordance, outlining a model update workflow for\nlong-term urban-scale analysis, and implementing a two-stage prediction and\ninference approach (IF-VPI-VATA) to guide future streetscape improvements. This\nframework can inform streetscape design to support sustainable, liveable, and\nresilient urban environments."
                },
                "authors": [
                    {
                        "name": "Sijie Yang"
                    },
                    {
                        "name": "Adrian Chong"
                    },
                    {
                        "name": "Pengyuan Liu"
                    },
                    {
                        "name": "Filip Biljecki"
                    }
                ],
                "author_detail": {
                    "name": "Filip Biljecki"
                },
                "author": "Filip Biljecki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11887v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11887v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18241v1",
                "updated": "2024-12-24T07:51:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    7,
                    51,
                    29,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T07:51:29Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    7,
                    51,
                    29,
                    1,
                    359,
                    0
                ],
                "title": "An Automatic Graph Construction Framework based on Large Language Models\n  for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Automatic Graph Construction Framework based on Large Language Models\n  for Recommendation"
                },
                "summary": "Graph neural networks (GNNs) have emerged as state-of-the-art methods to\nlearn from graph-structured data for recommendation. However, most existing\nGNN-based recommendation methods focus on the optimization of model structures\nand learning strategies based on pre-defined graphs, neglecting the importance\nof the graph construction stage. Earlier works for graph construction usually\nrely on speciffic rules or crowdsourcing, which are either too simplistic or\ntoo labor-intensive. Recent works start to utilize large language models (LLMs)\nto automate the graph construction, in view of their abundant open-world\nknowledge and remarkable reasoning capabilities. Nevertheless, they generally\nsuffer from two limitations: (1) invisibility of global view (e.g., overlooking\ncontextual information) and (2) construction inefficiency. To this end, we\nintroduce AutoGraph, an automatic graph construction framework based on LLMs\nfor recommendation. Specifically, we first use LLMs to infer the user\npreference and item knowledge, which is encoded as semantic vectors. Next, we\nemploy vector quantization to extract the latent factors from the semantic\nvectors. The latent factors are then incorporated as extra nodes to link the\nuser/item nodes, resulting in a graph with in-depth global-view semantics. We\nfurther design metapath-based message aggregation to effectively aggregate the\nsemantic and collaborative information. The framework is model-agnostic and\ncompatible with different backbone models. Extensive experiments on three\nreal-world datasets demonstrate the efficacy and efffciency of AutoGraph\ncompared to existing baseline methods. We have deployed AutoGraph in Huawei\nadvertising platform, and gain a 2.69% improvement on RPM and a 7.31%\nimprovement on eCPM in the online A/B test. Currently AutoGraph has been used\nas the main trafffc model, serving hundreds of millions of people.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) have emerged as state-of-the-art methods to\nlearn from graph-structured data for recommendation. However, most existing\nGNN-based recommendation methods focus on the optimization of model structures\nand learning strategies based on pre-defined graphs, neglecting the importance\nof the graph construction stage. Earlier works for graph construction usually\nrely on speciffic rules or crowdsourcing, which are either too simplistic or\ntoo labor-intensive. Recent works start to utilize large language models (LLMs)\nto automate the graph construction, in view of their abundant open-world\nknowledge and remarkable reasoning capabilities. Nevertheless, they generally\nsuffer from two limitations: (1) invisibility of global view (e.g., overlooking\ncontextual information) and (2) construction inefficiency. To this end, we\nintroduce AutoGraph, an automatic graph construction framework based on LLMs\nfor recommendation. Specifically, we first use LLMs to infer the user\npreference and item knowledge, which is encoded as semantic vectors. Next, we\nemploy vector quantization to extract the latent factors from the semantic\nvectors. The latent factors are then incorporated as extra nodes to link the\nuser/item nodes, resulting in a graph with in-depth global-view semantics. We\nfurther design metapath-based message aggregation to effectively aggregate the\nsemantic and collaborative information. The framework is model-agnostic and\ncompatible with different backbone models. Extensive experiments on three\nreal-world datasets demonstrate the efficacy and efffciency of AutoGraph\ncompared to existing baseline methods. We have deployed AutoGraph in Huawei\nadvertising platform, and gain a 2.69% improvement on RPM and a 7.31%\nimprovement on eCPM in the online A/B test. Currently AutoGraph has been used\nas the main trafffc model, serving hundreds of millions of people."
                },
                "authors": [
                    {
                        "name": "Rong Shan"
                    },
                    {
                        "name": "Jianghao Lin"
                    },
                    {
                        "name": "Chenxu Zhu"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Menghui Zhu"
                    },
                    {
                        "name": "Kangning Zhang"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Yong Yu"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02688v2",
                "updated": "2024-12-24T07:47:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    7,
                    47,
                    2,
                    1,
                    359,
                    0
                ],
                "published": "2024-11-05T00:16:01Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    0,
                    16,
                    1,
                    1,
                    310,
                    0
                ],
                "title": "On the loss of context-awareness in general instruction fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the loss of context-awareness in general instruction fine-tuning"
                },
                "summary": "Pre-trained Large Language Models (LLMs) require post-training methods such\nas supervised fine-tuning (SFT) on instruction-response pairs to enable\ninstruction following. However, this process can potentially harm existing\ncapabilities learned during pre-training. In this paper, we investigate the\nloss of context awareness after SFT, where context awareness is defined as the\nability to extract and understand information from user-provided context and\nrespond accordingly. We are the first to identify and show that the loss of\ncontext awareness, as reflected by the performance drop in the\nNeedle-in-a-Haystack test, occurs in instruction fine-tuned LLMs when the chat\ntemplate is applied to input prompts. We identify that the performance decline\nis partially caused by an attention bias toward different roles learned during\nconversational instruction fine-tuning. We validate our hypothesis by\nvisualizing changes in attention allocation after the chat template is applied\nand manually steering the attention heads. Based on these observations, we\npropose a metric to select context-dependent examples from general instruction\nfine-tuning datasets. We then apply conditional instruction fine-tuning with a\ncontext-dependency indicator, enabling the model to learn context awareness\nfrom these selected examples. Empirical experiments on four context-dependent\ndownstream tasks and three pre-trained LLMs of different sizes show that our\nmethod effectively mitigates the loss of context awareness without compromising\ngeneral instruction-following capabilities. Given our findings, we strongly\nadvocate for careful benchmarking of context awareness after instruction\nfine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained Large Language Models (LLMs) require post-training methods such\nas supervised fine-tuning (SFT) on instruction-response pairs to enable\ninstruction following. However, this process can potentially harm existing\ncapabilities learned during pre-training. In this paper, we investigate the\nloss of context awareness after SFT, where context awareness is defined as the\nability to extract and understand information from user-provided context and\nrespond accordingly. We are the first to identify and show that the loss of\ncontext awareness, as reflected by the performance drop in the\nNeedle-in-a-Haystack test, occurs in instruction fine-tuned LLMs when the chat\ntemplate is applied to input prompts. We identify that the performance decline\nis partially caused by an attention bias toward different roles learned during\nconversational instruction fine-tuning. We validate our hypothesis by\nvisualizing changes in attention allocation after the chat template is applied\nand manually steering the attention heads. Based on these observations, we\npropose a metric to select context-dependent examples from general instruction\nfine-tuning datasets. We then apply conditional instruction fine-tuning with a\ncontext-dependency indicator, enabling the model to learn context awareness\nfrom these selected examples. Empirical experiments on four context-dependent\ndownstream tasks and three pre-trained LLMs of different sizes show that our\nmethod effectively mitigates the loss of context awareness without compromising\ngeneral instruction-following capabilities. Given our findings, we strongly\nadvocate for careful benchmarking of context awareness after instruction\nfine-tuning."
                },
                "authors": [
                    {
                        "name": "Yihan Wang"
                    },
                    {
                        "name": "Andrew Bai"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Cho-Jui Hsieh"
                    }
                ],
                "author_detail": {
                    "name": "Cho-Jui Hsieh"
                },
                "author": "Cho-Jui Hsieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2110.09823v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2110.09823v5",
                "updated": "2024-12-24T07:36:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    7,
                    36,
                    33,
                    1,
                    359,
                    0
                ],
                "published": "2021-10-19T10:15:00Z",
                "published_parsed": [
                    2021,
                    10,
                    19,
                    10,
                    15,
                    0,
                    1,
                    292,
                    0
                ],
                "title": "An Empirical Study: Extensive Deep Temporal Point Process",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study: Extensive Deep Temporal Point Process"
                },
                "summary": "Temporal point process as the stochastic process on continuous domain of time\nis commonly used to model the asynchronous event sequence featuring with\noccurrence timestamps. Thanks to the strong expressivity of deep neural\nnetworks, they are emerging as a promising choice for capturing the patterns in\nasynchronous sequences, in the context of temporal point process. In this\npaper, we first review recent research emphasis and difficulties in modeling\nasynchronous event sequences with deep temporal point process, which can be\nconcluded into four fields: encoding of history sequence, formulation of\nconditional intensity function, relational discovery of events and learning\napproaches for optimization. We introduce most of recently proposed models by\ndismantling them into the four parts, and conduct experiments by remodularizing\nthe first three parts with the same learning strategy for a fair empirical\nevaluation. Besides, we extend the history encoders and conditional intensity\nfunction family, and propose a Granger causality discovery framework for\nexploiting the relations among multi-types of events. Because the Granger\ncausality can be represented by the Granger causality graph, discrete graph\nstructure learning in the framework of Variational Inference is employed to\nreveal latent structures of the graph. Further experiments show that the\nproposed framework with latent graph discovery can both capture the relations\nand achieve an improved fitting and predicting performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal point process as the stochastic process on continuous domain of time\nis commonly used to model the asynchronous event sequence featuring with\noccurrence timestamps. Thanks to the strong expressivity of deep neural\nnetworks, they are emerging as a promising choice for capturing the patterns in\nasynchronous sequences, in the context of temporal point process. In this\npaper, we first review recent research emphasis and difficulties in modeling\nasynchronous event sequences with deep temporal point process, which can be\nconcluded into four fields: encoding of history sequence, formulation of\nconditional intensity function, relational discovery of events and learning\napproaches for optimization. We introduce most of recently proposed models by\ndismantling them into the four parts, and conduct experiments by remodularizing\nthe first three parts with the same learning strategy for a fair empirical\nevaluation. Besides, we extend the history encoders and conditional intensity\nfunction family, and propose a Granger causality discovery framework for\nexploiting the relations among multi-types of events. Because the Granger\ncausality can be represented by the Granger causality graph, discrete graph\nstructure learning in the framework of Variational Inference is employed to\nreveal latent structures of the graph. Further experiments show that the\nproposed framework with latent graph discovery can both capture the relations\nand achieve an improved fitting and predicting performance."
                },
                "authors": [
                    {
                        "name": "Haitao Lin"
                    },
                    {
                        "name": "Cheng Tan"
                    },
                    {
                        "name": "Lirong Wu"
                    },
                    {
                        "name": "Zhangyang Gao"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Stan. Z. Li"
                    }
                ],
                "author_detail": {
                    "name": "Stan. Z. Li"
                },
                "author": "Stan. Z. Li",
                "arxiv_comment": "22 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2110.09823v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2110.09823v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18232v1",
                "updated": "2024-12-24T07:30:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    7,
                    30,
                    55,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T07:30:55Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    7,
                    30,
                    55,
                    1,
                    359,
                    0
                ],
                "title": "Efficient Long Context Language Model Retrieval with Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Long Context Language Model Retrieval with Compression"
                },
                "summary": "Long Context Language Models (LCLMs) have emerged as a new paradigm to\nperform Information Retrieval (IR), which enables the direct ingestion and\nretrieval of information by processing an entire corpus in their single\ncontext, showcasing the potential to surpass traditional sparse and dense\nretrieval methods. However, processing a large number of passages within\nin-context for retrieval is computationally expensive, and handling their\nrepresentations during inference further exacerbates the processing time; thus,\nwe aim to make LCLM retrieval more efficient and potentially more effective\nwith passage compression. Specifically, we propose a new compression approach\ntailored for LCLM retrieval, which is trained to maximize the retrieval\nperformance while minimizing the length of the compressed passages. To\naccomplish this, we generate the synthetic data, where compressed passages are\nautomatically created and labeled as chosen or rejected according to their\nretrieval success for a given query, and we train the proposed Compression\nmodel for Long context Retrieval (CoLoR) with this data via preference\noptimization while adding the length regularization loss on top of it to\nenforce brevity. Through extensive experiments on 9 datasets, we show that\nCoLoR improves the retrieval performance by 6% while compressing the in-context\nsize by a factor of 1.91.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Context Language Models (LCLMs) have emerged as a new paradigm to\nperform Information Retrieval (IR), which enables the direct ingestion and\nretrieval of information by processing an entire corpus in their single\ncontext, showcasing the potential to surpass traditional sparse and dense\nretrieval methods. However, processing a large number of passages within\nin-context for retrieval is computationally expensive, and handling their\nrepresentations during inference further exacerbates the processing time; thus,\nwe aim to make LCLM retrieval more efficient and potentially more effective\nwith passage compression. Specifically, we propose a new compression approach\ntailored for LCLM retrieval, which is trained to maximize the retrieval\nperformance while minimizing the length of the compressed passages. To\naccomplish this, we generate the synthetic data, where compressed passages are\nautomatically created and labeled as chosen or rejected according to their\nretrieval success for a given query, and we train the proposed Compression\nmodel for Long context Retrieval (CoLoR) with this data via preference\noptimization while adding the length regularization loss on top of it to\nenforce brevity. Through extensive experiments on 9 datasets, we show that\nCoLoR improves the retrieval performance by 6% while compressing the in-context\nsize by a factor of 1.91."
                },
                "authors": [
                    {
                        "name": "Minju Seo"
                    },
                    {
                        "name": "Jinheon Baek"
                    },
                    {
                        "name": "Seongyun Lee"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18225v1",
                "updated": "2024-12-24T07:15:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    7,
                    15,
                    48,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T07:15:48Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    7,
                    15,
                    48,
                    1,
                    359,
                    0
                ],
                "title": "Combining GPT and Code-Based Similarity Checking for Effective Smart\n  Contract Vulnerability Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining GPT and Code-Based Similarity Checking for Effective Smart\n  Contract Vulnerability Detection"
                },
                "summary": "With the rapid growth of blockchain technology, smart contracts are now\ncrucial to Decentralized Finance (DeFi) applications. Effective vulnerability\ndetection is vital for securing these contracts against hackers and enhancing\nthe accuracy and efficiency of security audits. In this paper, we present\nSimilarGPT, a unique vulnerability identification tool for smart contract,\nwhich combines Generative Pretrained Transformer (GPT) models with Code-based\nsimilarity checking methods. The main concept of the SimilarGPT tool is to\nmeasure the similarity between the code under inspection and the secure code\nfrom third-party libraries. To identify potential vulnerabilities, we connect\nthe semantic understanding capability of large language models (LLMs) with\nCode-based similarity checking techniques. We propose optimizing the detection\nsequence using topological ordering to enhance logical coherence and reduce\nfalse positives during detection. Through analysis of code reuse patterns in\nsmart contracts, we compile and process extensive third-party library code to\nestablish a comprehensive reference codebase. Then, we utilize LLM to conduct\nan indepth analysis of similar codes to identify and explain potential\nvulnerabilities in the codes. The experimental findings indicate that\nSimilarGPT excels in detecting vulnerabilities in smart contracts, particularly\nin missed detections and minimizing false positives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of blockchain technology, smart contracts are now\ncrucial to Decentralized Finance (DeFi) applications. Effective vulnerability\ndetection is vital for securing these contracts against hackers and enhancing\nthe accuracy and efficiency of security audits. In this paper, we present\nSimilarGPT, a unique vulnerability identification tool for smart contract,\nwhich combines Generative Pretrained Transformer (GPT) models with Code-based\nsimilarity checking methods. The main concept of the SimilarGPT tool is to\nmeasure the similarity between the code under inspection and the secure code\nfrom third-party libraries. To identify potential vulnerabilities, we connect\nthe semantic understanding capability of large language models (LLMs) with\nCode-based similarity checking techniques. We propose optimizing the detection\nsequence using topological ordering to enhance logical coherence and reduce\nfalse positives during detection. Through analysis of code reuse patterns in\nsmart contracts, we compile and process extensive third-party library code to\nestablish a comprehensive reference codebase. Then, we utilize LLM to conduct\nan indepth analysis of similar codes to identify and explain potential\nvulnerabilities in the codes. The experimental findings indicate that\nSimilarGPT excels in detecting vulnerabilities in smart contracts, particularly\nin missed detections and minimizing false positives."
                },
                "authors": [
                    {
                        "name": "Jango Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jango Zhang"
                },
                "author": "Jango Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08685v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08685v3",
                "updated": "2024-12-24T07:08:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    7,
                    8,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-08-16T11:58:34Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    58,
                    34,
                    4,
                    229,
                    0
                ],
                "title": "Can Large Language Models Improve the Adversarial Robustness of Graph\n  Neural Networks?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Improve the Adversarial Robustness of Graph\n  Neural Networks?"
                },
                "summary": "Graph neural networks (GNNs) are vulnerable to adversarial attacks,\nespecially for topology perturbations, and many methods that improve the\nrobustness of GNNs have received considerable attention. Recently, we have\nwitnessed the significant success of large language models (LLMs), leading many\nto explore the great potential of LLMs on GNNs. However, they mainly focus on\nimproving the performance of GNNs by utilizing LLMs to enhance the node\nfeatures. Therefore, we ask: Will the robustness of GNNs also be enhanced with\nthe powerful understanding and inference capabilities of LLMs? By presenting\nthe empirical results, we find that despite that LLMs can improve the\nrobustness of GNNs, there is still an average decrease of 23.1% in accuracy,\nimplying that the GNNs remain extremely vulnerable against topology attacks.\nTherefore, another question is how to extend the capabilities of LLMs on graph\nadversarial robustness. In this paper, we propose an LLM-based robust graph\nstructure inference framework, LLM4RGNN, which distills the inference\ncapabilities of GPT-4 into a local LLM for identifying malicious edges and an\nLM-based edge predictor for finding missing important edges, so as to recover a\nrobust graph structure. Extensive experiments demonstrate that LLM4RGNN\nconsistently improves the robustness across various GNNs. Even in some cases\nwhere the perturbation ratio increases to 40%, the accuracy of GNNs is still\nbetter than that on the clean graph. The source code can be found in\nhttps://github.com/zhongjian-zhang/LLM4RGNN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) are vulnerable to adversarial attacks,\nespecially for topology perturbations, and many methods that improve the\nrobustness of GNNs have received considerable attention. Recently, we have\nwitnessed the significant success of large language models (LLMs), leading many\nto explore the great potential of LLMs on GNNs. However, they mainly focus on\nimproving the performance of GNNs by utilizing LLMs to enhance the node\nfeatures. Therefore, we ask: Will the robustness of GNNs also be enhanced with\nthe powerful understanding and inference capabilities of LLMs? By presenting\nthe empirical results, we find that despite that LLMs can improve the\nrobustness of GNNs, there is still an average decrease of 23.1% in accuracy,\nimplying that the GNNs remain extremely vulnerable against topology attacks.\nTherefore, another question is how to extend the capabilities of LLMs on graph\nadversarial robustness. In this paper, we propose an LLM-based robust graph\nstructure inference framework, LLM4RGNN, which distills the inference\ncapabilities of GPT-4 into a local LLM for identifying malicious edges and an\nLM-based edge predictor for finding missing important edges, so as to recover a\nrobust graph structure. Extensive experiments demonstrate that LLM4RGNN\nconsistently improves the robustness across various GNNs. Even in some cases\nwhere the perturbation ratio increases to 40%, the accuracy of GNNs is still\nbetter than that on the clean graph. The source code can be found in\nhttps://github.com/zhongjian-zhang/LLM4RGNN."
                },
                "authors": [
                    {
                        "name": "Zhongjian Zhang"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Huichi Zhou"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Mengmei Zhang"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Chuan Shi"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Shi"
                },
                "author": "Chuan Shi",
                "arxiv_doi": "10.1145/3690624.3709256",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3690624.3709256",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.08685v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08685v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "accepted by KDD 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18219v1",
                "updated": "2024-12-24T06:57:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    6,
                    57,
                    16,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T06:57:16Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    6,
                    57,
                    16,
                    1,
                    359,
                    0
                ],
                "title": "Adapter Merging with Centroid Prototype Mapping for Scalable\n  Class-Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapter Merging with Centroid Prototype Mapping for Scalable\n  Class-Incremental Learning"
                },
                "summary": "We propose Adapter Merging with Centroid Prototype Mapping (ACMap), an\nexemplar-free framework for class-incremental learning (CIL) that addresses\nboth catastrophic forgetting and scalability. While existing methods trade-off\nbetween inference time and accuracy, ACMap consolidates task-specific adapters\ninto a single adapter, ensuring constant inference time across tasks without\ncompromising accuracy. The framework employs adapter merging to build a shared\nsubspace that aligns task representations and mitigates forgetting, while\ncentroid prototype mapping maintains high accuracy through consistent\nadaptation in the shared subspace. To further improve scalability, an early\nstopping strategy limits adapter merging as tasks increase. Extensive\nexperiments on five benchmark datasets demonstrate that ACMap matches\nstate-of-the-art accuracy while maintaining inference time comparable to the\nfastest existing methods. The code is available at\nhttps://github.com/tf63/ACMap",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Adapter Merging with Centroid Prototype Mapping (ACMap), an\nexemplar-free framework for class-incremental learning (CIL) that addresses\nboth catastrophic forgetting and scalability. While existing methods trade-off\nbetween inference time and accuracy, ACMap consolidates task-specific adapters\ninto a single adapter, ensuring constant inference time across tasks without\ncompromising accuracy. The framework employs adapter merging to build a shared\nsubspace that aligns task representations and mitigates forgetting, while\ncentroid prototype mapping maintains high accuracy through consistent\nadaptation in the shared subspace. To further improve scalability, an early\nstopping strategy limits adapter merging as tasks increase. Extensive\nexperiments on five benchmark datasets demonstrate that ACMap matches\nstate-of-the-art accuracy while maintaining inference time comparable to the\nfastest existing methods. The code is available at\nhttps://github.com/tf63/ACMap"
                },
                "authors": [
                    {
                        "name": "Takuma Fukuda"
                    },
                    {
                        "name": "Hiroshi Kera"
                    },
                    {
                        "name": "Kazuhiko Kawamoto"
                    }
                ],
                "author_detail": {
                    "name": "Kazuhiko Kawamoto"
                },
                "author": "Kazuhiko Kawamoto",
                "arxiv_comment": "11 pages (main text), 6 pages (supplementary material)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16220v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16220v2",
                "updated": "2024-12-24T06:17:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    6,
                    17,
                    31,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-18T10:56:40Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    10,
                    56,
                    40,
                    2,
                    353,
                    0
                ],
                "title": "Cross-Attention Graph Neural Networks for Inferring Gene Regulatory\n  Networks with Skewed Degree Distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Attention Graph Neural Networks for Inferring Gene Regulatory\n  Networks with Skewed Degree Distribution"
                },
                "summary": "Inferencing Gene Regulatory Networks (GRNs) from gene expression data is a\npivotal challenge in systems biology, and several innovative computational\nmethods have been introduced. However, most of these studies have not\nconsidered the skewed degree distribution of genes. Specifically, some genes\nmay regulate multiple target genes while some genes may be regulated by\nmultiple regulator genes. Such a skewed degree distribution issue significantly\ncomplicates the application of directed graph embedding methods. To tackle this\nissue, we propose the Cross-Attention Complex Dual Graph Embedding Model\n(XATGRN). Our XATGRN employs a cross-attention mechanism to effectively capture\nintricate gene interactions from gene expression profiles. Additionally, it\nuses a Dual Complex Graph Embedding approach to manage the skewed degree\ndistribution, thereby ensuring precise prediction of regulatory relationships\nand their directionality. Our model consistently outperforms existing\nstate-of-the-art methods across various datasets, underscoring its efficacy in\nelucidating complex gene regulatory mechanisms. Our codes used in this paper\nare publicly available at: https://github.com/kikixiong/XATGRN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferencing Gene Regulatory Networks (GRNs) from gene expression data is a\npivotal challenge in systems biology, and several innovative computational\nmethods have been introduced. However, most of these studies have not\nconsidered the skewed degree distribution of genes. Specifically, some genes\nmay regulate multiple target genes while some genes may be regulated by\nmultiple regulator genes. Such a skewed degree distribution issue significantly\ncomplicates the application of directed graph embedding methods. To tackle this\nissue, we propose the Cross-Attention Complex Dual Graph Embedding Model\n(XATGRN). Our XATGRN employs a cross-attention mechanism to effectively capture\nintricate gene interactions from gene expression profiles. Additionally, it\nuses a Dual Complex Graph Embedding approach to manage the skewed degree\ndistribution, thereby ensuring precise prediction of regulatory relationships\nand their directionality. Our model consistently outperforms existing\nstate-of-the-art methods across various datasets, underscoring its efficacy in\nelucidating complex gene regulatory mechanisms. Our codes used in this paper\nare publicly available at: https://github.com/kikixiong/XATGRN."
                },
                "authors": [
                    {
                        "name": "Jiaqi Xiong"
                    },
                    {
                        "name": "Nan Yin"
                    },
                    {
                        "name": "Yifan Sun"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yingxu Wang"
                    },
                    {
                        "name": "Duo Ai"
                    },
                    {
                        "name": "Fang Pan"
                    },
                    {
                        "name": "Shiyang Liang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyang Liang"
                },
                "author": "Shiyang Liang",
                "arxiv_comment": "11 pages, 6 figures,1 tabels",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16220v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16220v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18200v1",
                "updated": "2024-12-24T06:11:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    6,
                    11,
                    10,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T06:11:10Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    6,
                    11,
                    10,
                    1,
                    359,
                    0
                ],
                "title": "Adapting Large Language Models for Improving TCP Fairness over WiFi",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Large Language Models for Improving TCP Fairness over WiFi"
                },
                "summary": "The new transmission control protocol (TCP) relies on Deep Learning (DL) for\nprediction and optimization, but requires significant manual effort to design\ndeep neural networks (DNNs) and struggles with generalization in dynamic\nenvironments. Inspired by the success of large language models (LLMs), this\nstudy proposes TCP-LLM, a novel framework leveraging LLMs for TCP applications.\nTCP-LLM utilizes pre-trained knowledge to reduce engineering effort, enhance\ngeneralization, and deliver superior performance across diverse TCP tasks.\nApplied to reducing flow unfairness, adapting congestion control, and\npreventing starvation, TCP-LLM demonstrates significant improvements over TCP\nwith minimal fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new transmission control protocol (TCP) relies on Deep Learning (DL) for\nprediction and optimization, but requires significant manual effort to design\ndeep neural networks (DNNs) and struggles with generalization in dynamic\nenvironments. Inspired by the success of large language models (LLMs), this\nstudy proposes TCP-LLM, a novel framework leveraging LLMs for TCP applications.\nTCP-LLM utilizes pre-trained knowledge to reduce engineering effort, enhance\ngeneralization, and deliver superior performance across diverse TCP tasks.\nApplied to reducing flow unfairness, adapting congestion control, and\npreventing starvation, TCP-LLM demonstrates significant improvements over TCP\nwith minimal fine-tuning."
                },
                "authors": [
                    {
                        "name": "Shyam Kumar Shrestha"
                    },
                    {
                        "name": "Shiva Raj Pokhrel"
                    },
                    {
                        "name": "Jonathan Kua"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Kua"
                },
                "author": "Jonathan Kua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01787v2",
                "updated": "2024-12-24T06:06:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    6,
                    6,
                    25,
                    1,
                    359,
                    0
                ],
                "published": "2024-09-03T11:06:45Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    11,
                    6,
                    45,
                    1,
                    247,
                    0
                ],
                "title": "LLM-GAN: Construct Generative Adversarial Network Through Large Language\n  Models For Explainable Fake News Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-GAN: Construct Generative Adversarial Network Through Large Language\n  Models For Explainable Fake News Detection"
                },
                "summary": "Explainable fake news detection predicts the authenticity of news items with\nannotated explanations. Today, Large Language Models (LLMs) are known for their\npowerful natural language understanding and explanation generation abilities.\nHowever, presenting LLMs for explainable fake news detection remains two main\nchallenges. Firstly, fake news appears reasonable and could easily mislead\nLLMs, leaving them unable to understand the complex news-faking process.\nSecondly, utilizing LLMs for this task would generate both correct and\nincorrect explanations, which necessitates abundant labor in the loop. In this\npaper, we propose LLM-GAN, a novel framework that utilizes prompting mechanisms\nto enable an LLM to become Generator and Detector and for realistic fake news\ngeneration and detection. Our results demonstrate LLM-GAN's effectiveness in\nboth prediction performance and explanation quality. We further showcase the\nintegration of LLM-GAN to a cloud-native AI platform to provide better fake\nnews detection service in the cloud.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable fake news detection predicts the authenticity of news items with\nannotated explanations. Today, Large Language Models (LLMs) are known for their\npowerful natural language understanding and explanation generation abilities.\nHowever, presenting LLMs for explainable fake news detection remains two main\nchallenges. Firstly, fake news appears reasonable and could easily mislead\nLLMs, leaving them unable to understand the complex news-faking process.\nSecondly, utilizing LLMs for this task would generate both correct and\nincorrect explanations, which necessitates abundant labor in the loop. In this\npaper, we propose LLM-GAN, a novel framework that utilizes prompting mechanisms\nto enable an LLM to become Generator and Detector and for realistic fake news\ngeneration and detection. Our results demonstrate LLM-GAN's effectiveness in\nboth prediction performance and explanation quality. We further showcase the\nintegration of LLM-GAN to a cloud-native AI platform to provide better fake\nnews detection service in the cloud."
                },
                "authors": [
                    {
                        "name": "Yifeng Wang"
                    },
                    {
                        "name": "Zhouhong Gu"
                    },
                    {
                        "name": "Siwei Zhang"
                    },
                    {
                        "name": "Suhang Zheng"
                    },
                    {
                        "name": "Tao Wang"
                    },
                    {
                        "name": "Tianyu Li"
                    },
                    {
                        "name": "Hongwei Feng"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15639v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15639v2",
                "updated": "2024-12-24T06:05:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    6,
                    5,
                    22,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-20T07:55:59Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    55,
                    59,
                    4,
                    355,
                    0
                ],
                "title": "Tacit Learning with Adaptive Information Selection for Cooperative\n  Multi-Agent Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tacit Learning with Adaptive Information Selection for Cooperative\n  Multi-Agent Reinforcement Learning"
                },
                "summary": "In multi-agent reinforcement learning (MARL), the centralized training with\ndecentralized execution (CTDE) framework has gained widespread adoption due to\nits strong performance. However, the further development of CTDE faces two key\nchallenges. First, agents struggle to autonomously assess the relevance of\ninput information for cooperative tasks, impairing their decision-making\nabilities. Second, in communication-limited scenarios with partial\nobservability, agents are unable to access global information, restricting\ntheir ability to collaborate effectively from a global perspective. To address\nthese challenges, we introduce a novel cooperative MARL framework based on\ninformation selection and tacit learning. In this framework, agents gradually\ndevelop implicit coordination during training, enabling them to infer the\ncooperative behavior of others in a discrete space without communication,\nrelying solely on local information. Moreover, we integrate gating and\nselection mechanisms, allowing agents to adaptively filter information based on\nenvironmental changes, thereby enhancing their decision-making capabilities.\nExperiments on popular MARL benchmarks show that our framework can be\nseamlessly integrated with state-of-the-art algorithms, leading to significant\nperformance improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multi-agent reinforcement learning (MARL), the centralized training with\ndecentralized execution (CTDE) framework has gained widespread adoption due to\nits strong performance. However, the further development of CTDE faces two key\nchallenges. First, agents struggle to autonomously assess the relevance of\ninput information for cooperative tasks, impairing their decision-making\nabilities. Second, in communication-limited scenarios with partial\nobservability, agents are unable to access global information, restricting\ntheir ability to collaborate effectively from a global perspective. To address\nthese challenges, we introduce a novel cooperative MARL framework based on\ninformation selection and tacit learning. In this framework, agents gradually\ndevelop implicit coordination during training, enabling them to infer the\ncooperative behavior of others in a discrete space without communication,\nrelying solely on local information. Moreover, we integrate gating and\nselection mechanisms, allowing agents to adaptively filter information based on\nenvironmental changes, thereby enhancing their decision-making capabilities.\nExperiments on popular MARL benchmarks show that our framework can be\nseamlessly integrated with state-of-the-art algorithms, leading to significant\nperformance improvements."
                },
                "authors": [
                    {
                        "name": "Lunjun Liu"
                    },
                    {
                        "name": "Weilai Jiang"
                    },
                    {
                        "name": "Yaonan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yaonan Wang"
                },
                "author": "Yaonan Wang",
                "arxiv_comment": "Accepted by AAMAS 2025 (Extended Abstract)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15639v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15639v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18196v1",
                "updated": "2024-12-24T06:05:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    6,
                    5,
                    8,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T06:05:08Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    6,
                    5,
                    8,
                    1,
                    359,
                    0
                ],
                "title": "Robustness-aware Automatic Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustness-aware Automatic Prompt Optimization"
                },
                "summary": "The performance of Large Language Models (LLMs) is based on the quality of\nthe prompts and the semantic and structural integrity information of the input\ndata. However, current prompt generation methods primarily focus on generating\nprompts for clean input data, often overlooking the impact of perturbed inputs\non prompt performance. To address this limitation, we propose BATprompt (By\nAdversarial Training prompt), a novel method for prompt generation designed to\nwithstand input perturbations (such as typos in the input). Inspired by\nadversarial training techniques, BATprompt demonstrates strong performance on a\nvariety of perturbed tasks through a two-step process: adversarial perturbation\nand iterative optimization on unperturbed input via LLM. Unlike conventional\nadversarial attack methods, BATprompt avoids reliance on real gradients or\nmodel parameters. Instead, it leverages the advanced reasoning, language\nunderstanding and self reflection capabilities of LLMs to simulate gradients,\nguiding the generation of adversarial perturbations and optimizing prompt\nperformance. In our experiments, we evaluate BATprompt on multiple datasets\nacross both language understanding and generation tasks. The results indicate\nthat BATprompt outperforms existing prompt generation methods, delivering\nsuperior robustness and performance under diverse perturbation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of Large Language Models (LLMs) is based on the quality of\nthe prompts and the semantic and structural integrity information of the input\ndata. However, current prompt generation methods primarily focus on generating\nprompts for clean input data, often overlooking the impact of perturbed inputs\non prompt performance. To address this limitation, we propose BATprompt (By\nAdversarial Training prompt), a novel method for prompt generation designed to\nwithstand input perturbations (such as typos in the input). Inspired by\nadversarial training techniques, BATprompt demonstrates strong performance on a\nvariety of perturbed tasks through a two-step process: adversarial perturbation\nand iterative optimization on unperturbed input via LLM. Unlike conventional\nadversarial attack methods, BATprompt avoids reliance on real gradients or\nmodel parameters. Instead, it leverages the advanced reasoning, language\nunderstanding and self reflection capabilities of LLMs to simulate gradients,\nguiding the generation of adversarial perturbations and optimizing prompt\nperformance. In our experiments, we evaluate BATprompt on multiple datasets\nacross both language understanding and generation tasks. The results indicate\nthat BATprompt outperforms existing prompt generation methods, delivering\nsuperior robustness and performance under diverse perturbation scenarios."
                },
                "authors": [
                    {
                        "name": "Zeru Shi"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Yongye Su"
                    },
                    {
                        "name": "Weidi Luo"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18194v1",
                "updated": "2024-12-24T06:03:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    6,
                    3,
                    42,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T06:03:42Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    6,
                    3,
                    42,
                    1,
                    359,
                    0
                ],
                "title": "VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics\n  Manipulation with Long-Horizon Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics\n  Manipulation with Long-Horizon Reasoning Tasks"
                },
                "summary": "General-purposed embodied agents are designed to understand the users'\nnatural instructions or intentions and act precisely to complete universal\ntasks. Recently, methods based on foundation models especially\nVision-Language-Action models (VLAs) have shown a substantial potential to\nsolve language-conditioned manipulation (LCM) tasks well. However, existing\nbenchmarks do not adequately meet the needs of VLAs and relative algorithms. To\nbetter define such general-purpose tasks in the context of LLMs and advance the\nresearch in VLAs, we present VLABench, an open-source benchmark for evaluating\nuniversal LCM task learning. VLABench provides 100 carefully designed\ncategories of tasks, with strong randomization in each category of task and a\ntotal of 2000+ objects. VLABench stands out from previous benchmarks in four\nkey aspects: 1) tasks requiring world knowledge and common sense transfer, 2)\nnatural language instructions with implicit human intentions rather than\ntemplates, 3) long-horizon tasks demanding multi-step reasoning, and 4)\nevaluation of both action policies and language model capabilities. The\nbenchmark assesses multiple competencies including understanding of\nmesh\\&texture, spatial relationship, semantic instruction, physical laws,\nknowledge transfer and reasoning, etc. To support the downstream finetuning, we\nprovide high-quality training data collected via an automated framework\nincorporating heuristic skills and prior information. The experimental results\nindicate that both the current state-of-the-art pretrained VLAs and the\nworkflow based on VLMs face challenges in our tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General-purposed embodied agents are designed to understand the users'\nnatural instructions or intentions and act precisely to complete universal\ntasks. Recently, methods based on foundation models especially\nVision-Language-Action models (VLAs) have shown a substantial potential to\nsolve language-conditioned manipulation (LCM) tasks well. However, existing\nbenchmarks do not adequately meet the needs of VLAs and relative algorithms. To\nbetter define such general-purpose tasks in the context of LLMs and advance the\nresearch in VLAs, we present VLABench, an open-source benchmark for evaluating\nuniversal LCM task learning. VLABench provides 100 carefully designed\ncategories of tasks, with strong randomization in each category of task and a\ntotal of 2000+ objects. VLABench stands out from previous benchmarks in four\nkey aspects: 1) tasks requiring world knowledge and common sense transfer, 2)\nnatural language instructions with implicit human intentions rather than\ntemplates, 3) long-horizon tasks demanding multi-step reasoning, and 4)\nevaluation of both action policies and language model capabilities. The\nbenchmark assesses multiple competencies including understanding of\nmesh\\&texture, spatial relationship, semantic instruction, physical laws,\nknowledge transfer and reasoning, etc. To support the downstream finetuning, we\nprovide high-quality training data collected via an automated framework\nincorporating heuristic skills and prior information. The experimental results\nindicate that both the current state-of-the-art pretrained VLAs and the\nworkflow based on VLMs face challenges in our tasks."
                },
                "authors": [
                    {
                        "name": "Shiduo Zhang"
                    },
                    {
                        "name": "Zhe Xu"
                    },
                    {
                        "name": "Peiju Liu"
                    },
                    {
                        "name": "Xiaopeng Yu"
                    },
                    {
                        "name": "Yuan Li"
                    },
                    {
                        "name": "Qinghui Gao"
                    },
                    {
                        "name": "Zhaoye Fei"
                    },
                    {
                        "name": "Zhangyue Yin"
                    },
                    {
                        "name": "Zuxuan Wu"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16468v2",
                "updated": "2024-12-24T05:54:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    54,
                    15,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-21T03:51:04Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    3,
                    51,
                    4,
                    5,
                    356,
                    0
                ],
                "title": "The Road to Artificial SuperIntelligence: A Comprehensive Survey of\n  Superalignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Road to Artificial SuperIntelligence: A Comprehensive Survey of\n  Superalignment"
                },
                "summary": "The emergence of large language models (LLMs) has sparked the possibility of\nabout Artificial Superintelligence (ASI), a hypothetical AI system surpassing\nhuman intelligence. However, existing alignment paradigms struggle to guide\nsuch advanced AI systems. Superalignment, the alignment of AI systems with\nhuman values and safety requirements at superhuman levels of capability aims to\naddresses two primary goals -- scalability in supervision to provide\nhigh-quality guidance signals and robust governance to ensure alignment with\nhuman values. In this survey, we examine scalable oversight methods and\npotential solutions for superalignment. Specifically, we explore the concept of\nASI, the challenges it poses, and the limitations of current alignment\nparadigms in addressing the superalignment problem. Then we review scalable\noversight methods for superalignment. Finally, we discuss the key challenges\nand propose pathways for the safe and continual improvement of ASI systems. By\ncomprehensively reviewing the current literature, our goal is provide a\nsystematical introduction of existing methods, analyze their strengths and\nlimitations, and discuss potential future directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) has sparked the possibility of\nabout Artificial Superintelligence (ASI), a hypothetical AI system surpassing\nhuman intelligence. However, existing alignment paradigms struggle to guide\nsuch advanced AI systems. Superalignment, the alignment of AI systems with\nhuman values and safety requirements at superhuman levels of capability aims to\naddresses two primary goals -- scalability in supervision to provide\nhigh-quality guidance signals and robust governance to ensure alignment with\nhuman values. In this survey, we examine scalable oversight methods and\npotential solutions for superalignment. Specifically, we explore the concept of\nASI, the challenges it poses, and the limitations of current alignment\nparadigms in addressing the superalignment problem. Then we review scalable\noversight methods for superalignment. Finally, we discuss the key challenges\nand propose pathways for the safe and continual improvement of ASI systems. By\ncomprehensively reviewing the current literature, our goal is provide a\nsystematical introduction of existing methods, analyze their strengths and\nlimitations, and discuss potential future directions."
                },
                "authors": [
                    {
                        "name": "HyunJin Kim"
                    },
                    {
                        "name": "Xiaoyuan Yi"
                    },
                    {
                        "name": "Jing Yao"
                    },
                    {
                        "name": "Jianxun Lian"
                    },
                    {
                        "name": "Muhua Huang"
                    },
                    {
                        "name": "Shitong Duan"
                    },
                    {
                        "name": "JinYeong Bak"
                    },
                    {
                        "name": "Xing Xie"
                    }
                ],
                "author_detail": {
                    "name": "Xing Xie"
                },
                "author": "Xing Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18185v1",
                "updated": "2024-12-24T05:38:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    38,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T05:38:45Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    38,
                    45,
                    1,
                    359,
                    0
                ],
                "title": "TextMatch: Enhancing Image-Text Consistency Through Multimodal\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextMatch: Enhancing Image-Text Consistency Through Multimodal\n  Optimization"
                },
                "summary": "Text-to-image generative models excel in creating images from text but\nstruggle with ensuring alignment and consistency between outputs and prompts.\nThis paper introduces TextMatch, a novel framework that leverages multimodal\noptimization to address image-text discrepancies in text-to-image (T2I)\ngeneration and editing. TextMatch employs a scoring strategy powered by large\nlanguage models (LLMs) and visual question-answering (VQA) models to evaluate\nsemantic consistency between prompts and generated images. By integrating\nmultimodal in-context learning and chain of thought reasoning, our method\ndynamically refines prompts through iterative optimization. This process\nensures that the generated images better capture user intent of, resulting in\nhigher fidelity and relevance. Extensive experiments demonstrate that TextMatch\nsignificantly improves text-image consistency across multiple benchmarks,\nestablishing a reliable framework for advancing the capabilities of\ntext-to-image generative models. Our code is available at\nhttps://anonymous.4open.science/r/TextMatch-F55C/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generative models excel in creating images from text but\nstruggle with ensuring alignment and consistency between outputs and prompts.\nThis paper introduces TextMatch, a novel framework that leverages multimodal\noptimization to address image-text discrepancies in text-to-image (T2I)\ngeneration and editing. TextMatch employs a scoring strategy powered by large\nlanguage models (LLMs) and visual question-answering (VQA) models to evaluate\nsemantic consistency between prompts and generated images. By integrating\nmultimodal in-context learning and chain of thought reasoning, our method\ndynamically refines prompts through iterative optimization. This process\nensures that the generated images better capture user intent of, resulting in\nhigher fidelity and relevance. Extensive experiments demonstrate that TextMatch\nsignificantly improves text-image consistency across multiple benchmarks,\nestablishing a reliable framework for advancing the capabilities of\ntext-to-image generative models. Our code is available at\nhttps://anonymous.4open.science/r/TextMatch-F55C/."
                },
                "authors": [
                    {
                        "name": "Yucong Luo"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Jie Ouyang"
                    },
                    {
                        "name": "Xiaoyu Tao"
                    },
                    {
                        "name": "Qi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Liu"
                },
                "author": "Qi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18176v1",
                "updated": "2024-12-24T05:23:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    23,
                    13,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T05:23:13Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    23,
                    13,
                    1,
                    359,
                    0
                ],
                "title": "Molar: Multimodal LLMs with Collaborative Filtering Alignment for\n  Enhanced Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molar: Multimodal LLMs with Collaborative Filtering Alignment for\n  Enhanced Sequential Recommendation"
                },
                "summary": "Sequential recommendation (SR) systems have evolved significantly over the\npast decade, transitioning from traditional collaborative filtering to deep\nlearning approaches and, more recently, to large language models (LLMs). While\nthe adoption of LLMs has driven substantial advancements, these models\ninherently lack collaborative filtering information, relying primarily on\ntextual content data neglecting other modalities and thus failing to achieve\noptimal recommendation performance. To address this limitation, we propose\nMolar, a Multimodal large language sequential recommendation framework that\nintegrates multiple content modalities with ID information to capture\ncollaborative signals effectively. Molar employs an MLLM to generate unified\nitem representations from both textual and non-textual data, facilitating\ncomprehensive multimodal modeling and enriching item embeddings. Additionally,\nit incorporates collaborative filtering signals through a post-alignment\nmechanism, which aligns user representations from content-based and ID-based\nmodels, ensuring precise personalization and robust performance. By seamlessly\ncombining multimodal content with collaborative filtering insights, Molar\ncaptures both user interests and contextual semantics, leading to superior\nrecommendation accuracy. Extensive experiments validate that Molar\nsignificantly outperforms traditional and LLM-based baselines, highlighting its\nstrength in utilizing multimodal data and collaborative signals for sequential\nrecommendation tasks. The source code is available at\nhttps://anonymous.4open.science/r/Molar-8B06/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommendation (SR) systems have evolved significantly over the\npast decade, transitioning from traditional collaborative filtering to deep\nlearning approaches and, more recently, to large language models (LLMs). While\nthe adoption of LLMs has driven substantial advancements, these models\ninherently lack collaborative filtering information, relying primarily on\ntextual content data neglecting other modalities and thus failing to achieve\noptimal recommendation performance. To address this limitation, we propose\nMolar, a Multimodal large language sequential recommendation framework that\nintegrates multiple content modalities with ID information to capture\ncollaborative signals effectively. Molar employs an MLLM to generate unified\nitem representations from both textual and non-textual data, facilitating\ncomprehensive multimodal modeling and enriching item embeddings. Additionally,\nit incorporates collaborative filtering signals through a post-alignment\nmechanism, which aligns user representations from content-based and ID-based\nmodels, ensuring precise personalization and robust performance. By seamlessly\ncombining multimodal content with collaborative filtering insights, Molar\ncaptures both user interests and contextual semantics, leading to superior\nrecommendation accuracy. Extensive experiments validate that Molar\nsignificantly outperforms traditional and LLM-based baselines, highlighting its\nstrength in utilizing multimodal data and collaborative signals for sequential\nrecommendation tasks. The source code is available at\nhttps://anonymous.4open.science/r/Molar-8B06/."
                },
                "authors": [
                    {
                        "name": "Yucong Luo"
                    },
                    {
                        "name": "Qitao Qin"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Ruiran Yan"
                    },
                    {
                        "name": "Kefan Wang"
                    },
                    {
                        "name": "Jie Ouyang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Ouyang"
                },
                "author": "Jie Ouyang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20898v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20898v2",
                "updated": "2024-12-24T05:22:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    22,
                    40,
                    1,
                    359,
                    0
                ],
                "published": "2024-10-28T10:26:19Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    10,
                    26,
                    19,
                    0,
                    302,
                    0
                ],
                "title": "Diff-Instruct*: Towards Human-Preferred One-step Text-to-image\n  Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diff-Instruct*: Towards Human-Preferred One-step Text-to-image\n  Generative Models"
                },
                "summary": "In this paper, we introduce the Diff-Instruct* (DI*), an image data-free\napproach for building one-step text-to-image generative models that align with\nhuman preference while maintaining the ability to generate highly realistic\nimages. We frame human preference alignment as online reinforcement learning\nusing human feedback (RLHF), where the goal is to maximize the reward function\nwhile regularizing the generator distribution to remain close to a reference\ndiffusion process. Unlike traditional RLHF approaches, which rely on the KL\ndivergence for regularization, we introduce a novel score-based divergence\nregularization, which leads to significantly better performances. Although the\ndirect calculation of this preference alignment objective remains intractable,\nwe demonstrate that we can efficiently compute its gradient by deriving an\nequivalent yet tractable loss function. Remarkably, we used Diff-Instruct* to\ntrain a Stable Diffusion-XL-based 1-step model, the 2.6B DI*-SDXL-1step\ntext-to-image model, which can generate images of a resolution of 1024x1024\nwith only 1 generation step. DI*-SDXL-1step model uses only 1.88% inference\ntime and 29.30% GPU memory cost to outperform 12B FLUX-dev-50step significantly\nin PickScore, ImageReward, and CLIPScore on Parti prompt benchmark and HPSv2.1\non Human Preference Score benchmark, establishing a new state-of-the-art\nbenchmark of human-preferred 1-step text-to-image generative models. Besides\nthe strong quantitative performances, extensive qualitative comparisons also\nconfirm the advantages of DI* in terms of maintaining diversity, improving\nimage layouts, and enhancing aesthetic colors. We have released our\nindustry-ready model on the homepage:\n\\url{https://github.com/pkulwj1994/diff_instruct_star}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce the Diff-Instruct* (DI*), an image data-free\napproach for building one-step text-to-image generative models that align with\nhuman preference while maintaining the ability to generate highly realistic\nimages. We frame human preference alignment as online reinforcement learning\nusing human feedback (RLHF), where the goal is to maximize the reward function\nwhile regularizing the generator distribution to remain close to a reference\ndiffusion process. Unlike traditional RLHF approaches, which rely on the KL\ndivergence for regularization, we introduce a novel score-based divergence\nregularization, which leads to significantly better performances. Although the\ndirect calculation of this preference alignment objective remains intractable,\nwe demonstrate that we can efficiently compute its gradient by deriving an\nequivalent yet tractable loss function. Remarkably, we used Diff-Instruct* to\ntrain a Stable Diffusion-XL-based 1-step model, the 2.6B DI*-SDXL-1step\ntext-to-image model, which can generate images of a resolution of 1024x1024\nwith only 1 generation step. DI*-SDXL-1step model uses only 1.88% inference\ntime and 29.30% GPU memory cost to outperform 12B FLUX-dev-50step significantly\nin PickScore, ImageReward, and CLIPScore on Parti prompt benchmark and HPSv2.1\non Human Preference Score benchmark, establishing a new state-of-the-art\nbenchmark of human-preferred 1-step text-to-image generative models. Besides\nthe strong quantitative performances, extensive qualitative comparisons also\nconfirm the advantages of DI* in terms of maintaining diversity, improving\nimage layouts, and enhancing aesthetic colors. We have released our\nindustry-ready model on the homepage:\n\\url{https://github.com/pkulwj1994/diff_instruct_star}."
                },
                "authors": [
                    {
                        "name": "Weijian Luo"
                    },
                    {
                        "name": "Colin Zhang"
                    },
                    {
                        "name": "Debing Zhang"
                    },
                    {
                        "name": "Zhengyang Geng"
                    }
                ],
                "author_detail": {
                    "name": "Zhengyang Geng"
                },
                "author": "Zhengyang Geng",
                "arxiv_comment": "revision: 2.6B 1-step text-to-image model outperforms 12B\n  Flux-dev-50step model in human preferences",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20898v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20898v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18174v1",
                "updated": "2024-12-24T05:22:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    22,
                    33,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T05:22:33Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    22,
                    33,
                    1,
                    359,
                    0
                ],
                "title": "INVESTORBENCH: A Benchmark for Financial Decision-Making Tasks with\n  LLM-based Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INVESTORBENCH: A Benchmark for Financial Decision-Making Tasks with\n  LLM-based Agent"
                },
                "summary": "Recent advancements have underscored the potential of large language model\n(LLM)-based agents in financial decision-making. Despite this progress, the\nfield currently encounters two main challenges: (1) the lack of a comprehensive\nLLM agent framework adaptable to a variety of financial tasks, and (2) the\nabsence of standardized benchmarks and consistent datasets for assessing agent\nperformance. To tackle these issues, we introduce \\textsc{InvestorBench}, the\nfirst benchmark specifically designed for evaluating LLM-based agents in\ndiverse financial decision-making contexts. InvestorBench enhances the\nversatility of LLM-enabled agents by providing a comprehensive suite of tasks\napplicable to different financial products, including single equities like\nstocks, cryptocurrencies and exchange-traded funds (ETFs). Additionally, we\nassess the reasoning and decision-making capabilities of our agent framework\nusing thirteen different LLMs as backbone models, across various market\nenvironments and tasks. Furthermore, we have curated a diverse collection of\nopen-source, multi-modal datasets and developed a comprehensive suite of\nenvironments for financial decision-making. This establishes a highly\naccessible platform for evaluating financial agents' performance across various\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements have underscored the potential of large language model\n(LLM)-based agents in financial decision-making. Despite this progress, the\nfield currently encounters two main challenges: (1) the lack of a comprehensive\nLLM agent framework adaptable to a variety of financial tasks, and (2) the\nabsence of standardized benchmarks and consistent datasets for assessing agent\nperformance. To tackle these issues, we introduce \\textsc{InvestorBench}, the\nfirst benchmark specifically designed for evaluating LLM-based agents in\ndiverse financial decision-making contexts. InvestorBench enhances the\nversatility of LLM-enabled agents by providing a comprehensive suite of tasks\napplicable to different financial products, including single equities like\nstocks, cryptocurrencies and exchange-traded funds (ETFs). Additionally, we\nassess the reasoning and decision-making capabilities of our agent framework\nusing thirteen different LLMs as backbone models, across various market\nenvironments and tasks. Furthermore, we have curated a diverse collection of\nopen-source, multi-modal datasets and developed a comprehensive suite of\nenvironments for financial decision-making. This establishes a highly\naccessible platform for evaluating financial agents' performance across various\nscenarios."
                },
                "authors": [
                    {
                        "name": "Haohang Li"
                    },
                    {
                        "name": "Yupeng Cao"
                    },
                    {
                        "name": "Yangyang Yu"
                    },
                    {
                        "name": "Shashidhar Reddy Javaji"
                    },
                    {
                        "name": "Zhiyang Deng"
                    },
                    {
                        "name": "Yueru He"
                    },
                    {
                        "name": "Yuechen Jiang"
                    },
                    {
                        "name": "Zining Zhu"
                    },
                    {
                        "name": "Koduvayur Subbalakshmi"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Lingfei Qian"
                    },
                    {
                        "name": "Xueqing Peng"
                    },
                    {
                        "name": "Qianqian Xie"
                    },
                    {
                        "name": "Jordan W. Suchow"
                    }
                ],
                "author_detail": {
                    "name": "Jordan W. Suchow"
                },
                "author": "Jordan W. Suchow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2103.04021v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2103.04021v3",
                "updated": "2024-12-24T05:20:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    20,
                    59,
                    1,
                    359,
                    0
                ],
                "published": "2021-03-06T03:57:46Z",
                "published_parsed": [
                    2021,
                    3,
                    6,
                    3,
                    57,
                    46,
                    5,
                    65,
                    0
                ],
                "title": "Asymptotic Theory for IV-Based Reinforcement Learning with Potential\n  Endogeneity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotic Theory for IV-Based Reinforcement Learning with Potential\n  Endogeneity"
                },
                "summary": "In the standard data analysis framework, data is collected (once and for\nall), and then data analysis is carried out. However, with the advancement of\ndigital technology, decision-makers constantly analyze past data and generate\nnew data through their decisions. We model this as a Markov decision process\nand show that the dynamic interaction between data generation and data analysis\nleads to a new type of bias -- reinforcement bias -- that exacerbates the\nendogeneity problem in standard data analysis. We propose a class of instrument\nvariable (IV)-based reinforcement learning (RL) algorithms to correct for the\nbias and establish their theoretical properties by incorporating them into a\nstochastic approximation (SA) framework. Our analysis accommodates\niterate-dependent Markovian structures and, therefore, can be used to study RL\nalgorithms with policy improvement. We also provide formulas for inference on\noptimal policies of the IV-RL algorithms. These formulas highlight how\nintertemporal dependencies of the Markovian environment affect the inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the standard data analysis framework, data is collected (once and for\nall), and then data analysis is carried out. However, with the advancement of\ndigital technology, decision-makers constantly analyze past data and generate\nnew data through their decisions. We model this as a Markov decision process\nand show that the dynamic interaction between data generation and data analysis\nleads to a new type of bias -- reinforcement bias -- that exacerbates the\nendogeneity problem in standard data analysis. We propose a class of instrument\nvariable (IV)-based reinforcement learning (RL) algorithms to correct for the\nbias and establish their theoretical properties by incorporating them into a\nstochastic approximation (SA) framework. Our analysis accommodates\niterate-dependent Markovian structures and, therefore, can be used to study RL\nalgorithms with policy improvement. We also provide formulas for inference on\noptimal policies of the IV-RL algorithms. These formulas highlight how\nintertemporal dependencies of the Markovian environment affect the inference."
                },
                "authors": [
                    {
                        "name": "Jin Li"
                    },
                    {
                        "name": "Ye Luo"
                    },
                    {
                        "name": "Zigan Wang"
                    },
                    {
                        "name": "Xiaowei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowei Zhang"
                },
                "author": "Xiaowei Zhang",
                "arxiv_comment": "main body: 42 pages; supplemental material: 14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2103.04021v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2103.04021v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18171v1",
                "updated": "2024-12-24T05:10:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    10,
                    2,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T05:10:02Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    10,
                    2,
                    1,
                    359,
                    0
                ],
                "title": "Token Highlighter: Inspecting and Mitigating Jailbreak Prompts for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Highlighter: Inspecting and Mitigating Jailbreak Prompts for Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are increasingly being integrated into services\nsuch as ChatGPT to provide responses to user queries. To mitigate potential\nharm and prevent misuse, there have been concerted efforts to align the LLMs\nwith human values and legal compliance by incorporating various techniques,\nsuch as Reinforcement Learning from Human Feedback (RLHF), into the training of\nthe LLMs. However, recent research has exposed that even aligned LLMs are\nsusceptible to adversarial manipulations known as Jailbreak Attacks. To address\nthis challenge, this paper proposes a method called Token Highlighter to\ninspect and mitigate the potential jailbreak threats in the user query. Token\nHighlighter introduced a concept called Affirmation Loss to measure the LLM's\nwillingness to answer the user query. It then uses the gradient of Affirmation\nLoss for each token in the user query to locate the jailbreak-critical tokens.\nFurther, Token Highlighter exploits our proposed Soft Removal technique to\nmitigate the jailbreak effects of critical tokens via shrinking their token\nembeddings. Experimental results on two aligned LLMs (LLaMA-2 and Vicuna-V1.5)\ndemonstrate that the proposed method can effectively defend against a variety\nof Jailbreak Attacks while maintaining competent performance on benign\nquestions of the AlpacaEval benchmark. In addition, Token Highlighter is a\ncost-effective and interpretable defense because it only needs to query the\nprotected LLM once to compute the Affirmation Loss and can highlight the\ncritical tokens upon refusal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being integrated into services\nsuch as ChatGPT to provide responses to user queries. To mitigate potential\nharm and prevent misuse, there have been concerted efforts to align the LLMs\nwith human values and legal compliance by incorporating various techniques,\nsuch as Reinforcement Learning from Human Feedback (RLHF), into the training of\nthe LLMs. However, recent research has exposed that even aligned LLMs are\nsusceptible to adversarial manipulations known as Jailbreak Attacks. To address\nthis challenge, this paper proposes a method called Token Highlighter to\ninspect and mitigate the potential jailbreak threats in the user query. Token\nHighlighter introduced a concept called Affirmation Loss to measure the LLM's\nwillingness to answer the user query. It then uses the gradient of Affirmation\nLoss for each token in the user query to locate the jailbreak-critical tokens.\nFurther, Token Highlighter exploits our proposed Soft Removal technique to\nmitigate the jailbreak effects of critical tokens via shrinking their token\nembeddings. Experimental results on two aligned LLMs (LLaMA-2 and Vicuna-V1.5)\ndemonstrate that the proposed method can effectively defend against a variety\nof Jailbreak Attacks while maintaining competent performance on benign\nquestions of the AlpacaEval benchmark. In addition, Token Highlighter is a\ncost-effective and interpretable defense because it only needs to query the\nprotected LLM once to compute the Affirmation Loss and can highlight the\ncritical tokens upon refusal."
                },
                "authors": [
                    {
                        "name": "Xiaomeng Hu"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Tsung-Yi Ho"
                    }
                ],
                "author_detail": {
                    "name": "Tsung-Yi Ho"
                },
                "author": "Tsung-Yi Ho",
                "arxiv_comment": "Accepted by AAAI 2025. Project page:\n  https://huggingface.co/spaces/TrustSafeAI/Token-Highlighter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18169v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18169v1",
                "updated": "2024-12-24T05:07:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    7,
                    46,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T05:07:46Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    7,
                    46,
                    1,
                    359,
                    0
                ],
                "title": "KunServe: Elastic and Efficient Large Language Model Serving with\n  Parameter-centric Memory Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KunServe: Elastic and Efficient Large Language Model Serving with\n  Parameter-centric Memory Management"
                },
                "summary": "The stateful nature of large language model (LLM) servingcan easily throttle\nprecious GPU memory under load burstor long-generation requests like\nchain-of-thought reasoning,causing latency spikes due to queuing incoming\nrequests. However, state-of-the-art KVCache centric approaches handleload\nspikes by dropping, migrating, or swapping KVCache,which faces an essential\ntradeoff between the performance ofongoing vs. incoming requests and thus still\nseverely violatesSLO.This paper makes a key observation such that model\nparam-eters are independent of the requests and are replicated acrossGPUs, and\nthus proposes a parameter-centric approach byselectively dropping replicated\nparameters to leave preciousmemory for requests. However, LLM requires KVCache\ntobe saved in bound with model parameters and thus droppingparameters can cause\neither huge computation waste or longnetwork delay, affecting all ongoing\nrequests. Based on the ob-servation that attention operators can be decoupled\nfrom otheroperators, this paper further proposes a novel remote\nattentionmechanism through pipeline parallelism so as to serve up-coming\nrequests with the additional memory borrowed fromparameters on remote GPUs.\nThis paper further addresses sev-eral other challenges including lively\nexchanging KVCachewith incomplete parameters, generating an appropriate\nplanthat balances memory requirements with cooperative exe-cution overhead, and\nseamlessly restoring parameters whenthe throttling has gone. Evaluations show\nthatKUNSERVEreduces the tail TTFT of requests under throttling by up to 27.3x\ncompared to the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The stateful nature of large language model (LLM) servingcan easily throttle\nprecious GPU memory under load burstor long-generation requests like\nchain-of-thought reasoning,causing latency spikes due to queuing incoming\nrequests. However, state-of-the-art KVCache centric approaches handleload\nspikes by dropping, migrating, or swapping KVCache,which faces an essential\ntradeoff between the performance ofongoing vs. incoming requests and thus still\nseverely violatesSLO.This paper makes a key observation such that model\nparam-eters are independent of the requests and are replicated acrossGPUs, and\nthus proposes a parameter-centric approach byselectively dropping replicated\nparameters to leave preciousmemory for requests. However, LLM requires KVCache\ntobe saved in bound with model parameters and thus droppingparameters can cause\neither huge computation waste or longnetwork delay, affecting all ongoing\nrequests. Based on the ob-servation that attention operators can be decoupled\nfrom otheroperators, this paper further proposes a novel remote\nattentionmechanism through pipeline parallelism so as to serve up-coming\nrequests with the additional memory borrowed fromparameters on remote GPUs.\nThis paper further addresses sev-eral other challenges including lively\nexchanging KVCachewith incomplete parameters, generating an appropriate\nplanthat balances memory requirements with cooperative exe-cution overhead, and\nseamlessly restoring parameters whenthe throttling has gone. Evaluations show\nthatKUNSERVEreduces the tail TTFT of requests under throttling by up to 27.3x\ncompared to the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Rongxin Cheng"
                    },
                    {
                        "name": "Yifan Peng"
                    },
                    {
                        "name": "Yuxin Lai"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18169v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18167v1",
                "updated": "2024-12-24T04:59:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    59,
                    24,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T04:59:24Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    59,
                    24,
                    1,
                    359,
                    0
                ],
                "title": "Dependence of the estimated electric potential in thunderstorms observed\n  at GRAPES-3 on the hadronic interaction generators used in simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dependence of the estimated electric potential in thunderstorms observed\n  at GRAPES-3 on the hadronic interaction generators used in simulations"
                },
                "summary": "A potential difference of 1.3 Giga-Volts (GV) was inferred across a\nthundercloud using data from the GRAPES-3 muon telescope (G3MT). This was the\nfirst-ever estimation of gigavolt potential in thunderstorms, confirming\nprediction of C.T.R. Wilson almost a century ago. To infer the thundercloud\npotential required acceleration of muons in atmospheric electric field to be\nincorporated in the Monte Carlo simulation software CORSIKA. The G3MT records\nover 4 billion muons daily that are grouped into 169 directions covering 2.3 sr\nsky. This enabled changes as small as 0.1% in the muon flux on minute\ntimescale, caused by thunderstorms to be accurately measured. But that requires\nhigh statistics simulation of muon fluxes in thunderstorm electric fields. The\nCORSIKA offers a choice of several generators for low- (FLUKA, GHEISHA, and\nUrQMD) and high-energy (SIBYLL, EPOS-LHC, and QGSJETII) hadronic interactions.\nSince it is unclear which combination of the low- and high-energy generators\nprovides the correct description of hadronic interactions, all nine\ncombinations of generators were explored, and they yielded thundercloud\npotentials ranging from 1.3 GV to 1.6 GV for the event recorded on 1 December\n2014. The result of SIBYLL-FLUKA combination yielded the lowest electric\npotential of 1.3 GV was reported. Furthermore, another seven major thunderstorm\nevents recorded between April 2011 and December 2020 were analyzed to measure\nthe dependence of their thundercloud potential on the hadronic interaction\ngenerators. It is observed that the low-energy generators produce larger\nvariation ($\\sim$14%) in thundercloud potential than the high-energy generators\n($\\sim$8%). This probably reflects the fact that the GeV muons are\npredominantly produced in low-energy ($<$80 GeV) interactions, which\neffectively magnifies the differences in the meson production cross-sections\namong the low-energy generators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A potential difference of 1.3 Giga-Volts (GV) was inferred across a\nthundercloud using data from the GRAPES-3 muon telescope (G3MT). This was the\nfirst-ever estimation of gigavolt potential in thunderstorms, confirming\nprediction of C.T.R. Wilson almost a century ago. To infer the thundercloud\npotential required acceleration of muons in atmospheric electric field to be\nincorporated in the Monte Carlo simulation software CORSIKA. The G3MT records\nover 4 billion muons daily that are grouped into 169 directions covering 2.3 sr\nsky. This enabled changes as small as 0.1% in the muon flux on minute\ntimescale, caused by thunderstorms to be accurately measured. But that requires\nhigh statistics simulation of muon fluxes in thunderstorm electric fields. The\nCORSIKA offers a choice of several generators for low- (FLUKA, GHEISHA, and\nUrQMD) and high-energy (SIBYLL, EPOS-LHC, and QGSJETII) hadronic interactions.\nSince it is unclear which combination of the low- and high-energy generators\nprovides the correct description of hadronic interactions, all nine\ncombinations of generators were explored, and they yielded thundercloud\npotentials ranging from 1.3 GV to 1.6 GV for the event recorded on 1 December\n2014. The result of SIBYLL-FLUKA combination yielded the lowest electric\npotential of 1.3 GV was reported. Furthermore, another seven major thunderstorm\nevents recorded between April 2011 and December 2020 were analyzed to measure\nthe dependence of their thundercloud potential on the hadronic interaction\ngenerators. It is observed that the low-energy generators produce larger\nvariation ($\\sim$14%) in thundercloud potential than the high-energy generators\n($\\sim$8%). This probably reflects the fact that the GeV muons are\npredominantly produced in low-energy ($<$80 GeV) interactions, which\neffectively magnifies the differences in the meson production cross-sections\namong the low-energy generators."
                },
                "authors": [
                    {
                        "name": "B. Hariharan"
                    },
                    {
                        "name": "S. K. Gupta"
                    },
                    {
                        "name": "Y. Hayashi"
                    },
                    {
                        "name": "P. Jagadeesan"
                    },
                    {
                        "name": "A. Jain"
                    },
                    {
                        "name": "S. Kawakami"
                    },
                    {
                        "name": "H. Kojima"
                    },
                    {
                        "name": "P. K. Mohanty"
                    },
                    {
                        "name": "Y. Muraki"
                    },
                    {
                        "name": "P. K. Nayak"
                    },
                    {
                        "name": "A. Oshima"
                    },
                    {
                        "name": "M. Rameez"
                    },
                    {
                        "name": "K. Ramesh"
                    },
                    {
                        "name": "L. V. Reddy"
                    },
                    {
                        "name": "S. Shibata"
                    },
                    {
                        "name": "M. Zuberi"
                    }
                ],
                "author_detail": {
                    "name": "M. Zuberi"
                },
                "author": "M. Zuberi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18165v1",
                "updated": "2024-12-24T04:56:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    56,
                    32,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T04:56:32Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    56,
                    32,
                    1,
                    359,
                    0
                ],
                "title": "Parallel Neural Computing for Scene Understanding from LiDAR Perception\n  in Autonomous Racing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Neural Computing for Scene Understanding from LiDAR Perception\n  in Autonomous Racing"
                },
                "summary": "Autonomous driving in high-speed racing, as opposed to urban environments,\npresents significant challenges in scene understanding due to rapid changes in\nthe track environment. Traditional sequential network approaches may struggle\nto meet the real-time knowledge and decision-making demands of an autonomous\nagent covering large displacements in a short time. This paper proposes a novel\nbaseline architecture for developing sophisticated models capable of true\nhardware-enabled parallelism, achieving neural processing speeds that mirror\nthe agent's high velocity. The proposed model (Parallel Perception Network\n(PPN)) consists of two independent neural networks, segmentation and\nreconstruction networks, running parallelly on separate accelerated hardware.\nThe model takes raw 3D point cloud data from the LiDAR sensor as input and\nconverts it into a 2D Bird's Eye View Map on both devices. Each network\nindependently extracts its input features along space and time dimensions and\nproduces outputs parallelly. The proposed method's model is trained on a system\nwith two NVIDIA T4 GPUs, using a combination of loss functions, including edge\npreservation, and demonstrates a 2x speedup in model inference time compared to\na sequential configuration. Implementation is available at:\nhttps://github.com/suwesh/Parallel-Perception-Network. Learned parameters of\nthe trained networks are provided at:\nhttps://huggingface.co/suwesh/ParallelPerceptionNetwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous driving in high-speed racing, as opposed to urban environments,\npresents significant challenges in scene understanding due to rapid changes in\nthe track environment. Traditional sequential network approaches may struggle\nto meet the real-time knowledge and decision-making demands of an autonomous\nagent covering large displacements in a short time. This paper proposes a novel\nbaseline architecture for developing sophisticated models capable of true\nhardware-enabled parallelism, achieving neural processing speeds that mirror\nthe agent's high velocity. The proposed model (Parallel Perception Network\n(PPN)) consists of two independent neural networks, segmentation and\nreconstruction networks, running parallelly on separate accelerated hardware.\nThe model takes raw 3D point cloud data from the LiDAR sensor as input and\nconverts it into a 2D Bird's Eye View Map on both devices. Each network\nindependently extracts its input features along space and time dimensions and\nproduces outputs parallelly. The proposed method's model is trained on a system\nwith two NVIDIA T4 GPUs, using a combination of loss functions, including edge\npreservation, and demonstrates a 2x speedup in model inference time compared to\na sequential configuration. Implementation is available at:\nhttps://github.com/suwesh/Parallel-Perception-Network. Learned parameters of\nthe trained networks are provided at:\nhttps://huggingface.co/suwesh/ParallelPerceptionNetwork."
                },
                "authors": [
                    {
                        "name": "Suwesh Prasad Sah"
                    }
                ],
                "author_detail": {
                    "name": "Suwesh Prasad Sah"
                },
                "author": "Suwesh Prasad Sah",
                "arxiv_comment": "IEEE/ISED 2024",
                "arxiv_journal_ref": "12th International Conference on Intelligent Systems and Embedded\n  Design (ISED-2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02068v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02068v4",
                "updated": "2024-12-24T04:45:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    45,
                    51,
                    1,
                    359,
                    0
                ],
                "published": "2024-07-02T08:58:19Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    8,
                    58,
                    19,
                    1,
                    184,
                    0
                ],
                "title": "LPViT: Low-Power Semi-structured Pruning for Vision Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LPViT: Low-Power Semi-structured Pruning for Vision Transformers"
                },
                "summary": "Vision transformers have emerged as a promising alternative to convolutional\nneural networks for various image analysis tasks, offering comparable or\nsuperior performance. However, one significant drawback of ViTs is their\nresource-intensive nature, leading to increased memory footprint, computation\ncomplexity, and power consumption. To democratize this high-performance\ntechnology and make it more environmentally friendly, it is essential to\ncompress ViT models, reducing their resource requirements while maintaining\nhigh performance. In this paper, we introduce a new block-structured pruning to\naddress the resource-intensive issue for ViTs, offering a balanced trade-off\nbetween accuracy and hardware acceleration. Unlike unstructured pruning or\nchannel-wise structured pruning, block pruning leverages the block-wise\nstructure of linear layers, resulting in more efficient matrix multiplications.\nTo optimize this pruning scheme, our paper proposes a novel hardware-aware\nlearning objective that simultaneously maximizes speedup and minimizes power\nconsumption during inference, tailored to the block sparsity structure. This\nobjective eliminates the need for empirical look-up tables and focuses solely\non reducing parametrized layer connections. Moreover, our paper provides a\nlightweight algorithm to achieve post-training pruning for ViTs, utilizing\nsecond-order Taylor approximation and empirical optimization to solve the\nproposed hardware-aware objective. Extensive experiments on ImageNet are\nconducted across various ViT architectures, including DeiT-B and DeiT-S,\ndemonstrating competitive performance with other pruning methods and achieving\na remarkable balance between accuracy preservation and power savings.\nEspecially, we achieve up to 3.93x and 1.79x speedups on dedicated hardware and\nGPUs respectively for DeiT-B, and also observe an inference power reduction by\n1.4x on real-world GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision transformers have emerged as a promising alternative to convolutional\nneural networks for various image analysis tasks, offering comparable or\nsuperior performance. However, one significant drawback of ViTs is their\nresource-intensive nature, leading to increased memory footprint, computation\ncomplexity, and power consumption. To democratize this high-performance\ntechnology and make it more environmentally friendly, it is essential to\ncompress ViT models, reducing their resource requirements while maintaining\nhigh performance. In this paper, we introduce a new block-structured pruning to\naddress the resource-intensive issue for ViTs, offering a balanced trade-off\nbetween accuracy and hardware acceleration. Unlike unstructured pruning or\nchannel-wise structured pruning, block pruning leverages the block-wise\nstructure of linear layers, resulting in more efficient matrix multiplications.\nTo optimize this pruning scheme, our paper proposes a novel hardware-aware\nlearning objective that simultaneously maximizes speedup and minimizes power\nconsumption during inference, tailored to the block sparsity structure. This\nobjective eliminates the need for empirical look-up tables and focuses solely\non reducing parametrized layer connections. Moreover, our paper provides a\nlightweight algorithm to achieve post-training pruning for ViTs, utilizing\nsecond-order Taylor approximation and empirical optimization to solve the\nproposed hardware-aware objective. Extensive experiments on ImageNet are\nconducted across various ViT architectures, including DeiT-B and DeiT-S,\ndemonstrating competitive performance with other pruning methods and achieving\na remarkable balance between accuracy preservation and power savings.\nEspecially, we achieve up to 3.93x and 1.79x speedups on dedicated hardware and\nGPUs respectively for DeiT-B, and also observe an inference power reduction by\n1.4x on real-world GPUs."
                },
                "authors": [
                    {
                        "name": "Kaixin Xu"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Chunyun Chen"
                    },
                    {
                        "name": "Xue Geng"
                    },
                    {
                        "name": "Jie Lin"
                    },
                    {
                        "name": "Mohamed M. Sabry Aly"
                    },
                    {
                        "name": "Xulei Yang"
                    },
                    {
                        "name": "Min Wu"
                    },
                    {
                        "name": "Xiaoli Li"
                    },
                    {
                        "name": "Weisi Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weisi Lin"
                },
                "author": "Weisi Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02068v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02068v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15484v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15484v2",
                "updated": "2024-12-24T04:42:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    42,
                    49,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-20T01:37:22Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    1,
                    37,
                    22,
                    4,
                    355,
                    0
                ],
                "title": "Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and\n  Dual Evaluation Metrics for Factuality and Coverage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and\n  Dual Evaluation Metrics for Factuality and Coverage"
                },
                "summary": "Multimodal large language models (MLLMs) excel at generating highly detailed\ncaptions but often produce hallucinations. Our analysis reveals that existing\nhallucination detection methods struggle with detailed captions. We attribute\nthis to the increasing reliance of MLLMs on their generated text, rather than\nthe input image, as the sequence length grows. To address this issue, we\npropose a multiagent approach that leverages LLM-MLLM collaboration to correct\ngiven captions. Additionally, we introduce an evaluation framework and a\nbenchmark dataset to facilitate the systematic analysis of detailed captions.\nOur experiments demonstrate that our proposed evaluation method better aligns\nwith human judgments of factuality than existing metrics and that existing\napproaches to improve the MLLM factuality may fall short in hyper-detailed\nimage captioning tasks. In contrast, our proposed method significantly enhances\nthe factual accuracy of captions, even improving those generated by GPT-4V.\nFinally, we highlight a limitation of VQA-centric benchmarking by demonstrating\nthat an MLLM's performance on VQA benchmarks may not correlate with its ability\nto generate detailed image captions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) excel at generating highly detailed\ncaptions but often produce hallucinations. Our analysis reveals that existing\nhallucination detection methods struggle with detailed captions. We attribute\nthis to the increasing reliance of MLLMs on their generated text, rather than\nthe input image, as the sequence length grows. To address this issue, we\npropose a multiagent approach that leverages LLM-MLLM collaboration to correct\ngiven captions. Additionally, we introduce an evaluation framework and a\nbenchmark dataset to facilitate the systematic analysis of detailed captions.\nOur experiments demonstrate that our proposed evaluation method better aligns\nwith human judgments of factuality than existing metrics and that existing\napproaches to improve the MLLM factuality may fall short in hyper-detailed\nimage captioning tasks. In contrast, our proposed method significantly enhances\nthe factual accuracy of captions, even improving those generated by GPT-4V.\nFinally, we highlight a limitation of VQA-centric benchmarking by demonstrating\nthat an MLLM's performance on VQA benchmarks may not correlate with its ability\nto generate detailed image captions."
                },
                "authors": [
                    {
                        "name": "Saehyung Lee"
                    },
                    {
                        "name": "Seunghyun Yoon"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Jing Shi"
                    },
                    {
                        "name": "Sungroh Yoon"
                    }
                ],
                "author_detail": {
                    "name": "Sungroh Yoon"
                },
                "author": "Sungroh Yoon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15484v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15484v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15268v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15268v2",
                "updated": "2024-12-24T04:38:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    38,
                    57,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-17T06:28:28Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    6,
                    28,
                    28,
                    1,
                    352,
                    0
                ],
                "title": "Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic\n  Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic\n  Knowledge Graph"
                },
                "summary": "The rapid growth of social media platforms has raised significant concerns\nregarding online content toxicity. When Large Language Models (LLMs) are used\nfor toxicity detection, two key challenges emerge: 1) the absence of\ndomain-specific toxic knowledge leads to false negatives; 2) the excessive\nsensitivity of LLMs to toxic speech results in false positives, limiting\nfreedom of speech. To address these issues, we propose a novel method called\nMetaTox, leveraging graph search on a meta-toxic knowledge graph to enhance\nhatred and toxicity detection. First, we construct a comprehensive meta-toxic\nknowledge graph by utilizing LLMs to extract toxic information through a\nthree-step pipeline, with toxic benchmark datasets serving as corpora. Second,\nwe query the graph via retrieval and ranking processes to supplement accurate,\nrelevant toxic knowledge. Extensive experiments and in-depth case studies\nacross multiple datasets demonstrate that our MetaTox significantly decreases\nthe false positive rate while boosting overall toxicity detection performance.\nOur code will be available soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of social media platforms has raised significant concerns\nregarding online content toxicity. When Large Language Models (LLMs) are used\nfor toxicity detection, two key challenges emerge: 1) the absence of\ndomain-specific toxic knowledge leads to false negatives; 2) the excessive\nsensitivity of LLMs to toxic speech results in false positives, limiting\nfreedom of speech. To address these issues, we propose a novel method called\nMetaTox, leveraging graph search on a meta-toxic knowledge graph to enhance\nhatred and toxicity detection. First, we construct a comprehensive meta-toxic\nknowledge graph by utilizing LLMs to extract toxic information through a\nthree-step pipeline, with toxic benchmark datasets serving as corpora. Second,\nwe query the graph via retrieval and ranking processes to supplement accurate,\nrelevant toxic knowledge. Extensive experiments and in-depth case studies\nacross multiple datasets demonstrate that our MetaTox significantly decreases\nthe false positive rate while boosting overall toxicity detection performance.\nOur code will be available soon."
                },
                "authors": [
                    {
                        "name": "Yibo Zhao"
                    },
                    {
                        "name": "Jiapeng Zhu"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Xiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Li"
                },
                "author": "Xiang Li",
                "arxiv_comment": "8 pages of content",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15268v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15268v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18161v1",
                "updated": "2024-12-24T04:37:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    37,
                    7,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T04:37:07Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    37,
                    7,
                    1,
                    359,
                    0
                ],
                "title": "VISION: A Modular AI Assistant for Natural Human-Instrument Interaction\n  at Scientific User Facilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VISION: A Modular AI Assistant for Natural Human-Instrument Interaction\n  at Scientific User Facilities"
                },
                "summary": "Scientific user facilities, such as synchrotron beamlines, are equipped with\na wide array of hardware and software tools that require a codebase for\nhuman-computer-interaction. This often necessitates developers to be involved\nto establish connection between users/researchers and the complex\ninstrumentation. The advent of generative AI presents an opportunity to bridge\nthis knowledge gap, enabling seamless communication and efficient experimental\nworkflows. Here we present a modular architecture for the Virtual Scientific\nCompanion (VISION) by assembling multiple AI-enabled cognitive blocks that each\nscaffolds large language models (LLMs) for a specialized task. With VISION, we\nperformed LLM-based operation on the beamline workstation with low latency and\ndemonstrated the first voice-controlled experiment at an X-ray scattering\nbeamline. The modular and scalable architecture allows for easy adaptation to\nnew instrument and capabilities. Development on natural language-based\nscientific experimentation is a building block for an impending future where a\nscience exocortex -- a synthetic extension to the cognition of scientists --\nmay radically transform scientific practice and discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific user facilities, such as synchrotron beamlines, are equipped with\na wide array of hardware and software tools that require a codebase for\nhuman-computer-interaction. This often necessitates developers to be involved\nto establish connection between users/researchers and the complex\ninstrumentation. The advent of generative AI presents an opportunity to bridge\nthis knowledge gap, enabling seamless communication and efficient experimental\nworkflows. Here we present a modular architecture for the Virtual Scientific\nCompanion (VISION) by assembling multiple AI-enabled cognitive blocks that each\nscaffolds large language models (LLMs) for a specialized task. With VISION, we\nperformed LLM-based operation on the beamline workstation with low latency and\ndemonstrated the first voice-controlled experiment at an X-ray scattering\nbeamline. The modular and scalable architecture allows for easy adaptation to\nnew instrument and capabilities. Development on natural language-based\nscientific experimentation is a building block for an impending future where a\nscience exocortex -- a synthetic extension to the cognition of scientists --\nmay radically transform scientific practice and discovery."
                },
                "authors": [
                    {
                        "name": "Shray Mathur"
                    },
                    {
                        "name": "Noah van der Vleuten"
                    },
                    {
                        "name": "Kevin Yager"
                    },
                    {
                        "name": "Esther Tsai"
                    }
                ],
                "author_detail": {
                    "name": "Esther Tsai"
                },
                "author": "Esther Tsai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18156v1",
                "updated": "2024-12-24T04:28:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    28,
                    42,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T04:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    28,
                    42,
                    1,
                    359,
                    0
                ],
                "title": "scReader: Prompting Large Language Models to Interpret scRNA-seq Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "scReader: Prompting Large Language Models to Interpret scRNA-seq Data"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable advancements,\nprimarily due to their capabilities in modeling the hidden relationships within\ntext sequences. This innovation presents a unique opportunity in the field of\nlife sciences, where vast collections of single-cell omics data from multiple\nspecies provide a foundation for training foundational models. However, the\nchallenge lies in the disparity of data scales across different species,\nhindering the development of a comprehensive model for interpreting genetic\ndata across diverse organisms. In this study, we propose an innovative hybrid\napproach that integrates the general knowledge capabilities of LLMs with\ndomain-specific representation models for single-cell omics data\ninterpretation. We begin by focusing on genes as the fundamental unit of\nrepresentation. Gene representations are initialized using functional\ndescriptions, leveraging the strengths of mature language models such as\nLLaMA-2. By inputting single-cell gene-level expression data with prompts, we\neffectively model cellular representations based on the differential expression\nlevels of genes across various species and cell types. In the experiments, we\nconstructed developmental cells from humans and mice, specifically targeting\ncells that are challenging to annotate. We evaluated our methodology through\nbasic tasks such as cell annotation and visualization analysis. The results\ndemonstrate the efficacy of our approach compared to other methods using LLMs,\nhighlighting significant improvements in accuracy and interoperability. Our\nhybrid approach enhances the representation of single-cell data and offers a\nrobust framework for future research in cross-species genetic analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable advancements,\nprimarily due to their capabilities in modeling the hidden relationships within\ntext sequences. This innovation presents a unique opportunity in the field of\nlife sciences, where vast collections of single-cell omics data from multiple\nspecies provide a foundation for training foundational models. However, the\nchallenge lies in the disparity of data scales across different species,\nhindering the development of a comprehensive model for interpreting genetic\ndata across diverse organisms. In this study, we propose an innovative hybrid\napproach that integrates the general knowledge capabilities of LLMs with\ndomain-specific representation models for single-cell omics data\ninterpretation. We begin by focusing on genes as the fundamental unit of\nrepresentation. Gene representations are initialized using functional\ndescriptions, leveraging the strengths of mature language models such as\nLLaMA-2. By inputting single-cell gene-level expression data with prompts, we\neffectively model cellular representations based on the differential expression\nlevels of genes across various species and cell types. In the experiments, we\nconstructed developmental cells from humans and mice, specifically targeting\ncells that are challenging to annotate. We evaluated our methodology through\nbasic tasks such as cell annotation and visualization analysis. The results\ndemonstrate the efficacy of our approach compared to other methods using LLMs,\nhighlighting significant improvements in accuracy and interoperability. Our\nhybrid approach enhances the representation of single-cell data and offers a\nrobust framework for future research in cross-species genetic analysis."
                },
                "authors": [
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Qingqing Long"
                    },
                    {
                        "name": "Yuanchun Zhou"
                    },
                    {
                        "name": "Meng Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Meng Xiao"
                },
                "author": "Meng Xiao",
                "arxiv_comment": "8 pages, Accepted by ICDM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.14014v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.14014v4",
                "updated": "2024-12-24T04:27:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    27,
                    37,
                    1,
                    359,
                    0
                ],
                "published": "2023-05-23T12:51:20Z",
                "published_parsed": [
                    2023,
                    5,
                    23,
                    12,
                    51,
                    20,
                    1,
                    143,
                    0
                ],
                "title": "CLIP4STR: A Simple Baseline for Scene Text Recognition with Pre-trained\n  Vision-Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLIP4STR: A Simple Baseline for Scene Text Recognition with Pre-trained\n  Vision-Language Model"
                },
                "summary": "Pre-trained vision-language models~(VLMs) are the de-facto foundation models\nfor various downstream tasks. However, scene text recognition methods still\nprefer backbones pre-trained on a single modality, namely, the visual modality,\ndespite the potential of VLMs to serve as powerful scene text readers. For\nexample, CLIP can robustly identify regular (horizontal) and irregular\n(rotated, curved, blurred, or occluded) text in images. With such merits, we\ntransform CLIP into a scene text reader and introduce CLIP4STR, a simple yet\neffective STR method built upon image and text encoders of CLIP. It has two\nencoder-decoder branches: a visual branch and a cross-modal branch. The visual\nbranch provides an initial prediction based on the visual feature, and the\ncross-modal branch refines this prediction by addressing the discrepancy\nbetween the visual feature and text semantics. To fully leverage the\ncapabilities of both branches, we design a dual predict-and-refine decoding\nscheme for inference. We scale CLIP4STR in terms of the model size,\npre-training data, and training data, achieving state-of-the-art performance on\n13 STR benchmarks. Additionally, a comprehensive empirical study is provided to\nenhance the understanding of the adaptation of CLIP to STR. Our method\nestablishes a simple yet strong baseline for future STR research with VLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained vision-language models~(VLMs) are the de-facto foundation models\nfor various downstream tasks. However, scene text recognition methods still\nprefer backbones pre-trained on a single modality, namely, the visual modality,\ndespite the potential of VLMs to serve as powerful scene text readers. For\nexample, CLIP can robustly identify regular (horizontal) and irregular\n(rotated, curved, blurred, or occluded) text in images. With such merits, we\ntransform CLIP into a scene text reader and introduce CLIP4STR, a simple yet\neffective STR method built upon image and text encoders of CLIP. It has two\nencoder-decoder branches: a visual branch and a cross-modal branch. The visual\nbranch provides an initial prediction based on the visual feature, and the\ncross-modal branch refines this prediction by addressing the discrepancy\nbetween the visual feature and text semantics. To fully leverage the\ncapabilities of both branches, we design a dual predict-and-refine decoding\nscheme for inference. We scale CLIP4STR in terms of the model size,\npre-training data, and training data, achieving state-of-the-art performance on\n13 STR benchmarks. Additionally, a comprehensive empirical study is provided to\nenhance the understanding of the adaptation of CLIP to STR. Our method\nestablishes a simple yet strong baseline for future STR research with VLMs."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Ruijie Quan"
                    },
                    {
                        "name": "Linchao Zhu"
                    },
                    {
                        "name": "Yi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yang"
                },
                "author": "Yi Yang",
                "arxiv_doi": "10.1109/TIP.2024.3512354",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TIP.2024.3512354",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2305.14014v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.14014v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by T-IP. A PyTorch re-implementation is at\n  https://github.com/VamosC/CLIP4STR (Credit on GitHub@VamosC)",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18154v1",
                "updated": "2024-12-24T04:20:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    20,
                    43,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T04:20:43Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    20,
                    43,
                    1,
                    359,
                    0
                ],
                "title": "GeneSUM: Large Language Model-based Gene Summary Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeneSUM: Large Language Model-based Gene Summary Extraction"
                },
                "summary": "Emerging topics in biomedical research are continuously expanding, providing\na wealth of information about genes and their function. This rapid\nproliferation of knowledge presents unprecedented opportunities for scientific\ndiscovery and formidable challenges for researchers striving to keep abreast of\nthe latest advancements. One significant challenge is navigating the vast\ncorpus of literature to extract vital gene-related information, a\ntime-consuming and cumbersome task. To enhance the efficiency of this process,\nit is crucial to address several key challenges: (1) the overwhelming volume of\nliterature, (2) the complexity of gene functions, and (3) the automated\nintegration and generation. In response, we propose GeneSUM, a two-stage\nautomated gene summary extractor utilizing a large language model (LLM). Our\napproach retrieves and eliminates redundancy of target gene literature and then\nfine-tunes the LLM to refine and streamline the summarization process. We\nconducted extensive experiments to validate the efficacy of our proposed\nframework. The results demonstrate that LLM significantly enhances the\nintegration of gene-specific information, allowing more efficient\ndecision-making in ongoing research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging topics in biomedical research are continuously expanding, providing\na wealth of information about genes and their function. This rapid\nproliferation of knowledge presents unprecedented opportunities for scientific\ndiscovery and formidable challenges for researchers striving to keep abreast of\nthe latest advancements. One significant challenge is navigating the vast\ncorpus of literature to extract vital gene-related information, a\ntime-consuming and cumbersome task. To enhance the efficiency of this process,\nit is crucial to address several key challenges: (1) the overwhelming volume of\nliterature, (2) the complexity of gene functions, and (3) the automated\nintegration and generation. In response, we propose GeneSUM, a two-stage\nautomated gene summary extractor utilizing a large language model (LLM). Our\napproach retrieves and eliminates redundancy of target gene literature and then\nfine-tunes the LLM to refine and streamline the summarization process. We\nconducted extensive experiments to validate the efficacy of our proposed\nframework. The results demonstrate that LLM significantly enhances the\nintegration of gene-specific information, allowing more efficient\ndecision-making in ongoing research."
                },
                "authors": [
                    {
                        "name": "Zhijian Chen"
                    },
                    {
                        "name": "Chuan Hu"
                    },
                    {
                        "name": "Min Wu"
                    },
                    {
                        "name": "Qingqing Long"
                    },
                    {
                        "name": "Xuezhi Wang"
                    },
                    {
                        "name": "Yuanchun Zhou"
                    },
                    {
                        "name": "Meng Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Meng Xiao"
                },
                "author": "Meng Xiao",
                "arxiv_comment": "7 pages, Accepted by BIBM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16642v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16642v2",
                "updated": "2024-12-24T04:20:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    20,
                    18,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-21T14:24:32Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    14,
                    24,
                    32,
                    5,
                    356,
                    0
                ],
                "title": "L3TC: Leveraging RWKV for Learned Lossless Low-Complexity Text\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L3TC: Leveraging RWKV for Learned Lossless Low-Complexity Text\n  Compression"
                },
                "summary": "Learning-based probabilistic models can be combined with an entropy coder for\ndata compression. However, due to the high complexity of learning-based models,\ntheir practical application as text compressors has been largely overlooked. To\naddress this issue, our work focuses on a low-complexity design while\nmaintaining compression performance. We introduce a novel Learned Lossless\nLow-complexity Text Compression method (L3TC). Specifically, we conduct\nextensive experiments demonstrating that RWKV models achieve the fastest\ndecoding speed with a moderate compression ratio, making it the most suitable\nbackbone for our method. Second, we propose an outlier-aware tokenizer that\nuses a limited vocabulary to cover frequent tokens while allowing outliers to\nbypass the prediction and encoding. Third, we propose a novel high-rank\nreparameterization strategy that enhances the learning capability during\ntraining without increasing complexity during inference. Experimental results\nvalidate that our method achieves 48% bit saving compared to gzip compressor.\nBesides, L3TC offers compression performance comparable to other learned\ncompressors, with a 50x reduction in model parameters. More importantly, L3TC\nis the fastest among all learned compressors, providing real-time decoding\nspeeds up to megabytes per second. Our code is available at\nhttps://github.com/alipay/L3TC-leveraging-rwkv-for-learned-lossless-low-complexity-text-compression.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-based probabilistic models can be combined with an entropy coder for\ndata compression. However, due to the high complexity of learning-based models,\ntheir practical application as text compressors has been largely overlooked. To\naddress this issue, our work focuses on a low-complexity design while\nmaintaining compression performance. We introduce a novel Learned Lossless\nLow-complexity Text Compression method (L3TC). Specifically, we conduct\nextensive experiments demonstrating that RWKV models achieve the fastest\ndecoding speed with a moderate compression ratio, making it the most suitable\nbackbone for our method. Second, we propose an outlier-aware tokenizer that\nuses a limited vocabulary to cover frequent tokens while allowing outliers to\nbypass the prediction and encoding. Third, we propose a novel high-rank\nreparameterization strategy that enhances the learning capability during\ntraining without increasing complexity during inference. Experimental results\nvalidate that our method achieves 48% bit saving compared to gzip compressor.\nBesides, L3TC offers compression performance comparable to other learned\ncompressors, with a 50x reduction in model parameters. More importantly, L3TC\nis the fastest among all learned compressors, providing real-time decoding\nspeeds up to megabytes per second. Our code is available at\nhttps://github.com/alipay/L3TC-leveraging-rwkv-for-learned-lossless-low-complexity-text-compression.git."
                },
                "authors": [
                    {
                        "name": "Junxuan Zhang"
                    },
                    {
                        "name": "Zhengxue Cheng"
                    },
                    {
                        "name": "Yan Zhao"
                    },
                    {
                        "name": "Shihao Wang"
                    },
                    {
                        "name": "Dajiang Zhou"
                    },
                    {
                        "name": "Guo Lu"
                    },
                    {
                        "name": "Li Song"
                    }
                ],
                "author_detail": {
                    "name": "Li Song"
                },
                "author": "Li Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16642v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16642v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16746v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16746v2",
                "updated": "2024-12-24T04:04:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    4,
                    54,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-21T19:42:40Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    19,
                    42,
                    40,
                    5,
                    356,
                    0
                ],
                "title": "Unpacking Political Bias in Large Language Models: Insights Across Topic\n  Polarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unpacking Political Bias in Large Language Models: Insights Across Topic\n  Polarization"
                },
                "summary": "Large Language Models (LLMs) have been widely used to generate responses on\nsocial topics due to their world knowledge and generative capabilities. Beyond\nreasoning and generation performance, political bias is an essential issue that\nwarrants attention. Political bias, as a universal phenomenon in human society,\nmay be transferred to LLMs and distort LLMs' behaviors of information\nacquisition and dissemination with humans, leading to unequal access among\ndifferent groups of people. To prevent LLMs from reproducing and reinforcing\npolitical biases, and to encourage fairer LLM-human interactions,\ncomprehensively examining political bias in popular LLMs becomes urgent and\ncrucial.\n  In this study, we systematically measure the political biases in a wide range\nof LLMs, using a curated set of questions addressing political bias in various\ncontexts. Our findings reveal distinct patterns in how LLMs respond to\npolitical topics. For highly polarized topics, most LLMs exhibit a pronounced\nleft-leaning bias. Conversely, less polarized topics elicit greater consensus,\nwith similar response patterns across different LLMs. Additionally, we analyze\nhow LLM characteristics, including release date, model scale, and region of\norigin affect political bias. The results indicate political biases evolve with\nmodel scale and release date, and are also influenced by regional factors of\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely used to generate responses on\nsocial topics due to their world knowledge and generative capabilities. Beyond\nreasoning and generation performance, political bias is an essential issue that\nwarrants attention. Political bias, as a universal phenomenon in human society,\nmay be transferred to LLMs and distort LLMs' behaviors of information\nacquisition and dissemination with humans, leading to unequal access among\ndifferent groups of people. To prevent LLMs from reproducing and reinforcing\npolitical biases, and to encourage fairer LLM-human interactions,\ncomprehensively examining political bias in popular LLMs becomes urgent and\ncrucial.\n  In this study, we systematically measure the political biases in a wide range\nof LLMs, using a curated set of questions addressing political bias in various\ncontexts. Our findings reveal distinct patterns in how LLMs respond to\npolitical topics. For highly polarized topics, most LLMs exhibit a pronounced\nleft-leaning bias. Conversely, less polarized topics elicit greater consensus,\nwith similar response patterns across different LLMs. Additionally, we analyze\nhow LLM characteristics, including release date, model scale, and region of\norigin affect political bias. The results indicate political biases evolve with\nmodel scale and release date, and are also influenced by regional factors of\nLLMs."
                },
                "authors": [
                    {
                        "name": "Kaiqi Yang"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Yucheng Chu"
                    },
                    {
                        "name": "Yuping Lin"
                    },
                    {
                        "name": "Tai-Quan Peng"
                    },
                    {
                        "name": "Hui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hui Liu"
                },
                "author": "Hui Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16746v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16746v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.18603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18603v1",
                "updated": "2024-12-24T18:56:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    18,
                    56,
                    46,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T18:56:46Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    18,
                    56,
                    46,
                    1,
                    359,
                    0
                ],
                "title": "Long-Form Speech Generation with Spoken Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Form Speech Generation with Spoken Language Models"
                },
                "summary": "We consider the generative modeling of speech over multiple minutes, a\nrequirement for long-form multimedia generation and audio-native voice\nassistants. However, current spoken language models struggle to generate\nplausible speech past tens of seconds, from high temporal resolution of speech\ntokens causing loss of coherence, to architectural issues with long-sequence\ntraining or extrapolation, to memory costs at inference time. With these\nconsiderations we propose SpeechSSM, the first speech language model to learn\nfrom and sample long-form spoken audio (e.g., 16 minutes of read or\nextemporaneous speech) in a single decoding session without text intermediates,\nbased on recent advances in linear-time sequence modeling. Furthermore, to\naddress growing challenges in spoken language evaluation, especially in this\nnew long-form setting, we propose: new embedding-based and LLM-judged metrics;\nquality measurements over length and time; and a new benchmark for long-form\nspeech processing and generation, LibriSpeech-Long. Speech samples and the\ndataset are released at\nhttps://google.github.io/tacotron/publications/speechssm/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the generative modeling of speech over multiple minutes, a\nrequirement for long-form multimedia generation and audio-native voice\nassistants. However, current spoken language models struggle to generate\nplausible speech past tens of seconds, from high temporal resolution of speech\ntokens causing loss of coherence, to architectural issues with long-sequence\ntraining or extrapolation, to memory costs at inference time. With these\nconsiderations we propose SpeechSSM, the first speech language model to learn\nfrom and sample long-form spoken audio (e.g., 16 minutes of read or\nextemporaneous speech) in a single decoding session without text intermediates,\nbased on recent advances in linear-time sequence modeling. Furthermore, to\naddress growing challenges in spoken language evaluation, especially in this\nnew long-form setting, we propose: new embedding-based and LLM-judged metrics;\nquality measurements over length and time; and a new benchmark for long-form\nspeech processing and generation, LibriSpeech-Long. Speech samples and the\ndataset are released at\nhttps://google.github.io/tacotron/publications/speechssm/"
                },
                "authors": [
                    {
                        "name": "Se Jin Park"
                    },
                    {
                        "name": "Julian Salazar"
                    },
                    {
                        "name": "Aren Jansen"
                    },
                    {
                        "name": "Keisuke Kinoshita"
                    },
                    {
                        "name": "Yong Man Ro"
                    },
                    {
                        "name": "RJ Skerry-Ryan"
                    }
                ],
                "author_detail": {
                    "name": "RJ Skerry-Ryan"
                },
                "author": "RJ Skerry-Ryan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18601v1",
                "updated": "2024-12-24T18:56:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    18,
                    56,
                    0,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T18:56:00Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    18,
                    56,
                    0,
                    1,
                    359,
                    0
                ],
                "title": "Decentralized Intelligence in GameFi: Embodied AI Agents and the\n  Convergence of DeFi and Virtual Ecosystems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Intelligence in GameFi: Embodied AI Agents and the\n  Convergence of DeFi and Virtual Ecosystems"
                },
                "summary": "In the rapidly evolving landscape of GameFi, a fusion of gaming and\ndecentralized finance (DeFi), there exists a critical need to enhance player\nengagement and economic interaction within gaming ecosystems. Our GameFi\necosystem aims to fundamentally transform this landscape by integrating\nadvanced embodied AI agents into GameFi platforms. These AI agents, developed\nusing cutting-edge large language models (LLMs), such as GPT-4 and Claude AI,\nare capable of proactive, adaptive, and contextually rich interactions with\nplayers. By going beyond traditional scripted responses, these agents become\nintegral participants in the game's narrative and economic systems, directly\ninfluencing player strategies and in-game economies. We address the limitations\nof current GameFi platforms, which often lack immersive AI interactions and\nmechanisms for community engagement or creator monetization. Through the deep\nintegration of AI agents with blockchain technology, we establish a\nconsensus-driven, decentralized GameFi ecosystem. This ecosystem empowers\ncreators to monetize their contributions and fosters democratic collaboration\namong players and creators. Furthermore, by embedding DeFi mechanisms into the\ngaming experience, we enhance economic participation and provide new\nopportunities for financial interactions within the game. Our approach enhances\nplayer immersion and retention and advances the GameFi ecosystem by bridging\ntraditional gaming with Web3 technologies. By integrating sophisticated AI and\nDeFi elements, we contribute to the development of more engaging, economically\nrobust, and community-centric gaming environments. This project represents a\nsignificant advancement in the state-of-the-art in GameFi, offering insights\nand methodologies that can be applied throughout the gaming industry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving landscape of GameFi, a fusion of gaming and\ndecentralized finance (DeFi), there exists a critical need to enhance player\nengagement and economic interaction within gaming ecosystems. Our GameFi\necosystem aims to fundamentally transform this landscape by integrating\nadvanced embodied AI agents into GameFi platforms. These AI agents, developed\nusing cutting-edge large language models (LLMs), such as GPT-4 and Claude AI,\nare capable of proactive, adaptive, and contextually rich interactions with\nplayers. By going beyond traditional scripted responses, these agents become\nintegral participants in the game's narrative and economic systems, directly\ninfluencing player strategies and in-game economies. We address the limitations\nof current GameFi platforms, which often lack immersive AI interactions and\nmechanisms for community engagement or creator monetization. Through the deep\nintegration of AI agents with blockchain technology, we establish a\nconsensus-driven, decentralized GameFi ecosystem. This ecosystem empowers\ncreators to monetize their contributions and fosters democratic collaboration\namong players and creators. Furthermore, by embedding DeFi mechanisms into the\ngaming experience, we enhance economic participation and provide new\nopportunities for financial interactions within the game. Our approach enhances\nplayer immersion and retention and advances the GameFi ecosystem by bridging\ntraditional gaming with Web3 technologies. By integrating sophisticated AI and\nDeFi elements, we contribute to the development of more engaging, economically\nrobust, and community-centric gaming environments. This project represents a\nsignificant advancement in the state-of-the-art in GameFi, offering insights\nand methodologies that can be applied throughout the gaming industry."
                },
                "authors": [
                    {
                        "name": "Fernando Jia"
                    },
                    {
                        "name": "Jade Zheng"
                    },
                    {
                        "name": "Florence Li"
                    }
                ],
                "author_detail": {
                    "name": "Florence Li"
                },
                "author": "Florence Li",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18588v1",
                "updated": "2024-12-24T18:41:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    18,
                    41,
                    15,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T18:41:15Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    18,
                    41,
                    15,
                    1,
                    359,
                    0
                ],
                "title": "A Paragraph is All It Takes: Rich Robot Behaviors from Interacting,\n  Trusted LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Paragraph is All It Takes: Rich Robot Behaviors from Interacting,\n  Trusted LLMs"
                },
                "summary": "Large Language Models (LLMs) are compact representations of all public\nknowledge of our physical environment and animal and human behaviors. The\napplication of LLMs to robotics may offer a path to highly capable robots that\nperform well across most human tasks with limited or even zero tuning. Aside\nfrom increasingly sophisticated reasoning and task planning, networks of\n(suitably designed) LLMs offer ease of upgrading capabilities and allow humans\nto directly observe the robot's thinking. Here we explore the advantages,\nlimitations, and particularities of using LLMs to control physical robots. The\nbasic system consists of four LLMs communicating via a human language data bus\nimplemented via web sockets and ROS2 message passing. Surprisingly, rich robot\nbehaviors and good performance across different tasks could be achieved despite\nthe robot's data fusion cycle running at only 1Hz and the central data bus\nrunning at the extremely limited rates of the human brain, of around 40 bits/s.\nThe use of natural language for inter-LLM communication allowed the robot's\nreasoning and decision making to be directly observed by humans and made it\ntrivial to bias the system's behavior with sets of rules written in plain\nEnglish. These rules were immutably written into Ethereum, a global, public,\nand censorship resistant Turing-complete computer. We suggest that by using\nnatural language as the data bus among interacting AIs, and immutable public\nledgers to store behavior constraints, it is possible to build robots that\ncombine unexpectedly rich performance, upgradability, and durable alignment\nwith humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are compact representations of all public\nknowledge of our physical environment and animal and human behaviors. The\napplication of LLMs to robotics may offer a path to highly capable robots that\nperform well across most human tasks with limited or even zero tuning. Aside\nfrom increasingly sophisticated reasoning and task planning, networks of\n(suitably designed) LLMs offer ease of upgrading capabilities and allow humans\nto directly observe the robot's thinking. Here we explore the advantages,\nlimitations, and particularities of using LLMs to control physical robots. The\nbasic system consists of four LLMs communicating via a human language data bus\nimplemented via web sockets and ROS2 message passing. Surprisingly, rich robot\nbehaviors and good performance across different tasks could be achieved despite\nthe robot's data fusion cycle running at only 1Hz and the central data bus\nrunning at the extremely limited rates of the human brain, of around 40 bits/s.\nThe use of natural language for inter-LLM communication allowed the robot's\nreasoning and decision making to be directly observed by humans and made it\ntrivial to bias the system's behavior with sets of rules written in plain\nEnglish. These rules were immutably written into Ethereum, a global, public,\nand censorship resistant Turing-complete computer. We suggest that by using\nnatural language as the data bus among interacting AIs, and immutable public\nledgers to store behavior constraints, it is possible to build robots that\ncombine unexpectedly rich performance, upgradability, and durable alignment\nwith humans."
                },
                "authors": [
                    {
                        "name": "OpenMind"
                    },
                    {
                        "name": "Shaohong Zhong"
                    },
                    {
                        "name": "Adam Zhou"
                    },
                    {
                        "name": "Boyuan Chen"
                    },
                    {
                        "name": "Homin Luo"
                    },
                    {
                        "name": "Jan Liphardt"
                    }
                ],
                "author_detail": {
                    "name": "Jan Liphardt"
                },
                "author": "Jan Liphardt",
                "arxiv_comment": "10 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10924v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10924v3",
                "updated": "2024-12-24T17:56:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    17,
                    56,
                    50,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-14T18:18:52Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    18,
                    18,
                    52,
                    5,
                    349,
                    0
                ],
                "title": "Tokens, the oft-overlooked appetizer: Large language models, the\n  distributional hypothesis, and meaning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokens, the oft-overlooked appetizer: Large language models, the\n  distributional hypothesis, and meaning"
                },
                "summary": "Tokenization is a necessary component within the current architecture of many\nlanguage models, including the transformer-based large language models (LLMs)\nof Generative AI, yet its impact on the model's cognition is often overlooked.\nWe argue that LLMs demonstrate that the Distributional Hypothesis (DH) is\nsufficient for reasonably human-like language performance, and that the\nemergence of human-meaningful linguistic units among tokens motivates\nlinguistically-informed interventions in existing, linguistically-agnostic\ntokenization techniques, particularly with respect to their roles as (1)\nsemantic primitives and as (2) vehicles for conveying salient distributional\npatterns from human language to the model. We explore tokenizations from a BPE\ntokenizer; extant model vocabularies obtained from Hugging Face and tiktoken;\nand the information in exemplar token vectors as they move through the layers\nof a RoBERTa (large) model. Besides creating sub-optimal semantic building\nblocks and obscuring the model's access to the necessary distributional\npatterns, we describe how tokenization pretraining can be a backdoor for bias\nand other unwanted content, which current alignment practices may not\nremediate. Additionally, we relay evidence that the tokenization algorithm's\nobjective function impacts the LLM's cognition, despite being meaningfully\ninsulated from the main system intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization is a necessary component within the current architecture of many\nlanguage models, including the transformer-based large language models (LLMs)\nof Generative AI, yet its impact on the model's cognition is often overlooked.\nWe argue that LLMs demonstrate that the Distributional Hypothesis (DH) is\nsufficient for reasonably human-like language performance, and that the\nemergence of human-meaningful linguistic units among tokens motivates\nlinguistically-informed interventions in existing, linguistically-agnostic\ntokenization techniques, particularly with respect to their roles as (1)\nsemantic primitives and as (2) vehicles for conveying salient distributional\npatterns from human language to the model. We explore tokenizations from a BPE\ntokenizer; extant model vocabularies obtained from Hugging Face and tiktoken;\nand the information in exemplar token vectors as they move through the layers\nof a RoBERTa (large) model. Besides creating sub-optimal semantic building\nblocks and obscuring the model's access to the necessary distributional\npatterns, we describe how tokenization pretraining can be a backdoor for bias\nand other unwanted content, which current alignment practices may not\nremediate. Additionally, we relay evidence that the tokenization algorithm's\nobjective function impacts the LLM's cognition, despite being meaningfully\ninsulated from the main system intelligence."
                },
                "authors": [
                    {
                        "name": "Julia Witte Zimmerman"
                    },
                    {
                        "name": "Denis Hudon"
                    },
                    {
                        "name": "Kathryn Cramer"
                    },
                    {
                        "name": "Alejandro J. Ruiz"
                    },
                    {
                        "name": "Calla Beauregard"
                    },
                    {
                        "name": "Ashley Fehr"
                    },
                    {
                        "name": "Mikaela Irene Fudolig"
                    },
                    {
                        "name": "Bradford Demarest"
                    },
                    {
                        "name": "Yoshi Meke Bird"
                    },
                    {
                        "name": "Milo Z. Trujillo"
                    },
                    {
                        "name": "Christopher M. Danforth"
                    },
                    {
                        "name": "Peter Sheridan Dodds"
                    }
                ],
                "author_detail": {
                    "name": "Peter Sheridan Dodds"
                },
                "author": "Peter Sheridan Dodds",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10924v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10924v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18573v1",
                "updated": "2024-12-24T17:56:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    17,
                    56,
                    8,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T17:56:08Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    17,
                    56,
                    8,
                    1,
                    359,
                    0
                ],
                "title": "How Well Do LLMs Generate Code for Different Application Domains?\n  Benchmark and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Well Do LLMs Generate Code for Different Application Domains?\n  Benchmark and Evaluation"
                },
                "summary": "Recently, an increasing number of AI-driven programming assistants powered by\ncode LLMs have been integrated into various real-world software development\nenvironments, significantly boosting developer productivity. However, existing\ncode generation benchmarks primarily focus on general-purpose scenarios,\nleaving the code generation performance of LLMs for specific application\ndomains largely unknown. In this paper, we introduce a new benchmark,\nMultiCodeBench, to fill this gap. MultiCodeBench comprises 2,400 programming\ntasks, covering 12 popular software development domains and 15 programming\nlanguages. Specifically, we perform in-depth research to identify these 12\napplication domains. Given that each domain may involve multiple technical\nframeworks, and that different frameworks present distinct challenges in the\ncoding process, we categorize the commonly used frameworks and platforms within\neach domain. We then sample programming problems from GitHub repositories\nrelated to these subdomains. To ensure the quality of the tasks and mitigate\ndata leakage issues, we invite annotators to rewrite the docstrings for each\ntask in MultiCodeBench. Additionally, we build a static analysis-based\ndependency parsing tool to extract the dependencies in the ground truth for\neach task, enabling deeper performance analysis. Through extensive experiments\non MultiCodeBench with eleven representative mainstream LLMs, we reveal the\ncode generation performance of the LLMs across different application domains,\nproviding practical insights for developers in downstream fields when selecting\nLLMs. Furthermore, we analyze the reasons behind the models' failures in\ncompleting software application development tasks, offering guidance for model\ndevelopers to enhance domain-specific code generation capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, an increasing number of AI-driven programming assistants powered by\ncode LLMs have been integrated into various real-world software development\nenvironments, significantly boosting developer productivity. However, existing\ncode generation benchmarks primarily focus on general-purpose scenarios,\nleaving the code generation performance of LLMs for specific application\ndomains largely unknown. In this paper, we introduce a new benchmark,\nMultiCodeBench, to fill this gap. MultiCodeBench comprises 2,400 programming\ntasks, covering 12 popular software development domains and 15 programming\nlanguages. Specifically, we perform in-depth research to identify these 12\napplication domains. Given that each domain may involve multiple technical\nframeworks, and that different frameworks present distinct challenges in the\ncoding process, we categorize the commonly used frameworks and platforms within\neach domain. We then sample programming problems from GitHub repositories\nrelated to these subdomains. To ensure the quality of the tasks and mitigate\ndata leakage issues, we invite annotators to rewrite the docstrings for each\ntask in MultiCodeBench. Additionally, we build a static analysis-based\ndependency parsing tool to extract the dependencies in the ground truth for\neach task, enabling deeper performance analysis. Through extensive experiments\non MultiCodeBench with eleven representative mainstream LLMs, we reveal the\ncode generation performance of the LLMs across different application domains,\nproviding practical insights for developers in downstream fields when selecting\nLLMs. Furthermore, we analyze the reasons behind the models' failures in\ncompleting software application development tasks, offering guidance for model\ndevelopers to enhance domain-specific code generation capabilities."
                },
                "authors": [
                    {
                        "name": "Dewu Zheng"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Ensheng Shi"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16135v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16135v2",
                "updated": "2024-12-24T17:50:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    17,
                    50,
                    1,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-20T18:31:24Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    31,
                    24,
                    4,
                    355,
                    0
                ],
                "title": "Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models\n  into Assembly Code Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models\n  into Assembly Code Obfuscation"
                },
                "summary": "Malware authors often employ code obfuscations to make their malware harder\nto detect. Existing tools for generating obfuscated code often require access\nto the original source code (e.g., C++ or Java), and adding new obfuscations is\na non-trivial, labor-intensive process. In this study, we ask the following\nquestion: Can Large Language Models (LLMs) potentially generate a new\nobfuscated assembly code? If so, this poses a risk to anti-virus engines and\npotentially increases the flexibility of attackers to create new obfuscation\npatterns. We answer this in the affirmative by developing the MetamorphASM\nbenchmark comprising MetamorphASM Dataset (MAD) along with three code\nobfuscation techniques: dead code, register substitution, and control flow\nchange. The MetamorphASM systematically evaluates the ability of LLMs to\ngenerate and analyze obfuscated code using MAD, which contains 328,200\nobfuscated assembly code samples. We release this dataset and analyze the\nsuccess rate of various LLMs (e.g., GPT-3.5/4, GPT-4o-mini, Starcoder,\nCodeGemma, CodeLlama, CodeT5, and LLaMA 3.1) in generating obfuscated assembly\ncode. The evaluation was performed using established information-theoretic\nmetrics and manual human review to ensure correctness and provide the\nfoundation for researchers to study and develop remediations to this risk. The\nsource code can be found at the following GitHub link:\nhttps://github.com/mohammadi-ali/MetamorphASM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Malware authors often employ code obfuscations to make their malware harder\nto detect. Existing tools for generating obfuscated code often require access\nto the original source code (e.g., C++ or Java), and adding new obfuscations is\na non-trivial, labor-intensive process. In this study, we ask the following\nquestion: Can Large Language Models (LLMs) potentially generate a new\nobfuscated assembly code? If so, this poses a risk to anti-virus engines and\npotentially increases the flexibility of attackers to create new obfuscation\npatterns. We answer this in the affirmative by developing the MetamorphASM\nbenchmark comprising MetamorphASM Dataset (MAD) along with three code\nobfuscation techniques: dead code, register substitution, and control flow\nchange. The MetamorphASM systematically evaluates the ability of LLMs to\ngenerate and analyze obfuscated code using MAD, which contains 328,200\nobfuscated assembly code samples. We release this dataset and analyze the\nsuccess rate of various LLMs (e.g., GPT-3.5/4, GPT-4o-mini, Starcoder,\nCodeGemma, CodeLlama, CodeT5, and LLaMA 3.1) in generating obfuscated assembly\ncode. The evaluation was performed using established information-theoretic\nmetrics and manual human review to ensure correctness and provide the\nfoundation for researchers to study and develop remediations to this risk. The\nsource code can be found at the following GitHub link:\nhttps://github.com/mohammadi-ali/MetamorphASM."
                },
                "authors": [
                    {
                        "name": "Seyedreza Mohseni"
                    },
                    {
                        "name": "Seyedali Mohammadi"
                    },
                    {
                        "name": "Deepa Tilwani"
                    },
                    {
                        "name": "Yash Saxena"
                    },
                    {
                        "name": "Gerald Ndawula"
                    },
                    {
                        "name": "Sriram Vema"
                    },
                    {
                        "name": "Edward Raff"
                    },
                    {
                        "name": "Manas Gaur"
                    }
                ],
                "author_detail": {
                    "name": "Manas Gaur"
                },
                "author": "Manas Gaur",
                "arxiv_comment": "To appear in AAAI 2025, Main Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16135v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16135v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18566v1",
                "updated": "2024-12-24T17:37:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    17,
                    37,
                    11,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T17:37:11Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    17,
                    37,
                    11,
                    1,
                    359,
                    0
                ],
                "title": "Zero-resource Speech Translation and Recognition with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-resource Speech Translation and Recognition with LLMs"
                },
                "summary": "Despite recent advancements in speech processing, zero-resource speech\ntranslation (ST) and automatic speech recognition (ASR) remain challenging\nproblems. In this work, we propose to leverage a multilingual Large Language\nModel (LLM) to perform ST and ASR in languages for which the model has never\nseen paired audio-text data. We achieve this by using a pre-trained\nmultilingual speech encoder, a multilingual LLM, and a lightweight adaptation\nmodule that maps the audio representations to the token embedding space of the\nLLM. We perform several experiments both in ST and ASR to understand how to\nbest train the model and what data has the most impact on performance in\npreviously unseen languages. In ST, our best model is capable to achieve BLEU\nscores over 23 in CoVoST2 for two previously unseen languages, while in ASR, we\nachieve WERs of up to 28.2\\%. We finally show that the performance of our\nsystem is bounded by the ability of the LLM to output text in the desired\nlanguage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advancements in speech processing, zero-resource speech\ntranslation (ST) and automatic speech recognition (ASR) remain challenging\nproblems. In this work, we propose to leverage a multilingual Large Language\nModel (LLM) to perform ST and ASR in languages for which the model has never\nseen paired audio-text data. We achieve this by using a pre-trained\nmultilingual speech encoder, a multilingual LLM, and a lightweight adaptation\nmodule that maps the audio representations to the token embedding space of the\nLLM. We perform several experiments both in ST and ASR to understand how to\nbest train the model and what data has the most impact on performance in\npreviously unseen languages. In ST, our best model is capable to achieve BLEU\nscores over 23 in CoVoST2 for two previously unseen languages, while in ASR, we\nachieve WERs of up to 28.2\\%. We finally show that the performance of our\nsystem is bounded by the ability of the LLM to output text in the desired\nlanguage."
                },
                "authors": [
                    {
                        "name": "Karel Mundnich"
                    },
                    {
                        "name": "Xing Niu"
                    },
                    {
                        "name": "Prashant Mathur"
                    },
                    {
                        "name": "Srikanth Ronanki"
                    },
                    {
                        "name": "Brady Houston"
                    },
                    {
                        "name": "Veera Raghavendra Elluru"
                    },
                    {
                        "name": "Nilaksh Das"
                    },
                    {
                        "name": "Zejiang Hou"
                    },
                    {
                        "name": "Goeric Huybrechts"
                    },
                    {
                        "name": "Anshu Bhatia"
                    },
                    {
                        "name": "Daniel Garcia-Romero"
                    },
                    {
                        "name": "Kyu J. Han"
                    },
                    {
                        "name": "Katrin Kirchhoff"
                    }
                ],
                "author_detail": {
                    "name": "Katrin Kirchhoff"
                },
                "author": "Katrin Kirchhoff",
                "arxiv_comment": "ICASSP 2025, 5 pages, 2 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18552v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18552v1",
                "updated": "2024-12-24T17:05:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    17,
                    5,
                    26,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T17:05:26Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    17,
                    5,
                    26,
                    1,
                    359,
                    0
                ],
                "title": "Distilling Fine-grained Sentiment Understanding from Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Fine-grained Sentiment Understanding from Large Language\n  Models"
                },
                "summary": "Fine-grained sentiment analysis (FSA) aims to extract and summarize user\nopinions from vast opinionated text. Recent studies demonstrate that large\nlanguage models (LLMs) possess exceptional sentiment understanding\ncapabilities. However, directly deploying LLMs for FSA applications incurs high\ninference costs. Therefore, this paper investigates the distillation of\nfine-grained sentiment understanding from LLMs into small language models\n(SLMs). We prompt LLMs to examine and interpret the sentiments of given reviews\nand then utilize the generated content to pretrain SLMs. Additionally, we\ndevelop a comprehensive FSA benchmark to evaluate both SLMs and LLMs. Extensive\nexperiments on this benchmark reveal that: (1) distillation significantly\nenhances the performance of SLMs in FSA tasks, achieving a 6.00\\% improvement\nin $F_1$-score, and the distilled model can outperform Llama-2-7b with only\n220M parameters; (2) distillation equips SLMs with excellent zero-shot\nsentiment classification capabilities, enabling them to match or even exceed\ntheir teacher models. These results suggest that distillation from LLMs is a\nhighly promising direction for FSA. We will release our code, data, and\npretrained model weights at\n\\url{https://github.com/HITSZ-HLT/FSA-Distillation}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained sentiment analysis (FSA) aims to extract and summarize user\nopinions from vast opinionated text. Recent studies demonstrate that large\nlanguage models (LLMs) possess exceptional sentiment understanding\ncapabilities. However, directly deploying LLMs for FSA applications incurs high\ninference costs. Therefore, this paper investigates the distillation of\nfine-grained sentiment understanding from LLMs into small language models\n(SLMs). We prompt LLMs to examine and interpret the sentiments of given reviews\nand then utilize the generated content to pretrain SLMs. Additionally, we\ndevelop a comprehensive FSA benchmark to evaluate both SLMs and LLMs. Extensive\nexperiments on this benchmark reveal that: (1) distillation significantly\nenhances the performance of SLMs in FSA tasks, achieving a 6.00\\% improvement\nin $F_1$-score, and the distilled model can outperform Llama-2-7b with only\n220M parameters; (2) distillation equips SLMs with excellent zero-shot\nsentiment classification capabilities, enabling them to match or even exceed\ntheir teacher models. These results suggest that distillation from LLMs is a\nhighly promising direction for FSA. We will release our code, data, and\npretrained model weights at\n\\url{https://github.com/HITSZ-HLT/FSA-Distillation}."
                },
                "authors": [
                    {
                        "name": "Yice Zhang"
                    },
                    {
                        "name": "Guangyu Xie"
                    },
                    {
                        "name": "Hongling Xu"
                    },
                    {
                        "name": "Kaiheng Hou"
                    },
                    {
                        "name": "Jianzhu Bao"
                    },
                    {
                        "name": "Qianlong Wang"
                    },
                    {
                        "name": "Shiwei Chen"
                    },
                    {
                        "name": "Ruifeng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruifeng Xu"
                },
                "author": "Ruifeng Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18552v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18552v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18551v1",
                "updated": "2024-12-24T17:03:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    17,
                    3,
                    44,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T17:03:44Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    17,
                    3,
                    44,
                    1,
                    359,
                    0
                ],
                "title": "Libra-Leaderboard: Towards Responsible AI through a Balanced Leaderboard\n  of Safety and Capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Libra-Leaderboard: Towards Responsible AI through a Balanced Leaderboard\n  of Safety and Capability"
                },
                "summary": "To address this gap, we introduce Libra-Leaderboard, a comprehensive\nframework designed to rank LLMs through a balanced evaluation of performance\nand safety. Combining a dynamic leaderboard with an interactive LLM arena,\nLibra-Leaderboard encourages the joint optimization of capability and safety.\nUnlike traditional approaches that average performance and safety metrics,\nLibra-Leaderboard uses a distance-to-optimal-score method to calculate the\noverall rankings. This approach incentivizes models to achieve a balance rather\nthan excelling in one dimension at the expense of some other ones. In the first\nrelease, Libra-Leaderboard evaluates 26 mainstream LLMs from 14 leading\norganizations, identifying critical safety challenges even in state-of-the-art\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To address this gap, we introduce Libra-Leaderboard, a comprehensive\nframework designed to rank LLMs through a balanced evaluation of performance\nand safety. Combining a dynamic leaderboard with an interactive LLM arena,\nLibra-Leaderboard encourages the joint optimization of capability and safety.\nUnlike traditional approaches that average performance and safety metrics,\nLibra-Leaderboard uses a distance-to-optimal-score method to calculate the\noverall rankings. This approach incentivizes models to achieve a balance rather\nthan excelling in one dimension at the expense of some other ones. In the first\nrelease, Libra-Leaderboard evaluates 26 mainstream LLMs from 14 leading\norganizations, identifying critical safety challenges even in state-of-the-art\nmodels."
                },
                "authors": [
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Xudong Han"
                    },
                    {
                        "name": "Zenan Zhai"
                    },
                    {
                        "name": "Honglin Mu"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Zhenxuan Zhang"
                    },
                    {
                        "name": "Yilin Geng"
                    },
                    {
                        "name": "Shom Lin"
                    },
                    {
                        "name": "Renxi Wang"
                    },
                    {
                        "name": "Artem Shelmanov"
                    },
                    {
                        "name": "Xiangyu Qi"
                    },
                    {
                        "name": "Yuxia Wang"
                    },
                    {
                        "name": "Donghai Hong"
                    },
                    {
                        "name": "Youliang Yuan"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Haoqin Tu"
                    },
                    {
                        "name": "Fajri Koto"
                    },
                    {
                        "name": "Tatsuki Kuribayashi"
                    },
                    {
                        "name": "Cong Zeng"
                    },
                    {
                        "name": "Rishabh Bhardwaj"
                    },
                    {
                        "name": "Bingchen Zhao"
                    },
                    {
                        "name": "Yawen Duan"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Emad A. Alghamdi"
                    },
                    {
                        "name": "Yaodong Yang"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Soujanya Poria"
                    },
                    {
                        "name": "Pengfei Liu"
                    },
                    {
                        "name": "Zhengzhong Liu"
                    },
                    {
                        "name": "Xuguang Ren"
                    },
                    {
                        "name": "Eduard Hovy"
                    },
                    {
                        "name": "Iryna Gurevych"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Monojit Choudhury"
                    },
                    {
                        "name": "Timothy Baldwin"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Baldwin"
                },
                "author": "Timothy Baldwin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18547v1",
                "updated": "2024-12-24T16:55:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    55,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T16:55:45Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    55,
                    45,
                    1,
                    359,
                    0
                ],
                "title": "Token-Budget-Aware LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-Budget-Aware LLM Reasoning"
                },
                "summary": "Reasoning is critical for large language models (LLMs) to excel in a wide\nrange of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM\nperformance by decomposing problems into intermediate steps, they also incur\nsignificant overhead in token usage, leading to increased costs. We find that\nthe reasoning process of current LLMs is unnecessarily lengthy and it can be\ncompressed by including a reasonable token budget in the prompt, but the choice\nof token budget plays a crucial role in the actual compression effectiveness.\nWe then propose a token-budget-aware LLM reasoning framework, which dynamically\nestimates token budgets for different problems based on reasoning complexity\nand uses the estimated token budgets to guide the reasoning process.\nExperiments show that our method effectively reduces token costs in CoT\nreasoning with only a slight performance reduction, offering a practical\nsolution to balance efficiency and accuracy in LLM reasoning. Code:\nhttps://github.com/GeniusHTX/TALE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning is critical for large language models (LLMs) to excel in a wide\nrange of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM\nperformance by decomposing problems into intermediate steps, they also incur\nsignificant overhead in token usage, leading to increased costs. We find that\nthe reasoning process of current LLMs is unnecessarily lengthy and it can be\ncompressed by including a reasonable token budget in the prompt, but the choice\nof token budget plays a crucial role in the actual compression effectiveness.\nWe then propose a token-budget-aware LLM reasoning framework, which dynamically\nestimates token budgets for different problems based on reasoning complexity\nand uses the estimated token budgets to guide the reasoning process.\nExperiments show that our method effectively reduces token costs in CoT\nreasoning with only a slight performance reduction, offering a practical\nsolution to balance efficiency and accuracy in LLM reasoning. Code:\nhttps://github.com/GeniusHTX/TALE."
                },
                "authors": [
                    {
                        "name": "Tingxu Han"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Shiyu Zhao"
                    },
                    {
                        "name": "Shiqing Ma"
                    },
                    {
                        "name": "Zhenyu Chen"
                    },
                    {
                        "name": "Zhenting Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhenting Wang"
                },
                "author": "Zhenting Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18544v1",
                "updated": "2024-12-24T16:51:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    51,
                    35,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T16:51:35Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    51,
                    35,
                    1,
                    359,
                    0
                ],
                "title": "Consistency Checks for Language Model Forecasters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consistency Checks for Language Model Forecasters"
                },
                "summary": "Forecasting is a task that is difficult to evaluate: the ground truth can\nonly be known in the future. Recent work showing LLM forecasters rapidly\napproaching human-level performance begs the question: how can we benchmark and\nevaluate these forecasters instantaneously? Following the consistency check\nframework, we measure the performance of forecasters in terms of the\nconsistency of their predictions on different logically-related questions. We\npropose a new, general consistency metric based on arbitrage: for example, if a\nforecasting AI illogically predicts that both the Democratic and Republican\nparties have 60% probability of winning the 2024 US presidential election, an\narbitrageur can trade against the forecaster's predictions and make a profit.\nWe build an automated evaluation system that generates a set of base questions,\ninstantiates consistency checks from these questions, elicits the predictions\nof the forecaster, and measures the consistency of the predictions. We then\nbuild a standard, proper-scoring-rule forecasting benchmark, and show that our\n(instantaneous) consistency metrics correlate with LLM forecasters' ground\ntruth Brier scores (which are only known in the future). We also release a\nconsistency benchmark that resolves in 2028, providing a long-term evaluation\ntool for forecasting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting is a task that is difficult to evaluate: the ground truth can\nonly be known in the future. Recent work showing LLM forecasters rapidly\napproaching human-level performance begs the question: how can we benchmark and\nevaluate these forecasters instantaneously? Following the consistency check\nframework, we measure the performance of forecasters in terms of the\nconsistency of their predictions on different logically-related questions. We\npropose a new, general consistency metric based on arbitrage: for example, if a\nforecasting AI illogically predicts that both the Democratic and Republican\nparties have 60% probability of winning the 2024 US presidential election, an\narbitrageur can trade against the forecaster's predictions and make a profit.\nWe build an automated evaluation system that generates a set of base questions,\ninstantiates consistency checks from these questions, elicits the predictions\nof the forecaster, and measures the consistency of the predictions. We then\nbuild a standard, proper-scoring-rule forecasting benchmark, and show that our\n(instantaneous) consistency metrics correlate with LLM forecasters' ground\ntruth Brier scores (which are only known in the future). We also release a\nconsistency benchmark that resolves in 2028, providing a long-term evaluation\ntool for forecasting."
                },
                "authors": [
                    {
                        "name": "Daniel Paleka"
                    },
                    {
                        "name": "Abhimanyu Pallavi Sudhir"
                    },
                    {
                        "name": "Alejandro Alvarez"
                    },
                    {
                        "name": "Vineeth Bhat"
                    },
                    {
                        "name": "Adam Shen"
                    },
                    {
                        "name": "Evan Wang"
                    },
                    {
                        "name": "Florian TramÃ¨r"
                    }
                ],
                "author_detail": {
                    "name": "Florian TramÃ¨r"
                },
                "author": "Florian TramÃ¨r",
                "arxiv_comment": "56 pages, 25 figures. Submitted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12564v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12564v2",
                "updated": "2024-12-24T16:41:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    41,
                    40,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-17T05:48:48Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    48,
                    48,
                    1,
                    352,
                    0
                ],
                "title": "Evaluating Zero-Shot Multilingual Aspect-Based Sentiment Analysis with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Zero-Shot Multilingual Aspect-Based Sentiment Analysis with\n  Large Language Models"
                },
                "summary": "Aspect-based sentiment analysis (ABSA), a sequence labeling task, has\nattracted increasing attention in multilingual contexts. While previous\nresearch has focused largely on fine-tuning or training models specifically for\nABSA, we evaluate large language models (LLMs) under zero-shot conditions to\nexplore their potential to tackle this challenge with minimal task-specific\nadaptation. We conduct a comprehensive empirical evaluation of a series of LLMs\non multilingual ABSA tasks, investigating various prompting strategies,\nincluding vanilla zero-shot, chain-of-thought (CoT), self-improvement,\nself-debate, and self-consistency, across nine different models. Results\nindicate that while LLMs show promise in handling multilingual ABSA, they\ngenerally fall short of fine-tuned, task-specific models. Notably, simpler\nzero-shot prompts often outperform more complex strategies, especially in\nhigh-resource languages like English. These findings underscore the need for\nfurther refinement of LLM-based approaches to effectively address ABSA task\nacross diverse languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-based sentiment analysis (ABSA), a sequence labeling task, has\nattracted increasing attention in multilingual contexts. While previous\nresearch has focused largely on fine-tuning or training models specifically for\nABSA, we evaluate large language models (LLMs) under zero-shot conditions to\nexplore their potential to tackle this challenge with minimal task-specific\nadaptation. We conduct a comprehensive empirical evaluation of a series of LLMs\non multilingual ABSA tasks, investigating various prompting strategies,\nincluding vanilla zero-shot, chain-of-thought (CoT), self-improvement,\nself-debate, and self-consistency, across nine different models. Results\nindicate that while LLMs show promise in handling multilingual ABSA, they\ngenerally fall short of fine-tuned, task-specific models. Notably, simpler\nzero-shot prompts often outperform more complex strategies, especially in\nhigh-resource languages like English. These findings underscore the need for\nfurther refinement of LLM-based approaches to effectively address ABSA task\nacross diverse languages."
                },
                "authors": [
                    {
                        "name": "Chengyan Wu"
                    },
                    {
                        "name": "Bolei Ma"
                    },
                    {
                        "name": "Zheyu Zhang"
                    },
                    {
                        "name": "Ningyuan Deng"
                    },
                    {
                        "name": "Yanqing He"
                    },
                    {
                        "name": "Yun Xue"
                    }
                ],
                "author_detail": {
                    "name": "Yun Xue"
                },
                "author": "Yun Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12564v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12564v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18537v1",
                "updated": "2024-12-24T16:38:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    38,
                    4,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T16:38:04Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    38,
                    4,
                    1,
                    359,
                    0
                ],
                "title": "Harnessing Large Language Models for Knowledge Graph Question Answering\n  via Adaptive Multi-Aspect Retrieval-Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Large Language Models for Knowledge Graph Question Answering\n  via Adaptive Multi-Aspect Retrieval-Augmentation"
                },
                "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities, yet\nstruggle with hallucination and outdated knowledge when tasked with complex\nknowledge reasoning, resulting in factually incorrect outputs. Previous studies\nhave attempted to mitigate it by retrieving factual knowledge from large-scale\nknowledge graphs (KGs) to assist LLMs in logical reasoning and prediction of\nanswers. However, this kind of approach often introduces noise and irrelevant\ndata, especially in situations with extensive context from multiple knowledge\naspects. In this way, LLM attention can be potentially mislead from question\nand relevant information. In our study, we introduce an Adaptive Multi-Aspect\nRetrieval-augmented over KGs (Amar) framework. This method retrieves knowledge\nincluding entities, relations, and subgraphs, and converts each piece of\nretrieved text into prompt embeddings. The Amar framework comprises two key\nsub-components: 1) a self-alignment module that aligns commonalities among\nentities, relations, and subgraphs to enhance retrieved text, thereby reducing\nnoise interference; 2) a relevance gating module that employs a soft gate to\nlearn the relevance score between question and multi-aspect retrieved data, to\ndetermine which information should be used to enhance LLMs' output, or even\nfiltered altogether. Our method has achieved state-of-the-art performance on\ntwo common datasets, WebQSP and CWQ, showing a 1.9\\% improvement in accuracy\nover its best competitor and a 6.6\\% improvement in logical form generation\nover a method that directly uses retrieved text as context prompts. These\nresults demonstrate the effectiveness of Amar in improving the reasoning of\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate remarkable capabilities, yet\nstruggle with hallucination and outdated knowledge when tasked with complex\nknowledge reasoning, resulting in factually incorrect outputs. Previous studies\nhave attempted to mitigate it by retrieving factual knowledge from large-scale\nknowledge graphs (KGs) to assist LLMs in logical reasoning and prediction of\nanswers. However, this kind of approach often introduces noise and irrelevant\ndata, especially in situations with extensive context from multiple knowledge\naspects. In this way, LLM attention can be potentially mislead from question\nand relevant information. In our study, we introduce an Adaptive Multi-Aspect\nRetrieval-augmented over KGs (Amar) framework. This method retrieves knowledge\nincluding entities, relations, and subgraphs, and converts each piece of\nretrieved text into prompt embeddings. The Amar framework comprises two key\nsub-components: 1) a self-alignment module that aligns commonalities among\nentities, relations, and subgraphs to enhance retrieved text, thereby reducing\nnoise interference; 2) a relevance gating module that employs a soft gate to\nlearn the relevance score between question and multi-aspect retrieved data, to\ndetermine which information should be used to enhance LLMs' output, or even\nfiltered altogether. Our method has achieved state-of-the-art performance on\ntwo common datasets, WebQSP and CWQ, showing a 1.9\\% improvement in accuracy\nover its best competitor and a 6.6\\% improvement in logical form generation\nover a method that directly uses retrieved text as context prompts. These\nresults demonstrate the effectiveness of Amar in improving the reasoning of\nLLMs."
                },
                "authors": [
                    {
                        "name": "Derong Xu Xinhang Li"
                    },
                    {
                        "name": "Ziheng Zhang"
                    },
                    {
                        "name": "Zhenxi Lin"
                    },
                    {
                        "name": "Zhihong Zhu"
                    },
                    {
                        "name": "Zhi Zheng"
                    },
                    {
                        "name": "Xian Wu"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "arxiv_comment": "Accepted by AAAI'2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14909v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14909v2",
                "updated": "2024-12-24T16:25:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    25,
                    27,
                    1,
                    359,
                    0
                ],
                "published": "2024-08-27T09:35:49Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    35,
                    49,
                    1,
                    240,
                    0
                ],
                "title": "SpikingSSMs: Learning Long Sequences with Sparse and Parallel Spiking\n  State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpikingSSMs: Learning Long Sequences with Sparse and Parallel Spiking\n  State Space Models"
                },
                "summary": "Known as low energy consumption networks, spiking neural networks (SNNs) have\ngained a lot of attention within the past decades. While SNNs are increasing\ncompetitive with artificial neural networks (ANNs) for vision tasks, they are\nrarely used for long sequence tasks, despite their intrinsic temporal dynamics.\nIn this work, we develop spiking state space models (SpikingSSMs) for long\nsequence learning by leveraging on the sequence learning abilities of state\nspace models (SSMs). Inspired by dendritic neuron structure, we hierarchically\nintegrate neuronal dynamics with the original SSM block, meanwhile realizing\nsparse synaptic computation. Furthermore, to solve the conflict of event-driven\nneuronal dynamics with parallel computing, we propose a light-weight surrogate\ndynamic network which accurately predicts the after-reset membrane potential\nand compatible to learnable thresholds, enabling orders of acceleration in\ntraining speed compared with conventional iterative methods. On the long range\narena benchmark task, SpikingSSM achieves competitive performance to\nstate-of-the-art SSMs meanwhile realizing on average 90\\% of network sparsity.\nOn language modeling, our network significantly surpasses existing spiking\nlarge language models (spikingLLMs) on the WikiText-103 dataset with only a\nthird of the model size, demonstrating its potential as backbone architecture\nfor low computation cost LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Known as low energy consumption networks, spiking neural networks (SNNs) have\ngained a lot of attention within the past decades. While SNNs are increasing\ncompetitive with artificial neural networks (ANNs) for vision tasks, they are\nrarely used for long sequence tasks, despite their intrinsic temporal dynamics.\nIn this work, we develop spiking state space models (SpikingSSMs) for long\nsequence learning by leveraging on the sequence learning abilities of state\nspace models (SSMs). Inspired by dendritic neuron structure, we hierarchically\nintegrate neuronal dynamics with the original SSM block, meanwhile realizing\nsparse synaptic computation. Furthermore, to solve the conflict of event-driven\nneuronal dynamics with parallel computing, we propose a light-weight surrogate\ndynamic network which accurately predicts the after-reset membrane potential\nand compatible to learnable thresholds, enabling orders of acceleration in\ntraining speed compared with conventional iterative methods. On the long range\narena benchmark task, SpikingSSM achieves competitive performance to\nstate-of-the-art SSMs meanwhile realizing on average 90\\% of network sparsity.\nOn language modeling, our network significantly surpasses existing spiking\nlarge language models (spikingLLMs) on the WikiText-103 dataset with only a\nthird of the model size, demonstrating its potential as backbone architecture\nfor low computation cost LLMs."
                },
                "authors": [
                    {
                        "name": "Shuaijie Shen"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Renzhuo Huang"
                    },
                    {
                        "name": "Yan Zhong"
                    },
                    {
                        "name": "Qinghai Guo"
                    },
                    {
                        "name": "Zhichao Lu"
                    },
                    {
                        "name": "Jianguo Zhang"
                    },
                    {
                        "name": "Luziwei Leng"
                    }
                ],
                "author_detail": {
                    "name": "Luziwei Leng"
                },
                "author": "Luziwei Leng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14909v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18531v1",
                "updated": "2024-12-24T16:24:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    24,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T16:24:45Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    24,
                    45,
                    1,
                    359,
                    0
                ],
                "title": "Automated Code Review In Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Code Review In Practice"
                },
                "summary": "Code review is a widespread practice to improve software quality and transfer\nknowledge. It is often seen as time-consuming due to the need for manual effort\nand potential delays. Several AI-assisted tools, such as Qodo, GitHub Copilot,\nand Coderabbit, provide automated reviews using large language models (LLMs).\nThe effects of such tools in the industry are yet to be examined.\n  This study examines the impact of LLM-based automated code review tools in an\nindustrial setting. The study was conducted within a software development\nenvironment that adopted an AI-assisted review tool (based on open-source Qodo\nPR Agent). Around 238 practitioners across ten projects had access to the tool.\nWe focused on three projects with 4,335 pull requests, 1,568 of which underwent\nautomated reviews. Data collection comprised three sources: (1) a quantitative\nanalysis of pull request data, including comment labels indicating whether\ndevelopers acted on the automated comments, (2) surveys sent to developers\nregarding their experience with reviews on individual pull requests, and (3) a\nbroader survey of 22 practitioners capturing their general opinions on\nautomated reviews.\n  73.8% of automated comments were resolved. However, the average pull request\nclosure duration increased from five hours 52 minutes to eight hours 20\nminutes, with varying trends across projects. Most practitioners reported a\nminor improvement in code quality due to automated reviews.\n  The LLM-based tool proved useful in software development, enhancing bug\ndetection, increasing awareness of code quality, and promoting best practices.\nHowever, it also led to longer pull request closure times and introduced\ndrawbacks like faulty reviews, unnecessary corrections, and irrelevant\ncomments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code review is a widespread practice to improve software quality and transfer\nknowledge. It is often seen as time-consuming due to the need for manual effort\nand potential delays. Several AI-assisted tools, such as Qodo, GitHub Copilot,\nand Coderabbit, provide automated reviews using large language models (LLMs).\nThe effects of such tools in the industry are yet to be examined.\n  This study examines the impact of LLM-based automated code review tools in an\nindustrial setting. The study was conducted within a software development\nenvironment that adopted an AI-assisted review tool (based on open-source Qodo\nPR Agent). Around 238 practitioners across ten projects had access to the tool.\nWe focused on three projects with 4,335 pull requests, 1,568 of which underwent\nautomated reviews. Data collection comprised three sources: (1) a quantitative\nanalysis of pull request data, including comment labels indicating whether\ndevelopers acted on the automated comments, (2) surveys sent to developers\nregarding their experience with reviews on individual pull requests, and (3) a\nbroader survey of 22 practitioners capturing their general opinions on\nautomated reviews.\n  73.8% of automated comments were resolved. However, the average pull request\nclosure duration increased from five hours 52 minutes to eight hours 20\nminutes, with varying trends across projects. Most practitioners reported a\nminor improvement in code quality due to automated reviews.\n  The LLM-based tool proved useful in software development, enhancing bug\ndetection, increasing awareness of code quality, and promoting best practices.\nHowever, it also led to longer pull request closure times and introduced\ndrawbacks like faulty reviews, unnecessary corrections, and irrelevant\ncomments."
                },
                "authors": [
                    {
                        "name": "Umut Cihan"
                    },
                    {
                        "name": "Vahid Haratian"
                    },
                    {
                        "name": "Arda Ä°Ã§Ã¶z"
                    },
                    {
                        "name": "Mert Kaan GÃ¼l"
                    },
                    {
                        "name": "Ãmercan Devran"
                    },
                    {
                        "name": "Emircan Furkan Bayendur"
                    },
                    {
                        "name": "Baykal Mehmet UÃ§ar"
                    },
                    {
                        "name": "Eray TÃ¼zÃ¼n"
                    }
                ],
                "author_detail": {
                    "name": "Eray TÃ¼zÃ¼n"
                },
                "author": "Eray TÃ¼zÃ¼n",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17743v2",
                "updated": "2024-12-24T16:07:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    7,
                    47,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-23T17:47:53Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    47,
                    53,
                    0,
                    358,
                    0
                ],
                "title": "YuLan-Mini: An Open Data-efficient Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "YuLan-Mini: An Open Data-efficient Language Model"
                },
                "summary": "Effective pre-training of large language models (LLMs) has been challenging\ndue to the immense resource demands and the complexity of the technical\nprocesses involved. This paper presents a detailed technical report on\nYuLan-Mini, a highly capable base model with 2.42B parameters that achieves\ntop-tier performance among models of similar parameter scale. Our pre-training\napproach focuses on enhancing training efficacy through three key technical\ncontributions: an elaborate data pipeline combines data cleaning with data\nschedule strategies, a robust optimization method to mitigate training\ninstability, and an effective annealing approach that incorporates targeted\ndata selection and long context training. Remarkably, YuLan-Mini, trained on\n1.08T tokens, achieves performance comparable to industry-leading models that\nrequire significantly more data. To facilitate reproduction, we release the\nfull details of the data composition for each training phase. Project details\ncan be accessed at the following link: https://github.com/RUC-GSAI/YuLan-Mini.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective pre-training of large language models (LLMs) has been challenging\ndue to the immense resource demands and the complexity of the technical\nprocesses involved. This paper presents a detailed technical report on\nYuLan-Mini, a highly capable base model with 2.42B parameters that achieves\ntop-tier performance among models of similar parameter scale. Our pre-training\napproach focuses on enhancing training efficacy through three key technical\ncontributions: an elaborate data pipeline combines data cleaning with data\nschedule strategies, a robust optimization method to mitigate training\ninstability, and an effective annealing approach that incorporates targeted\ndata selection and long context training. Remarkably, YuLan-Mini, trained on\n1.08T tokens, achieves performance comparable to industry-leading models that\nrequire significantly more data. To facilitate reproduction, we release the\nfull details of the data composition for each training phase. Project details\ncan be accessed at the following link: https://github.com/RUC-GSAI/YuLan-Mini."
                },
                "authors": [
                    {
                        "name": "Yiwen Hu"
                    },
                    {
                        "name": "Huatong Song"
                    },
                    {
                        "name": "Jia Deng"
                    },
                    {
                        "name": "Jiapeng Wang"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Kun Zhou"
                    },
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Zican Dong"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18511v1",
                "updated": "2024-12-24T15:50:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    15,
                    50,
                    10,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T15:50:10Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    15,
                    50,
                    10,
                    1,
                    359,
                    0
                ],
                "title": "Large Language Model guided Deep Reinforcement Learning for Decision\n  Making in Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model guided Deep Reinforcement Learning for Decision\n  Making in Autonomous Driving"
                },
                "summary": "Deep reinforcement learning (DRL) shows promising potential for autonomous\ndriving decision-making. However, DRL demands extensive computational resources\nto achieve a qualified policy in complex driving scenarios due to its low\nlearning efficiency. Moreover, leveraging expert guidance from human to enhance\nDRL performance incurs prohibitively high labor costs, which limits its\npractical application. In this study, we propose a novel large language model\n(LLM) guided deep reinforcement learning (LGDRL) framework for addressing the\ndecision-making problem of autonomous vehicles. Within this framework, an\nLLM-based driving expert is integrated into the DRL to provide intelligent\nguidance for the learning process of DRL. Subsequently, in order to efficiently\nutilize the guidance of the LLM expert to enhance the performance of DRL\ndecision-making policies, the learning and interaction process of DRL is\nenhanced through an innovative expert policy constrained algorithm and a novel\nLLM-intervened interaction mechanism. Experimental results demonstrate that our\nmethod not only achieves superior driving performance with a 90\\% task success\nrate but also significantly improves the learning efficiency and expert\nguidance utilization efficiency compared to state-of-the-art baseline\nalgorithms. Moreover, the proposed method enables the DRL agent to maintain\nconsistent and reliable performance in the absence of LLM expert guidance. The\ncode and supplementary videos are available at\nhttps://bitmobility.github.io/LGDRL/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep reinforcement learning (DRL) shows promising potential for autonomous\ndriving decision-making. However, DRL demands extensive computational resources\nto achieve a qualified policy in complex driving scenarios due to its low\nlearning efficiency. Moreover, leveraging expert guidance from human to enhance\nDRL performance incurs prohibitively high labor costs, which limits its\npractical application. In this study, we propose a novel large language model\n(LLM) guided deep reinforcement learning (LGDRL) framework for addressing the\ndecision-making problem of autonomous vehicles. Within this framework, an\nLLM-based driving expert is integrated into the DRL to provide intelligent\nguidance for the learning process of DRL. Subsequently, in order to efficiently\nutilize the guidance of the LLM expert to enhance the performance of DRL\ndecision-making policies, the learning and interaction process of DRL is\nenhanced through an innovative expert policy constrained algorithm and a novel\nLLM-intervened interaction mechanism. Experimental results demonstrate that our\nmethod not only achieves superior driving performance with a 90\\% task success\nrate but also significantly improves the learning efficiency and expert\nguidance utilization efficiency compared to state-of-the-art baseline\nalgorithms. Moreover, the proposed method enables the DRL agent to maintain\nconsistent and reliable performance in the absence of LLM expert guidance. The\ncode and supplementary videos are available at\nhttps://bitmobility.github.io/LGDRL/."
                },
                "authors": [
                    {
                        "name": "Hao Pang"
                    },
                    {
                        "name": "Zhenpo Wang"
                    },
                    {
                        "name": "Guoqiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Guoqiang Li"
                },
                "author": "Guoqiang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18497v1",
                "updated": "2024-12-24T15:28:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    15,
                    28,
                    56,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T15:28:56Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    15,
                    28,
                    56,
                    1,
                    359,
                    0
                ],
                "title": "Think or Remember? Detecting and Directing LLMs Towards Memorization or\n  Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think or Remember? Detecting and Directing LLMs Towards Memorization or\n  Generalization"
                },
                "summary": "In this paper, we explore the foundational mechanisms of memorization and\ngeneralization in Large Language Models (LLMs), inspired by the functional\nspecialization observed in the human brain. Our investigation serves as a case\nstudy leveraging specially designed datasets and experimental-scale LLMs to lay\nthe groundwork for understanding these behaviors. Specifically, we aim to first\nenable LLMs to exhibit both memorization and generalization by training with\nthe designed dataset, then (a) examine whether LLMs exhibit neuron-level\nspatial differentiation for memorization and generalization, (b) predict these\nbehaviors using model internal representations, and (c) steer the behaviors\nthrough inference-time interventions. Our findings reveal that neuron-wise\ndifferentiation of memorization and generalization is observable in LLMs, and\ntargeted interventions can successfully direct their behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we explore the foundational mechanisms of memorization and\ngeneralization in Large Language Models (LLMs), inspired by the functional\nspecialization observed in the human brain. Our investigation serves as a case\nstudy leveraging specially designed datasets and experimental-scale LLMs to lay\nthe groundwork for understanding these behaviors. Specifically, we aim to first\nenable LLMs to exhibit both memorization and generalization by training with\nthe designed dataset, then (a) examine whether LLMs exhibit neuron-level\nspatial differentiation for memorization and generalization, (b) predict these\nbehaviors using model internal representations, and (c) steer the behaviors\nthrough inference-time interventions. Our findings reveal that neuron-wise\ndifferentiation of memorization and generalization is observable in LLMs, and\ntargeted interventions can successfully direct their behavior."
                },
                "authors": [
                    {
                        "name": "Yi-Fu Fu"
                    },
                    {
                        "name": "Yu-Chieh Tu"
                    },
                    {
                        "name": "Tzu-Ling Cheng"
                    },
                    {
                        "name": "Cheng-Yu Lin"
                    },
                    {
                        "name": "Yi-Ting Yang"
                    },
                    {
                        "name": "Heng-Yi Liu"
                    },
                    {
                        "name": "Keng-Te Liao"
                    },
                    {
                        "name": "Da-Cheng Juan"
                    },
                    {
                        "name": "Shou-De Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shou-De Lin"
                },
                "author": "Shou-De Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04220v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04220v4",
                "updated": "2024-12-24T15:08:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    15,
                    8,
                    40,
                    1,
                    359,
                    0
                ],
                "published": "2024-06-06T16:18:30Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    16,
                    18,
                    30,
                    3,
                    158,
                    0
                ],
                "title": "BEADs: Bias Evaluation Across Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEADs: Bias Evaluation Across Domains"
                },
                "summary": "Recent advancements in large language models (LLMs) have greatly enhanced\nnatural language processing (NLP) applications. Nevertheless, these models\noften inherit biases from their training data. Despite the availability of\nvarious datasets for bias detection, most are limited to one or two NLP tasks\n(typically classification or evaluation) and lack comprehensive evaluations\nacross a broader range of NLP tasks. To address this gap, we introduce the Bias\nEvaluations Across Domains BEADs dataset, designed to support a wide array of\nNLP tasks, including text classification, token classification, bias\nquantification, and benign language generation. A key focus of this paper is\nthe gold label dataset that is annotated by GPT4 for scalabilty and verified by\nexperts to ensure high reliability. BEADs provides data for both fine-tuning,\nincluding classification and language generation tasks, and for evaluating\nLLMs. Our findings indicate that BEADs effectively identifies numerous biases\nwhen fine-tuned on this dataset. It also reduces biases when used for\nfine-tuning language generation task, while preserving language quality. The\nresults also reveal some prevalent demographic biases in LLMs when BEADs is\nused for evaluation in demographic task. We provide the BEADs dataset for\ndetecting biases in various domains, and this dataset is readily usable for\nresponsible AI development and application. The dataset can be accessed at\nhttps://huggingface.co/datasets/shainar/BEAD .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have greatly enhanced\nnatural language processing (NLP) applications. Nevertheless, these models\noften inherit biases from their training data. Despite the availability of\nvarious datasets for bias detection, most are limited to one or two NLP tasks\n(typically classification or evaluation) and lack comprehensive evaluations\nacross a broader range of NLP tasks. To address this gap, we introduce the Bias\nEvaluations Across Domains BEADs dataset, designed to support a wide array of\nNLP tasks, including text classification, token classification, bias\nquantification, and benign language generation. A key focus of this paper is\nthe gold label dataset that is annotated by GPT4 for scalabilty and verified by\nexperts to ensure high reliability. BEADs provides data for both fine-tuning,\nincluding classification and language generation tasks, and for evaluating\nLLMs. Our findings indicate that BEADs effectively identifies numerous biases\nwhen fine-tuned on this dataset. It also reduces biases when used for\nfine-tuning language generation task, while preserving language quality. The\nresults also reveal some prevalent demographic biases in LLMs when BEADs is\nused for evaluation in demographic task. We provide the BEADs dataset for\ndetecting biases in various domains, and this dataset is readily usable for\nresponsible AI development and application. The dataset can be accessed at\nhttps://huggingface.co/datasets/shainar/BEAD ."
                },
                "authors": [
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Mizanur Rahman"
                    },
                    {
                        "name": "Michael R. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Zhang"
                },
                "author": "Michael R. Zhang",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04220v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04220v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11006v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11006v3",
                "updated": "2024-12-24T15:04:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    15,
                    4,
                    50,
                    1,
                    359,
                    0
                ],
                "published": "2024-08-20T17:00:04Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    0,
                    4,
                    1,
                    233,
                    0
                ],
                "title": "Security Attacks on LLM-based Code Completion Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security Attacks on LLM-based Code Completion Tools"
                },
                "summary": "The rapid development of large language models (LLMs) has significantly\nadvanced code completion capabilities, giving rise to a new generation of\nLLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, these\ntools possess unique workflows, integrating multiple information sources as\ninput and prioritizing code suggestions over natural language interaction,\nwhich introduces distinct security challenges. Additionally, LCCTs often rely\non proprietary code datasets for training, raising concerns about the potential\nexposure of sensitive data. This paper exploits these distinct characteristics\nof LCCTs to develop targeted attack methodologies on two critical security\nrisks: jailbreaking and training data extraction attacks. Our experimental\nresults expose significant vulnerabilities within LCCTs, including a 99.4%\nsuccess rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rate\non Amazon Q. Furthermore, We successfully extracted sensitive user data from\nGitHub Copilot, including 54 real email addresses and 314 physical addresses\nassociated with GitHub usernames. Our study also demonstrates that these\ncode-based attack methods are effective against general-purpose LLMs, such as\nthe GPT series, highlighting a broader security misalignment in the handling of\ncode by modern LLMs. These findings underscore critical security challenges\nassociated with LCCTs and suggest essential directions for strengthening their\nsecurity frameworks. The example code and attack samples from our research are\nprovided at https://github.com/Sensente/Security-Attacks-on-LCCTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) has significantly\nadvanced code completion capabilities, giving rise to a new generation of\nLLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, these\ntools possess unique workflows, integrating multiple information sources as\ninput and prioritizing code suggestions over natural language interaction,\nwhich introduces distinct security challenges. Additionally, LCCTs often rely\non proprietary code datasets for training, raising concerns about the potential\nexposure of sensitive data. This paper exploits these distinct characteristics\nof LCCTs to develop targeted attack methodologies on two critical security\nrisks: jailbreaking and training data extraction attacks. Our experimental\nresults expose significant vulnerabilities within LCCTs, including a 99.4%\nsuccess rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rate\non Amazon Q. Furthermore, We successfully extracted sensitive user data from\nGitHub Copilot, including 54 real email addresses and 314 physical addresses\nassociated with GitHub usernames. Our study also demonstrates that these\ncode-based attack methods are effective against general-purpose LLMs, such as\nthe GPT series, highlighting a broader security misalignment in the handling of\ncode by modern LLMs. These findings underscore critical security challenges\nassociated with LCCTs and suggest essential directions for strengthening their\nsecurity frameworks. The example code and attack samples from our research are\nprovided at https://github.com/Sensente/Security-Attacks-on-LCCTs."
                },
                "authors": [
                    {
                        "name": "Wen Cheng"
                    },
                    {
                        "name": "Ke Sun"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "arxiv_comment": "Paper accepted at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11006v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11006v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06736v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06736v3",
                "updated": "2024-12-24T14:44:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    14,
                    44,
                    32,
                    1,
                    359,
                    0
                ],
                "published": "2024-11-11T06:04:53Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    6,
                    4,
                    53,
                    0,
                    316,
                    0
                ],
                "title": "MrSteve: Instruction-Following Agents in Minecraft with What-Where-When\n  Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MrSteve: Instruction-Following Agents in Minecraft with What-Where-When\n  Memory"
                },
                "summary": "Significant advances have been made in developing general-purpose embodied AI\nin environments like Minecraft through the adoption of LLM-augmented\nhierarchical approaches. While these approaches, which combine high-level\nplanners with low-level controllers, show promise, low-level controllers\nfrequently become performance bottlenecks due to repeated failures. In this\npaper, we argue that the primary cause of failure in many low-level controllers\nis the absence of an episodic memory system. To address this, we introduce\nMrSteve (Memory Recall Steve-1), a novel low-level controller equipped with\nPlace Event Memory (PEM), a form of episodic memory that captures what, where,\nand when information from episodes. This directly addresses the main limitation\nof the popular low-level controller, Steve-1. Unlike previous models that rely\non short-term memory, PEM organizes spatial and event-based data, enabling\nefficient recall and navigation in long-horizon tasks. Additionally, we propose\nan Exploration Strategy and a Memory-Augmented Task Solving Framework, allowing\nagents to alternate between exploration and task-solving based on recalled\nevents. Our approach significantly improves task-solving and exploration\nefficiency compared to existing methods. We will release our code and demos on\nthe project page: https://sites.google.com/view/mr-steve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant advances have been made in developing general-purpose embodied AI\nin environments like Minecraft through the adoption of LLM-augmented\nhierarchical approaches. While these approaches, which combine high-level\nplanners with low-level controllers, show promise, low-level controllers\nfrequently become performance bottlenecks due to repeated failures. In this\npaper, we argue that the primary cause of failure in many low-level controllers\nis the absence of an episodic memory system. To address this, we introduce\nMrSteve (Memory Recall Steve-1), a novel low-level controller equipped with\nPlace Event Memory (PEM), a form of episodic memory that captures what, where,\nand when information from episodes. This directly addresses the main limitation\nof the popular low-level controller, Steve-1. Unlike previous models that rely\non short-term memory, PEM organizes spatial and event-based data, enabling\nefficient recall and navigation in long-horizon tasks. Additionally, we propose\nan Exploration Strategy and a Memory-Augmented Task Solving Framework, allowing\nagents to alternate between exploration and task-solving based on recalled\nevents. Our approach significantly improves task-solving and exploration\nefficiency compared to existing methods. We will release our code and demos on\nthe project page: https://sites.google.com/view/mr-steve."
                },
                "authors": [
                    {
                        "name": "Junyeong Park"
                    },
                    {
                        "name": "Junmo Cho"
                    },
                    {
                        "name": "Sungjin Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Sungjin Ahn"
                },
                "author": "Sungjin Ahn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06736v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06736v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18118v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18118v4",
                "updated": "2024-12-24T14:26:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    14,
                    26,
                    36,
                    1,
                    359,
                    0
                ],
                "published": "2024-06-26T07:15:44Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    7,
                    15,
                    44,
                    2,
                    178,
                    0
                ],
                "title": "SafeAligner: Safety Alignment against Jailbreak Attacks via Response\n  Disparity Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeAligner: Safety Alignment against Jailbreak Attacks via Response\n  Disparity Guidance"
                },
                "summary": "As the development of large language models (LLMs) rapidly advances, securing\nthese models effectively without compromising their utility has become a\npivotal area of research. However, current defense strategies against jailbreak\nattacks (i.e., efforts to bypass security protocols) often suffer from limited\nadaptability, restricted general capability, and high cost. To address these\nchallenges, we introduce SafeAligner, a methodology implemented at the decoding\nstage to fortify defenses against jailbreak attacks. We begin by developing two\nspecialized models: the Sentinel Model, which is trained to foster safety, and\nthe Intruder Model, designed to generate riskier responses. SafeAligner\nleverages the disparity in security levels between the responses from these\nmodels to differentiate between harmful and beneficial tokens, effectively\nguiding the safety alignment by altering the output token distribution of the\ntarget model. Extensive experiments show that SafeAligner can increase the\nlikelihood of beneficial tokens, while reducing the occurrence of harmful ones,\nthereby ensuring secure alignment with minimal loss to generality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the development of large language models (LLMs) rapidly advances, securing\nthese models effectively without compromising their utility has become a\npivotal area of research. However, current defense strategies against jailbreak\nattacks (i.e., efforts to bypass security protocols) often suffer from limited\nadaptability, restricted general capability, and high cost. To address these\nchallenges, we introduce SafeAligner, a methodology implemented at the decoding\nstage to fortify defenses against jailbreak attacks. We begin by developing two\nspecialized models: the Sentinel Model, which is trained to foster safety, and\nthe Intruder Model, designed to generate riskier responses. SafeAligner\nleverages the disparity in security levels between the responses from these\nmodels to differentiate between harmful and beneficial tokens, effectively\nguiding the safety alignment by altering the output token distribution of the\ntarget model. Extensive experiments show that SafeAligner can increase the\nlikelihood of beneficial tokens, while reducing the occurrence of harmful ones,\nthereby ensuring secure alignment with minimal loss to generality."
                },
                "authors": [
                    {
                        "name": "Caishuang Huang"
                    },
                    {
                        "name": "Wanxu Zhao"
                    },
                    {
                        "name": "Rui Zheng"
                    },
                    {
                        "name": "Huijie Lv"
                    },
                    {
                        "name": "Wenyu Zhan"
                    },
                    {
                        "name": "Shihan Dou"
                    },
                    {
                        "name": "Sixian Li"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Enyu Zhou"
                    },
                    {
                        "name": "Junjie Ye"
                    },
                    {
                        "name": "Yuming Yang"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18118v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18118v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18450v1",
                "updated": "2024-12-24T14:21:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    14,
                    21,
                    58,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T14:21:58Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    14,
                    21,
                    58,
                    1,
                    359,
                    0
                ],
                "title": "3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D\n  Scene Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D\n  Scene Understanding"
                },
                "summary": "A 3D scene graph represents a compact scene model, storing information about\nthe objects and the semantic relationships between them, making its use\npromising for robotic tasks. When interacting with a user, an embodied\nintelligent agent should be capable of responding to various queries about the\nscene formulated in natural language. Large Language Models (LLMs) are\nbeneficial solutions for user-robot interaction due to their natural language\nunderstanding and reasoning abilities. Recent methods for creating learnable\nrepresentations of 3D scenes have demonstrated the potential to improve the\nquality of LLMs responses by adapting to the 3D world. However, the existing\nmethods do not explicitly utilize information about the semantic relationships\nbetween objects, limiting themselves to information about their coordinates. In\nthis work, we propose a method 3DGraphLLM for constructing a learnable\nrepresentation of a 3D scene graph. The learnable representation is used as\ninput for LLMs to perform 3D vision-language tasks. In our experiments on\npopular ScanRefer, RIORefer, Multi3DRefer, ScanQA, Sqa3D, and Scan2cap\ndatasets, we demonstrate the advantage of this approach over baseline methods\nthat do not use information about the semantic relationships between objects.\nThe code is publicly available at\nhttps://github.com/CognitiveAISystems/3DGraphLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A 3D scene graph represents a compact scene model, storing information about\nthe objects and the semantic relationships between them, making its use\npromising for robotic tasks. When interacting with a user, an embodied\nintelligent agent should be capable of responding to various queries about the\nscene formulated in natural language. Large Language Models (LLMs) are\nbeneficial solutions for user-robot interaction due to their natural language\nunderstanding and reasoning abilities. Recent methods for creating learnable\nrepresentations of 3D scenes have demonstrated the potential to improve the\nquality of LLMs responses by adapting to the 3D world. However, the existing\nmethods do not explicitly utilize information about the semantic relationships\nbetween objects, limiting themselves to information about their coordinates. In\nthis work, we propose a method 3DGraphLLM for constructing a learnable\nrepresentation of a 3D scene graph. The learnable representation is used as\ninput for LLMs to perform 3D vision-language tasks. In our experiments on\npopular ScanRefer, RIORefer, Multi3DRefer, ScanQA, Sqa3D, and Scan2cap\ndatasets, we demonstrate the advantage of this approach over baseline methods\nthat do not use information about the semantic relationships between objects.\nThe code is publicly available at\nhttps://github.com/CognitiveAISystems/3DGraphLLM."
                },
                "authors": [
                    {
                        "name": "Tatiana Zemskova"
                    },
                    {
                        "name": "Dmitry Yudin"
                    }
                ],
                "author_detail": {
                    "name": "Dmitry Yudin"
                },
                "author": "Dmitry Yudin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18443v1",
                "updated": "2024-12-24T14:03:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    14,
                    3,
                    7,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T14:03:07Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    14,
                    3,
                    7,
                    1,
                    359,
                    0
                ],
                "title": "Is Large Language Model Good at Triple Set Prediction? An Empirical\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Large Language Model Good at Triple Set Prediction? An Empirical\n  Study"
                },
                "summary": "The core of the Knowledge Graph Completion (KGC) task is to predict and\ncomplete the missing relations or nodes in a KG. Common KGC tasks are mostly\nabout inferring unknown elements with one or two elements being known in a\ntriple. In comparison, the Triple Set Prediction (TSP) task is a more realistic\nknowledge graph completion task. It aims to predict all elements of unknown\ntriples based on the information from known triples. In recent years, large\nlanguage models (LLMs) have exhibited significant advancements in language\ncomprehension, demonstrating considerable potential for KGC tasks. However, the\npotential of LLM on the TSP task has not yet to be investigated. Thus in this\npaper we proposed a new framework to explore the strengths and limitations of\nLLM in the TSP task. Specifically, the framework consists of LLM-based rule\nmining and LLM-based triple set prediction. The relation list of KG embedded\nwithin rich semantic information is first leveraged to prompt LLM in the\ngeneration of rules. This process is both efficient and independent of\nstatistical information, making it easier to mine effective and realistic\nrules. For each subgraph, the specified rule is applied in conjunction with the\nrelevant triples within that subgraph to guide the LLM in predicting the\nmissing triples. Subsequently, the predictions from all subgraphs are\nconsolidated to derive the complete set of predicted triples on KG. Finally,\nthe method is evaluated on the relatively complete CFamily dataset. The\nexperimental results indicate that when LLMs are required to adhere to a large\namount of factual knowledge to predict missing triples, significant\nhallucinations occurs, leading to a noticeable decline in performance. To\nfurther explore the causes of this phenomenon, this paper presents a\ncomprehensive analysis supported by a detailed case study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The core of the Knowledge Graph Completion (KGC) task is to predict and\ncomplete the missing relations or nodes in a KG. Common KGC tasks are mostly\nabout inferring unknown elements with one or two elements being known in a\ntriple. In comparison, the Triple Set Prediction (TSP) task is a more realistic\nknowledge graph completion task. It aims to predict all elements of unknown\ntriples based on the information from known triples. In recent years, large\nlanguage models (LLMs) have exhibited significant advancements in language\ncomprehension, demonstrating considerable potential for KGC tasks. However, the\npotential of LLM on the TSP task has not yet to be investigated. Thus in this\npaper we proposed a new framework to explore the strengths and limitations of\nLLM in the TSP task. Specifically, the framework consists of LLM-based rule\nmining and LLM-based triple set prediction. The relation list of KG embedded\nwithin rich semantic information is first leveraged to prompt LLM in the\ngeneration of rules. This process is both efficient and independent of\nstatistical information, making it easier to mine effective and realistic\nrules. For each subgraph, the specified rule is applied in conjunction with the\nrelevant triples within that subgraph to guide the LLM in predicting the\nmissing triples. Subsequently, the predictions from all subgraphs are\nconsolidated to derive the complete set of predicted triples on KG. Finally,\nthe method is evaluated on the relatively complete CFamily dataset. The\nexperimental results indicate that when LLMs are required to adhere to a large\namount of factual knowledge to predict missing triples, significant\nhallucinations occurs, leading to a noticeable decline in performance. To\nfurther explore the causes of this phenomenon, this paper presents a\ncomprehensive analysis supported by a detailed case study."
                },
                "authors": [
                    {
                        "name": "Yuan Yuan"
                    },
                    {
                        "name": "Yajing Xu"
                    },
                    {
                        "name": "Wen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wen Zhang"
                },
                "author": "Wen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18442v1",
                "updated": "2024-12-24T14:02:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    14,
                    2,
                    44,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T14:02:44Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    14,
                    2,
                    44,
                    1,
                    359,
                    0
                ],
                "title": "SoK: On the Offensive Potential of AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: On the Offensive Potential of AI"
                },
                "summary": "Our society increasingly benefits from Artificial Intelligence (AI).\nUnfortunately, more and more evidence shows that AI is also used for offensive\npurposes. Prior works have revealed various examples of use cases in which the\ndeployment of AI can lead to violation of security and privacy objectives. No\nextant work, however, has been able to draw a holistic picture of the offensive\npotential of AI. In this SoK paper we seek to lay the ground for a systematic\nanalysis of the heterogeneous capabilities of offensive AI. In particular we\n(i) account for AI risks to both humans and systems while (ii) consolidating\nand distilling knowledge from academic literature, expert opinions, industrial\nvenues, as well as laymen -- all of which being valuable sources of information\non offensive AI.\n  To enable alignment of such diverse sources of knowledge, we devise a common\nset of criteria reflecting essential technological factors related to offensive\nAI. With the help of such criteria, we systematically analyze: 95 research\npapers; 38 InfoSec briefings (from, e.g., BlackHat); the responses of a user\nstudy (N=549) entailing individuals with diverse backgrounds and expertise; and\nthe opinion of 12 experts. Our contributions not only reveal concerning ways\n(some of which overlooked by prior work) in which AI can be offensively used\ntoday, but also represent a foothold to address this threat in the years to\ncome.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our society increasingly benefits from Artificial Intelligence (AI).\nUnfortunately, more and more evidence shows that AI is also used for offensive\npurposes. Prior works have revealed various examples of use cases in which the\ndeployment of AI can lead to violation of security and privacy objectives. No\nextant work, however, has been able to draw a holistic picture of the offensive\npotential of AI. In this SoK paper we seek to lay the ground for a systematic\nanalysis of the heterogeneous capabilities of offensive AI. In particular we\n(i) account for AI risks to both humans and systems while (ii) consolidating\nand distilling knowledge from academic literature, expert opinions, industrial\nvenues, as well as laymen -- all of which being valuable sources of information\non offensive AI.\n  To enable alignment of such diverse sources of knowledge, we devise a common\nset of criteria reflecting essential technological factors related to offensive\nAI. With the help of such criteria, we systematically analyze: 95 research\npapers; 38 InfoSec briefings (from, e.g., BlackHat); the responses of a user\nstudy (N=549) entailing individuals with diverse backgrounds and expertise; and\nthe opinion of 12 experts. Our contributions not only reveal concerning ways\n(some of which overlooked by prior work) in which AI can be offensively used\ntoday, but also represent a foothold to address this threat in the years to\ncome."
                },
                "authors": [
                    {
                        "name": "Saskia Laura SchrÃ¶er"
                    },
                    {
                        "name": "Giovanni Apruzzese"
                    },
                    {
                        "name": "Soheil Human"
                    },
                    {
                        "name": "Pavel Laskov"
                    },
                    {
                        "name": "Hyrum S. Anderson"
                    },
                    {
                        "name": "Edward W. N. Bernroider"
                    },
                    {
                        "name": "Aurore Fass"
                    },
                    {
                        "name": "Ben Nassi"
                    },
                    {
                        "name": "Vera Rimmer"
                    },
                    {
                        "name": "Fabio Roli"
                    },
                    {
                        "name": "Samer Salam"
                    },
                    {
                        "name": "Ashley Shen"
                    },
                    {
                        "name": "Ali Sunyaev"
                    },
                    {
                        "name": "Tim Wadwha-Brown"
                    },
                    {
                        "name": "Isabel Wagner"
                    },
                    {
                        "name": "Gang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Gang Wang"
                },
                "author": "Gang Wang",
                "arxiv_comment": "Systemization of Knowledge (SoK) paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18428v1",
                "updated": "2024-12-24T13:42:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    42,
                    44,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T13:42:44Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    42,
                    44,
                    1,
                    359,
                    0
                ],
                "title": "Explainable Multi-Modal Data Exploration in Natural Language via LLM\n  Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable Multi-Modal Data Exploration in Natural Language via LLM\n  Agent"
                },
                "summary": "International enterprises, organizations, or hospitals collect large amounts\nof multi-modal data stored in databases, text documents, images, and videos.\nWhile there has been recent progress in the separate fields of multi-modal data\nexploration as well as in database systems that automatically translate natural\nlanguage questions to database query languages, the research challenge of\nquerying database systems combined with other unstructured modalities such as\nimages in natural language is widely unexplored.\n  In this paper, we propose XMODE - a system that enables explainable,\nmulti-modal data exploration in natural language. Our approach is based on the\nfollowing research contributions: (1) Our system is inspired by a real-world\nuse case that enables users to explore multi-modal information systems. (2)\nXMODE leverages a LLM-based agentic AI framework to decompose a natural\nlanguage question into subtasks such as text-to-SQL generation and image\nanalysis. (3) Experimental results on multi-modal datasets over relational data\nand images demonstrate that our system outperforms state-of-the-art multi-modal\nexploration systems, excelling not only in accuracy but also in various\nperformance metrics such as query latency, API costs, planning efficiency, and\nexplanation quality, thanks to the more effective utilization of the reasoning\ncapabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "International enterprises, organizations, or hospitals collect large amounts\nof multi-modal data stored in databases, text documents, images, and videos.\nWhile there has been recent progress in the separate fields of multi-modal data\nexploration as well as in database systems that automatically translate natural\nlanguage questions to database query languages, the research challenge of\nquerying database systems combined with other unstructured modalities such as\nimages in natural language is widely unexplored.\n  In this paper, we propose XMODE - a system that enables explainable,\nmulti-modal data exploration in natural language. Our approach is based on the\nfollowing research contributions: (1) Our system is inspired by a real-world\nuse case that enables users to explore multi-modal information systems. (2)\nXMODE leverages a LLM-based agentic AI framework to decompose a natural\nlanguage question into subtasks such as text-to-SQL generation and image\nanalysis. (3) Experimental results on multi-modal datasets over relational data\nand images demonstrate that our system outperforms state-of-the-art multi-modal\nexploration systems, excelling not only in accuracy but also in various\nperformance metrics such as query latency, API costs, planning efficiency, and\nexplanation quality, thanks to the more effective utilization of the reasoning\ncapabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Farhad Nooralahzadeh"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Jonathan Furst"
                    },
                    {
                        "name": "Kurt Stockinger"
                    }
                ],
                "author_detail": {
                    "name": "Kurt Stockinger"
                },
                "author": "Kurt Stockinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06504v2",
                "updated": "2024-12-24T13:41:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    41,
                    8,
                    1,
                    359,
                    0
                ],
                "published": "2024-03-11T08:25:53Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    8,
                    25,
                    53,
                    0,
                    71,
                    0
                ],
                "title": "LoHan: Low-Cost High-Performance Framework to Fine-Tune 100B Model on a\n  Consumer GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoHan: Low-Cost High-Performance Framework to Fine-Tune 100B Model on a\n  Consumer GPU"
                },
                "summary": "Nowadays, AI researchers become more and more interested in fine-tuning a\npre-trained LLM, whose size has grown to up to over 100B parameters, for their\ndownstream tasks. One approach to fine-tune such huge models is to aggregate\ndevice memory from many GPUs. However, this approach introduces prohibitive\ncosts for most data scientists with a limited budget for high-end GPU servers.\nIn this paper, we focus on LLM fine-tuning on a single consumer-grade GPU in a\ncommodity server with limited main memory capacity, which is accessible to most\nAI researchers. In such a scenario, existing offloading-based methods fail to\nfine-tune an LLM efficiently due to a lack of holistic intra-server tensor\nmovement management. To this end, we present LoHan, a low-cost,\nhigh-performance deep learning training framework that enables efficient\n100B-scale model fine-tuning on a commodity server with a consumer-grade GPU\nand limited main memory capacity. The key idea is to add holistic offloading\ntraffic as an optimization dimension for 1)active gradient offloading, and\n2)holistic traffic-aware activation swapping mechanism. The experimental\nresults show that 1)LoHan is the first to fine-tune a 175B model on an RTX 4090\nand 256 GB main memory, 2)LoHan achieves 2.32x throughput than the\nstate-of-the-art baselines when fine-tuning a small 13B model, and 3)LoHan\nenables a cheap low-end consumer GPU to have higher cost-effectiveness than a\nDGX-A100 cluster when fine-tuning a 175B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, AI researchers become more and more interested in fine-tuning a\npre-trained LLM, whose size has grown to up to over 100B parameters, for their\ndownstream tasks. One approach to fine-tune such huge models is to aggregate\ndevice memory from many GPUs. However, this approach introduces prohibitive\ncosts for most data scientists with a limited budget for high-end GPU servers.\nIn this paper, we focus on LLM fine-tuning on a single consumer-grade GPU in a\ncommodity server with limited main memory capacity, which is accessible to most\nAI researchers. In such a scenario, existing offloading-based methods fail to\nfine-tune an LLM efficiently due to a lack of holistic intra-server tensor\nmovement management. To this end, we present LoHan, a low-cost,\nhigh-performance deep learning training framework that enables efficient\n100B-scale model fine-tuning on a commodity server with a consumer-grade GPU\nand limited main memory capacity. The key idea is to add holistic offloading\ntraffic as an optimization dimension for 1)active gradient offloading, and\n2)holistic traffic-aware activation swapping mechanism. The experimental\nresults show that 1)LoHan is the first to fine-tune a 175B model on an RTX 4090\nand 256 GB main memory, 2)LoHan achieves 2.32x throughput than the\nstate-of-the-art baselines when fine-tuning a small 13B model, and 3)LoHan\nenables a cheap low-end consumer GPU to have higher cost-effectiveness than a\nDGX-A100 cluster when fine-tuning a 175B model."
                },
                "authors": [
                    {
                        "name": "Changyue Liao"
                    },
                    {
                        "name": "Mo Sun"
                    },
                    {
                        "name": "Zihan Yang"
                    },
                    {
                        "name": "Jun Xie"
                    },
                    {
                        "name": "Kaiqi Chen"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Fei Wu"
                    },
                    {
                        "name": "Zeke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zeke Wang"
                },
                "author": "Zeke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.08309v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.08309v3",
                "updated": "2024-12-24T13:37:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    37,
                    49,
                    1,
                    359,
                    0
                ],
                "published": "2024-02-13T09:12:55Z",
                "published_parsed": [
                    2024,
                    2,
                    13,
                    9,
                    12,
                    55,
                    1,
                    44,
                    0
                ],
                "title": "Prompted Contextual Vectors for Spear-Phishing Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompted Contextual Vectors for Spear-Phishing Detection"
                },
                "summary": "Spear-phishing attacks present a significant security challenge, with large\nlanguage models (LLMs) escalating the threat by generating convincing emails\nand facilitating target reconnaissance. To address this, we propose a detection\napproach based on a novel document vectorization method that utilizes an\nensemble of LLMs to create representation vectors. By prompting LLMs to reason\nand respond to human-crafted questions, we quantify the presence of common\npersuasion principles in the email's content, producing prompted contextual\ndocument vectors for a downstream supervised machine learning model. We\nevaluate our method using a unique dataset generated by a proprietary system\nthat automates target reconnaissance and spear-phishing email creation. Our\nmethod achieves a 91\\% F1 score in identifying LLM-generated spear-phishing\nemails, with the training set comprising only traditional phishing and benign\nemails. Key contributions include a novel document vectorization method\nutilizing LLM reasoning, a publicly available dataset of high-quality\nspear-phishing emails, and the demonstrated effectiveness of our method in\ndetecting such emails. This methodology can be utilized for various document\nclassification tasks, particularly in adversarial problem domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spear-phishing attacks present a significant security challenge, with large\nlanguage models (LLMs) escalating the threat by generating convincing emails\nand facilitating target reconnaissance. To address this, we propose a detection\napproach based on a novel document vectorization method that utilizes an\nensemble of LLMs to create representation vectors. By prompting LLMs to reason\nand respond to human-crafted questions, we quantify the presence of common\npersuasion principles in the email's content, producing prompted contextual\ndocument vectors for a downstream supervised machine learning model. We\nevaluate our method using a unique dataset generated by a proprietary system\nthat automates target reconnaissance and spear-phishing email creation. Our\nmethod achieves a 91\\% F1 score in identifying LLM-generated spear-phishing\nemails, with the training set comprising only traditional phishing and benign\nemails. Key contributions include a novel document vectorization method\nutilizing LLM reasoning, a publicly available dataset of high-quality\nspear-phishing emails, and the demonstrated effectiveness of our method in\ndetecting such emails. This methodology can be utilized for various document\nclassification tasks, particularly in adversarial problem domains."
                },
                "authors": [
                    {
                        "name": "Daniel Nahmias"
                    },
                    {
                        "name": "Gal Engelberg"
                    },
                    {
                        "name": "Dan Klein"
                    },
                    {
                        "name": "Asaf Shabtai"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Shabtai"
                },
                "author": "Asaf Shabtai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.08309v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.08309v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18419v1",
                "updated": "2024-12-24T13:24:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    24,
                    1,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T13:24:01Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    24,
                    1,
                    1,
                    359,
                    0
                ],
                "title": "Research on the Proximity Relationships of Psychosomatic Disease\n  Knowledge Graph Modules Extracted by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on the Proximity Relationships of Psychosomatic Disease\n  Knowledge Graph Modules Extracted by Large Language Models"
                },
                "summary": "As social changes accelerate, the incidence of psychosomatic disorders has\nsignificantly increased, becoming a major challenge in global health issues.\nThis necessitates an innovative knowledge system and analytical methods to aid\nin diagnosis and treatment. Here, we establish the ontology model and entity\ntypes, using the BERT model and LoRA-tuned LLM for named entity recognition,\nconstructing the knowledge graph with 9668 triples. Next, by analyzing the\nnetwork distances between disease, symptom, and drug modules, it was found that\ncloser network distances among diseases can predict greater similarities in\ntheir clinical manifestations, treatment approaches, and psychological\nmechanisms, and closer distances between symptoms indicate that they are more\nlikely to co-occur. Lastly, by comparing the proximity d and proximity z score,\nit was shown that symptom-disease pairs in primary diagnostic relationships\nhave a stronger association and are of higher referential value than those in\ndiagnostic relationships. The research results revealed the potential\nconnections between diseases, co-occurring symptoms, and similarities in\ntreatment strategies, providing new perspectives for the diagnosis and\ntreatment of psychosomatic disorders and valuable information for future mental\nhealth research and practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As social changes accelerate, the incidence of psychosomatic disorders has\nsignificantly increased, becoming a major challenge in global health issues.\nThis necessitates an innovative knowledge system and analytical methods to aid\nin diagnosis and treatment. Here, we establish the ontology model and entity\ntypes, using the BERT model and LoRA-tuned LLM for named entity recognition,\nconstructing the knowledge graph with 9668 triples. Next, by analyzing the\nnetwork distances between disease, symptom, and drug modules, it was found that\ncloser network distances among diseases can predict greater similarities in\ntheir clinical manifestations, treatment approaches, and psychological\nmechanisms, and closer distances between symptoms indicate that they are more\nlikely to co-occur. Lastly, by comparing the proximity d and proximity z score,\nit was shown that symptom-disease pairs in primary diagnostic relationships\nhave a stronger association and are of higher referential value than those in\ndiagnostic relationships. The research results revealed the potential\nconnections between diseases, co-occurring symptoms, and similarities in\ntreatment strategies, providing new perspectives for the diagnosis and\ntreatment of psychosomatic disorders and valuable information for future mental\nhealth research and practice."
                },
                "authors": [
                    {
                        "name": "Zihan Zhou"
                    },
                    {
                        "name": "Ziyi Zeng"
                    },
                    {
                        "name": "Wenhao Jiang"
                    },
                    {
                        "name": "Yihui Zhu"
                    },
                    {
                        "name": "Jiaxin Mao"
                    },
                    {
                        "name": "Yonggui Yuan"
                    },
                    {
                        "name": "Min Xia"
                    },
                    {
                        "name": "Shubin Zhao"
                    },
                    {
                        "name": "Mengyu Yao"
                    },
                    {
                        "name": "Yunqian Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yunqian Chen"
                },
                "author": "Yunqian Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18416v1",
                "updated": "2024-12-24T13:08:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    8,
                    34,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T13:08:34Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    8,
                    34,
                    1,
                    359,
                    0
                ],
                "title": "Muse: A Multimodal Conversational Recommendation Dataset with\n  Scenario-Grounded User Profiles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Muse: A Multimodal Conversational Recommendation Dataset with\n  Scenario-Grounded User Profiles"
                },
                "summary": "Current conversational recommendation systems focus predominantly on text.\nHowever, real-world recommendation settings are generally multimodal, causing a\nsignificant gap between existing research and practical applications. To\naddress this issue, we propose Muse, the first multimodal conversational\nrecommendation dataset. Muse comprises 83,148 utterances from 7,000\nconversations centered around the Clothing domain. Each conversation contains\ncomprehensive multimodal interactions, rich elements, and natural dialogues.\nData in Muse are automatically synthesized by a multi-agent framework powered\nby multimodal large language models (MLLMs). It innovatively derives user\nprofiles from real-world scenarios rather than depending on manual design and\nhistory data for better scalability, and then it fulfills conversation\nsimulation and optimization. Both human and LLM evaluations demonstrate the\nhigh quality of conversations in Muse. Additionally, fine-tuning experiments on\nthree MLLMs demonstrate Muse's learnable patterns for recommendations and\nresponses, confirming its value for multimodal conversational recommendation.\nOur dataset and codes are available at\n\\url{https://anonymous.4open.science/r/Muse-0086}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current conversational recommendation systems focus predominantly on text.\nHowever, real-world recommendation settings are generally multimodal, causing a\nsignificant gap between existing research and practical applications. To\naddress this issue, we propose Muse, the first multimodal conversational\nrecommendation dataset. Muse comprises 83,148 utterances from 7,000\nconversations centered around the Clothing domain. Each conversation contains\ncomprehensive multimodal interactions, rich elements, and natural dialogues.\nData in Muse are automatically synthesized by a multi-agent framework powered\nby multimodal large language models (MLLMs). It innovatively derives user\nprofiles from real-world scenarios rather than depending on manual design and\nhistory data for better scalability, and then it fulfills conversation\nsimulation and optimization. Both human and LLM evaluations demonstrate the\nhigh quality of conversations in Muse. Additionally, fine-tuning experiments on\nthree MLLMs demonstrate Muse's learnable patterns for recommendations and\nresponses, confirming its value for multimodal conversational recommendation.\nOur dataset and codes are available at\n\\url{https://anonymous.4open.science/r/Muse-0086}."
                },
                "authors": [
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Xiaocui Yang"
                    },
                    {
                        "name": "Yongkang Liu"
                    },
                    {
                        "name": "Shi Feng"
                    },
                    {
                        "name": "Daling Wang"
                    },
                    {
                        "name": "Yifei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yifei Zhang"
                },
                "author": "Yifei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18415v1",
                "updated": "2024-12-24T13:07:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    7,
                    29,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T13:07:29Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    7,
                    29,
                    1,
                    359,
                    0
                ],
                "title": "Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi\n  and English",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi\n  and English"
                },
                "summary": "Large Language Models (LLMs) excel in linguistic tasks but struggle with\nmathematical reasoning, particularly in non English languages like Hindi. This\nresearch aims to enhance the mathematical reasoning skills of smaller, resource\nefficient open-source LLMs in both Hindi and English. We evaluate models like\nOpenHathi 7B, LLaMA-2 7B, WizardMath 7B, Mistral 7B, LLeMMa 7B, MAmmoTH 7B,\nGemini Pro, and GPT-4 using zero-shot, few-shot chain-of-thought (CoT) methods,\nand supervised fine-tuning. Our approach incorporates curriculum learning,\nprogressively training models on increasingly difficult problems, a novel\nDecomposition Strategy to simplify complex arithmetic operations, and a\nStructured Solution Design that divides solutions into phases. Our experiments\nresult in notable performance enhancements. WizardMath 7B exceeds Gemini's\naccuracy on English datasets by +6% and matches Gemini's performance on Hindi\ndatasets. Adopting a bilingual approach that combines English and Hindi samples\nachieves results comparable to individual language models, demonstrating the\ncapability to learn mathematical reasoning in both languages. This research\nhighlights the potential for improving mathematical reasoning in open-source\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in linguistic tasks but struggle with\nmathematical reasoning, particularly in non English languages like Hindi. This\nresearch aims to enhance the mathematical reasoning skills of smaller, resource\nefficient open-source LLMs in both Hindi and English. We evaluate models like\nOpenHathi 7B, LLaMA-2 7B, WizardMath 7B, Mistral 7B, LLeMMa 7B, MAmmoTH 7B,\nGemini Pro, and GPT-4 using zero-shot, few-shot chain-of-thought (CoT) methods,\nand supervised fine-tuning. Our approach incorporates curriculum learning,\nprogressively training models on increasingly difficult problems, a novel\nDecomposition Strategy to simplify complex arithmetic operations, and a\nStructured Solution Design that divides solutions into phases. Our experiments\nresult in notable performance enhancements. WizardMath 7B exceeds Gemini's\naccuracy on English datasets by +6% and matches Gemini's performance on Hindi\ndatasets. Adopting a bilingual approach that combines English and Hindi samples\nachieves results comparable to individual language models, demonstrating the\ncapability to learn mathematical reasoning in both languages. This research\nhighlights the potential for improving mathematical reasoning in open-source\nLLMs."
                },
                "authors": [
                    {
                        "name": "Avinash Anand"
                    },
                    {
                        "name": "Kritarth Prasad"
                    },
                    {
                        "name": "Chhavi Kirtani"
                    },
                    {
                        "name": "Ashwin R Nair"
                    },
                    {
                        "name": "Manvendra Kumar Nema"
                    },
                    {
                        "name": "Raj Jaiswal"
                    },
                    {
                        "name": "Rajiv Ratn Shah"
                    }
                ],
                "author_detail": {
                    "name": "Rajiv Ratn Shah"
                },
                "author": "Rajiv Ratn Shah",
                "arxiv_comment": "Accepted at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16187v2",
                "updated": "2024-12-24T13:04:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    4,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-13T06:00:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    0,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing"
                },
                "summary": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks."
                },
                "authors": [
                    {
                        "name": "Minghui Liu"
                    },
                    {
                        "name": "Tahseen Rabbani"
                    },
                    {
                        "name": "Tony O'Halloran"
                    },
                    {
                        "name": "Ananth Sankaralingam"
                    },
                    {
                        "name": "Mary-Anne Hartley"
                    },
                    {
                        "name": "Brian Gravelle"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Cornelia FermÃ¼ller"
                    },
                    {
                        "name": "Yiannis Aloimonos"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Aloimonos"
                },
                "author": "Yiannis Aloimonos",
                "arxiv_comment": "10 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18407v1",
                "updated": "2024-12-24T12:54:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    12,
                    54,
                    19,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T12:54:19Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    12,
                    54,
                    19,
                    1,
                    359,
                    0
                ],
                "title": "A Statistical Framework for Ranking LLM-Based Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Statistical Framework for Ranking LLM-Based Chatbots"
                },
                "summary": "Large language models (LLMs) have transformed natural language processing,\nwith frameworks like Chatbot Arena providing pioneering platforms for\nevaluating these models. By facilitating millions of pairwise comparisons based\non human judgments, Chatbot Arena has become a cornerstone in LLM evaluation,\noffering rich datasets for ranking models in open-ended conversational tasks.\nBuilding upon this foundation, we propose a statistical framework that\nincorporates key advancements to address specific challenges in pairwise\ncomparison analysis. First, we introduce a factored tie model that enhances the\nability to handle ties -- an integral aspect of human-judged comparisons --\nsignificantly improving the model's fit to observed data. Second, we extend the\nframework to model covariance between competitors, enabling deeper insights\ninto performance relationships and facilitating intuitive groupings into\nperformance tiers. Third, we resolve optimization challenges arising from\nparameter non-uniqueness by introducing novel constraints, ensuring stable and\ninterpretable parameter estimation. Through rigorous evaluation and extensive\nexperimentation, our framework demonstrates substantial improvements over\nexisting methods in modeling pairwise comparison data. To support\nreproducibility and practical adoption, we release leaderbot, an open-source\nPython package implementing our models and analyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have transformed natural language processing,\nwith frameworks like Chatbot Arena providing pioneering platforms for\nevaluating these models. By facilitating millions of pairwise comparisons based\non human judgments, Chatbot Arena has become a cornerstone in LLM evaluation,\noffering rich datasets for ranking models in open-ended conversational tasks.\nBuilding upon this foundation, we propose a statistical framework that\nincorporates key advancements to address specific challenges in pairwise\ncomparison analysis. First, we introduce a factored tie model that enhances the\nability to handle ties -- an integral aspect of human-judged comparisons --\nsignificantly improving the model's fit to observed data. Second, we extend the\nframework to model covariance between competitors, enabling deeper insights\ninto performance relationships and facilitating intuitive groupings into\nperformance tiers. Third, we resolve optimization challenges arising from\nparameter non-uniqueness by introducing novel constraints, ensuring stable and\ninterpretable parameter estimation. Through rigorous evaluation and extensive\nexperimentation, our framework demonstrates substantial improvements over\nexisting methods in modeling pairwise comparison data. To support\nreproducibility and practical adoption, we release leaderbot, an open-source\nPython package implementing our models and analyses."
                },
                "authors": [
                    {
                        "name": "Siavash Ameli"
                    },
                    {
                        "name": "Siyuan Zhuang"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    }
                ],
                "author_detail": {
                    "name": "Michael W. Mahoney"
                },
                "author": "Michael W. Mahoney",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18390v1",
                "updated": "2024-12-24T12:28:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    12,
                    28,
                    19,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T12:28:19Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    12,
                    28,
                    19,
                    1,
                    359,
                    0
                ],
                "title": "RDPM: Solve Diffusion Probabilistic Models via Recurrent Token\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RDPM: Solve Diffusion Probabilistic Models via Recurrent Token\n  Prediction"
                },
                "summary": "Diffusion Probabilistic Models (DPMs) have emerged as the de facto approach\nfor high-fidelity image synthesis, operating diffusion processes on continuous\nVAE latent, which significantly differ from the text generation methods\nemployed by Large Language Models (LLMs). In this paper, we introduce a novel\ngenerative framework, the Recurrent Diffusion Probabilistic Model (RDPM), which\nenhances the diffusion process through a recurrent token prediction mechanism,\nthereby pioneering the field of Discrete Diffusion. By progressively\nintroducing Gaussian noise into the latent representations of images and\nencoding them into vector-quantized tokens in a recurrent manner, RDPM\nfacilitates a unique diffusion process on discrete-value domains. This process\niteratively predicts the token codes for subsequent timesteps, transforming the\ninitial standard Gaussian noise into the source data distribution, aligning\nwith GPT-style models in terms of the loss function. RDPM demonstrates superior\nperformance while benefiting from the speed advantage of requiring only a few\ninference steps. This model not only leverages the diffusion process to ensure\nhigh-quality generation but also converts continuous signals into a series of\nhigh-fidelity discrete tokens, thereby maintaining a unified optimization\nstrategy with other discrete tokens, such as text. We anticipate that this work\nwill contribute to the development of a unified model for multimodal\ngeneration, specifically by integrating continuous signal domains such as\nimages, videos, and audio with text. We will release the code and model weights\nto the open-source community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Probabilistic Models (DPMs) have emerged as the de facto approach\nfor high-fidelity image synthesis, operating diffusion processes on continuous\nVAE latent, which significantly differ from the text generation methods\nemployed by Large Language Models (LLMs). In this paper, we introduce a novel\ngenerative framework, the Recurrent Diffusion Probabilistic Model (RDPM), which\nenhances the diffusion process through a recurrent token prediction mechanism,\nthereby pioneering the field of Discrete Diffusion. By progressively\nintroducing Gaussian noise into the latent representations of images and\nencoding them into vector-quantized tokens in a recurrent manner, RDPM\nfacilitates a unique diffusion process on discrete-value domains. This process\niteratively predicts the token codes for subsequent timesteps, transforming the\ninitial standard Gaussian noise into the source data distribution, aligning\nwith GPT-style models in terms of the loss function. RDPM demonstrates superior\nperformance while benefiting from the speed advantage of requiring only a few\ninference steps. This model not only leverages the diffusion process to ensure\nhigh-quality generation but also converts continuous signals into a series of\nhigh-fidelity discrete tokens, thereby maintaining a unified optimization\nstrategy with other discrete tokens, such as text. We anticipate that this work\nwill contribute to the development of a unified model for multimodal\ngeneration, specifically by integrating continuous signal domains such as\nimages, videos, and audio with text. We will release the code and model weights\nto the open-source community."
                },
                "authors": [
                    {
                        "name": "Wu Xiaoping"
                    },
                    {
                        "name": "Hu Jie"
                    },
                    {
                        "name": "Wei Xiaoming"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xiaoming"
                },
                "author": "Wei Xiaoming",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18377v1",
                "updated": "2024-12-24T12:03:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    12,
                    3,
                    36,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T12:03:36Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    12,
                    3,
                    36,
                    1,
                    359,
                    0
                ],
                "title": "ChaI-TeA: A Benchmark for Evaluating Autocompletion of Interactions with\n  LLM-based Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChaI-TeA: A Benchmark for Evaluating Autocompletion of Interactions with\n  LLM-based Chatbots"
                },
                "summary": "The rise of LLMs has deflected a growing portion of human-computer\ninteractions towards LLM-based chatbots. The remarkable abilities of these\nmodels allow users to interact using long, diverse natural language text\ncovering a wide range of topics and styles. Phrasing these messages is a time\nand effort consuming task, calling for an autocomplete solution to assist\nusers. We introduce the task of chatbot interaction autocomplete. We present\nChaI-TeA: CHat InTEraction Autocomplete; An autcomplete evaluation framework\nfor LLM-based chatbot interactions. The framework includes a formal definition\nof the task, coupled with suitable datasets and metrics. We use the framework\nto evaluate After formally defining the task along with suitable datasets and\nmetrics, we test 9 models on the defined auto completion task, finding that\nwhile current off-the-shelf models perform fairly, there is still much room for\nimprovement, mainly in ranking of the generated suggestions. We provide\ninsights for practitioners working on this task and open new research\ndirections for researchers in the field. We release our framework to serve as a\nfoundation for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of LLMs has deflected a growing portion of human-computer\ninteractions towards LLM-based chatbots. The remarkable abilities of these\nmodels allow users to interact using long, diverse natural language text\ncovering a wide range of topics and styles. Phrasing these messages is a time\nand effort consuming task, calling for an autocomplete solution to assist\nusers. We introduce the task of chatbot interaction autocomplete. We present\nChaI-TeA: CHat InTEraction Autocomplete; An autcomplete evaluation framework\nfor LLM-based chatbot interactions. The framework includes a formal definition\nof the task, coupled with suitable datasets and metrics. We use the framework\nto evaluate After formally defining the task along with suitable datasets and\nmetrics, we test 9 models on the defined auto completion task, finding that\nwhile current off-the-shelf models perform fairly, there is still much room for\nimprovement, mainly in ranking of the generated suggestions. We provide\ninsights for practitioners working on this task and open new research\ndirections for researchers in the field. We release our framework to serve as a\nfoundation for future research."
                },
                "authors": [
                    {
                        "name": "Shani Goren"
                    },
                    {
                        "name": "Oren Kalinsky"
                    },
                    {
                        "name": "Tomer Stav"
                    },
                    {
                        "name": "Yuri Rapoport"
                    },
                    {
                        "name": "Yaron Fairstein"
                    },
                    {
                        "name": "Ram Yazdy"
                    },
                    {
                        "name": "Nachshon Cohen"
                    },
                    {
                        "name": "Alexander Libov"
                    },
                    {
                        "name": "Guy Kushilevitz"
                    }
                ],
                "author_detail": {
                    "name": "Guy Kushilevitz"
                },
                "author": "Guy Kushilevitz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18371v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18371v1",
                "updated": "2024-12-24T11:54:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    11,
                    54,
                    14,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T11:54:14Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    11,
                    54,
                    14,
                    1,
                    359,
                    0
                ],
                "title": "Defining and Detecting the Defects of the Large Language Model-based\n  Autonomous Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defining and Detecting the Defects of the Large Language Model-based\n  Autonomous Agents"
                },
                "summary": "AI agents are systems capable of perceiving their environment, autonomously\nplanning and executing tasks. Recent advancements in LLM have introduced a\ntransformative paradigm for AI agents, enabling them to interact with external\nresources and tools through prompts. In such agents, the workflow integrates\ndeveloper-written code, which manages framework construction and logic control,\nwith LLM-generated natural language that enhances dynamic decision-making and\ninteraction. However, discrepancies between developer-implemented logic and the\ndynamically generated content of LLMs in terms of behavior and expected\noutcomes can lead to defects, such as tool invocation failures and task\nexecution errors. These issues introduce specific risks, leading to various\ndefects in LLM-based AI Agents, such as service interruptions. Despite the\nimportance of these issues, there is a lack of systematic work that focuses on\nanalyzing LLM-based AI Agents to uncover defects in their code. In this paper,\nwe present the first study focused on identifying and detecting defects in LLM\nAgents. We collected and analyzed 6,854 relevant posts from StackOverflow to\ndefine 8 types of agent defects. For each type, we provided detailed\ndescriptions with an example. Then, we designed a static analysis tool, named\nAgentable, to detect the defects. Agentable leverages Code Property Graphs and\nLLMs to analyze Agent workflows by efficiently identifying specific code\npatterns and analyzing natural language descriptions. To evaluate Agentable, we\nconstructed two datasets: AgentSet, consists of 84 real-world Agents, and\nAgentTest, which contains 78 Agents specifically designed to include various\ntypes of defects. Our results show that Agentable achieved an overall accuracy\nof 88.79% and a recall rate of 91.03%. Furthermore, our analysis reveals the\n889 defects of the AgentSet, highlighting the prevalence of these defects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents are systems capable of perceiving their environment, autonomously\nplanning and executing tasks. Recent advancements in LLM have introduced a\ntransformative paradigm for AI agents, enabling them to interact with external\nresources and tools through prompts. In such agents, the workflow integrates\ndeveloper-written code, which manages framework construction and logic control,\nwith LLM-generated natural language that enhances dynamic decision-making and\ninteraction. However, discrepancies between developer-implemented logic and the\ndynamically generated content of LLMs in terms of behavior and expected\noutcomes can lead to defects, such as tool invocation failures and task\nexecution errors. These issues introduce specific risks, leading to various\ndefects in LLM-based AI Agents, such as service interruptions. Despite the\nimportance of these issues, there is a lack of systematic work that focuses on\nanalyzing LLM-based AI Agents to uncover defects in their code. In this paper,\nwe present the first study focused on identifying and detecting defects in LLM\nAgents. We collected and analyzed 6,854 relevant posts from StackOverflow to\ndefine 8 types of agent defects. For each type, we provided detailed\ndescriptions with an example. Then, we designed a static analysis tool, named\nAgentable, to detect the defects. Agentable leverages Code Property Graphs and\nLLMs to analyze Agent workflows by efficiently identifying specific code\npatterns and analyzing natural language descriptions. To evaluate Agentable, we\nconstructed two datasets: AgentSet, consists of 84 real-world Agents, and\nAgentTest, which contains 78 Agents specifically designed to include various\ntypes of defects. Our results show that Agentable achieved an overall accuracy\nof 88.79% and a recall rate of 91.03%. Furthermore, our analysis reveals the\n889 defects of the AgentSet, highlighting the prevalence of these defects."
                },
                "authors": [
                    {
                        "name": "Kaiwen Ning"
                    },
                    {
                        "name": "Jiachi Chen"
                    },
                    {
                        "name": "Jingwen Zhang"
                    },
                    {
                        "name": "Wei Lia"
                    },
                    {
                        "name": "Zexu Wang"
                    },
                    {
                        "name": "Yuming Feng"
                    },
                    {
                        "name": "Weizhe Zhang"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18371v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18371v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18367v1",
                "updated": "2024-12-24T11:50:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    11,
                    50,
                    18,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T11:50:18Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    11,
                    50,
                    18,
                    1,
                    359,
                    0
                ],
                "title": "Towards Global AI Inclusivity: A Large-Scale Multilingual Terminology\n  Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Global AI Inclusivity: A Large-Scale Multilingual Terminology\n  Dataset"
                },
                "summary": "The field of machine translation has achieved significant advancements, yet\ndomain-specific terminology translation, particularly in AI, remains\nchallenging. We introduced GIST, a large-scale multilingual AI terminology\ndataset containing 5K terms extracted from top AI conference papers spanning\n2000 to 2023. The terms were translated into Arabic, Chinese, French, Japanese,\nand Russian using a hybrid framework that combines LLMs for extraction with\nhuman expertise for translation. The dataset's quality was benchmarked against\nexisting resources, demonstrating superior translation accuracy through\ncrowdsourced evaluation. GIST was integrated into translation workflows using\npost-translation refinement methods that required no retraining, where LLM\nprompting consistently improved BLEU and COMET scores. A web demonstration on\nthe ACL Anthology platform highlights its practical application, showcasing\nimproved accessibility for non-English speakers. This work aims to address\ncritical gaps in AI terminology resources and fosters global inclusivity and\ncollaboration in AI research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of machine translation has achieved significant advancements, yet\ndomain-specific terminology translation, particularly in AI, remains\nchallenging. We introduced GIST, a large-scale multilingual AI terminology\ndataset containing 5K terms extracted from top AI conference papers spanning\n2000 to 2023. The terms were translated into Arabic, Chinese, French, Japanese,\nand Russian using a hybrid framework that combines LLMs for extraction with\nhuman expertise for translation. The dataset's quality was benchmarked against\nexisting resources, demonstrating superior translation accuracy through\ncrowdsourced evaluation. GIST was integrated into translation workflows using\npost-translation refinement methods that required no retraining, where LLM\nprompting consistently improved BLEU and COMET scores. A web demonstration on\nthe ACL Anthology platform highlights its practical application, showcasing\nimproved accessibility for non-English speakers. This work aims to address\ncritical gaps in AI terminology resources and fosters global inclusivity and\ncollaboration in AI research."
                },
                "authors": [
                    {
                        "name": "Jiarui Liu"
                    },
                    {
                        "name": "Iman Ouzzani"
                    },
                    {
                        "name": "Wenkai Li"
                    },
                    {
                        "name": "Lechen Zhang"
                    },
                    {
                        "name": "Tianyue Ou"
                    },
                    {
                        "name": "Houda Bouamor"
                    },
                    {
                        "name": "Zhijing Jin"
                    },
                    {
                        "name": "Mona Diab"
                    }
                ],
                "author_detail": {
                    "name": "Mona Diab"
                },
                "author": "Mona Diab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12698v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12698v3",
                "updated": "2024-12-24T11:46:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    11,
                    46,
                    32,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-17T09:16:28Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    16,
                    28,
                    1,
                    352,
                    0
                ],
                "title": "Audio Array-Based 3D UAV Trajectory Estimation with LiDAR\n  Pseudo-Labeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio Array-Based 3D UAV Trajectory Estimation with LiDAR\n  Pseudo-Labeling"
                },
                "summary": "As small unmanned aerial vehicles (UAVs) become increasingly prevalent, there\nis growing concern regarding their impact on public safety and privacy,\nhighlighting the need for advanced tracking and trajectory estimation\nsolutions. In response, this paper introduces a novel framework that utilizes\naudio array for 3D UAV trajectory estimation. Our approach incorporates a\nself-supervised learning model, starting with the conversion of audio data into\nmel-spectrograms, which are analyzed through an encoder to extract crucial\ntemporal and spectral information. Simultaneously, UAV trajectories are\nestimated using LiDAR point clouds via unsupervised methods. These LiDAR-based\nestimations act as pseudo labels, enabling the training of an Audio Perception\nNetwork without requiring labeled data. In this architecture, the LiDAR-based\nsystem operates as the Teacher Network, guiding the Audio Perception Network,\nwhich serves as the Student Network. Once trained, the model can independently\npredict 3D trajectories using only audio signals, with no need for LiDAR data\nor external ground truth during deployment. To further enhance precision, we\napply Gaussian Process modeling for improved spatiotemporal tracking. Our\nmethod delivers top-tier performance on the MMAUD dataset, establishing a new\nbenchmark in trajectory estimation using self-supervised learning techniques\nwithout reliance on ground truth annotations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As small unmanned aerial vehicles (UAVs) become increasingly prevalent, there\nis growing concern regarding their impact on public safety and privacy,\nhighlighting the need for advanced tracking and trajectory estimation\nsolutions. In response, this paper introduces a novel framework that utilizes\naudio array for 3D UAV trajectory estimation. Our approach incorporates a\nself-supervised learning model, starting with the conversion of audio data into\nmel-spectrograms, which are analyzed through an encoder to extract crucial\ntemporal and spectral information. Simultaneously, UAV trajectories are\nestimated using LiDAR point clouds via unsupervised methods. These LiDAR-based\nestimations act as pseudo labels, enabling the training of an Audio Perception\nNetwork without requiring labeled data. In this architecture, the LiDAR-based\nsystem operates as the Teacher Network, guiding the Audio Perception Network,\nwhich serves as the Student Network. Once trained, the model can independently\npredict 3D trajectories using only audio signals, with no need for LiDAR data\nor external ground truth during deployment. To further enhance precision, we\napply Gaussian Process modeling for improved spatiotemporal tracking. Our\nmethod delivers top-tier performance on the MMAUD dataset, establishing a new\nbenchmark in trajectory estimation using self-supervised learning techniques\nwithout reliance on ground truth annotations."
                },
                "authors": [
                    {
                        "name": "Allen Lei"
                    },
                    {
                        "name": "Tianchen Deng"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Jianfei Yang"
                    },
                    {
                        "name": "Shenghai Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Shenghai Yuan"
                },
                "author": "Shenghai Yuan",
                "arxiv_comment": "Accepted for ICASSP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12698v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12698v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16964v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16964v2",
                "updated": "2024-12-24T11:43:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    11,
                    43,
                    25,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-22T10:49:27Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    10,
                    49,
                    27,
                    6,
                    357,
                    0
                ],
                "title": "System-2 Mathematical Reasoning via Enriched Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "System-2 Mathematical Reasoning via Enriched Instruction Tuning"
                },
                "summary": "Solving complex mathematical problems via system-2 reasoning is a natural\nhuman skill, yet it remains a significant challenge for current large language\nmodels (LLMs). We identify the scarcity of deliberate multi-step reasoning data\nas a primary limiting factor. To this end, we introduce Enriched Instruction\nTuning (EIT), a method that enriches existing human-annotated mathematical\ndatasets by synergizing human and AI feedback to create fine-grained reasoning\ntrajectories. These datasets are then used to fine-tune open-source LLMs,\nenhancing their mathematical reasoning abilities without reliance on any\nsymbolic verification program. Concretely, EIT is composed of two critical\nsteps: Enriching with Reasoning Plan (ERP) and Enriching with Reasoning Step\n(ERS). The former generates a high-level plan that breaks down complex\ninstructions into a sequence of simpler objectives, while ERS fills in\nreasoning contexts often overlooked by human annotators, creating a smoother\nreasoning trajectory for LLM fine-tuning. Unlike existing CoT prompting methods\nthat generate reasoning chains only depending on LLM's internal knowledge, our\nmethod leverages human-annotated initial answers as ``meta-knowledge'' to help\nLLMs generate more detailed and precise reasoning processes, leading to a more\ntrustworthy LLM expert for complex mathematical problems. In experiments, EIT\nachieves an accuracy of 84.1% on GSM8K and 32.5% on MATH, surpassing\nstate-of-the-art fine-tuning and prompting methods, and even matching the\nperformance of tool-augmented methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving complex mathematical problems via system-2 reasoning is a natural\nhuman skill, yet it remains a significant challenge for current large language\nmodels (LLMs). We identify the scarcity of deliberate multi-step reasoning data\nas a primary limiting factor. To this end, we introduce Enriched Instruction\nTuning (EIT), a method that enriches existing human-annotated mathematical\ndatasets by synergizing human and AI feedback to create fine-grained reasoning\ntrajectories. These datasets are then used to fine-tune open-source LLMs,\nenhancing their mathematical reasoning abilities without reliance on any\nsymbolic verification program. Concretely, EIT is composed of two critical\nsteps: Enriching with Reasoning Plan (ERP) and Enriching with Reasoning Step\n(ERS). The former generates a high-level plan that breaks down complex\ninstructions into a sequence of simpler objectives, while ERS fills in\nreasoning contexts often overlooked by human annotators, creating a smoother\nreasoning trajectory for LLM fine-tuning. Unlike existing CoT prompting methods\nthat generate reasoning chains only depending on LLM's internal knowledge, our\nmethod leverages human-annotated initial answers as ``meta-knowledge'' to help\nLLMs generate more detailed and precise reasoning processes, leading to a more\ntrustworthy LLM expert for complex mathematical problems. In experiments, EIT\nachieves an accuracy of 84.1% on GSM8K and 32.5% on MATH, surpassing\nstate-of-the-art fine-tuning and prompting methods, and even matching the\nperformance of tool-augmented methods."
                },
                "authors": [
                    {
                        "name": "Huanqia Cai"
                    },
                    {
                        "name": "Yijun Yang"
                    },
                    {
                        "name": "Zhifeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhifeng Li"
                },
                "author": "Zhifeng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16964v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16964v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18351v1",
                "updated": "2024-12-24T11:24:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    11,
                    24,
                    56,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T11:24:56Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    11,
                    24,
                    56,
                    1,
                    359,
                    0
                ],
                "title": "Multi-Agents Based on Large Language Models for Knowledge-based Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agents Based on Large Language Models for Knowledge-based Visual\n  Question Answering"
                },
                "summary": "Large Language Models (LLMs) have achieved impressive results in\nknowledge-based Visual Question Answering (VQA). However existing methods still\nhave challenges: the inability to use external tools autonomously, and the\ninability to work in teams. Humans tend to know whether they need to use\nexternal tools when they encounter a new question, e.g., they tend to be able\nto give a direct answer to a familiar question, whereas they tend to use tools\nsuch as search engines when they encounter an unfamiliar question. In addition,\nhumans also tend to collaborate and discuss with others to get better answers.\nInspired by this, we propose the multi-agent voting framework. We design three\nLLM-based agents that simulate different levels of staff in a team, and assign\nthe available tools according to the levels. Each agent provides the\ncorresponding answer, and finally all the answers provided by the agents are\nvoted to get the final answer. Experiments on OK-VQA and A-OKVQA show that our\napproach outperforms other baselines by 2.2 and 1.0, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved impressive results in\nknowledge-based Visual Question Answering (VQA). However existing methods still\nhave challenges: the inability to use external tools autonomously, and the\ninability to work in teams. Humans tend to know whether they need to use\nexternal tools when they encounter a new question, e.g., they tend to be able\nto give a direct answer to a familiar question, whereas they tend to use tools\nsuch as search engines when they encounter an unfamiliar question. In addition,\nhumans also tend to collaborate and discuss with others to get better answers.\nInspired by this, we propose the multi-agent voting framework. We design three\nLLM-based agents that simulate different levels of staff in a team, and assign\nthe available tools according to the levels. Each agent provides the\ncorresponding answer, and finally all the answers provided by the agents are\nvoted to get the final answer. Experiments on OK-VQA and A-OKVQA show that our\napproach outperforms other baselines by 2.2 and 1.0, respectively."
                },
                "authors": [
                    {
                        "name": "Zhongjian Hu"
                    },
                    {
                        "name": "Peng Yang"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Zhenqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhenqi Wang"
                },
                "author": "Zhenqi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18346v1",
                "updated": "2024-12-24T11:08:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    11,
                    8,
                    15,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T11:08:15Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    11,
                    8,
                    15,
                    1,
                    359,
                    0
                ],
                "title": "ORAN Drives Higher Returns on Investments in Urban and Suburban Regions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORAN Drives Higher Returns on Investments in Urban and Suburban Regions"
                },
                "summary": "This paper provides the first incentive analysis of open radio access\nnetworks (ORAN) using game theory. We assess strategic interactions between\ntelecom supply chain stakeholders: mobile network operators (MNOs), network\ninfrastructure suppliers (NIS), and original equipment manufacturers (OEMs)\nacross three procurement scenarios: (i) Traditional, (ii) Predatory as\nmonolithic radio access networks (MRAN), and (iii) DirectOEM as ORAN. We use\nrandom forest and gradient boosting models to evaluate the optimal margins\nacross urban, suburban, and rural U.S. regions. Results suggest that ORAN\ndeployment consistently demonstrates higher net present value (NPV) of profits\nin urban and suburban regions, outperforming the traditional procurement\nstrategy by 11% to 31%. However, rural areas present lower NPVs across all\nscenarios, with significant variability at the county level. This analysis\noffers actionable insights for telecom investment strategies, bridging\ntechnical innovation with economic outcomes and addressing strategic supply\nchain dynamics through a game-theoretic lens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides the first incentive analysis of open radio access\nnetworks (ORAN) using game theory. We assess strategic interactions between\ntelecom supply chain stakeholders: mobile network operators (MNOs), network\ninfrastructure suppliers (NIS), and original equipment manufacturers (OEMs)\nacross three procurement scenarios: (i) Traditional, (ii) Predatory as\nmonolithic radio access networks (MRAN), and (iii) DirectOEM as ORAN. We use\nrandom forest and gradient boosting models to evaluate the optimal margins\nacross urban, suburban, and rural U.S. regions. Results suggest that ORAN\ndeployment consistently demonstrates higher net present value (NPV) of profits\nin urban and suburban regions, outperforming the traditional procurement\nstrategy by 11% to 31%. However, rural areas present lower NPVs across all\nscenarios, with significant variability at the county level. This analysis\noffers actionable insights for telecom investment strategies, bridging\ntechnical innovation with economic outcomes and addressing strategic supply\nchain dynamics through a game-theoretic lens."
                },
                "authors": [
                    {
                        "name": "Priyanka Sharma"
                    },
                    {
                        "name": "Edward J. Oughton"
                    },
                    {
                        "name": "Aleksan Shanoyan"
                    }
                ],
                "author_detail": {
                    "name": "Aleksan Shanoyan"
                },
                "author": "Aleksan Shanoyan",
                "arxiv_comment": "46 pages with 8 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.17790v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.17790v2",
                "updated": "2024-12-24T11:04:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    11,
                    4,
                    52,
                    1,
                    359,
                    0
                ],
                "published": "2024-03-26T15:21:18Z",
                "published_parsed": [
                    2024,
                    3,
                    26,
                    15,
                    21,
                    18,
                    1,
                    86,
                    0
                ],
                "title": "A PAC-Bayesian Framework for Optimal Control with Stability Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A PAC-Bayesian Framework for Optimal Control with Stability Guarantees"
                },
                "summary": "Stochastic Nonlinear Optimal Control (SNOC) involves minimizing a cost\nfunction that averages out the random uncertainties affecting the dynamics of\nnonlinear systems. For tractability reasons, this problem is typically\naddressed by minimizing an empirical cost, which represents the average cost\nacross a finite dataset of sampled disturbances. However, this approach raises\nthe challenge of quantifying the control performance against out-of-sample\nuncertainties. Particularly, in scenarios where the training dataset is small,\nSNOC policies are prone to overfitting, resulting in significant discrepancies\nbetween the empirical cost and the true cost, i.e., the average SNOC cost\nincurred during control deployment. Therefore, establishing generalization\nbounds on the true cost is crucial for ensuring reliability in real-world\napplications. In this paper, we introduce a novel approach that leverages\nPAC-Bayes theory to provide rigorous generalization bounds for SNOC. Based on\nthese bounds, we propose a new method for designing optimal controllers,\noffering a principled way to incorporate prior knowledge into the synthesis\nprocess, which aids in improving the control policy and mitigating overfitting.\nFurthermore, by leveraging recent parametrizations of stabilizing controllers\nfor nonlinear systems, our framework inherently ensures closed-loop stability.\nThe effectiveness of our proposed method in incorporating prior knowledge and\ncombating overfitting is shown by designing neural network controllers for\ntasks in cooperative robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Nonlinear Optimal Control (SNOC) involves minimizing a cost\nfunction that averages out the random uncertainties affecting the dynamics of\nnonlinear systems. For tractability reasons, this problem is typically\naddressed by minimizing an empirical cost, which represents the average cost\nacross a finite dataset of sampled disturbances. However, this approach raises\nthe challenge of quantifying the control performance against out-of-sample\nuncertainties. Particularly, in scenarios where the training dataset is small,\nSNOC policies are prone to overfitting, resulting in significant discrepancies\nbetween the empirical cost and the true cost, i.e., the average SNOC cost\nincurred during control deployment. Therefore, establishing generalization\nbounds on the true cost is crucial for ensuring reliability in real-world\napplications. In this paper, we introduce a novel approach that leverages\nPAC-Bayes theory to provide rigorous generalization bounds for SNOC. Based on\nthese bounds, we propose a new method for designing optimal controllers,\noffering a principled way to incorporate prior knowledge into the synthesis\nprocess, which aids in improving the control policy and mitigating overfitting.\nFurthermore, by leveraging recent parametrizations of stabilizing controllers\nfor nonlinear systems, our framework inherently ensures closed-loop stability.\nThe effectiveness of our proposed method in incorporating prior knowledge and\ncombating overfitting is shown by designing neural network controllers for\ntasks in cooperative robotics."
                },
                "authors": [
                    {
                        "name": "Mahrokh Ghoddousi Boroujeni"
                    },
                    {
                        "name": "Clara LucÃ­a Galimberti"
                    },
                    {
                        "name": "Andreas Krause"
                    },
                    {
                        "name": "Giancarlo Ferrari-Trecate"
                    }
                ],
                "author_detail": {
                    "name": "Giancarlo Ferrari-Trecate"
                },
                "author": "Giancarlo Ferrari-Trecate",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.17790v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.17790v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17690v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17690v2",
                "updated": "2024-12-24T11:03:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    11,
                    3,
                    42,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-23T16:16:30Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    16,
                    30,
                    0,
                    358,
                    0
                ],
                "title": "RAGONITE: Iterative Retrieval on Induced Databases and Verbalized RDF\n  for Conversational QA over KGs with RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGONITE: Iterative Retrieval on Induced Databases and Verbalized RDF\n  for Conversational QA over KGs with RAG"
                },
                "summary": "Conversational question answering (ConvQA) is a convenient means of searching\nover RDF knowledge graphs (KGs), where a prevalent approach is to translate\nnatural language questions to SPARQL queries. However, SPARQL has certain\nshortcomings: (i) it is brittle for complex intents and conversational\nquestions, and (ii) it is not suitable for more abstract needs. Instead, we\npropose a novel two-pronged system where we fuse: (i) SQL-query results over a\ndatabase automatically derived from the KG, and (ii) text-search results over\nverbalizations of KG facts. Our pipeline supports iterative retrieval: when the\nresults of any branch are found to be unsatisfactory, the system can\nautomatically opt for further rounds. We put everything together in a retrieval\naugmented generation (RAG) setup, where an LLM generates a coherent response\nfrom accumulated search results. We demonstrate the superiority of our proposed\nsystem over several baselines on a knowledge graph of BMW automobiles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational question answering (ConvQA) is a convenient means of searching\nover RDF knowledge graphs (KGs), where a prevalent approach is to translate\nnatural language questions to SPARQL queries. However, SPARQL has certain\nshortcomings: (i) it is brittle for complex intents and conversational\nquestions, and (ii) it is not suitable for more abstract needs. Instead, we\npropose a novel two-pronged system where we fuse: (i) SQL-query results over a\ndatabase automatically derived from the KG, and (ii) text-search results over\nverbalizations of KG facts. Our pipeline supports iterative retrieval: when the\nresults of any branch are found to be unsatisfactory, the system can\nautomatically opt for further rounds. We put everything together in a retrieval\naugmented generation (RAG) setup, where an LLM generates a coherent response\nfrom accumulated search results. We demonstrate the superiority of our proposed\nsystem over several baselines on a knowledge graph of BMW automobiles."
                },
                "authors": [
                    {
                        "name": "Rishiraj Saha Roy"
                    },
                    {
                        "name": "Chris Hinze"
                    },
                    {
                        "name": "Joel Schlotthauer"
                    },
                    {
                        "name": "Farzad Naderi"
                    },
                    {
                        "name": "Viktor Hangya"
                    },
                    {
                        "name": "Andreas Foltyn"
                    },
                    {
                        "name": "Luzian Hahn"
                    },
                    {
                        "name": "Fabian Kuech"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Kuech"
                },
                "author": "Fabian Kuech",
                "arxiv_comment": "Accepted at BTW 2025, 10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17690v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17690v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15529v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15529v2",
                "updated": "2024-12-24T10:32:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    10,
                    32,
                    13,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-20T03:37:07Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    3,
                    37,
                    7,
                    4,
                    355,
                    0
                ],
                "title": "XRAG: eXamining the Core -- Benchmarking Foundational Components in\n  Advanced Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XRAG: eXamining the Core -- Benchmarking Foundational Components in\n  Advanced Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) synergizes the retrieval of pertinent\ndata with the generative capabilities of Large Language Models (LLMs), ensuring\nthat the generated output is not only contextually relevant but also accurate\nand current. We introduce XRAG, an open-source, modular codebase that\nfacilitates exhaustive evaluation of the performance of foundational components\nof advanced RAG modules. These components are systematically categorized into\nfour core phases: pre-retrieval, retrieval, post-retrieval, and generation. We\nsystematically analyse them across reconfigured datasets, providing a\ncomprehensive benchmark for their effectiveness. As the complexity of RAG\nsystems continues to escalate, we underscore the critical need to identify\npotential failure points in RAG systems. We formulate a suite of experimental\nmethodologies and diagnostic testing protocols to dissect the failure points\ninherent in RAG engineering. Subsequently, we proffer bespoke solutions aimed\nat bolstering the overall performance of these modules. Our work thoroughly\nevaluates the performance of advanced core components in RAG systems, providing\ninsights into optimizations for prevalent failure points.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) synergizes the retrieval of pertinent\ndata with the generative capabilities of Large Language Models (LLMs), ensuring\nthat the generated output is not only contextually relevant but also accurate\nand current. We introduce XRAG, an open-source, modular codebase that\nfacilitates exhaustive evaluation of the performance of foundational components\nof advanced RAG modules. These components are systematically categorized into\nfour core phases: pre-retrieval, retrieval, post-retrieval, and generation. We\nsystematically analyse them across reconfigured datasets, providing a\ncomprehensive benchmark for their effectiveness. As the complexity of RAG\nsystems continues to escalate, we underscore the critical need to identify\npotential failure points in RAG systems. We formulate a suite of experimental\nmethodologies and diagnostic testing protocols to dissect the failure points\ninherent in RAG engineering. Subsequently, we proffer bespoke solutions aimed\nat bolstering the overall performance of these modules. Our work thoroughly\nevaluates the performance of advanced core components in RAG systems, providing\ninsights into optimizations for prevalent failure points."
                },
                "authors": [
                    {
                        "name": "Qianren Mao"
                    },
                    {
                        "name": "Yangyifei Luo"
                    },
                    {
                        "name": "Jinlong Zhang"
                    },
                    {
                        "name": "Hanwen Hao"
                    },
                    {
                        "name": "Zhilong Cao"
                    },
                    {
                        "name": "Xiaolong Wang"
                    },
                    {
                        "name": "Xiao Guan"
                    },
                    {
                        "name": "Zhenting Huang"
                    },
                    {
                        "name": "Weifeng Jiang"
                    },
                    {
                        "name": "Shuyu Guo"
                    },
                    {
                        "name": "Zhentao Han"
                    },
                    {
                        "name": "Qili Zhang"
                    },
                    {
                        "name": "Siyuan Tao"
                    },
                    {
                        "name": "Yujie Liu"
                    },
                    {
                        "name": "Junnan Liu"
                    },
                    {
                        "name": "Zhixing Tan"
                    },
                    {
                        "name": "Jie Sun"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xudong Liu"
                    },
                    {
                        "name": "Richong Zhang"
                    },
                    {
                        "name": "Jianxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianxin Li"
                },
                "author": "Jianxin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15529v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15529v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17915v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17915v4",
                "updated": "2024-12-24T09:35:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    9,
                    35,
                    5,
                    1,
                    359,
                    0
                ],
                "published": "2024-07-25T10:09:21Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    10,
                    9,
                    21,
                    3,
                    207,
                    0
                ],
                "title": "The Dark Side of Function Calling: Pathways to Jailbreaking Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dark Side of Function Calling: Pathways to Jailbreaking Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities, but\ntheir power comes with significant security considerations. While extensive\nresearch has been conducted on the safety of LLMs in chat mode, the security\nimplications of their function calling feature have been largely overlooked.\nThis paper uncovers a critical vulnerability in the function calling process of\nLLMs, introducing a novel \"jailbreak function\" attack method that exploits\nalignment discrepancies, user coercion, and the absence of rigorous safety\nfilters. Our empirical study, conducted on six state-of-the-art LLMs including\nGPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-pro, reveals an alarming average\nsuccess rate of over 90\\% for this attack. We provide a comprehensive analysis\nof why function calls are susceptible to such attacks and propose defensive\nstrategies, including the use of defensive prompts. Our findings highlight the\nurgent need for enhanced security measures in the function calling capabilities\nof LLMs, contributing to the field of AI safety by identifying a previously\nunexplored risk, designing an effective attack method, and suggesting practical\ndefensive measures. Our code is available at\nhttps://github.com/wooozihui/jailbreakfunction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities, but\ntheir power comes with significant security considerations. While extensive\nresearch has been conducted on the safety of LLMs in chat mode, the security\nimplications of their function calling feature have been largely overlooked.\nThis paper uncovers a critical vulnerability in the function calling process of\nLLMs, introducing a novel \"jailbreak function\" attack method that exploits\nalignment discrepancies, user coercion, and the absence of rigorous safety\nfilters. Our empirical study, conducted on six state-of-the-art LLMs including\nGPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-pro, reveals an alarming average\nsuccess rate of over 90\\% for this attack. We provide a comprehensive analysis\nof why function calls are susceptible to such attacks and propose defensive\nstrategies, including the use of defensive prompts. Our findings highlight the\nurgent need for enhanced security measures in the function calling capabilities\nof LLMs, contributing to the field of AI safety by identifying a previously\nunexplored risk, designing an effective attack method, and suggesting practical\ndefensive measures. Our code is available at\nhttps://github.com/wooozihui/jailbreakfunction."
                },
                "authors": [
                    {
                        "name": "Zihui Wu"
                    },
                    {
                        "name": "Haichang Gao"
                    },
                    {
                        "name": "Jianping He"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17915v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17915v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18299v1",
                "updated": "2024-12-24T09:06:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    9,
                    6,
                    58,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T09:06:58Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    9,
                    6,
                    58,
                    1,
                    359,
                    0
                ],
                "title": "M-Ped: Multi-Prompt Ensemble Decoding for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M-Ped: Multi-Prompt Ensemble Decoding for Large Language Models"
                },
                "summary": "With the widespread application of Large Language Models (LLMs) in the field\nof Natural Language Processing (NLP), enhancing their performance has become a\nresearch hotspot. This paper presents a novel multi-prompt ensemble decoding\napproach designed to bolster the generation quality of LLMs by leveraging the\naggregation of outcomes from multiple prompts. Given a unique input $X$, we\nsubmit $n$ variations of prompts with $X$ to LLMs in batch mode to decode and\nderive probability distributions. For each token prediction, we calculate the\nensemble probability by averaging the $n$ probability distributions within the\nbatch, utilizing this aggregated probability to generate the token. This\ntechnique is dubbed Inner-Batch Ensemble. To facilitate efficient batch\ninference, we implement a Left-Padding strategy to maintain uniform input\nlengths across the n prompts. Through extensive experimentation on diverse NLP\ntasks, including machine translation, code generation, and text simplification,\nwe demonstrate the efficacy of our method in enhancing LLM performance. The\nresults show substantial improvements in BLEU scores, pass@$k$ rates, and LENS\nmetrics over conventional methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread application of Large Language Models (LLMs) in the field\nof Natural Language Processing (NLP), enhancing their performance has become a\nresearch hotspot. This paper presents a novel multi-prompt ensemble decoding\napproach designed to bolster the generation quality of LLMs by leveraging the\naggregation of outcomes from multiple prompts. Given a unique input $X$, we\nsubmit $n$ variations of prompts with $X$ to LLMs in batch mode to decode and\nderive probability distributions. For each token prediction, we calculate the\nensemble probability by averaging the $n$ probability distributions within the\nbatch, utilizing this aggregated probability to generate the token. This\ntechnique is dubbed Inner-Batch Ensemble. To facilitate efficient batch\ninference, we implement a Left-Padding strategy to maintain uniform input\nlengths across the n prompts. Through extensive experimentation on diverse NLP\ntasks, including machine translation, code generation, and text simplification,\nwe demonstrate the efficacy of our method in enhancing LLM performance. The\nresults show substantial improvements in BLEU scores, pass@$k$ rates, and LENS\nmetrics over conventional methods."
                },
                "authors": [
                    {
                        "name": "Jiaxin Guo"
                    },
                    {
                        "name": "Daimeng Wei"
                    },
                    {
                        "name": "Yuanchang Luo"
                    },
                    {
                        "name": "Shimin Tao"
                    },
                    {
                        "name": "Hengchao Shang"
                    },
                    {
                        "name": "Zongyao Li"
                    },
                    {
                        "name": "Shaojun Li"
                    },
                    {
                        "name": "Jinlong Yang"
                    },
                    {
                        "name": "Zhanglin Wu"
                    },
                    {
                        "name": "Zhiqiang Rao"
                    },
                    {
                        "name": "Hao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Yang"
                },
                "author": "Hao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18298v1",
                "updated": "2024-12-24T09:05:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    9,
                    5,
                    37,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T09:05:37Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    9,
                    5,
                    37,
                    1,
                    359,
                    0
                ],
                "title": "Quo Vadis, Anomaly Detection? LLMs and VLMs in the Spotlight",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quo Vadis, Anomaly Detection? LLMs and VLMs in the Spotlight"
                },
                "summary": "Video anomaly detection (VAD) has witnessed significant advancements through\nthe integration of large language models (LLMs) and vision-language models\n(VLMs), addressing critical challenges such as interpretability, temporal\nreasoning, and generalization in dynamic, open-world scenarios. This paper\npresents an in-depth review of cutting-edge LLM-/VLM-based methods in 2024,\nfocusing on four key aspects: (i) enhancing interpretability through semantic\ninsights and textual explanations, making visual anomalies more understandable;\n(ii) capturing intricate temporal relationships to detect and localize dynamic\nanomalies across video frames; (iii) enabling few-shot and zero-shot detection\nto minimize reliance on large, annotated datasets; and (iv) addressing\nopen-world and class-agnostic anomalies by using semantic understanding and\nmotion features for spatiotemporal coherence. We highlight their potential to\nredefine the landscape of VAD. Additionally, we explore the synergy between\nvisual and textual modalities offered by LLMs and VLMs, highlighting their\ncombined strengths and proposing future directions to fully exploit the\npotential in enhancing video anomaly detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video anomaly detection (VAD) has witnessed significant advancements through\nthe integration of large language models (LLMs) and vision-language models\n(VLMs), addressing critical challenges such as interpretability, temporal\nreasoning, and generalization in dynamic, open-world scenarios. This paper\npresents an in-depth review of cutting-edge LLM-/VLM-based methods in 2024,\nfocusing on four key aspects: (i) enhancing interpretability through semantic\ninsights and textual explanations, making visual anomalies more understandable;\n(ii) capturing intricate temporal relationships to detect and localize dynamic\nanomalies across video frames; (iii) enabling few-shot and zero-shot detection\nto minimize reliance on large, annotated datasets; and (iv) addressing\nopen-world and class-agnostic anomalies by using semantic understanding and\nmotion features for spatiotemporal coherence. We highlight their potential to\nredefine the landscape of VAD. Additionally, we explore the synergy between\nvisual and textual modalities offered by LLMs and VLMs, highlighting their\ncombined strengths and proposing future directions to fully exploit the\npotential in enhancing video anomaly detection."
                },
                "authors": [
                    {
                        "name": "Xi Ding"
                    },
                    {
                        "name": "Lei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Wang"
                },
                "author": "Lei Wang",
                "arxiv_comment": "Research report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18295v1",
                "updated": "2024-12-24T09:03:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    9,
                    3,
                    57,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T09:03:57Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    9,
                    3,
                    57,
                    1,
                    359,
                    0
                ],
                "title": "Pirates of the RAG: Adaptively Attacking LLMs to Leak Knowledge Bases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pirates of the RAG: Adaptively Attacking LLMs to Leak Knowledge Bases"
                },
                "summary": "The growing ubiquity of Retrieval-Augmented Generation (RAG) systems in\nseveral real-world services triggers severe concerns about their security. A\nRAG system improves the generative capabilities of a Large Language Models\n(LLM) by a retrieval mechanism which operates on a private knowledge base,\nwhose unintended exposure could lead to severe consequences, including breaches\nof private and sensitive information. This paper presents a black-box attack to\nforce a RAG system to leak its private knowledge base which, differently from\nexisting approaches, is adaptive and automatic. A relevance-based mechanism and\nan attacker-side open-source LLM favor the generation of effective queries to\nleak most of the (hidden) knowledge base. Extensive experimentation proves the\nquality of the proposed algorithm in different RAG pipelines and domains,\ncomparing to very recent related approaches, which turn out to be either not\nfully black-box, not adaptive, or not based on open-source models. The findings\nfrom our study remark the urgent need for more robust privacy safeguards in the\ndesign and deployment of RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing ubiquity of Retrieval-Augmented Generation (RAG) systems in\nseveral real-world services triggers severe concerns about their security. A\nRAG system improves the generative capabilities of a Large Language Models\n(LLM) by a retrieval mechanism which operates on a private knowledge base,\nwhose unintended exposure could lead to severe consequences, including breaches\nof private and sensitive information. This paper presents a black-box attack to\nforce a RAG system to leak its private knowledge base which, differently from\nexisting approaches, is adaptive and automatic. A relevance-based mechanism and\nan attacker-side open-source LLM favor the generation of effective queries to\nleak most of the (hidden) knowledge base. Extensive experimentation proves the\nquality of the proposed algorithm in different RAG pipelines and domains,\ncomparing to very recent related approaches, which turn out to be either not\nfully black-box, not adaptive, or not based on open-source models. The findings\nfrom our study remark the urgent need for more robust privacy safeguards in the\ndesign and deployment of RAG systems."
                },
                "authors": [
                    {
                        "name": "Christian Di Maio"
                    },
                    {
                        "name": "Cristian Cosci"
                    },
                    {
                        "name": "Marco Maggini"
                    },
                    {
                        "name": "Valentina Poggioni"
                    },
                    {
                        "name": "Stefano Melacci"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Melacci"
                },
                "author": "Stefano Melacci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11465v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11465v3",
                "updated": "2024-12-24T09:03:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    9,
                    3,
                    2,
                    1,
                    359,
                    0
                ],
                "published": "2024-11-18T10:58:46Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    10,
                    58,
                    46,
                    0,
                    323,
                    0
                ],
                "title": "Re-examining learning linear functions in context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-examining learning linear functions in context"
                },
                "summary": "In-context learning (ICL) has emerged as a powerful paradigm for easily\nadapting Large Language Models (LLMs) to various tasks. However, our\nunderstanding of how ICL works remains limited. We explore a simple model of\nICL in a controlled setup with synthetic training data to investigate ICL of\nunivariate linear functions. We experiment with a range of GPT-2-like\ntransformer models trained from scratch. Our findings challenge the prevailing\nnarrative that transformers adopt algorithmic approaches like linear regression\nto learn a linear function in-context. These models fail to generalize beyond\ntheir training distribution, highlighting fundamental limitations in their\ncapacity to infer abstract task structures. Our experiments lead us to propose\na mathematically precise hypothesis of what the model might be learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) has emerged as a powerful paradigm for easily\nadapting Large Language Models (LLMs) to various tasks. However, our\nunderstanding of how ICL works remains limited. We explore a simple model of\nICL in a controlled setup with synthetic training data to investigate ICL of\nunivariate linear functions. We experiment with a range of GPT-2-like\ntransformer models trained from scratch. Our findings challenge the prevailing\nnarrative that transformers adopt algorithmic approaches like linear regression\nto learn a linear function in-context. These models fail to generalize beyond\ntheir training distribution, highlighting fundamental limitations in their\ncapacity to infer abstract task structures. Our experiments lead us to propose\na mathematically precise hypothesis of what the model might be learning."
                },
                "authors": [
                    {
                        "name": "Omar Naim"
                    },
                    {
                        "name": "Guilhem FouilhÃ©"
                    },
                    {
                        "name": "Nicholas Asher"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Asher"
                },
                "author": "Nicholas Asher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11465v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11465v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18291v1",
                "updated": "2024-12-24T08:53:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    8,
                    53,
                    54,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T08:53:54Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    8,
                    53,
                    54,
                    1,
                    359,
                    0
                ],
                "title": "DeepCRCEval: Revisiting the Evaluation of Code Review Comment Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepCRCEval: Revisiting the Evaluation of Code Review Comment Generation"
                },
                "summary": "Code review is a vital but demanding aspect of software development,\ngenerating significant interest in automating review comments. Traditional\nevaluation methods for these comments, primarily based on text similarity, face\ntwo major challenges: inconsistent reliability of human-authored comments in\nopen-source projects and the weak correlation of text similarity with\nobjectives like enhancing code quality and detecting defects.\n  This study empirically analyzes benchmark comments using a novel set of\ncriteria informed by prior research and developer interviews. We then similarly\nrevisit the evaluation of existing methodologies. Our evaluation framework,\nDeepCRCEval, integrates human evaluators and Large Language Models (LLMs) for a\ncomprehensive reassessment of current techniques based on the criteria set.\nBesides, we also introduce an innovative and efficient baseline, LLM-Reviewer,\nleveraging the few-shot learning capabilities of LLMs for a target-oriented\ncomparison.\n  Our research highlights the limitations of text similarity metrics, finding\nthat less than 10% of benchmark comments are high quality for automation. In\ncontrast, DeepCRCEval effectively distinguishes between high and low-quality\ncomments, proving to be a more reliable evaluation mechanism. Incorporating LLM\nevaluators into DeepCRCEval significantly boosts efficiency, reducing time and\ncost by 88.78% and 90.32%, respectively. Furthermore, LLM-Reviewer demonstrates\nsignificant potential of focusing task real targets in comment generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code review is a vital but demanding aspect of software development,\ngenerating significant interest in automating review comments. Traditional\nevaluation methods for these comments, primarily based on text similarity, face\ntwo major challenges: inconsistent reliability of human-authored comments in\nopen-source projects and the weak correlation of text similarity with\nobjectives like enhancing code quality and detecting defects.\n  This study empirically analyzes benchmark comments using a novel set of\ncriteria informed by prior research and developer interviews. We then similarly\nrevisit the evaluation of existing methodologies. Our evaluation framework,\nDeepCRCEval, integrates human evaluators and Large Language Models (LLMs) for a\ncomprehensive reassessment of current techniques based on the criteria set.\nBesides, we also introduce an innovative and efficient baseline, LLM-Reviewer,\nleveraging the few-shot learning capabilities of LLMs for a target-oriented\ncomparison.\n  Our research highlights the limitations of text similarity metrics, finding\nthat less than 10% of benchmark comments are high quality for automation. In\ncontrast, DeepCRCEval effectively distinguishes between high and low-quality\ncomments, proving to be a more reliable evaluation mechanism. Incorporating LLM\nevaluators into DeepCRCEval significantly boosts efficiency, reducing time and\ncost by 88.78% and 90.32%, respectively. Furthermore, LLM-Reviewer demonstrates\nsignificant potential of focusing task real targets in comment generation."
                },
                "authors": [
                    {
                        "name": "Junyi Lu"
                    },
                    {
                        "name": "Xiaojia Li"
                    },
                    {
                        "name": "Zihan Hua"
                    },
                    {
                        "name": "Lei Yu"
                    },
                    {
                        "name": "Shiqi Cheng"
                    },
                    {
                        "name": "Li Yang"
                    },
                    {
                        "name": "Fengjun Zhang"
                    },
                    {
                        "name": "Chun Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Chun Zuo"
                },
                "author": "Chun Zuo",
                "arxiv_comment": "Accepted to the 28th International Conference on Fundamental\n  Approaches to Software Engineering (FASE 2025), part of the 28th European\n  Joint Conferences on Theory and Practice of Software (ETAPS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18279v1",
                "updated": "2024-12-24T08:39:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    8,
                    39,
                    35,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T08:39:35Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    8,
                    39,
                    35,
                    1,
                    359,
                    0
                ],
                "title": "Improving Multi-Step Reasoning Abilities of Large Language Models with\n  Direct Advantage Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Multi-Step Reasoning Abilities of Large Language Models with\n  Direct Advantage Policy Optimization"
                },
                "summary": "The role of reinforcement learning (RL) in enhancing the reasoning of large\nlanguage models (LLMs) is becoming increasingly significant. Despite the\nsuccess of RL in many scenarios, there are still many challenges in improving\nthe reasoning of LLMs. One challenge is the sparse reward, which makes\noptimization difficult for RL and necessitates a large amount of data samples.\nAnother challenge stems from the inherent instability of RL, particularly when\nusing Actor-Critic (AC) methods to derive optimal policies, which often leads\nto unstable training processes. To address these issues, we introduce Direct\nAdvantage Policy Optimization (DAPO), an novel step-level offline RL algorithm.\nUnlike standard alignment that rely solely outcome rewards to optimize policies\n(such as DPO), DAPO employs a critic function to predict the reasoning accuracy\nat each step, thereby generating dense signals to refine the generation\nstrategy. Additionally, the Actor and Critic components in DAPO are trained\nindependently, avoiding the co-training instability observed in standard AC\nalgorithms like PPO. We train DAPO on mathematical and code query datasets and\nthen evaluate its performance on multiple benchmarks. Our results show that\nDAPO can effectively enhance the mathematical and code capabilities on both SFT\nmodels and RL models, demonstrating the effectiveness of DAPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The role of reinforcement learning (RL) in enhancing the reasoning of large\nlanguage models (LLMs) is becoming increasingly significant. Despite the\nsuccess of RL in many scenarios, there are still many challenges in improving\nthe reasoning of LLMs. One challenge is the sparse reward, which makes\noptimization difficult for RL and necessitates a large amount of data samples.\nAnother challenge stems from the inherent instability of RL, particularly when\nusing Actor-Critic (AC) methods to derive optimal policies, which often leads\nto unstable training processes. To address these issues, we introduce Direct\nAdvantage Policy Optimization (DAPO), an novel step-level offline RL algorithm.\nUnlike standard alignment that rely solely outcome rewards to optimize policies\n(such as DPO), DAPO employs a critic function to predict the reasoning accuracy\nat each step, thereby generating dense signals to refine the generation\nstrategy. Additionally, the Actor and Critic components in DAPO are trained\nindependently, avoiding the co-training instability observed in standard AC\nalgorithms like PPO. We train DAPO on mathematical and code query datasets and\nthen evaluate its performance on multiple benchmarks. Our results show that\nDAPO can effectively enhance the mathematical and code capabilities on both SFT\nmodels and RL models, demonstrating the effectiveness of DAPO."
                },
                "authors": [
                    {
                        "name": "Jiacai Liu"
                    },
                    {
                        "name": "Chaojie Wang"
                    },
                    {
                        "name": "Chris Yuhao Liu"
                    },
                    {
                        "name": "Liang Zeng"
                    },
                    {
                        "name": "Rui Yan"
                    },
                    {
                        "name": "Yiwen Sun"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yahui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yahui Zhou"
                },
                "author": "Yahui Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18274v1",
                "updated": "2024-12-24T08:33:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    8,
                    33,
                    44,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T08:33:44Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    8,
                    33,
                    44,
                    1,
                    359,
                    0
                ],
                "title": "GenAI Content Detection Task 2: AI vs. Human -- Academic Essay\n  Authenticity Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenAI Content Detection Task 2: AI vs. Human -- Academic Essay\n  Authenticity Challenge"
                },
                "summary": "This paper presents a comprehensive overview of the first edition of the\nAcademic Essay Authenticity Challenge, organized as part of the GenAI Content\nDetection shared tasks collocated with COLING 2025. This challenge focuses on\ndetecting machine-generated vs. human-authored essays for academic purposes.\nThe task is defined as follows: \"Given an essay, identify whether it is\ngenerated by a machine or authored by a human.'' The challenge involves two\nlanguages: English and Arabic. During the evaluation phase, 25 teams submitted\nsystems for English and 21 teams for Arabic, reflecting substantial interest in\nthe task. Finally, seven teams submitted system description papers. The\nmajority of submissions utilized fine-tuned transformer-based models, with one\nteam employing Large Language Models (LLMs) such as Llama 2 and Llama 3. This\npaper outlines the task formulation, details the dataset construction process,\nand explains the evaluation framework. Additionally, we present a summary of\nthe approaches adopted by participating teams. Nearly all submitted systems\noutperformed the n-gram-based baseline, with the top-performing systems\nachieving F1 scores exceeding 0.98 for both languages, indicating significant\nprogress in the detection of machine-generated text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive overview of the first edition of the\nAcademic Essay Authenticity Challenge, organized as part of the GenAI Content\nDetection shared tasks collocated with COLING 2025. This challenge focuses on\ndetecting machine-generated vs. human-authored essays for academic purposes.\nThe task is defined as follows: \"Given an essay, identify whether it is\ngenerated by a machine or authored by a human.'' The challenge involves two\nlanguages: English and Arabic. During the evaluation phase, 25 teams submitted\nsystems for English and 21 teams for Arabic, reflecting substantial interest in\nthe task. Finally, seven teams submitted system description papers. The\nmajority of submissions utilized fine-tuned transformer-based models, with one\nteam employing Large Language Models (LLMs) such as Llama 2 and Llama 3. This\npaper outlines the task formulation, details the dataset construction process,\nand explains the evaluation framework. Additionally, we present a summary of\nthe approaches adopted by participating teams. Nearly all submitted systems\noutperformed the n-gram-based baseline, with the top-performing systems\nachieving F1 scores exceeding 0.98 for both languages, indicating significant\nprogress in the detection of machine-generated text."
                },
                "authors": [
                    {
                        "name": "Shammur Absar Chowdhury"
                    },
                    {
                        "name": "Hind Almerekhi"
                    },
                    {
                        "name": "Mucahid Kutlu"
                    },
                    {
                        "name": "Kaan Efe Keles"
                    },
                    {
                        "name": "Fatema Ahmad"
                    },
                    {
                        "name": "Tasnim Mohiuddin"
                    },
                    {
                        "name": "George Mikros"
                    },
                    {
                        "name": "Firoj Alam"
                    }
                ],
                "author_detail": {
                    "name": "Firoj Alam"
                },
                "author": "Firoj Alam",
                "arxiv_comment": "AI Generated Content, Academic Essay, LLMs, Arabic, English",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18270v1",
                "updated": "2024-12-24T08:29:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    8,
                    29,
                    0,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T08:29:00Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    8,
                    29,
                    0,
                    1,
                    359,
                    0
                ],
                "title": "Annotating References to Mythological Entities in French Literature",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Annotating References to Mythological Entities in French Literature"
                },
                "summary": "In this paper, we explore the relevance of large language models (LLMs) for\nannotating references to Roman and Greek mythological entities in modern and\ncontemporary French literature. We present an annotation scheme and demonstrate\nthat recent LLMs can be directly applied to follow this scheme effectively,\nalthough not without occasionally making significant analytical errors.\nAdditionally, we show that LLMs (and, more specifically, ChatGPT) are capable\nof offering interpretative insights into the use of mythological references by\nliterary authors. However, we also find that LLMs struggle to accurately\nidentify relevant passages in novels (when used as an information retrieval\nengine), often hallucinating and generating fabricated examples-an issue that\nraises significant ethical concerns. Nonetheless, when used carefully, LLMs\nremain valuable tools for performing annotations with high accuracy, especially\nfor tasks that would be difficult to annotate comprehensively on a large scale\nthrough manual methods alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we explore the relevance of large language models (LLMs) for\nannotating references to Roman and Greek mythological entities in modern and\ncontemporary French literature. We present an annotation scheme and demonstrate\nthat recent LLMs can be directly applied to follow this scheme effectively,\nalthough not without occasionally making significant analytical errors.\nAdditionally, we show that LLMs (and, more specifically, ChatGPT) are capable\nof offering interpretative insights into the use of mythological references by\nliterary authors. However, we also find that LLMs struggle to accurately\nidentify relevant passages in novels (when used as an information retrieval\nengine), often hallucinating and generating fabricated examples-an issue that\nraises significant ethical concerns. Nonetheless, when used carefully, LLMs\nremain valuable tools for performing annotations with high accuracy, especially\nfor tasks that would be difficult to annotate comprehensively on a large scale\nthrough manual methods alone."
                },
                "authors": [
                    {
                        "name": "Thierry Poibeau"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Poibeau"
                },
                "arxiv_affiliation": "Lattice",
                "author": "Thierry Poibeau",
                "arxiv_journal_ref": "CHR (Computational Humanities Research) -- Digital Methods for\n  Mythological Research Workshop, Dec 2024, Aarhus (Danemark), Denmark",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08719v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08719v2",
                "updated": "2024-12-24T08:24:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    8,
                    24,
                    5,
                    1,
                    359,
                    0
                ],
                "published": "2024-01-16T06:54:44Z",
                "published_parsed": [
                    2024,
                    1,
                    16,
                    6,
                    54,
                    44,
                    1,
                    16,
                    0
                ],
                "title": "CodeComplex: Dataset for Worst-Case Time Complexity Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeComplex: Dataset for Worst-Case Time Complexity Prediction"
                },
                "summary": "Reasoning ability of Large Language Models (LLMs) is a crucial ability,\nespecially in complex decision-making tasks. One significant task to show LLMs'\nreasoning capability is code time complexity prediction, which involves various\nintricate factors such as the input range of variables and conditional loops.\nCurrent benchmarks fall short of providing a rigorous assessment due to limited\ndata, language constraints, and insufficient labeling. They do not consider\ntime complexity based on input representation and merely evaluate whether\npredictions fall into the same class, lacking a measure of how close incorrect\npredictions are to the correct ones. To address these dependencies, we\nintroduce CodeComplex, the first robust and extensive dataset designed to\nevaluate LLMs' reasoning abilities in predicting code time complexity.\nCodeComplex comprises 4,900 Java codes and an equivalent number of Python\ncodes, overcoming language and labeling constraints, carefully annotated with\ncomplexity labels based on input characteristics by a panel of algorithmic\nexperts. Additionally, we propose specialized evaluation metrics for the\nreasoning of complexity prediction tasks, offering a more precise and reliable\nassessment of LLMs' reasoning capabilities. We release our dataset\n(https://github.com/sybaik1/CodeComplex-Data) and baseline models\n(https://github.com/sybaik1/CodeComplex-Models) publicly to encourage the\nrelevant (NLP, SE, and PL) communities to utilize and participate in this\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning ability of Large Language Models (LLMs) is a crucial ability,\nespecially in complex decision-making tasks. One significant task to show LLMs'\nreasoning capability is code time complexity prediction, which involves various\nintricate factors such as the input range of variables and conditional loops.\nCurrent benchmarks fall short of providing a rigorous assessment due to limited\ndata, language constraints, and insufficient labeling. They do not consider\ntime complexity based on input representation and merely evaluate whether\npredictions fall into the same class, lacking a measure of how close incorrect\npredictions are to the correct ones. To address these dependencies, we\nintroduce CodeComplex, the first robust and extensive dataset designed to\nevaluate LLMs' reasoning abilities in predicting code time complexity.\nCodeComplex comprises 4,900 Java codes and an equivalent number of Python\ncodes, overcoming language and labeling constraints, carefully annotated with\ncomplexity labels based on input characteristics by a panel of algorithmic\nexperts. Additionally, we propose specialized evaluation metrics for the\nreasoning of complexity prediction tasks, offering a more precise and reliable\nassessment of LLMs' reasoning capabilities. We release our dataset\n(https://github.com/sybaik1/CodeComplex-Data) and baseline models\n(https://github.com/sybaik1/CodeComplex-Models) publicly to encourage the\nrelevant (NLP, SE, and PL) communities to utilize and participate in this\nresearch."
                },
                "authors": [
                    {
                        "name": "Seung-Yeop Baik"
                    },
                    {
                        "name": "Joonghyuk Hahn"
                    },
                    {
                        "name": "Jungin Kim"
                    },
                    {
                        "name": "Mingi Jeon"
                    },
                    {
                        "name": "Aditi"
                    },
                    {
                        "name": "Yo-Sub Han"
                    },
                    {
                        "name": "Sang-Ki Ko"
                    }
                ],
                "author_detail": {
                    "name": "Sang-Ki Ko"
                },
                "author": "Sang-Ki Ko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08719v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08719v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18260v1",
                "updated": "2024-12-24T08:20:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    8,
                    20,
                    29,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T08:20:29Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    8,
                    20,
                    29,
                    1,
                    359,
                    0
                ],
                "title": "Investigating Large Language Models for Code Vulnerability Detection: An\n  Experimental Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Large Language Models for Code Vulnerability Detection: An\n  Experimental Study"
                },
                "summary": "Code vulnerability detection (CVD) is essential for addressing and preventing\nsystem security issues, playing a crucial role in ensuring software security.\nPrevious learning-based vulnerability detection methods rely on either\nfine-tuning medium-size sequence models or training smaller neural networks\nfrom scratch. Recent advancements in large pre-trained language models (LLMs)\nhave showcased remarkable capabilities in various code intelligence tasks\nincluding code understanding and generation. However, the effectiveness of LLMs\nin detecting code vulnerabilities is largely under-explored. This work aims to\ninvestigate the gap by fine-tuning LLMs for the CVD task, involving four\nwidely-used open-source LLMs. We also implement other five previous graph-based\nor medium-size sequence models for comparison. Experiments are conducted on\nfive commonly-used CVD datasets, including both the part of short samples and\nlong samples. In addition, we conduct quantitative experiments to investigate\nthe class imbalance issue and the model's performance on samples of different\nlengths, which are rarely studied in previous works. To better facilitate\ncommunities, we open-source all codes and resources of this study in\nhttps://github.com/SakiRinn/LLM4CVD and\nhttps://huggingface.co/datasets/xuefen/VulResource.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code vulnerability detection (CVD) is essential for addressing and preventing\nsystem security issues, playing a crucial role in ensuring software security.\nPrevious learning-based vulnerability detection methods rely on either\nfine-tuning medium-size sequence models or training smaller neural networks\nfrom scratch. Recent advancements in large pre-trained language models (LLMs)\nhave showcased remarkable capabilities in various code intelligence tasks\nincluding code understanding and generation. However, the effectiveness of LLMs\nin detecting code vulnerabilities is largely under-explored. This work aims to\ninvestigate the gap by fine-tuning LLMs for the CVD task, involving four\nwidely-used open-source LLMs. We also implement other five previous graph-based\nor medium-size sequence models for comparison. Experiments are conducted on\nfive commonly-used CVD datasets, including both the part of short samples and\nlong samples. In addition, we conduct quantitative experiments to investigate\nthe class imbalance issue and the model's performance on samples of different\nlengths, which are rarely studied in previous works. To better facilitate\ncommunities, we open-source all codes and resources of this study in\nhttps://github.com/SakiRinn/LLM4CVD and\nhttps://huggingface.co/datasets/xuefen/VulResource."
                },
                "authors": [
                    {
                        "name": "Xuefeng Jiang"
                    },
                    {
                        "name": "Lvhua Wu"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Jingjing Xue"
                    },
                    {
                        "name": "Yuwei Wang"
                    },
                    {
                        "name": "Tingting Wu"
                    },
                    {
                        "name": "Min Liu"
                    }
                ],
                "author_detail": {
                    "name": "Min Liu"
                },
                "author": "Min Liu",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16597v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16597v2",
                "updated": "2024-12-24T08:14:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    8,
                    14,
                    36,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-21T12:03:31Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    12,
                    3,
                    31,
                    5,
                    356,
                    0
                ],
                "title": "LLMs Enable Context-Aware Augmented Reality in Surgical Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Enable Context-Aware Augmented Reality in Surgical Navigation"
                },
                "summary": "Wearable Augmented Reality (AR) technologies are gaining recognition for\ntheir potential to transform surgical navigation systems. As these technologies\nevolve, selecting the right interaction method to control the system becomes\ncrucial. Our work introduces a voice-controlled user interface (VCUI) for\nsurgical AR assistance systems (ARAS), designed for pancreatic surgery, that\nintegrates Large Language Models (LLMs). Employing a mixed-method research\napproach, we assessed the usability of our LLM-based design in both simulated\nsurgical tasks and during pancreatic surgeries, comparing its performance\nagainst conventional VCUI for surgical ARAS using speech commands. Our findings\ndemonstrated the usability of our proposed LLM-based VCUI, yielding a\nsignificantly lower task completion time and cognitive workload compared to\nspeech commands. Additionally, qualitative insights from interviews with\nsurgeons aligned with the quantitative data, revealing a strong preference for\nthe LLM-based VCUI. Surgeons emphasized its intuitiveness and highlighted the\npotential of LLM-based VCUI in expediting decision-making in surgical\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wearable Augmented Reality (AR) technologies are gaining recognition for\ntheir potential to transform surgical navigation systems. As these technologies\nevolve, selecting the right interaction method to control the system becomes\ncrucial. Our work introduces a voice-controlled user interface (VCUI) for\nsurgical AR assistance systems (ARAS), designed for pancreatic surgery, that\nintegrates Large Language Models (LLMs). Employing a mixed-method research\napproach, we assessed the usability of our LLM-based design in both simulated\nsurgical tasks and during pancreatic surgeries, comparing its performance\nagainst conventional VCUI for surgical ARAS using speech commands. Our findings\ndemonstrated the usability of our proposed LLM-based VCUI, yielding a\nsignificantly lower task completion time and cognitive workload compared to\nspeech commands. Additionally, qualitative insights from interviews with\nsurgeons aligned with the quantitative data, revealing a strong preference for\nthe LLM-based VCUI. Surgeons emphasized its intuitiveness and highlighted the\npotential of LLM-based VCUI in expediting decision-making in surgical\nenvironments."
                },
                "authors": [
                    {
                        "name": "Hamraz Javaheri"
                    },
                    {
                        "name": "Omid Ghamarnejad"
                    },
                    {
                        "name": "Paul Lukowicz"
                    },
                    {
                        "name": "Gregor Alexander Stavrou"
                    },
                    {
                        "name": "Jakob Karolus"
                    }
                ],
                "author_detail": {
                    "name": "Jakob Karolus"
                },
                "author": "Jakob Karolus",
                "arxiv_comment": "32 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16597v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16597v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18241v1",
                "updated": "2024-12-24T07:51:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    7,
                    51,
                    29,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T07:51:29Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    7,
                    51,
                    29,
                    1,
                    359,
                    0
                ],
                "title": "An Automatic Graph Construction Framework based on Large Language Models\n  for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Automatic Graph Construction Framework based on Large Language Models\n  for Recommendation"
                },
                "summary": "Graph neural networks (GNNs) have emerged as state-of-the-art methods to\nlearn from graph-structured data for recommendation. However, most existing\nGNN-based recommendation methods focus on the optimization of model structures\nand learning strategies based on pre-defined graphs, neglecting the importance\nof the graph construction stage. Earlier works for graph construction usually\nrely on speciffic rules or crowdsourcing, which are either too simplistic or\ntoo labor-intensive. Recent works start to utilize large language models (LLMs)\nto automate the graph construction, in view of their abundant open-world\nknowledge and remarkable reasoning capabilities. Nevertheless, they generally\nsuffer from two limitations: (1) invisibility of global view (e.g., overlooking\ncontextual information) and (2) construction inefficiency. To this end, we\nintroduce AutoGraph, an automatic graph construction framework based on LLMs\nfor recommendation. Specifically, we first use LLMs to infer the user\npreference and item knowledge, which is encoded as semantic vectors. Next, we\nemploy vector quantization to extract the latent factors from the semantic\nvectors. The latent factors are then incorporated as extra nodes to link the\nuser/item nodes, resulting in a graph with in-depth global-view semantics. We\nfurther design metapath-based message aggregation to effectively aggregate the\nsemantic and collaborative information. The framework is model-agnostic and\ncompatible with different backbone models. Extensive experiments on three\nreal-world datasets demonstrate the efficacy and efffciency of AutoGraph\ncompared to existing baseline methods. We have deployed AutoGraph in Huawei\nadvertising platform, and gain a 2.69% improvement on RPM and a 7.31%\nimprovement on eCPM in the online A/B test. Currently AutoGraph has been used\nas the main trafffc model, serving hundreds of millions of people.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) have emerged as state-of-the-art methods to\nlearn from graph-structured data for recommendation. However, most existing\nGNN-based recommendation methods focus on the optimization of model structures\nand learning strategies based on pre-defined graphs, neglecting the importance\nof the graph construction stage. Earlier works for graph construction usually\nrely on speciffic rules or crowdsourcing, which are either too simplistic or\ntoo labor-intensive. Recent works start to utilize large language models (LLMs)\nto automate the graph construction, in view of their abundant open-world\nknowledge and remarkable reasoning capabilities. Nevertheless, they generally\nsuffer from two limitations: (1) invisibility of global view (e.g., overlooking\ncontextual information) and (2) construction inefficiency. To this end, we\nintroduce AutoGraph, an automatic graph construction framework based on LLMs\nfor recommendation. Specifically, we first use LLMs to infer the user\npreference and item knowledge, which is encoded as semantic vectors. Next, we\nemploy vector quantization to extract the latent factors from the semantic\nvectors. The latent factors are then incorporated as extra nodes to link the\nuser/item nodes, resulting in a graph with in-depth global-view semantics. We\nfurther design metapath-based message aggregation to effectively aggregate the\nsemantic and collaborative information. The framework is model-agnostic and\ncompatible with different backbone models. Extensive experiments on three\nreal-world datasets demonstrate the efficacy and efffciency of AutoGraph\ncompared to existing baseline methods. We have deployed AutoGraph in Huawei\nadvertising platform, and gain a 2.69% improvement on RPM and a 7.31%\nimprovement on eCPM in the online A/B test. Currently AutoGraph has been used\nas the main trafffc model, serving hundreds of millions of people."
                },
                "authors": [
                    {
                        "name": "Rong Shan"
                    },
                    {
                        "name": "Jianghao Lin"
                    },
                    {
                        "name": "Chenxu Zhu"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Menghui Zhu"
                    },
                    {
                        "name": "Kangning Zhang"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Yong Yu"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02688v2",
                "updated": "2024-12-24T07:47:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    7,
                    47,
                    2,
                    1,
                    359,
                    0
                ],
                "published": "2024-11-05T00:16:01Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    0,
                    16,
                    1,
                    1,
                    310,
                    0
                ],
                "title": "On the loss of context-awareness in general instruction fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the loss of context-awareness in general instruction fine-tuning"
                },
                "summary": "Pre-trained Large Language Models (LLMs) require post-training methods such\nas supervised fine-tuning (SFT) on instruction-response pairs to enable\ninstruction following. However, this process can potentially harm existing\ncapabilities learned during pre-training. In this paper, we investigate the\nloss of context awareness after SFT, where context awareness is defined as the\nability to extract and understand information from user-provided context and\nrespond accordingly. We are the first to identify and show that the loss of\ncontext awareness, as reflected by the performance drop in the\nNeedle-in-a-Haystack test, occurs in instruction fine-tuned LLMs when the chat\ntemplate is applied to input prompts. We identify that the performance decline\nis partially caused by an attention bias toward different roles learned during\nconversational instruction fine-tuning. We validate our hypothesis by\nvisualizing changes in attention allocation after the chat template is applied\nand manually steering the attention heads. Based on these observations, we\npropose a metric to select context-dependent examples from general instruction\nfine-tuning datasets. We then apply conditional instruction fine-tuning with a\ncontext-dependency indicator, enabling the model to learn context awareness\nfrom these selected examples. Empirical experiments on four context-dependent\ndownstream tasks and three pre-trained LLMs of different sizes show that our\nmethod effectively mitigates the loss of context awareness without compromising\ngeneral instruction-following capabilities. Given our findings, we strongly\nadvocate for careful benchmarking of context awareness after instruction\nfine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained Large Language Models (LLMs) require post-training methods such\nas supervised fine-tuning (SFT) on instruction-response pairs to enable\ninstruction following. However, this process can potentially harm existing\ncapabilities learned during pre-training. In this paper, we investigate the\nloss of context awareness after SFT, where context awareness is defined as the\nability to extract and understand information from user-provided context and\nrespond accordingly. We are the first to identify and show that the loss of\ncontext awareness, as reflected by the performance drop in the\nNeedle-in-a-Haystack test, occurs in instruction fine-tuned LLMs when the chat\ntemplate is applied to input prompts. We identify that the performance decline\nis partially caused by an attention bias toward different roles learned during\nconversational instruction fine-tuning. We validate our hypothesis by\nvisualizing changes in attention allocation after the chat template is applied\nand manually steering the attention heads. Based on these observations, we\npropose a metric to select context-dependent examples from general instruction\nfine-tuning datasets. We then apply conditional instruction fine-tuning with a\ncontext-dependency indicator, enabling the model to learn context awareness\nfrom these selected examples. Empirical experiments on four context-dependent\ndownstream tasks and three pre-trained LLMs of different sizes show that our\nmethod effectively mitigates the loss of context awareness without compromising\ngeneral instruction-following capabilities. Given our findings, we strongly\nadvocate for careful benchmarking of context awareness after instruction\nfine-tuning."
                },
                "authors": [
                    {
                        "name": "Yihan Wang"
                    },
                    {
                        "name": "Andrew Bai"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Cho-Jui Hsieh"
                    }
                ],
                "author_detail": {
                    "name": "Cho-Jui Hsieh"
                },
                "author": "Cho-Jui Hsieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18230v1",
                "updated": "2024-12-24T07:28:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    7,
                    28,
                    10,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T07:28:10Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    7,
                    28,
                    10,
                    1,
                    359,
                    0
                ],
                "title": "Efficient Detection Framework Adaptation for Edge Computing: A\n  Plug-and-play Neural Network Toolbox Enabling Edge Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Detection Framework Adaptation for Edge Computing: A\n  Plug-and-play Neural Network Toolbox Enabling Edge Deployment"
                },
                "summary": "Edge computing has emerged as a key paradigm for deploying deep\nlearning-based object detection in time-sensitive scenarios. However, existing\nedge detection methods face challenges: 1) difficulty balancing detection\nprecision with lightweight models, 2) limited adaptability of generalized\ndeployment designs, and 3) insufficient real-world validation. To address these\nissues, we propose the Edge Detection Toolbox (ED-TOOLBOX), which utilizes\ngeneralizable plug-and-play components to adapt object detection models for\nedge environments. Specifically, we introduce a lightweight Reparameterized\nDynamic Convolutional Network (Rep-DConvNet) featuring weighted multi-shape\nconvolutional branches to enhance detection performance. Additionally, we\ndesign a Sparse Cross-Attention (SC-A) network with a\nlocalized-mapping-assisted self-attention mechanism, enabling a well-crafted\njoint module for adaptive feature transfer. For real-world applications, we\nincorporate an Efficient Head into the YOLO framework to accelerate edge model\noptimization. To demonstrate practical impact, we identify a gap in helmet\ndetection -- overlooking band fastening, a critical safety factor -- and create\nthe Helmet Band Detection Dataset (HBDD). Using ED-TOOLBOX-optimized models, we\naddress this real-world task. Extensive experiments validate the effectiveness\nof ED-TOOLBOX, with edge detection models outperforming six state-of-the-art\nmethods in visual surveillance simulations, achieving real-time and accurate\nperformance. These results highlight ED-TOOLBOX as a superior solution for edge\nobject detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge computing has emerged as a key paradigm for deploying deep\nlearning-based object detection in time-sensitive scenarios. However, existing\nedge detection methods face challenges: 1) difficulty balancing detection\nprecision with lightweight models, 2) limited adaptability of generalized\ndeployment designs, and 3) insufficient real-world validation. To address these\nissues, we propose the Edge Detection Toolbox (ED-TOOLBOX), which utilizes\ngeneralizable plug-and-play components to adapt object detection models for\nedge environments. Specifically, we introduce a lightweight Reparameterized\nDynamic Convolutional Network (Rep-DConvNet) featuring weighted multi-shape\nconvolutional branches to enhance detection performance. Additionally, we\ndesign a Sparse Cross-Attention (SC-A) network with a\nlocalized-mapping-assisted self-attention mechanism, enabling a well-crafted\njoint module for adaptive feature transfer. For real-world applications, we\nincorporate an Efficient Head into the YOLO framework to accelerate edge model\noptimization. To demonstrate practical impact, we identify a gap in helmet\ndetection -- overlooking band fastening, a critical safety factor -- and create\nthe Helmet Band Detection Dataset (HBDD). Using ED-TOOLBOX-optimized models, we\naddress this real-world task. Extensive experiments validate the effectiveness\nof ED-TOOLBOX, with edge detection models outperforming six state-of-the-art\nmethods in visual surveillance simulations, achieving real-time and accurate\nperformance. These results highlight ED-TOOLBOX as a superior solution for edge\nobject detection."
                },
                "authors": [
                    {
                        "name": "Jiaqi Wu"
                    },
                    {
                        "name": "Shihao Zhang"
                    },
                    {
                        "name": "Simin Chen"
                    },
                    {
                        "name": "Lixu Wang"
                    },
                    {
                        "name": "Zehua Wang"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Fangyuan He"
                    },
                    {
                        "name": "Zijian Tian"
                    },
                    {
                        "name": "F. Richard Yu"
                    },
                    {
                        "name": "Victor C. M. Leung"
                    }
                ],
                "author_detail": {
                    "name": "Victor C. M. Leung"
                },
                "author": "Victor C. M. Leung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18225v1",
                "updated": "2024-12-24T07:15:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    7,
                    15,
                    48,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T07:15:48Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    7,
                    15,
                    48,
                    1,
                    359,
                    0
                ],
                "title": "Combining GPT and Code-Based Similarity Checking for Effective Smart\n  Contract Vulnerability Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining GPT and Code-Based Similarity Checking for Effective Smart\n  Contract Vulnerability Detection"
                },
                "summary": "With the rapid growth of blockchain technology, smart contracts are now\ncrucial to Decentralized Finance (DeFi) applications. Effective vulnerability\ndetection is vital for securing these contracts against hackers and enhancing\nthe accuracy and efficiency of security audits. In this paper, we present\nSimilarGPT, a unique vulnerability identification tool for smart contract,\nwhich combines Generative Pretrained Transformer (GPT) models with Code-based\nsimilarity checking methods. The main concept of the SimilarGPT tool is to\nmeasure the similarity between the code under inspection and the secure code\nfrom third-party libraries. To identify potential vulnerabilities, we connect\nthe semantic understanding capability of large language models (LLMs) with\nCode-based similarity checking techniques. We propose optimizing the detection\nsequence using topological ordering to enhance logical coherence and reduce\nfalse positives during detection. Through analysis of code reuse patterns in\nsmart contracts, we compile and process extensive third-party library code to\nestablish a comprehensive reference codebase. Then, we utilize LLM to conduct\nan indepth analysis of similar codes to identify and explain potential\nvulnerabilities in the codes. The experimental findings indicate that\nSimilarGPT excels in detecting vulnerabilities in smart contracts, particularly\nin missed detections and minimizing false positives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of blockchain technology, smart contracts are now\ncrucial to Decentralized Finance (DeFi) applications. Effective vulnerability\ndetection is vital for securing these contracts against hackers and enhancing\nthe accuracy and efficiency of security audits. In this paper, we present\nSimilarGPT, a unique vulnerability identification tool for smart contract,\nwhich combines Generative Pretrained Transformer (GPT) models with Code-based\nsimilarity checking methods. The main concept of the SimilarGPT tool is to\nmeasure the similarity between the code under inspection and the secure code\nfrom third-party libraries. To identify potential vulnerabilities, we connect\nthe semantic understanding capability of large language models (LLMs) with\nCode-based similarity checking techniques. We propose optimizing the detection\nsequence using topological ordering to enhance logical coherence and reduce\nfalse positives during detection. Through analysis of code reuse patterns in\nsmart contracts, we compile and process extensive third-party library code to\nestablish a comprehensive reference codebase. Then, we utilize LLM to conduct\nan indepth analysis of similar codes to identify and explain potential\nvulnerabilities in the codes. The experimental findings indicate that\nSimilarGPT excels in detecting vulnerabilities in smart contracts, particularly\nin missed detections and minimizing false positives."
                },
                "authors": [
                    {
                        "name": "Jango Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jango Zhang"
                },
                "author": "Jango Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08685v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08685v3",
                "updated": "2024-12-24T07:08:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    7,
                    8,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-08-16T11:58:34Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    58,
                    34,
                    4,
                    229,
                    0
                ],
                "title": "Can Large Language Models Improve the Adversarial Robustness of Graph\n  Neural Networks?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Improve the Adversarial Robustness of Graph\n  Neural Networks?"
                },
                "summary": "Graph neural networks (GNNs) are vulnerable to adversarial attacks,\nespecially for topology perturbations, and many methods that improve the\nrobustness of GNNs have received considerable attention. Recently, we have\nwitnessed the significant success of large language models (LLMs), leading many\nto explore the great potential of LLMs on GNNs. However, they mainly focus on\nimproving the performance of GNNs by utilizing LLMs to enhance the node\nfeatures. Therefore, we ask: Will the robustness of GNNs also be enhanced with\nthe powerful understanding and inference capabilities of LLMs? By presenting\nthe empirical results, we find that despite that LLMs can improve the\nrobustness of GNNs, there is still an average decrease of 23.1% in accuracy,\nimplying that the GNNs remain extremely vulnerable against topology attacks.\nTherefore, another question is how to extend the capabilities of LLMs on graph\nadversarial robustness. In this paper, we propose an LLM-based robust graph\nstructure inference framework, LLM4RGNN, which distills the inference\ncapabilities of GPT-4 into a local LLM for identifying malicious edges and an\nLM-based edge predictor for finding missing important edges, so as to recover a\nrobust graph structure. Extensive experiments demonstrate that LLM4RGNN\nconsistently improves the robustness across various GNNs. Even in some cases\nwhere the perturbation ratio increases to 40%, the accuracy of GNNs is still\nbetter than that on the clean graph. The source code can be found in\nhttps://github.com/zhongjian-zhang/LLM4RGNN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) are vulnerable to adversarial attacks,\nespecially for topology perturbations, and many methods that improve the\nrobustness of GNNs have received considerable attention. Recently, we have\nwitnessed the significant success of large language models (LLMs), leading many\nto explore the great potential of LLMs on GNNs. However, they mainly focus on\nimproving the performance of GNNs by utilizing LLMs to enhance the node\nfeatures. Therefore, we ask: Will the robustness of GNNs also be enhanced with\nthe powerful understanding and inference capabilities of LLMs? By presenting\nthe empirical results, we find that despite that LLMs can improve the\nrobustness of GNNs, there is still an average decrease of 23.1% in accuracy,\nimplying that the GNNs remain extremely vulnerable against topology attacks.\nTherefore, another question is how to extend the capabilities of LLMs on graph\nadversarial robustness. In this paper, we propose an LLM-based robust graph\nstructure inference framework, LLM4RGNN, which distills the inference\ncapabilities of GPT-4 into a local LLM for identifying malicious edges and an\nLM-based edge predictor for finding missing important edges, so as to recover a\nrobust graph structure. Extensive experiments demonstrate that LLM4RGNN\nconsistently improves the robustness across various GNNs. Even in some cases\nwhere the perturbation ratio increases to 40%, the accuracy of GNNs is still\nbetter than that on the clean graph. The source code can be found in\nhttps://github.com/zhongjian-zhang/LLM4RGNN."
                },
                "authors": [
                    {
                        "name": "Zhongjian Zhang"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Huichi Zhou"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Mengmei Zhang"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Chuan Shi"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Shi"
                },
                "author": "Chuan Shi",
                "arxiv_doi": "10.1145/3690624.3709256",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3690624.3709256",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.08685v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08685v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "accepted by KDD 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18212v1",
                "updated": "2024-12-24T06:40:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    6,
                    40,
                    13,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T06:40:13Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    6,
                    40,
                    13,
                    1,
                    359,
                    0
                ],
                "title": "Accelerating AIGC Services with Latent Action Diffusion Scheduling in\n  Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating AIGC Services with Latent Action Diffusion Scheduling in\n  Edge Networks"
                },
                "summary": "Artificial Intelligence Generated Content (AIGC) has gained significant\npopularity for creating diverse content. Current AIGC models primarily focus on\ncontent quality within a centralized framework, resulting in a high service\ndelay and negative user experiences. However, not only does the workload of an\nAIGC task depend on the AIGC model's complexity rather than the amount of data,\nbut the large model and its multi-layer encoder structure also result in a huge\ndemand for computational and memory resources. These unique characteristics\npose new challenges in its modeling, deployment, and scheduling at edge\nnetworks. Thus, we model an offloading problem among edges for providing real\nAIGC services and propose LAD-TS, a novel Latent Action Diffusion-based Task\nScheduling method that orchestrates multiple edge servers for expedited AIGC\nservices. The LAD-TS generates a near-optimal offloading decision by leveraging\nthe diffusion model's conditional generation capability and the reinforcement\nlearning's environment interaction ability, thereby minimizing the service\ndelays under multiple resource constraints. Meanwhile, a latent action\ndiffusion strategy is designed to guide decision generation by utilizing\nhistorical action probability, enabling rapid achievement of near-optimal\ndecisions. Furthermore, we develop DEdgeAI, a prototype edge system with a\nrefined AIGC model deployment to implement and evaluate our LAD-TS method.\nDEdgeAI provides a real AIGC service for users, demonstrating up to 29.18%\nshorter service delays than the current five representative AIGC platforms. We\nrelease our open-source code at https://github.com/ChangfuXu/DEdgeAI/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence Generated Content (AIGC) has gained significant\npopularity for creating diverse content. Current AIGC models primarily focus on\ncontent quality within a centralized framework, resulting in a high service\ndelay and negative user experiences. However, not only does the workload of an\nAIGC task depend on the AIGC model's complexity rather than the amount of data,\nbut the large model and its multi-layer encoder structure also result in a huge\ndemand for computational and memory resources. These unique characteristics\npose new challenges in its modeling, deployment, and scheduling at edge\nnetworks. Thus, we model an offloading problem among edges for providing real\nAIGC services and propose LAD-TS, a novel Latent Action Diffusion-based Task\nScheduling method that orchestrates multiple edge servers for expedited AIGC\nservices. The LAD-TS generates a near-optimal offloading decision by leveraging\nthe diffusion model's conditional generation capability and the reinforcement\nlearning's environment interaction ability, thereby minimizing the service\ndelays under multiple resource constraints. Meanwhile, a latent action\ndiffusion strategy is designed to guide decision generation by utilizing\nhistorical action probability, enabling rapid achievement of near-optimal\ndecisions. Furthermore, we develop DEdgeAI, a prototype edge system with a\nrefined AIGC model deployment to implement and evaluate our LAD-TS method.\nDEdgeAI provides a real AIGC service for users, demonstrating up to 29.18%\nshorter service delays than the current five representative AIGC platforms. We\nrelease our open-source code at https://github.com/ChangfuXu/DEdgeAI/."
                },
                "authors": [
                    {
                        "name": "Changfu Xu"
                    },
                    {
                        "name": "Jianxiong Guo"
                    },
                    {
                        "name": "Wanyu Lin"
                    },
                    {
                        "name": "Haodong Zou"
                    },
                    {
                        "name": "Wentao Fan"
                    },
                    {
                        "name": "Tian Wang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Jiannong Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jiannong Cao"
                },
                "author": "Jiannong Cao",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18200v1",
                "updated": "2024-12-24T06:11:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    6,
                    11,
                    10,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T06:11:10Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    6,
                    11,
                    10,
                    1,
                    359,
                    0
                ],
                "title": "Adapting Large Language Models for Improving TCP Fairness over WiFi",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Large Language Models for Improving TCP Fairness over WiFi"
                },
                "summary": "The new transmission control protocol (TCP) relies on Deep Learning (DL) for\nprediction and optimization, but requires significant manual effort to design\ndeep neural networks (DNNs) and struggles with generalization in dynamic\nenvironments. Inspired by the success of large language models (LLMs), this\nstudy proposes TCP-LLM, a novel framework leveraging LLMs for TCP applications.\nTCP-LLM utilizes pre-trained knowledge to reduce engineering effort, enhance\ngeneralization, and deliver superior performance across diverse TCP tasks.\nApplied to reducing flow unfairness, adapting congestion control, and\npreventing starvation, TCP-LLM demonstrates significant improvements over TCP\nwith minimal fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new transmission control protocol (TCP) relies on Deep Learning (DL) for\nprediction and optimization, but requires significant manual effort to design\ndeep neural networks (DNNs) and struggles with generalization in dynamic\nenvironments. Inspired by the success of large language models (LLMs), this\nstudy proposes TCP-LLM, a novel framework leveraging LLMs for TCP applications.\nTCP-LLM utilizes pre-trained knowledge to reduce engineering effort, enhance\ngeneralization, and deliver superior performance across diverse TCP tasks.\nApplied to reducing flow unfairness, adapting congestion control, and\npreventing starvation, TCP-LLM demonstrates significant improvements over TCP\nwith minimal fine-tuning."
                },
                "authors": [
                    {
                        "name": "Shyam Kumar Shrestha"
                    },
                    {
                        "name": "Shiva Raj Pokhrel"
                    },
                    {
                        "name": "Jonathan Kua"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Kua"
                },
                "author": "Jonathan Kua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01787v2",
                "updated": "2024-12-24T06:06:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    6,
                    6,
                    25,
                    1,
                    359,
                    0
                ],
                "published": "2024-09-03T11:06:45Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    11,
                    6,
                    45,
                    1,
                    247,
                    0
                ],
                "title": "LLM-GAN: Construct Generative Adversarial Network Through Large Language\n  Models For Explainable Fake News Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-GAN: Construct Generative Adversarial Network Through Large Language\n  Models For Explainable Fake News Detection"
                },
                "summary": "Explainable fake news detection predicts the authenticity of news items with\nannotated explanations. Today, Large Language Models (LLMs) are known for their\npowerful natural language understanding and explanation generation abilities.\nHowever, presenting LLMs for explainable fake news detection remains two main\nchallenges. Firstly, fake news appears reasonable and could easily mislead\nLLMs, leaving them unable to understand the complex news-faking process.\nSecondly, utilizing LLMs for this task would generate both correct and\nincorrect explanations, which necessitates abundant labor in the loop. In this\npaper, we propose LLM-GAN, a novel framework that utilizes prompting mechanisms\nto enable an LLM to become Generator and Detector and for realistic fake news\ngeneration and detection. Our results demonstrate LLM-GAN's effectiveness in\nboth prediction performance and explanation quality. We further showcase the\nintegration of LLM-GAN to a cloud-native AI platform to provide better fake\nnews detection service in the cloud.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable fake news detection predicts the authenticity of news items with\nannotated explanations. Today, Large Language Models (LLMs) are known for their\npowerful natural language understanding and explanation generation abilities.\nHowever, presenting LLMs for explainable fake news detection remains two main\nchallenges. Firstly, fake news appears reasonable and could easily mislead\nLLMs, leaving them unable to understand the complex news-faking process.\nSecondly, utilizing LLMs for this task would generate both correct and\nincorrect explanations, which necessitates abundant labor in the loop. In this\npaper, we propose LLM-GAN, a novel framework that utilizes prompting mechanisms\nto enable an LLM to become Generator and Detector and for realistic fake news\ngeneration and detection. Our results demonstrate LLM-GAN's effectiveness in\nboth prediction performance and explanation quality. We further showcase the\nintegration of LLM-GAN to a cloud-native AI platform to provide better fake\nnews detection service in the cloud."
                },
                "authors": [
                    {
                        "name": "Yifeng Wang"
                    },
                    {
                        "name": "Zhouhong Gu"
                    },
                    {
                        "name": "Siwei Zhang"
                    },
                    {
                        "name": "Suhang Zheng"
                    },
                    {
                        "name": "Tao Wang"
                    },
                    {
                        "name": "Tianyu Li"
                    },
                    {
                        "name": "Hongwei Feng"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18196v1",
                "updated": "2024-12-24T06:05:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    6,
                    5,
                    8,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T06:05:08Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    6,
                    5,
                    8,
                    1,
                    359,
                    0
                ],
                "title": "Robustness-aware Automatic Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustness-aware Automatic Prompt Optimization"
                },
                "summary": "The performance of Large Language Models (LLMs) is based on the quality of\nthe prompts and the semantic and structural integrity information of the input\ndata. However, current prompt generation methods primarily focus on generating\nprompts for clean input data, often overlooking the impact of perturbed inputs\non prompt performance. To address this limitation, we propose BATprompt (By\nAdversarial Training prompt), a novel method for prompt generation designed to\nwithstand input perturbations (such as typos in the input). Inspired by\nadversarial training techniques, BATprompt demonstrates strong performance on a\nvariety of perturbed tasks through a two-step process: adversarial perturbation\nand iterative optimization on unperturbed input via LLM. Unlike conventional\nadversarial attack methods, BATprompt avoids reliance on real gradients or\nmodel parameters. Instead, it leverages the advanced reasoning, language\nunderstanding and self reflection capabilities of LLMs to simulate gradients,\nguiding the generation of adversarial perturbations and optimizing prompt\nperformance. In our experiments, we evaluate BATprompt on multiple datasets\nacross both language understanding and generation tasks. The results indicate\nthat BATprompt outperforms existing prompt generation methods, delivering\nsuperior robustness and performance under diverse perturbation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of Large Language Models (LLMs) is based on the quality of\nthe prompts and the semantic and structural integrity information of the input\ndata. However, current prompt generation methods primarily focus on generating\nprompts for clean input data, often overlooking the impact of perturbed inputs\non prompt performance. To address this limitation, we propose BATprompt (By\nAdversarial Training prompt), a novel method for prompt generation designed to\nwithstand input perturbations (such as typos in the input). Inspired by\nadversarial training techniques, BATprompt demonstrates strong performance on a\nvariety of perturbed tasks through a two-step process: adversarial perturbation\nand iterative optimization on unperturbed input via LLM. Unlike conventional\nadversarial attack methods, BATprompt avoids reliance on real gradients or\nmodel parameters. Instead, it leverages the advanced reasoning, language\nunderstanding and self reflection capabilities of LLMs to simulate gradients,\nguiding the generation of adversarial perturbations and optimizing prompt\nperformance. In our experiments, we evaluate BATprompt on multiple datasets\nacross both language understanding and generation tasks. The results indicate\nthat BATprompt outperforms existing prompt generation methods, delivering\nsuperior robustness and performance under diverse perturbation scenarios."
                },
                "authors": [
                    {
                        "name": "Zeru Shi"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Yongye Su"
                    },
                    {
                        "name": "Weidi Luo"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18194v1",
                "updated": "2024-12-24T06:03:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    6,
                    3,
                    42,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T06:03:42Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    6,
                    3,
                    42,
                    1,
                    359,
                    0
                ],
                "title": "VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics\n  Manipulation with Long-Horizon Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics\n  Manipulation with Long-Horizon Reasoning Tasks"
                },
                "summary": "General-purposed embodied agents are designed to understand the users'\nnatural instructions or intentions and act precisely to complete universal\ntasks. Recently, methods based on foundation models especially\nVision-Language-Action models (VLAs) have shown a substantial potential to\nsolve language-conditioned manipulation (LCM) tasks well. However, existing\nbenchmarks do not adequately meet the needs of VLAs and relative algorithms. To\nbetter define such general-purpose tasks in the context of LLMs and advance the\nresearch in VLAs, we present VLABench, an open-source benchmark for evaluating\nuniversal LCM task learning. VLABench provides 100 carefully designed\ncategories of tasks, with strong randomization in each category of task and a\ntotal of 2000+ objects. VLABench stands out from previous benchmarks in four\nkey aspects: 1) tasks requiring world knowledge and common sense transfer, 2)\nnatural language instructions with implicit human intentions rather than\ntemplates, 3) long-horizon tasks demanding multi-step reasoning, and 4)\nevaluation of both action policies and language model capabilities. The\nbenchmark assesses multiple competencies including understanding of\nmesh\\&texture, spatial relationship, semantic instruction, physical laws,\nknowledge transfer and reasoning, etc. To support the downstream finetuning, we\nprovide high-quality training data collected via an automated framework\nincorporating heuristic skills and prior information. The experimental results\nindicate that both the current state-of-the-art pretrained VLAs and the\nworkflow based on VLMs face challenges in our tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General-purposed embodied agents are designed to understand the users'\nnatural instructions or intentions and act precisely to complete universal\ntasks. Recently, methods based on foundation models especially\nVision-Language-Action models (VLAs) have shown a substantial potential to\nsolve language-conditioned manipulation (LCM) tasks well. However, existing\nbenchmarks do not adequately meet the needs of VLAs and relative algorithms. To\nbetter define such general-purpose tasks in the context of LLMs and advance the\nresearch in VLAs, we present VLABench, an open-source benchmark for evaluating\nuniversal LCM task learning. VLABench provides 100 carefully designed\ncategories of tasks, with strong randomization in each category of task and a\ntotal of 2000+ objects. VLABench stands out from previous benchmarks in four\nkey aspects: 1) tasks requiring world knowledge and common sense transfer, 2)\nnatural language instructions with implicit human intentions rather than\ntemplates, 3) long-horizon tasks demanding multi-step reasoning, and 4)\nevaluation of both action policies and language model capabilities. The\nbenchmark assesses multiple competencies including understanding of\nmesh\\&texture, spatial relationship, semantic instruction, physical laws,\nknowledge transfer and reasoning, etc. To support the downstream finetuning, we\nprovide high-quality training data collected via an automated framework\nincorporating heuristic skills and prior information. The experimental results\nindicate that both the current state-of-the-art pretrained VLAs and the\nworkflow based on VLMs face challenges in our tasks."
                },
                "authors": [
                    {
                        "name": "Shiduo Zhang"
                    },
                    {
                        "name": "Zhe Xu"
                    },
                    {
                        "name": "Peiju Liu"
                    },
                    {
                        "name": "Xiaopeng Yu"
                    },
                    {
                        "name": "Yuan Li"
                    },
                    {
                        "name": "Qinghui Gao"
                    },
                    {
                        "name": "Zhaoye Fei"
                    },
                    {
                        "name": "Zhangyue Yin"
                    },
                    {
                        "name": "Zuxuan Wu"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16468v2",
                "updated": "2024-12-24T05:54:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    54,
                    15,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-21T03:51:04Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    3,
                    51,
                    4,
                    5,
                    356,
                    0
                ],
                "title": "The Road to Artificial SuperIntelligence: A Comprehensive Survey of\n  Superalignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Road to Artificial SuperIntelligence: A Comprehensive Survey of\n  Superalignment"
                },
                "summary": "The emergence of large language models (LLMs) has sparked the possibility of\nabout Artificial Superintelligence (ASI), a hypothetical AI system surpassing\nhuman intelligence. However, existing alignment paradigms struggle to guide\nsuch advanced AI systems. Superalignment, the alignment of AI systems with\nhuman values and safety requirements at superhuman levels of capability aims to\naddresses two primary goals -- scalability in supervision to provide\nhigh-quality guidance signals and robust governance to ensure alignment with\nhuman values. In this survey, we examine scalable oversight methods and\npotential solutions for superalignment. Specifically, we explore the concept of\nASI, the challenges it poses, and the limitations of current alignment\nparadigms in addressing the superalignment problem. Then we review scalable\noversight methods for superalignment. Finally, we discuss the key challenges\nand propose pathways for the safe and continual improvement of ASI systems. By\ncomprehensively reviewing the current literature, our goal is provide a\nsystematical introduction of existing methods, analyze their strengths and\nlimitations, and discuss potential future directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) has sparked the possibility of\nabout Artificial Superintelligence (ASI), a hypothetical AI system surpassing\nhuman intelligence. However, existing alignment paradigms struggle to guide\nsuch advanced AI systems. Superalignment, the alignment of AI systems with\nhuman values and safety requirements at superhuman levels of capability aims to\naddresses two primary goals -- scalability in supervision to provide\nhigh-quality guidance signals and robust governance to ensure alignment with\nhuman values. In this survey, we examine scalable oversight methods and\npotential solutions for superalignment. Specifically, we explore the concept of\nASI, the challenges it poses, and the limitations of current alignment\nparadigms in addressing the superalignment problem. Then we review scalable\noversight methods for superalignment. Finally, we discuss the key challenges\nand propose pathways for the safe and continual improvement of ASI systems. By\ncomprehensively reviewing the current literature, our goal is provide a\nsystematical introduction of existing methods, analyze their strengths and\nlimitations, and discuss potential future directions."
                },
                "authors": [
                    {
                        "name": "HyunJin Kim"
                    },
                    {
                        "name": "Xiaoyuan Yi"
                    },
                    {
                        "name": "Jing Yao"
                    },
                    {
                        "name": "Jianxun Lian"
                    },
                    {
                        "name": "Muhua Huang"
                    },
                    {
                        "name": "Shitong Duan"
                    },
                    {
                        "name": "JinYeong Bak"
                    },
                    {
                        "name": "Xing Xie"
                    }
                ],
                "author_detail": {
                    "name": "Xing Xie"
                },
                "author": "Xing Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18189v1",
                "updated": "2024-12-24T05:50:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    50,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T05:50:45Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    50,
                    45,
                    1,
                    359,
                    0
                ],
                "title": "Toward an Automated, Proactive Safety Warning System Development for\n  Truck Mounted Attenuators in Mobile Work Zones",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward an Automated, Proactive Safety Warning System Development for\n  Truck Mounted Attenuators in Mobile Work Zones"
                },
                "summary": "Even though Truck Mounted Attenuators (TMA)/Autonomous Truck Mounted\nAttenuators (ATMA) and traffic control devices are increasingly used in mobile\nwork zones to enhance safety, work zone collisions remain a significant safety\nconcern in the United States. In Missouri, there were 63 TMA-related crashes in\n2023, a 27% increase compared to 2022. Currently, all the signs in the mobile\nwork zones are passive safety measures, relying on drivers' recognition and\nattention. Some distracted drivers may ignore these signs and warnings, raising\nsafety concerns. In this study, we proposed an additional proactive warning\nsystem that could be applied to the TMA/ATMA to improve overall safety. A\nfeasible solution has been demonstrated by integrating a Panoptic Driving\nPerception algorithm into the Robot Operating System (ROS) and applying it to\nthe TMA/ATMA systems. This enables us to alert vehicles on a collision course\nwith the TMA. Our experimental setup, currently conducted in a laboratory\nenvironment with two ROS robots and a desktop GPU, demonstrates the system's\ncapability to calculate real-time distance and speed and activate warning\nsignals. Leveraging ROS's distributed computing capabilities allows for\nflexible system deployment and cost reduction. In future field tests, by\ncombining the stopping sight distance (SSD) standards from the AASHTO Green\nBook, the system enables real-time monitoring of oncoming vehicles and provides\nadditional proactive warnings to enhance the safety of mobile work zones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even though Truck Mounted Attenuators (TMA)/Autonomous Truck Mounted\nAttenuators (ATMA) and traffic control devices are increasingly used in mobile\nwork zones to enhance safety, work zone collisions remain a significant safety\nconcern in the United States. In Missouri, there were 63 TMA-related crashes in\n2023, a 27% increase compared to 2022. Currently, all the signs in the mobile\nwork zones are passive safety measures, relying on drivers' recognition and\nattention. Some distracted drivers may ignore these signs and warnings, raising\nsafety concerns. In this study, we proposed an additional proactive warning\nsystem that could be applied to the TMA/ATMA to improve overall safety. A\nfeasible solution has been demonstrated by integrating a Panoptic Driving\nPerception algorithm into the Robot Operating System (ROS) and applying it to\nthe TMA/ATMA systems. This enables us to alert vehicles on a collision course\nwith the TMA. Our experimental setup, currently conducted in a laboratory\nenvironment with two ROS robots and a desktop GPU, demonstrates the system's\ncapability to calculate real-time distance and speed and activate warning\nsignals. Leveraging ROS's distributed computing capabilities allows for\nflexible system deployment and cost reduction. In future field tests, by\ncombining the stopping sight distance (SSD) standards from the AASHTO Green\nBook, the system enables real-time monitoring of oncoming vehicles and provides\nadditional proactive warnings to enhance the safety of mobile work zones."
                },
                "authors": [
                    {
                        "name": "Xiang Yu"
                    },
                    {
                        "name": "Linlin Zhang"
                    },
                    {
                        "name": "Yaw"
                    },
                    {
                        "name": "Adu-Gyamfi"
                    }
                ],
                "author_detail": {
                    "name": "Adu-Gyamfi"
                },
                "author": "Adu-Gyamfi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18185v1",
                "updated": "2024-12-24T05:38:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    38,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T05:38:45Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    38,
                    45,
                    1,
                    359,
                    0
                ],
                "title": "TextMatch: Enhancing Image-Text Consistency Through Multimodal\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextMatch: Enhancing Image-Text Consistency Through Multimodal\n  Optimization"
                },
                "summary": "Text-to-image generative models excel in creating images from text but\nstruggle with ensuring alignment and consistency between outputs and prompts.\nThis paper introduces TextMatch, a novel framework that leverages multimodal\noptimization to address image-text discrepancies in text-to-image (T2I)\ngeneration and editing. TextMatch employs a scoring strategy powered by large\nlanguage models (LLMs) and visual question-answering (VQA) models to evaluate\nsemantic consistency between prompts and generated images. By integrating\nmultimodal in-context learning and chain of thought reasoning, our method\ndynamically refines prompts through iterative optimization. This process\nensures that the generated images better capture user intent of, resulting in\nhigher fidelity and relevance. Extensive experiments demonstrate that TextMatch\nsignificantly improves text-image consistency across multiple benchmarks,\nestablishing a reliable framework for advancing the capabilities of\ntext-to-image generative models. Our code is available at\nhttps://anonymous.4open.science/r/TextMatch-F55C/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generative models excel in creating images from text but\nstruggle with ensuring alignment and consistency between outputs and prompts.\nThis paper introduces TextMatch, a novel framework that leverages multimodal\noptimization to address image-text discrepancies in text-to-image (T2I)\ngeneration and editing. TextMatch employs a scoring strategy powered by large\nlanguage models (LLMs) and visual question-answering (VQA) models to evaluate\nsemantic consistency between prompts and generated images. By integrating\nmultimodal in-context learning and chain of thought reasoning, our method\ndynamically refines prompts through iterative optimization. This process\nensures that the generated images better capture user intent of, resulting in\nhigher fidelity and relevance. Extensive experiments demonstrate that TextMatch\nsignificantly improves text-image consistency across multiple benchmarks,\nestablishing a reliable framework for advancing the capabilities of\ntext-to-image generative models. Our code is available at\nhttps://anonymous.4open.science/r/TextMatch-F55C/."
                },
                "authors": [
                    {
                        "name": "Yucong Luo"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Jie Ouyang"
                    },
                    {
                        "name": "Xiaoyu Tao"
                    },
                    {
                        "name": "Qi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Liu"
                },
                "author": "Qi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18176v1",
                "updated": "2024-12-24T05:23:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    23,
                    13,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T05:23:13Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    23,
                    13,
                    1,
                    359,
                    0
                ],
                "title": "Molar: Multimodal LLMs with Collaborative Filtering Alignment for\n  Enhanced Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molar: Multimodal LLMs with Collaborative Filtering Alignment for\n  Enhanced Sequential Recommendation"
                },
                "summary": "Sequential recommendation (SR) systems have evolved significantly over the\npast decade, transitioning from traditional collaborative filtering to deep\nlearning approaches and, more recently, to large language models (LLMs). While\nthe adoption of LLMs has driven substantial advancements, these models\ninherently lack collaborative filtering information, relying primarily on\ntextual content data neglecting other modalities and thus failing to achieve\noptimal recommendation performance. To address this limitation, we propose\nMolar, a Multimodal large language sequential recommendation framework that\nintegrates multiple content modalities with ID information to capture\ncollaborative signals effectively. Molar employs an MLLM to generate unified\nitem representations from both textual and non-textual data, facilitating\ncomprehensive multimodal modeling and enriching item embeddings. Additionally,\nit incorporates collaborative filtering signals through a post-alignment\nmechanism, which aligns user representations from content-based and ID-based\nmodels, ensuring precise personalization and robust performance. By seamlessly\ncombining multimodal content with collaborative filtering insights, Molar\ncaptures both user interests and contextual semantics, leading to superior\nrecommendation accuracy. Extensive experiments validate that Molar\nsignificantly outperforms traditional and LLM-based baselines, highlighting its\nstrength in utilizing multimodal data and collaborative signals for sequential\nrecommendation tasks. The source code is available at\nhttps://anonymous.4open.science/r/Molar-8B06/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommendation (SR) systems have evolved significantly over the\npast decade, transitioning from traditional collaborative filtering to deep\nlearning approaches and, more recently, to large language models (LLMs). While\nthe adoption of LLMs has driven substantial advancements, these models\ninherently lack collaborative filtering information, relying primarily on\ntextual content data neglecting other modalities and thus failing to achieve\noptimal recommendation performance. To address this limitation, we propose\nMolar, a Multimodal large language sequential recommendation framework that\nintegrates multiple content modalities with ID information to capture\ncollaborative signals effectively. Molar employs an MLLM to generate unified\nitem representations from both textual and non-textual data, facilitating\ncomprehensive multimodal modeling and enriching item embeddings. Additionally,\nit incorporates collaborative filtering signals through a post-alignment\nmechanism, which aligns user representations from content-based and ID-based\nmodels, ensuring precise personalization and robust performance. By seamlessly\ncombining multimodal content with collaborative filtering insights, Molar\ncaptures both user interests and contextual semantics, leading to superior\nrecommendation accuracy. Extensive experiments validate that Molar\nsignificantly outperforms traditional and LLM-based baselines, highlighting its\nstrength in utilizing multimodal data and collaborative signals for sequential\nrecommendation tasks. The source code is available at\nhttps://anonymous.4open.science/r/Molar-8B06/."
                },
                "authors": [
                    {
                        "name": "Yucong Luo"
                    },
                    {
                        "name": "Qitao Qin"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Ruiran Yan"
                    },
                    {
                        "name": "Kefan Wang"
                    },
                    {
                        "name": "Jie Ouyang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Ouyang"
                },
                "author": "Jie Ouyang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18174v1",
                "updated": "2024-12-24T05:22:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    22,
                    33,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T05:22:33Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    22,
                    33,
                    1,
                    359,
                    0
                ],
                "title": "INVESTORBENCH: A Benchmark for Financial Decision-Making Tasks with\n  LLM-based Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INVESTORBENCH: A Benchmark for Financial Decision-Making Tasks with\n  LLM-based Agent"
                },
                "summary": "Recent advancements have underscored the potential of large language model\n(LLM)-based agents in financial decision-making. Despite this progress, the\nfield currently encounters two main challenges: (1) the lack of a comprehensive\nLLM agent framework adaptable to a variety of financial tasks, and (2) the\nabsence of standardized benchmarks and consistent datasets for assessing agent\nperformance. To tackle these issues, we introduce \\textsc{InvestorBench}, the\nfirst benchmark specifically designed for evaluating LLM-based agents in\ndiverse financial decision-making contexts. InvestorBench enhances the\nversatility of LLM-enabled agents by providing a comprehensive suite of tasks\napplicable to different financial products, including single equities like\nstocks, cryptocurrencies and exchange-traded funds (ETFs). Additionally, we\nassess the reasoning and decision-making capabilities of our agent framework\nusing thirteen different LLMs as backbone models, across various market\nenvironments and tasks. Furthermore, we have curated a diverse collection of\nopen-source, multi-modal datasets and developed a comprehensive suite of\nenvironments for financial decision-making. This establishes a highly\naccessible platform for evaluating financial agents' performance across various\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements have underscored the potential of large language model\n(LLM)-based agents in financial decision-making. Despite this progress, the\nfield currently encounters two main challenges: (1) the lack of a comprehensive\nLLM agent framework adaptable to a variety of financial tasks, and (2) the\nabsence of standardized benchmarks and consistent datasets for assessing agent\nperformance. To tackle these issues, we introduce \\textsc{InvestorBench}, the\nfirst benchmark specifically designed for evaluating LLM-based agents in\ndiverse financial decision-making contexts. InvestorBench enhances the\nversatility of LLM-enabled agents by providing a comprehensive suite of tasks\napplicable to different financial products, including single equities like\nstocks, cryptocurrencies and exchange-traded funds (ETFs). Additionally, we\nassess the reasoning and decision-making capabilities of our agent framework\nusing thirteen different LLMs as backbone models, across various market\nenvironments and tasks. Furthermore, we have curated a diverse collection of\nopen-source, multi-modal datasets and developed a comprehensive suite of\nenvironments for financial decision-making. This establishes a highly\naccessible platform for evaluating financial agents' performance across various\nscenarios."
                },
                "authors": [
                    {
                        "name": "Haohang Li"
                    },
                    {
                        "name": "Yupeng Cao"
                    },
                    {
                        "name": "Yangyang Yu"
                    },
                    {
                        "name": "Shashidhar Reddy Javaji"
                    },
                    {
                        "name": "Zhiyang Deng"
                    },
                    {
                        "name": "Yueru He"
                    },
                    {
                        "name": "Yuechen Jiang"
                    },
                    {
                        "name": "Zining Zhu"
                    },
                    {
                        "name": "Koduvayur Subbalakshmi"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Lingfei Qian"
                    },
                    {
                        "name": "Xueqing Peng"
                    },
                    {
                        "name": "Qianqian Xie"
                    },
                    {
                        "name": "Jordan W. Suchow"
                    }
                ],
                "author_detail": {
                    "name": "Jordan W. Suchow"
                },
                "author": "Jordan W. Suchow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18171v1",
                "updated": "2024-12-24T05:10:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    10,
                    2,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T05:10:02Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    10,
                    2,
                    1,
                    359,
                    0
                ],
                "title": "Token Highlighter: Inspecting and Mitigating Jailbreak Prompts for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Highlighter: Inspecting and Mitigating Jailbreak Prompts for Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are increasingly being integrated into services\nsuch as ChatGPT to provide responses to user queries. To mitigate potential\nharm and prevent misuse, there have been concerted efforts to align the LLMs\nwith human values and legal compliance by incorporating various techniques,\nsuch as Reinforcement Learning from Human Feedback (RLHF), into the training of\nthe LLMs. However, recent research has exposed that even aligned LLMs are\nsusceptible to adversarial manipulations known as Jailbreak Attacks. To address\nthis challenge, this paper proposes a method called Token Highlighter to\ninspect and mitigate the potential jailbreak threats in the user query. Token\nHighlighter introduced a concept called Affirmation Loss to measure the LLM's\nwillingness to answer the user query. It then uses the gradient of Affirmation\nLoss for each token in the user query to locate the jailbreak-critical tokens.\nFurther, Token Highlighter exploits our proposed Soft Removal technique to\nmitigate the jailbreak effects of critical tokens via shrinking their token\nembeddings. Experimental results on two aligned LLMs (LLaMA-2 and Vicuna-V1.5)\ndemonstrate that the proposed method can effectively defend against a variety\nof Jailbreak Attacks while maintaining competent performance on benign\nquestions of the AlpacaEval benchmark. In addition, Token Highlighter is a\ncost-effective and interpretable defense because it only needs to query the\nprotected LLM once to compute the Affirmation Loss and can highlight the\ncritical tokens upon refusal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being integrated into services\nsuch as ChatGPT to provide responses to user queries. To mitigate potential\nharm and prevent misuse, there have been concerted efforts to align the LLMs\nwith human values and legal compliance by incorporating various techniques,\nsuch as Reinforcement Learning from Human Feedback (RLHF), into the training of\nthe LLMs. However, recent research has exposed that even aligned LLMs are\nsusceptible to adversarial manipulations known as Jailbreak Attacks. To address\nthis challenge, this paper proposes a method called Token Highlighter to\ninspect and mitigate the potential jailbreak threats in the user query. Token\nHighlighter introduced a concept called Affirmation Loss to measure the LLM's\nwillingness to answer the user query. It then uses the gradient of Affirmation\nLoss for each token in the user query to locate the jailbreak-critical tokens.\nFurther, Token Highlighter exploits our proposed Soft Removal technique to\nmitigate the jailbreak effects of critical tokens via shrinking their token\nembeddings. Experimental results on two aligned LLMs (LLaMA-2 and Vicuna-V1.5)\ndemonstrate that the proposed method can effectively defend against a variety\nof Jailbreak Attacks while maintaining competent performance on benign\nquestions of the AlpacaEval benchmark. In addition, Token Highlighter is a\ncost-effective and interpretable defense because it only needs to query the\nprotected LLM once to compute the Affirmation Loss and can highlight the\ncritical tokens upon refusal."
                },
                "authors": [
                    {
                        "name": "Xiaomeng Hu"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Tsung-Yi Ho"
                    }
                ],
                "author_detail": {
                    "name": "Tsung-Yi Ho"
                },
                "author": "Tsung-Yi Ho",
                "arxiv_comment": "Accepted by AAAI 2025. Project page:\n  https://huggingface.co/spaces/TrustSafeAI/Token-Highlighter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18169v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18169v1",
                "updated": "2024-12-24T05:07:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    7,
                    46,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T05:07:46Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    7,
                    46,
                    1,
                    359,
                    0
                ],
                "title": "KunServe: Elastic and Efficient Large Language Model Serving with\n  Parameter-centric Memory Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KunServe: Elastic and Efficient Large Language Model Serving with\n  Parameter-centric Memory Management"
                },
                "summary": "The stateful nature of large language model (LLM) servingcan easily throttle\nprecious GPU memory under load burstor long-generation requests like\nchain-of-thought reasoning,causing latency spikes due to queuing incoming\nrequests. However, state-of-the-art KVCache centric approaches handleload\nspikes by dropping, migrating, or swapping KVCache,which faces an essential\ntradeoff between the performance ofongoing vs. incoming requests and thus still\nseverely violatesSLO.This paper makes a key observation such that model\nparam-eters are independent of the requests and are replicated acrossGPUs, and\nthus proposes a parameter-centric approach byselectively dropping replicated\nparameters to leave preciousmemory for requests. However, LLM requires KVCache\ntobe saved in bound with model parameters and thus droppingparameters can cause\neither huge computation waste or longnetwork delay, affecting all ongoing\nrequests. Based on the ob-servation that attention operators can be decoupled\nfrom otheroperators, this paper further proposes a novel remote\nattentionmechanism through pipeline parallelism so as to serve up-coming\nrequests with the additional memory borrowed fromparameters on remote GPUs.\nThis paper further addresses sev-eral other challenges including lively\nexchanging KVCachewith incomplete parameters, generating an appropriate\nplanthat balances memory requirements with cooperative exe-cution overhead, and\nseamlessly restoring parameters whenthe throttling has gone. Evaluations show\nthatKUNSERVEreduces the tail TTFT of requests under throttling by up to 27.3x\ncompared to the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The stateful nature of large language model (LLM) servingcan easily throttle\nprecious GPU memory under load burstor long-generation requests like\nchain-of-thought reasoning,causing latency spikes due to queuing incoming\nrequests. However, state-of-the-art KVCache centric approaches handleload\nspikes by dropping, migrating, or swapping KVCache,which faces an essential\ntradeoff between the performance ofongoing vs. incoming requests and thus still\nseverely violatesSLO.This paper makes a key observation such that model\nparam-eters are independent of the requests and are replicated acrossGPUs, and\nthus proposes a parameter-centric approach byselectively dropping replicated\nparameters to leave preciousmemory for requests. However, LLM requires KVCache\ntobe saved in bound with model parameters and thus droppingparameters can cause\neither huge computation waste or longnetwork delay, affecting all ongoing\nrequests. Based on the ob-servation that attention operators can be decoupled\nfrom otheroperators, this paper further proposes a novel remote\nattentionmechanism through pipeline parallelism so as to serve up-coming\nrequests with the additional memory borrowed fromparameters on remote GPUs.\nThis paper further addresses sev-eral other challenges including lively\nexchanging KVCachewith incomplete parameters, generating an appropriate\nplanthat balances memory requirements with cooperative exe-cution overhead, and\nseamlessly restoring parameters whenthe throttling has gone. Evaluations show\nthatKUNSERVEreduces the tail TTFT of requests under throttling by up to 27.3x\ncompared to the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Rongxin Cheng"
                    },
                    {
                        "name": "Yifan Peng"
                    },
                    {
                        "name": "Yuxin Lai"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18169v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.02431v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.02431v5",
                "updated": "2024-12-24T04:57:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    57,
                    32,
                    1,
                    359,
                    0
                ],
                "published": "2023-11-04T15:21:55Z",
                "published_parsed": [
                    2023,
                    11,
                    4,
                    15,
                    21,
                    55,
                    5,
                    308,
                    0
                ],
                "title": "The contribution of US broadband infrastructure subsidy and investment\n  programs to GDP using input-output modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The contribution of US broadband infrastructure subsidy and investment\n  programs to GDP using input-output modeling"
                },
                "summary": "More than one-fifth of the US population does not subscribe to a fixed\nbroadband service despite broadband being a recognized merit good. For example,\nless than 4% of citizens earning more than US \\$70k annually do not have\nbroadband, compared to 26% of those earning below US \\$20k annually. To address\nthis, the federal government has undertaken one of the largest broadband\ninvestment programs ever via The Bipartisan Infrastructure Law, with the aim of\naddressing this disparity and expanding broadband connectivity to all citizens.\nWe examine broadband availability, adoption, and need for each US state, and\nthen construct an Input-Output model to explore the potential structural\nmacroeconomic impacts of broadband spending on Gross Domestic Product (GDP) and\nsupply chain linkages. In terms of macroeconomic impact, the total potential\nindirect contribution to US GDP by the program could be as high as US \\$84.8\nbillion, \\$32.7 billion, and \\$9.78 billion for the Broadband Equity, Access,\nand Deployment program, the Affordable Connectivity Program, and additional\nprograms, respectively. Thus, overall, the broadband allocations could expand\nUS GDP by up to US \\$127.3 billion (0.10% of annual US GDP over the next five\nyears). Moreover, the broadband packages within the Bipartisan Infrastructure\nLaw could create up to 230,000 jobs (0.14% labor market increase). We\ncontribute one of the first economic impact assessments of the US Bipartisan\nInfrastructure Law to the literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More than one-fifth of the US population does not subscribe to a fixed\nbroadband service despite broadband being a recognized merit good. For example,\nless than 4% of citizens earning more than US \\$70k annually do not have\nbroadband, compared to 26% of those earning below US \\$20k annually. To address\nthis, the federal government has undertaken one of the largest broadband\ninvestment programs ever via The Bipartisan Infrastructure Law, with the aim of\naddressing this disparity and expanding broadband connectivity to all citizens.\nWe examine broadband availability, adoption, and need for each US state, and\nthen construct an Input-Output model to explore the potential structural\nmacroeconomic impacts of broadband spending on Gross Domestic Product (GDP) and\nsupply chain linkages. In terms of macroeconomic impact, the total potential\nindirect contribution to US GDP by the program could be as high as US \\$84.8\nbillion, \\$32.7 billion, and \\$9.78 billion for the Broadband Equity, Access,\nand Deployment program, the Affordable Connectivity Program, and additional\nprograms, respectively. Thus, overall, the broadband allocations could expand\nUS GDP by up to US \\$127.3 billion (0.10% of annual US GDP over the next five\nyears). Moreover, the broadband packages within the Bipartisan Infrastructure\nLaw could create up to 230,000 jobs (0.14% labor market increase). We\ncontribute one of the first economic impact assessments of the US Bipartisan\nInfrastructure Law to the literature."
                },
                "authors": [
                    {
                        "name": "Matthew Sprintson"
                    },
                    {
                        "name": "Edward Oughton"
                    }
                ],
                "author_detail": {
                    "name": "Edward Oughton"
                },
                "author": "Edward Oughton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.02431v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.02431v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15484v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15484v2",
                "updated": "2024-12-24T04:42:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    42,
                    49,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-20T01:37:22Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    1,
                    37,
                    22,
                    4,
                    355,
                    0
                ],
                "title": "Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and\n  Dual Evaluation Metrics for Factuality and Coverage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and\n  Dual Evaluation Metrics for Factuality and Coverage"
                },
                "summary": "Multimodal large language models (MLLMs) excel at generating highly detailed\ncaptions but often produce hallucinations. Our analysis reveals that existing\nhallucination detection methods struggle with detailed captions. We attribute\nthis to the increasing reliance of MLLMs on their generated text, rather than\nthe input image, as the sequence length grows. To address this issue, we\npropose a multiagent approach that leverages LLM-MLLM collaboration to correct\ngiven captions. Additionally, we introduce an evaluation framework and a\nbenchmark dataset to facilitate the systematic analysis of detailed captions.\nOur experiments demonstrate that our proposed evaluation method better aligns\nwith human judgments of factuality than existing metrics and that existing\napproaches to improve the MLLM factuality may fall short in hyper-detailed\nimage captioning tasks. In contrast, our proposed method significantly enhances\nthe factual accuracy of captions, even improving those generated by GPT-4V.\nFinally, we highlight a limitation of VQA-centric benchmarking by demonstrating\nthat an MLLM's performance on VQA benchmarks may not correlate with its ability\nto generate detailed image captions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) excel at generating highly detailed\ncaptions but often produce hallucinations. Our analysis reveals that existing\nhallucination detection methods struggle with detailed captions. We attribute\nthis to the increasing reliance of MLLMs on their generated text, rather than\nthe input image, as the sequence length grows. To address this issue, we\npropose a multiagent approach that leverages LLM-MLLM collaboration to correct\ngiven captions. Additionally, we introduce an evaluation framework and a\nbenchmark dataset to facilitate the systematic analysis of detailed captions.\nOur experiments demonstrate that our proposed evaluation method better aligns\nwith human judgments of factuality than existing metrics and that existing\napproaches to improve the MLLM factuality may fall short in hyper-detailed\nimage captioning tasks. In contrast, our proposed method significantly enhances\nthe factual accuracy of captions, even improving those generated by GPT-4V.\nFinally, we highlight a limitation of VQA-centric benchmarking by demonstrating\nthat an MLLM's performance on VQA benchmarks may not correlate with its ability\nto generate detailed image captions."
                },
                "authors": [
                    {
                        "name": "Saehyung Lee"
                    },
                    {
                        "name": "Seunghyun Yoon"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Jing Shi"
                    },
                    {
                        "name": "Sungroh Yoon"
                    }
                ],
                "author_detail": {
                    "name": "Sungroh Yoon"
                },
                "author": "Sungroh Yoon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15484v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15484v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15268v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15268v2",
                "updated": "2024-12-24T04:38:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    38,
                    57,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-17T06:28:28Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    6,
                    28,
                    28,
                    1,
                    352,
                    0
                ],
                "title": "Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic\n  Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic\n  Knowledge Graph"
                },
                "summary": "The rapid growth of social media platforms has raised significant concerns\nregarding online content toxicity. When Large Language Models (LLMs) are used\nfor toxicity detection, two key challenges emerge: 1) the absence of\ndomain-specific toxic knowledge leads to false negatives; 2) the excessive\nsensitivity of LLMs to toxic speech results in false positives, limiting\nfreedom of speech. To address these issues, we propose a novel method called\nMetaTox, leveraging graph search on a meta-toxic knowledge graph to enhance\nhatred and toxicity detection. First, we construct a comprehensive meta-toxic\nknowledge graph by utilizing LLMs to extract toxic information through a\nthree-step pipeline, with toxic benchmark datasets serving as corpora. Second,\nwe query the graph via retrieval and ranking processes to supplement accurate,\nrelevant toxic knowledge. Extensive experiments and in-depth case studies\nacross multiple datasets demonstrate that our MetaTox significantly decreases\nthe false positive rate while boosting overall toxicity detection performance.\nOur code will be available soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of social media platforms has raised significant concerns\nregarding online content toxicity. When Large Language Models (LLMs) are used\nfor toxicity detection, two key challenges emerge: 1) the absence of\ndomain-specific toxic knowledge leads to false negatives; 2) the excessive\nsensitivity of LLMs to toxic speech results in false positives, limiting\nfreedom of speech. To address these issues, we propose a novel method called\nMetaTox, leveraging graph search on a meta-toxic knowledge graph to enhance\nhatred and toxicity detection. First, we construct a comprehensive meta-toxic\nknowledge graph by utilizing LLMs to extract toxic information through a\nthree-step pipeline, with toxic benchmark datasets serving as corpora. Second,\nwe query the graph via retrieval and ranking processes to supplement accurate,\nrelevant toxic knowledge. Extensive experiments and in-depth case studies\nacross multiple datasets demonstrate that our MetaTox significantly decreases\nthe false positive rate while boosting overall toxicity detection performance.\nOur code will be available soon."
                },
                "authors": [
                    {
                        "name": "Yibo Zhao"
                    },
                    {
                        "name": "Jiapeng Zhu"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Xiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Li"
                },
                "author": "Xiang Li",
                "arxiv_comment": "8 pages of content",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15268v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15268v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18161v1",
                "updated": "2024-12-24T04:37:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    37,
                    7,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T04:37:07Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    37,
                    7,
                    1,
                    359,
                    0
                ],
                "title": "VISION: A Modular AI Assistant for Natural Human-Instrument Interaction\n  at Scientific User Facilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VISION: A Modular AI Assistant for Natural Human-Instrument Interaction\n  at Scientific User Facilities"
                },
                "summary": "Scientific user facilities, such as synchrotron beamlines, are equipped with\na wide array of hardware and software tools that require a codebase for\nhuman-computer-interaction. This often necessitates developers to be involved\nto establish connection between users/researchers and the complex\ninstrumentation. The advent of generative AI presents an opportunity to bridge\nthis knowledge gap, enabling seamless communication and efficient experimental\nworkflows. Here we present a modular architecture for the Virtual Scientific\nCompanion (VISION) by assembling multiple AI-enabled cognitive blocks that each\nscaffolds large language models (LLMs) for a specialized task. With VISION, we\nperformed LLM-based operation on the beamline workstation with low latency and\ndemonstrated the first voice-controlled experiment at an X-ray scattering\nbeamline. The modular and scalable architecture allows for easy adaptation to\nnew instrument and capabilities. Development on natural language-based\nscientific experimentation is a building block for an impending future where a\nscience exocortex -- a synthetic extension to the cognition of scientists --\nmay radically transform scientific practice and discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific user facilities, such as synchrotron beamlines, are equipped with\na wide array of hardware and software tools that require a codebase for\nhuman-computer-interaction. This often necessitates developers to be involved\nto establish connection between users/researchers and the complex\ninstrumentation. The advent of generative AI presents an opportunity to bridge\nthis knowledge gap, enabling seamless communication and efficient experimental\nworkflows. Here we present a modular architecture for the Virtual Scientific\nCompanion (VISION) by assembling multiple AI-enabled cognitive blocks that each\nscaffolds large language models (LLMs) for a specialized task. With VISION, we\nperformed LLM-based operation on the beamline workstation with low latency and\ndemonstrated the first voice-controlled experiment at an X-ray scattering\nbeamline. The modular and scalable architecture allows for easy adaptation to\nnew instrument and capabilities. Development on natural language-based\nscientific experimentation is a building block for an impending future where a\nscience exocortex -- a synthetic extension to the cognition of scientists --\nmay radically transform scientific practice and discovery."
                },
                "authors": [
                    {
                        "name": "Shray Mathur"
                    },
                    {
                        "name": "Noah van der Vleuten"
                    },
                    {
                        "name": "Kevin Yager"
                    },
                    {
                        "name": "Esther Tsai"
                    }
                ],
                "author_detail": {
                    "name": "Esther Tsai"
                },
                "author": "Esther Tsai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18156v1",
                "updated": "2024-12-24T04:28:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    28,
                    42,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T04:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    28,
                    42,
                    1,
                    359,
                    0
                ],
                "title": "scReader: Prompting Large Language Models to Interpret scRNA-seq Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "scReader: Prompting Large Language Models to Interpret scRNA-seq Data"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable advancements,\nprimarily due to their capabilities in modeling the hidden relationships within\ntext sequences. This innovation presents a unique opportunity in the field of\nlife sciences, where vast collections of single-cell omics data from multiple\nspecies provide a foundation for training foundational models. However, the\nchallenge lies in the disparity of data scales across different species,\nhindering the development of a comprehensive model for interpreting genetic\ndata across diverse organisms. In this study, we propose an innovative hybrid\napproach that integrates the general knowledge capabilities of LLMs with\ndomain-specific representation models for single-cell omics data\ninterpretation. We begin by focusing on genes as the fundamental unit of\nrepresentation. Gene representations are initialized using functional\ndescriptions, leveraging the strengths of mature language models such as\nLLaMA-2. By inputting single-cell gene-level expression data with prompts, we\neffectively model cellular representations based on the differential expression\nlevels of genes across various species and cell types. In the experiments, we\nconstructed developmental cells from humans and mice, specifically targeting\ncells that are challenging to annotate. We evaluated our methodology through\nbasic tasks such as cell annotation and visualization analysis. The results\ndemonstrate the efficacy of our approach compared to other methods using LLMs,\nhighlighting significant improvements in accuracy and interoperability. Our\nhybrid approach enhances the representation of single-cell data and offers a\nrobust framework for future research in cross-species genetic analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable advancements,\nprimarily due to their capabilities in modeling the hidden relationships within\ntext sequences. This innovation presents a unique opportunity in the field of\nlife sciences, where vast collections of single-cell omics data from multiple\nspecies provide a foundation for training foundational models. However, the\nchallenge lies in the disparity of data scales across different species,\nhindering the development of a comprehensive model for interpreting genetic\ndata across diverse organisms. In this study, we propose an innovative hybrid\napproach that integrates the general knowledge capabilities of LLMs with\ndomain-specific representation models for single-cell omics data\ninterpretation. We begin by focusing on genes as the fundamental unit of\nrepresentation. Gene representations are initialized using functional\ndescriptions, leveraging the strengths of mature language models such as\nLLaMA-2. By inputting single-cell gene-level expression data with prompts, we\neffectively model cellular representations based on the differential expression\nlevels of genes across various species and cell types. In the experiments, we\nconstructed developmental cells from humans and mice, specifically targeting\ncells that are challenging to annotate. We evaluated our methodology through\nbasic tasks such as cell annotation and visualization analysis. The results\ndemonstrate the efficacy of our approach compared to other methods using LLMs,\nhighlighting significant improvements in accuracy and interoperability. Our\nhybrid approach enhances the representation of single-cell data and offers a\nrobust framework for future research in cross-species genetic analysis."
                },
                "authors": [
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Qingqing Long"
                    },
                    {
                        "name": "Yuanchun Zhou"
                    },
                    {
                        "name": "Meng Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Meng Xiao"
                },
                "author": "Meng Xiao",
                "arxiv_comment": "8 pages, Accepted by ICDM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17153v2",
                "updated": "2024-12-24T04:21:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    21,
                    15,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-22T20:21:54Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    20,
                    21,
                    54,
                    6,
                    357,
                    0
                ],
                "title": "Distilled Decoding 1: One-step Sampling of Image Auto-regressive Models\n  with Flow Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilled Decoding 1: One-step Sampling of Image Auto-regressive Models\n  with Flow Matching"
                },
                "summary": "Autoregressive (AR) models have achieved state-of-the-art performance in text\nand image generation but suffer from slow generation due to the token-by-token\nprocess. We ask an ambitious question: can a pre-trained AR model be adapted to\ngenerate outputs in just one or two steps? If successful, this would\nsignificantly advance the development and deployment of AR models. We notice\nthat existing works that try to speed up AR generation by generating multiple\ntokens at once fundamentally cannot capture the output distribution due to the\nconditional dependencies between tokens, limiting their effectiveness for\nfew-step generation. To address this, we propose Distilled Decoding (DD), which\nuses flow matching to create a deterministic mapping from Gaussian distribution\nto the output distribution of the pre-trained AR model. We then train a network\nto distill this mapping, enabling few-step generation. DD doesn't need the\ntraining data of the original AR model, making it more practical. We evaluate\nDD on state-of-the-art image AR models and present promising results on\nImageNet-256. For VAR, which requires 10-step generation, DD enables one-step\ngeneration (6.3$\\times$ speed-up), with an acceptable increase in FID from 4.19\nto 9.96. For LlamaGen, DD reduces generation from 256 steps to 1, achieving an\n217.8$\\times$ speed-up with a comparable FID increase from 4.11 to 11.35. In\nboth cases, baseline methods completely fail with FID>100. DD also excels on\ntext-to-image generation, reducing the generation from 256 steps to 2 for\nLlamaGen with minimal FID increase from 25.70 to 28.95. As the first work to\ndemonstrate the possibility of one-step generation for image AR models, DD\nchallenges the prevailing notion that AR models are inherently slow, and opens\nup new opportunities for efficient AR generation. The project website is at\nhttps://imagination-research.github.io/distilled-decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) models have achieved state-of-the-art performance in text\nand image generation but suffer from slow generation due to the token-by-token\nprocess. We ask an ambitious question: can a pre-trained AR model be adapted to\ngenerate outputs in just one or two steps? If successful, this would\nsignificantly advance the development and deployment of AR models. We notice\nthat existing works that try to speed up AR generation by generating multiple\ntokens at once fundamentally cannot capture the output distribution due to the\nconditional dependencies between tokens, limiting their effectiveness for\nfew-step generation. To address this, we propose Distilled Decoding (DD), which\nuses flow matching to create a deterministic mapping from Gaussian distribution\nto the output distribution of the pre-trained AR model. We then train a network\nto distill this mapping, enabling few-step generation. DD doesn't need the\ntraining data of the original AR model, making it more practical. We evaluate\nDD on state-of-the-art image AR models and present promising results on\nImageNet-256. For VAR, which requires 10-step generation, DD enables one-step\ngeneration (6.3$\\times$ speed-up), with an acceptable increase in FID from 4.19\nto 9.96. For LlamaGen, DD reduces generation from 256 steps to 1, achieving an\n217.8$\\times$ speed-up with a comparable FID increase from 4.11 to 11.35. In\nboth cases, baseline methods completely fail with FID>100. DD also excels on\ntext-to-image generation, reducing the generation from 256 steps to 2 for\nLlamaGen with minimal FID increase from 25.70 to 28.95. As the first work to\ndemonstrate the possibility of one-step generation for image AR models, DD\nchallenges the prevailing notion that AR models are inherently slow, and opens\nup new opportunities for efficient AR generation. The project website is at\nhttps://imagination-research.github.io/distilled-decoding."
                },
                "authors": [
                    {
                        "name": "Enshu Liu"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Zinan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zinan Lin"
                },
                "author": "Zinan Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18154v1",
                "updated": "2024-12-24T04:20:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    20,
                    43,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T04:20:43Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    20,
                    43,
                    1,
                    359,
                    0
                ],
                "title": "GeneSUM: Large Language Model-based Gene Summary Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeneSUM: Large Language Model-based Gene Summary Extraction"
                },
                "summary": "Emerging topics in biomedical research are continuously expanding, providing\na wealth of information about genes and their function. This rapid\nproliferation of knowledge presents unprecedented opportunities for scientific\ndiscovery and formidable challenges for researchers striving to keep abreast of\nthe latest advancements. One significant challenge is navigating the vast\ncorpus of literature to extract vital gene-related information, a\ntime-consuming and cumbersome task. To enhance the efficiency of this process,\nit is crucial to address several key challenges: (1) the overwhelming volume of\nliterature, (2) the complexity of gene functions, and (3) the automated\nintegration and generation. In response, we propose GeneSUM, a two-stage\nautomated gene summary extractor utilizing a large language model (LLM). Our\napproach retrieves and eliminates redundancy of target gene literature and then\nfine-tunes the LLM to refine and streamline the summarization process. We\nconducted extensive experiments to validate the efficacy of our proposed\nframework. The results demonstrate that LLM significantly enhances the\nintegration of gene-specific information, allowing more efficient\ndecision-making in ongoing research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging topics in biomedical research are continuously expanding, providing\na wealth of information about genes and their function. This rapid\nproliferation of knowledge presents unprecedented opportunities for scientific\ndiscovery and formidable challenges for researchers striving to keep abreast of\nthe latest advancements. One significant challenge is navigating the vast\ncorpus of literature to extract vital gene-related information, a\ntime-consuming and cumbersome task. To enhance the efficiency of this process,\nit is crucial to address several key challenges: (1) the overwhelming volume of\nliterature, (2) the complexity of gene functions, and (3) the automated\nintegration and generation. In response, we propose GeneSUM, a two-stage\nautomated gene summary extractor utilizing a large language model (LLM). Our\napproach retrieves and eliminates redundancy of target gene literature and then\nfine-tunes the LLM to refine and streamline the summarization process. We\nconducted extensive experiments to validate the efficacy of our proposed\nframework. The results demonstrate that LLM significantly enhances the\nintegration of gene-specific information, allowing more efficient\ndecision-making in ongoing research."
                },
                "authors": [
                    {
                        "name": "Zhijian Chen"
                    },
                    {
                        "name": "Chuan Hu"
                    },
                    {
                        "name": "Min Wu"
                    },
                    {
                        "name": "Qingqing Long"
                    },
                    {
                        "name": "Xuezhi Wang"
                    },
                    {
                        "name": "Yuanchun Zhou"
                    },
                    {
                        "name": "Meng Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Meng Xiao"
                },
                "author": "Meng Xiao",
                "arxiv_comment": "7 pages, Accepted by BIBM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16746v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16746v2",
                "updated": "2024-12-24T04:04:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    4,
                    54,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-21T19:42:40Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    19,
                    42,
                    40,
                    5,
                    356,
                    0
                ],
                "title": "Unpacking Political Bias in Large Language Models: Insights Across Topic\n  Polarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unpacking Political Bias in Large Language Models: Insights Across Topic\n  Polarization"
                },
                "summary": "Large Language Models (LLMs) have been widely used to generate responses on\nsocial topics due to their world knowledge and generative capabilities. Beyond\nreasoning and generation performance, political bias is an essential issue that\nwarrants attention. Political bias, as a universal phenomenon in human society,\nmay be transferred to LLMs and distort LLMs' behaviors of information\nacquisition and dissemination with humans, leading to unequal access among\ndifferent groups of people. To prevent LLMs from reproducing and reinforcing\npolitical biases, and to encourage fairer LLM-human interactions,\ncomprehensively examining political bias in popular LLMs becomes urgent and\ncrucial.\n  In this study, we systematically measure the political biases in a wide range\nof LLMs, using a curated set of questions addressing political bias in various\ncontexts. Our findings reveal distinct patterns in how LLMs respond to\npolitical topics. For highly polarized topics, most LLMs exhibit a pronounced\nleft-leaning bias. Conversely, less polarized topics elicit greater consensus,\nwith similar response patterns across different LLMs. Additionally, we analyze\nhow LLM characteristics, including release date, model scale, and region of\norigin affect political bias. The results indicate political biases evolve with\nmodel scale and release date, and are also influenced by regional factors of\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely used to generate responses on\nsocial topics due to their world knowledge and generative capabilities. Beyond\nreasoning and generation performance, political bias is an essential issue that\nwarrants attention. Political bias, as a universal phenomenon in human society,\nmay be transferred to LLMs and distort LLMs' behaviors of information\nacquisition and dissemination with humans, leading to unequal access among\ndifferent groups of people. To prevent LLMs from reproducing and reinforcing\npolitical biases, and to encourage fairer LLM-human interactions,\ncomprehensively examining political bias in popular LLMs becomes urgent and\ncrucial.\n  In this study, we systematically measure the political biases in a wide range\nof LLMs, using a curated set of questions addressing political bias in various\ncontexts. Our findings reveal distinct patterns in how LLMs respond to\npolitical topics. For highly polarized topics, most LLMs exhibit a pronounced\nleft-leaning bias. Conversely, less polarized topics elicit greater consensus,\nwith similar response patterns across different LLMs. Additionally, we analyze\nhow LLM characteristics, including release date, model scale, and region of\norigin affect political bias. The results indicate political biases evolve with\nmodel scale and release date, and are also influenced by regional factors of\nLLMs."
                },
                "authors": [
                    {
                        "name": "Kaiqi Yang"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Yucheng Chu"
                    },
                    {
                        "name": "Yuping Lin"
                    },
                    {
                        "name": "Tai-Quan Peng"
                    },
                    {
                        "name": "Hui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hui Liu"
                },
                "author": "Hui Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16746v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16746v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18148v1",
                "updated": "2024-12-24T04:04:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    4,
                    54,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T04:04:54Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    4,
                    54,
                    1,
                    359,
                    0
                ],
                "title": "Are We in the AI-Generated Text World Already? Quantifying and\n  Monitoring AIGT on Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are We in the AI-Generated Text World Already? Quantifying and\n  Monitoring AIGT on Social Media"
                },
                "summary": "Social media platforms are experiencing a growing presence of AI-Generated\nTexts (AIGTs). However, the misuse of AIGTs could have profound implications\nfor public opinion, such as spreading misinformation and manipulating\nnarratives. Despite its importance, a systematic study to assess the prevalence\nof AIGTs on social media is still lacking. To address this gap, this paper aims\nto quantify, monitor, and analyze the AIGTs on online social media platforms.\nWe first collect a dataset (SM-D) with around 2.4M posts from 3 major social\nmedia platforms: Medium, Quora, and Reddit. Then, we construct a diverse\ndataset (AIGTBench) to train and evaluate AIGT detectors. AIGTBench combines\npopular open-source datasets and our AIGT datasets generated from social media\ntexts by 12 LLMs, serving as a benchmark for evaluating mainstream detectors.\nWith this setup, we identify the best-performing detector (OSM-Det). We then\napply OSM-Det to SM-D to track AIGTs over time and observe different trends of\nAI Attribution Rate (AAR) across social media platforms from January 2022 to\nOctober 2024. Specifically, Medium and Quora exhibit marked increases in AAR,\nrising from 1.77% to 37.03% and 2.06% to 38.95%, respectively. In contrast,\nReddit shows slower growth, with AAR increasing from 1.31% to 2.45% over the\nsame period. Our further analysis indicates that AIGTs differ from\nhuman-written texts across several dimensions, including linguistic patterns,\ntopic distributions, engagement levels, and the follower distribution of\nauthors. We envision our analysis and findings on AIGTs in social media can\nshed light on future research in this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social media platforms are experiencing a growing presence of AI-Generated\nTexts (AIGTs). However, the misuse of AIGTs could have profound implications\nfor public opinion, such as spreading misinformation and manipulating\nnarratives. Despite its importance, a systematic study to assess the prevalence\nof AIGTs on social media is still lacking. To address this gap, this paper aims\nto quantify, monitor, and analyze the AIGTs on online social media platforms.\nWe first collect a dataset (SM-D) with around 2.4M posts from 3 major social\nmedia platforms: Medium, Quora, and Reddit. Then, we construct a diverse\ndataset (AIGTBench) to train and evaluate AIGT detectors. AIGTBench combines\npopular open-source datasets and our AIGT datasets generated from social media\ntexts by 12 LLMs, serving as a benchmark for evaluating mainstream detectors.\nWith this setup, we identify the best-performing detector (OSM-Det). We then\napply OSM-Det to SM-D to track AIGTs over time and observe different trends of\nAI Attribution Rate (AAR) across social media platforms from January 2022 to\nOctober 2024. Specifically, Medium and Quora exhibit marked increases in AAR,\nrising from 1.77% to 37.03% and 2.06% to 38.95%, respectively. In contrast,\nReddit shows slower growth, with AAR increasing from 1.31% to 2.45% over the\nsame period. Our further analysis indicates that AIGTs differ from\nhuman-written texts across several dimensions, including linguistic patterns,\ntopic distributions, engagement levels, and the follower distribution of\nauthors. We envision our analysis and findings on AIGTs in social media can\nshed light on future research in this domain."
                },
                "authors": [
                    {
                        "name": "Zhen Sun"
                    },
                    {
                        "name": "Zongmin Zhang"
                    },
                    {
                        "name": "Xinyue Shen"
                    },
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Yule Liu"
                    },
                    {
                        "name": "Michael Backes"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Xinlei He"
                    }
                ],
                "author_detail": {
                    "name": "Xinlei He"
                },
                "author": "Xinlei He",
                "arxiv_comment": "24 pages,18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07985v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07985v3",
                "updated": "2024-12-24T04:04:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    4,
                    30,
                    1,
                    359,
                    0
                ],
                "published": "2024-10-10T14:39:33Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    39,
                    33,
                    3,
                    284,
                    0
                ],
                "title": "Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large\n  Language Models"
                },
                "summary": "Recent advancements in large language models (LLMs) have led to significant\nbreakthroughs in mathematical reasoning capabilities. However, existing\nbenchmarks like GSM8K or MATH are now being solved with high accuracy (e.g.,\nOpenAI o1 achieves 94.8\\% on MATH dataset), indicating their inadequacy for\ntruly challenging these models. To bridge this gap, we propose a comprehensive\nand challenging benchmark specifically designed to assess LLMs' mathematical\nreasoning at the Olympiad level. Unlike existing Olympiad-related benchmarks,\nour dataset focuses exclusively on mathematics and comprises a vast collection\nof 4428 competition-level problems with rigorous human annotation. These\nproblems are meticulously categorized into over 33 sub-domains and span more\nthan 10 distinct difficulty levels, enabling a holistic assessment of model\nperformance in Olympiad-mathematical reasoning. Furthermore, we conducted an\nin-depth analysis based on this benchmark. Our experimental results show that\neven the most advanced models, OpenAI o1-mini and OpenAI o1-preview, struggle\nwith highly challenging Olympiad-level problems, with 60.54\\% and 52.55\\%\naccuracy, highlighting significant challenges in Olympiad-level mathematical\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have led to significant\nbreakthroughs in mathematical reasoning capabilities. However, existing\nbenchmarks like GSM8K or MATH are now being solved with high accuracy (e.g.,\nOpenAI o1 achieves 94.8\\% on MATH dataset), indicating their inadequacy for\ntruly challenging these models. To bridge this gap, we propose a comprehensive\nand challenging benchmark specifically designed to assess LLMs' mathematical\nreasoning at the Olympiad level. Unlike existing Olympiad-related benchmarks,\nour dataset focuses exclusively on mathematics and comprises a vast collection\nof 4428 competition-level problems with rigorous human annotation. These\nproblems are meticulously categorized into over 33 sub-domains and span more\nthan 10 distinct difficulty levels, enabling a holistic assessment of model\nperformance in Olympiad-mathematical reasoning. Furthermore, we conducted an\nin-depth analysis based on this benchmark. Our experimental results show that\neven the most advanced models, OpenAI o1-mini and OpenAI o1-preview, struggle\nwith highly challenging Olympiad-level problems, with 60.54\\% and 52.55\\%\naccuracy, highlighting significant challenges in Olympiad-level mathematical\nreasoning."
                },
                "authors": [
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Feifan Song"
                    },
                    {
                        "name": "Zhe Yang"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Chenghao Ma"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Runxin Xu"
                    },
                    {
                        "name": "Zhengyang Tang"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Daoguang Zan"
                    },
                    {
                        "name": "Shanghaoran Quan"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Lei Sha"
                    },
                    {
                        "name": "Yichang Zhang"
                    },
                    {
                        "name": "Xuancheng Ren"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Baobao Chang"
                    }
                ],
                "author_detail": {
                    "name": "Baobao Chang"
                },
                "author": "Baobao Chang",
                "arxiv_comment": "30 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07985v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07985v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.06824v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.06824v4",
                "updated": "2024-12-24T04:03:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    4,
                    3,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-01-12T00:50:04Z",
                "published_parsed": [
                    2024,
                    1,
                    12,
                    0,
                    50,
                    4,
                    4,
                    12,
                    0
                ],
                "title": "Revisiting Jailbreaking for Large Language Models: A Representation\n  Engineering Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Jailbreaking for Large Language Models: A Representation\n  Engineering Perspective"
                },
                "summary": "The recent surge in jailbreaking attacks has revealed significant\nvulnerabilities in Large Language Models (LLMs) when exposed to malicious\ninputs. While various defense strategies have been proposed to mitigate these\nthreats, there has been limited research into the underlying mechanisms that\nmake LLMs vulnerable to such attacks. In this study, we suggest that the\nself-safeguarding capability of LLMs is linked to specific activity patterns\nwithin their representation space. Although these patterns have little impact\non the semantic content of the generated text, they play a crucial role in\nshaping LLM behavior under jailbreaking attacks. Our findings demonstrate that\nthese patterns can be detected with just a few pairs of contrastive queries.\nExtensive experimentation shows that the robustness of LLMs against\njailbreaking can be manipulated by weakening or strengthening these patterns.\nFurther visual analysis provides additional evidence for our conclusions,\nproviding new insights into the jailbreaking phenomenon. These findings\nhighlight the importance of addressing the potential misuse of open-source LLMs\nwithin the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent surge in jailbreaking attacks has revealed significant\nvulnerabilities in Large Language Models (LLMs) when exposed to malicious\ninputs. While various defense strategies have been proposed to mitigate these\nthreats, there has been limited research into the underlying mechanisms that\nmake LLMs vulnerable to such attacks. In this study, we suggest that the\nself-safeguarding capability of LLMs is linked to specific activity patterns\nwithin their representation space. Although these patterns have little impact\non the semantic content of the generated text, they play a crucial role in\nshaping LLM behavior under jailbreaking attacks. Our findings demonstrate that\nthese patterns can be detected with just a few pairs of contrastive queries.\nExtensive experimentation shows that the robustness of LLMs against\njailbreaking can be manipulated by weakening or strengthening these patterns.\nFurther visual analysis provides additional evidence for our conclusions,\nproviding new insights into the jailbreaking phenomenon. These findings\nhighlight the importance of addressing the potential misuse of open-source LLMs\nwithin the community."
                },
                "authors": [
                    {
                        "name": "Tianlong Li"
                    },
                    {
                        "name": "Zhenghua Wang"
                    },
                    {
                        "name": "Wenhao Liu"
                    },
                    {
                        "name": "Muling Wu"
                    },
                    {
                        "name": "Shihan Dou"
                    },
                    {
                        "name": "Changze Lv"
                    },
                    {
                        "name": "Xiaohua Wang"
                    },
                    {
                        "name": "Xiaoqing Zheng"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "Accepted by COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.06824v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.06824v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11934v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11934v2",
                "updated": "2024-12-24T03:55:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    3,
                    55,
                    40,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-16T16:20:41Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    20,
                    41,
                    0,
                    351,
                    0
                ],
                "title": "Stepwise Reasoning Error Disruption Attack of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stepwise Reasoning Error Disruption Attack of LLMs"
                },
                "summary": "Large language models (LLMs) have made remarkable strides in complex\nreasoning tasks, but their safety and robustness in reasoning processes remain\nunderexplored. Existing attacks on LLM reasoning are constrained by specific\nsettings or lack of imperceptibility, limiting their feasibility and\ngeneralizability. To address these challenges, we propose the Stepwise\nrEasoning Error Disruption (SEED) attack, which subtly injects errors into\nprior reasoning steps to mislead the model into producing incorrect subsequent\nreasoning and final answers. Unlike previous methods, SEED is compatible with\nzero-shot and few-shot settings, maintains the natural reasoning flow, and\nensures covert execution without modifying the instruction. Extensive\nexperiments on four datasets across four different models demonstrate SEED's\neffectiveness, revealing the vulnerabilities of LLMs to disruptions in\nreasoning processes. These findings underscore the need for greater attention\nto the robustness of LLM reasoning to ensure safety in practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made remarkable strides in complex\nreasoning tasks, but their safety and robustness in reasoning processes remain\nunderexplored. Existing attacks on LLM reasoning are constrained by specific\nsettings or lack of imperceptibility, limiting their feasibility and\ngeneralizability. To address these challenges, we propose the Stepwise\nrEasoning Error Disruption (SEED) attack, which subtly injects errors into\nprior reasoning steps to mislead the model into producing incorrect subsequent\nreasoning and final answers. Unlike previous methods, SEED is compatible with\nzero-shot and few-shot settings, maintains the natural reasoning flow, and\nensures covert execution without modifying the instruction. Extensive\nexperiments on four datasets across four different models demonstrate SEED's\neffectiveness, revealing the vulnerabilities of LLMs to disruptions in\nreasoning processes. These findings underscore the need for greater attention\nto the robustness of LLM reasoning to ensure safety in practical applications."
                },
                "authors": [
                    {
                        "name": "Jingyu Peng"
                    },
                    {
                        "name": "Maolin Wang"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Wanyu Wang"
                    },
                    {
                        "name": "Pengyue Jia"
                    },
                    {
                        "name": "Qidong Liu"
                    },
                    {
                        "name": "Ruocheng Guo"
                    },
                    {
                        "name": "Qi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Liu"
                },
                "author": "Qi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11934v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11934v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15295v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15295v2",
                "updated": "2024-12-24T03:43:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    3,
                    43,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-19T09:03:39Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    9,
                    3,
                    39,
                    3,
                    354,
                    0
                ],
                "title": "Log-Time K-Means Clustering for 1D Data: Novel Approaches with Proof and\n  Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Log-Time K-Means Clustering for 1D Data: Novel Approaches with Proof and\n  Implementation"
                },
                "summary": "Clustering is a key task in machine learning, with $k$-means being widely\nused for its simplicity and effectiveness. While 1D clustering is common,\nexisting methods often fail to exploit the structure of 1D data, leading to\ninefficiencies. This thesis introduces optimized algorithms for $k$-means++\ninitialization and Lloyd's algorithm, leveraging sorted data, prefix sums, and\nbinary search for improved computational performance. The main contributions\nare: (1) an optimized $k$-cluster algorithm achieving $O(l \\cdot k^2 \\cdot \\log\nn)$ complexity for greedy $k$-means++ initialization and $O(i \\cdot k \\cdot\n\\log n)$ for Lloyd's algorithm, where $l$ is the number of greedy $k$-means++\nlocal trials, and $i$ is the number of Lloyd's algorithm iterations, and (2) a\nbinary search-based two-cluster algorithm, achieving $O(\\log n)$ runtime with\ndeterministic convergence to a Lloyd's algorithm local minimum. Benchmarks\ndemonstrate over a 4500x speedup compared to scikit-learn for large datasets\nwhile maintaining clustering quality measured by within-cluster sum of squares\n(WCSS). Additionally, the algorithms achieve a 300x speedup in an LLM\nquantization task, highlighting their utility in emerging applications. This\nthesis bridges theory and practice for 1D $k$-means clustering, delivering\nefficient and sound algorithms implemented in a JIT-optimized open-source\nPython library.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clustering is a key task in machine learning, with $k$-means being widely\nused for its simplicity and effectiveness. While 1D clustering is common,\nexisting methods often fail to exploit the structure of 1D data, leading to\ninefficiencies. This thesis introduces optimized algorithms for $k$-means++\ninitialization and Lloyd's algorithm, leveraging sorted data, prefix sums, and\nbinary search for improved computational performance. The main contributions\nare: (1) an optimized $k$-cluster algorithm achieving $O(l \\cdot k^2 \\cdot \\log\nn)$ complexity for greedy $k$-means++ initialization and $O(i \\cdot k \\cdot\n\\log n)$ for Lloyd's algorithm, where $l$ is the number of greedy $k$-means++\nlocal trials, and $i$ is the number of Lloyd's algorithm iterations, and (2) a\nbinary search-based two-cluster algorithm, achieving $O(\\log n)$ runtime with\ndeterministic convergence to a Lloyd's algorithm local minimum. Benchmarks\ndemonstrate over a 4500x speedup compared to scikit-learn for large datasets\nwhile maintaining clustering quality measured by within-cluster sum of squares\n(WCSS). Additionally, the algorithms achieve a 300x speedup in an LLM\nquantization task, highlighting their utility in emerging applications. This\nthesis bridges theory and practice for 1D $k$-means clustering, delivering\nefficient and sound algorithms implemented in a JIT-optimized open-source\nPython library."
                },
                "authors": [
                    {
                        "name": "Jake Hyun"
                    }
                ],
                "author_detail": {
                    "name": "Jake Hyun"
                },
                "author": "Jake Hyun",
                "arxiv_comment": "Undergraduate thesis, Department of Computer Science and Engineering,\n  Seoul National University. Minor revisions incorporated post-submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15295v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18135v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18135v1",
                "updated": "2024-12-24T03:43:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    3,
                    43,
                    15,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T03:43:15Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    3,
                    43,
                    15,
                    1,
                    359,
                    0
                ],
                "title": "LSAQ: Layer-Specific Adaptive Quantization for Large Language Model\n  Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSAQ: Layer-Specific Adaptive Quantization for Large Language Model\n  Deployment"
                },
                "summary": "As large language models (LLMs) demonstrate exceptional performance across\nvarious domains, the deployment of these models on edge devices has emerged as\na new trend. Quantization techniques, which reduce the size and memory\nfootprint of LLMs, are effective for enabling deployment on\nresource-constrained edge devices. However, existing one-size-fits-all\nquantization methods often fail to dynamically adjust the memory consumption of\nLLMs based on specific hardware characteristics and usage scenarios. To address\nthis limitation, we propose LSAQ (Layer-Specific Adaptive Quantization), a\nsystem for adaptive quantization and dynamic deployment of LLMs based on layer\nimportance. LSAQ evaluates layer importance by constructing top-k token sets\nfrom the inputs and outputs of each layer and calculating their Jaccard\ncoefficient. Using this evaluation, the system adaptively adjusts quantization\nstrategies in real time according to the resource availability of edge devices,\nassigning different precision levels to layers of varying importance. This\napproach significantly reduces the storage requirements of LLMs while\nmaintaining model performance, enabling efficient deployment across diverse\nhardware platforms and usage scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) demonstrate exceptional performance across\nvarious domains, the deployment of these models on edge devices has emerged as\na new trend. Quantization techniques, which reduce the size and memory\nfootprint of LLMs, are effective for enabling deployment on\nresource-constrained edge devices. However, existing one-size-fits-all\nquantization methods often fail to dynamically adjust the memory consumption of\nLLMs based on specific hardware characteristics and usage scenarios. To address\nthis limitation, we propose LSAQ (Layer-Specific Adaptive Quantization), a\nsystem for adaptive quantization and dynamic deployment of LLMs based on layer\nimportance. LSAQ evaluates layer importance by constructing top-k token sets\nfrom the inputs and outputs of each layer and calculating their Jaccard\ncoefficient. Using this evaluation, the system adaptively adjusts quantization\nstrategies in real time according to the resource availability of edge devices,\nassigning different precision levels to layers of varying importance. This\napproach significantly reduces the storage requirements of LLMs while\nmaintaining model performance, enabling efficient deployment across diverse\nhardware platforms and usage scenarios."
                },
                "authors": [
                    {
                        "name": "Binrui Zeng"
                    },
                    {
                        "name": "Bin Ji"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Jie Yu"
                    },
                    {
                        "name": "Shasha Li"
                    },
                    {
                        "name": "Jun Ma"
                    },
                    {
                        "name": "Xiaopeng Li"
                    },
                    {
                        "name": "Shangwen Wang"
                    },
                    {
                        "name": "Xinran Hong"
                    }
                ],
                "author_detail": {
                    "name": "Xinran Hong"
                },
                "author": "Xinran Hong",
                "arxiv_comment": "8 pages, 4 figures, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18135v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18135v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14332v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14332v3",
                "updated": "2024-12-24T03:27:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    3,
                    27,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-10-18T09:44:25Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    44,
                    25,
                    4,
                    292,
                    0
                ],
                "title": "Croc: Pretraining Large Multimodal Models with Cross-Modal Comprehension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Croc: Pretraining Large Multimodal Models with Cross-Modal Comprehension"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have catalyzed the\ndevelopment of Large Multimodal Models (LMMs). However, existing research\nprimarily focuses on tuning language and image instructions, ignoring the\ncritical pretraining phase where models learn to process textual and visual\nmodalities jointly. In this paper, we propose a new pretraining paradigm for\nLMMs to enhance the visual comprehension capabilities of LLMs by introducing a\nnovel cross-modal comprehension stage. Specifically, we design a dynamically\nlearnable prompt token pool and employ the Hungarian algorithm to replace part\nof the original visual tokens with the most relevant prompt tokens. Then, we\nconceptualize visual tokens as analogous to a \"foreign language\" for the LLMs\nand propose a mixed attention mechanism with bidirectional visual attention and\nunidirectional textual attention to comprehensively enhance the understanding\nof visual tokens. Meanwhile, we integrate a detailed caption generation task,\nleveraging rich descriptions to further facilitate LLMs in understanding visual\nsemantic information. After pretraining on 1.5 million publicly accessible\ndata, we present a new foundation model called Croc. Experimental results\ndemonstrate that Croc achieves new state-of-the-art performance on massive\nvision-language benchmarks. To support reproducibility and facilitate further\nresearch, we release the training code and pre-trained model weights at\nhttps://github.com/deepglint/Croc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have catalyzed the\ndevelopment of Large Multimodal Models (LMMs). However, existing research\nprimarily focuses on tuning language and image instructions, ignoring the\ncritical pretraining phase where models learn to process textual and visual\nmodalities jointly. In this paper, we propose a new pretraining paradigm for\nLMMs to enhance the visual comprehension capabilities of LLMs by introducing a\nnovel cross-modal comprehension stage. Specifically, we design a dynamically\nlearnable prompt token pool and employ the Hungarian algorithm to replace part\nof the original visual tokens with the most relevant prompt tokens. Then, we\nconceptualize visual tokens as analogous to a \"foreign language\" for the LLMs\nand propose a mixed attention mechanism with bidirectional visual attention and\nunidirectional textual attention to comprehensively enhance the understanding\nof visual tokens. Meanwhile, we integrate a detailed caption generation task,\nleveraging rich descriptions to further facilitate LLMs in understanding visual\nsemantic information. After pretraining on 1.5 million publicly accessible\ndata, we present a new foundation model called Croc. Experimental results\ndemonstrate that Croc achieves new state-of-the-art performance on massive\nvision-language benchmarks. To support reproducibility and facilitate further\nresearch, we release the training code and pre-trained model weights at\nhttps://github.com/deepglint/Croc."
                },
                "authors": [
                    {
                        "name": "Yin Xie"
                    },
                    {
                        "name": "Kaicheng Yang"
                    },
                    {
                        "name": "Ninghua Yang"
                    },
                    {
                        "name": "Weimo Deng"
                    },
                    {
                        "name": "Xiangzi Dai"
                    },
                    {
                        "name": "Tiancheng Gu"
                    },
                    {
                        "name": "Yumeng Wang"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Yongle Zhao"
                    },
                    {
                        "name": "Ziyong Feng"
                    },
                    {
                        "name": "Roy Miles"
                    },
                    {
                        "name": "Ismail Elezi"
                    },
                    {
                        "name": "Jiankang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Jiankang Deng"
                },
                "author": "Jiankang Deng",
                "arxiv_comment": "14 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14332v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14332v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00852v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00852v2",
                "updated": "2024-12-24T03:24:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    3,
                    24,
                    55,
                    1,
                    359,
                    0
                ],
                "published": "2024-10-30T11:22:37Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    11,
                    22,
                    37,
                    2,
                    304,
                    0
                ],
                "title": "EF-LLM: Energy Forecasting LLM with AI-assisted Automation, Enhanced\n  Sparse Prediction, Hallucination Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EF-LLM: Energy Forecasting LLM with AI-assisted Automation, Enhanced\n  Sparse Prediction, Hallucination Detection"
                },
                "summary": "Accurate prediction helps to achieve supply-demand balance in energy systems,\nsupporting decision-making and scheduling. Traditional models, lacking\nAI-assisted automation, rely on experts, incur high costs, and struggle with\nsparse data prediction. To address these challenges, we propose the Energy\nForecasting Large Language Model (EF-LLM), which integrates domain knowledge\nand temporal data for time-series forecasting, supporting both pre-forecast\noperations and post-forecast decision-support. EF-LLM's human-AI interaction\ncapabilities lower the entry barrier in forecasting tasks, reducing the need\nfor extra expert involvement. To achieve this, we propose a continual learning\napproach with updatable LoRA and a multi-channel architecture for aligning\nheterogeneous multimodal data, enabling EF-LLM to continually learn\nheterogeneous multimodal knowledge. In addition, EF-LLM enables accurate\npredictions under sparse data conditions through its ability to process\nmultimodal data. We propose Fusion Parameter-Efficient Fine-Tuning (F-PEFT)\nmethod to effectively leverage both time-series data and text for this purpose.\nEF-LLM is also the first energy-specific LLM to detect hallucinations and\nquantify their occurrence rate, achieved via multi-task learning, semantic\nsimilarity analysis, and ANOVA. We have achieved success in energy prediction\nscenarios for load, photovoltaic, and wind power forecast.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate prediction helps to achieve supply-demand balance in energy systems,\nsupporting decision-making and scheduling. Traditional models, lacking\nAI-assisted automation, rely on experts, incur high costs, and struggle with\nsparse data prediction. To address these challenges, we propose the Energy\nForecasting Large Language Model (EF-LLM), which integrates domain knowledge\nand temporal data for time-series forecasting, supporting both pre-forecast\noperations and post-forecast decision-support. EF-LLM's human-AI interaction\ncapabilities lower the entry barrier in forecasting tasks, reducing the need\nfor extra expert involvement. To achieve this, we propose a continual learning\napproach with updatable LoRA and a multi-channel architecture for aligning\nheterogeneous multimodal data, enabling EF-LLM to continually learn\nheterogeneous multimodal knowledge. In addition, EF-LLM enables accurate\npredictions under sparse data conditions through its ability to process\nmultimodal data. We propose Fusion Parameter-Efficient Fine-Tuning (F-PEFT)\nmethod to effectively leverage both time-series data and text for this purpose.\nEF-LLM is also the first energy-specific LLM to detect hallucinations and\nquantify their occurrence rate, achieved via multi-task learning, semantic\nsimilarity analysis, and ANOVA. We have achieved success in energy prediction\nscenarios for load, photovoltaic, and wind power forecast."
                },
                "authors": [
                    {
                        "name": "Zihang Qiu"
                    },
                    {
                        "name": "Chaojie Li"
                    },
                    {
                        "name": "Zhongyang Wang"
                    },
                    {
                        "name": "Renyou Xie"
                    },
                    {
                        "name": "Borui Zhang"
                    },
                    {
                        "name": "Huadong Mo"
                    },
                    {
                        "name": "Guo Chen"
                    },
                    {
                        "name": "Zhaoyang Dong"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoyang Dong"
                },
                "author": "Zhaoyang Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00852v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00852v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16844v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16844v2",
                "updated": "2024-12-24T03:02:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    3,
                    2,
                    32,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-22T03:43:51Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    3,
                    43,
                    51,
                    6,
                    357,
                    0
                ],
                "title": "Sim911: Towards Effective and Equitable 9-1-1 Dispatcher Training with\n  an LLM-Enabled Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sim911: Towards Effective and Equitable 9-1-1 Dispatcher Training with\n  an LLM-Enabled Simulation"
                },
                "summary": "Emergency response services are vital for enhancing public safety by\nsafeguarding the environment, property, and human lives. As frontline members\nof these services, 9-1-1 dispatchers have a direct impact on response times and\nthe overall effectiveness of emergency operations. However, traditional\ndispatcher training methods, which rely on role-playing by experienced\npersonnel, are labor-intensive, time-consuming, and often neglect the specific\nneeds of underserved communities. To address these challenges, we introduce\nSim911, the first training simulation for 9-1-1 dispatchers powered by Large\nLanguage Models (LLMs). Sim911 enhances training through three key technical\ninnovations: (1) knowledge construction, which utilizes archived 9-1-1 call\ndata to generate simulations that closely mirror real-world scenarios; (2)\ncontext-aware controlled generation, which employs dynamic prompts and vector\nbases to ensure that LLM behavior aligns with training objectives; and (3)\nvalidation with looped correction, which filters out low-quality responses and\nrefines the system performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergency response services are vital for enhancing public safety by\nsafeguarding the environment, property, and human lives. As frontline members\nof these services, 9-1-1 dispatchers have a direct impact on response times and\nthe overall effectiveness of emergency operations. However, traditional\ndispatcher training methods, which rely on role-playing by experienced\npersonnel, are labor-intensive, time-consuming, and often neglect the specific\nneeds of underserved communities. To address these challenges, we introduce\nSim911, the first training simulation for 9-1-1 dispatchers powered by Large\nLanguage Models (LLMs). Sim911 enhances training through three key technical\ninnovations: (1) knowledge construction, which utilizes archived 9-1-1 call\ndata to generate simulations that closely mirror real-world scenarios; (2)\ncontext-aware controlled generation, which employs dynamic prompts and vector\nbases to ensure that LLM behavior aligns with training objectives; and (3)\nvalidation with looped correction, which filters out low-quality responses and\nrefines the system performance."
                },
                "authors": [
                    {
                        "name": "Zirong Chen"
                    },
                    {
                        "name": "Elizabeth Chason"
                    },
                    {
                        "name": "Noah Mladenovski"
                    },
                    {
                        "name": "Erin Wilson"
                    },
                    {
                        "name": "Kristin Mullen"
                    },
                    {
                        "name": "Stephen Martini"
                    },
                    {
                        "name": "Meiyi Ma"
                    }
                ],
                "author_detail": {
                    "name": "Meiyi Ma"
                },
                "author": "Meiyi Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16844v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16844v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18116v1",
                "updated": "2024-12-24T02:54:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    2,
                    54,
                    56,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T02:54:56Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    2,
                    54,
                    56,
                    1,
                    359,
                    0
                ],
                "title": "AutoDroid-V2: Boosting SLM-based GUI Agents via Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoDroid-V2: Boosting SLM-based GUI Agents via Code Generation"
                },
                "summary": "Large language models (LLMs) have brought exciting new advances to mobile UI\nagents, a long-standing research field that aims to complete arbitrary natural\nlanguage tasks through mobile UI interactions. However, existing UI agents\nusually demand high reasoning capabilities of powerful large models that are\ndifficult to be deployed locally on end-users' devices, which raises huge\nconcerns about user privacy and centralized serving cost. One way to reduce the\nrequired model size is to customize a smaller domain-specific model with\nhigh-quality training data, e.g. large-scale human demonstrations of diverse\ntypes of apps and tasks, while such datasets are extremely difficult to obtain.\nInspired by the remarkable coding abilities of recent small language models\n(SLMs), we propose to convert the UI task automation problem to a code\ngeneration problem, which can be effectively solved by an on-device SLM and\nefficiently executed with an on-device code interpreter. Unlike normal coding\ntasks that can be extensively pretrained with public datasets, generating UI\nautomation code is challenging due to the diversity, complexity, and\nvariability of target apps. Therefore, we adopt a document-centered approach\nthat automatically builds fine-grained API documentation for each app and\ngenerates diverse task samples based on this documentation. By guiding the\nagent with the synthetic documents and task samples, it learns to generate\nprecise and efficient scripts to complete unseen tasks. Based on detailed\ncomparisons with state-of-the-art mobile UI agents, our approach effectively\nimproves the mobile task automation with significantly higher success rates and\nlower latency/token consumption. Code will be open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have brought exciting new advances to mobile UI\nagents, a long-standing research field that aims to complete arbitrary natural\nlanguage tasks through mobile UI interactions. However, existing UI agents\nusually demand high reasoning capabilities of powerful large models that are\ndifficult to be deployed locally on end-users' devices, which raises huge\nconcerns about user privacy and centralized serving cost. One way to reduce the\nrequired model size is to customize a smaller domain-specific model with\nhigh-quality training data, e.g. large-scale human demonstrations of diverse\ntypes of apps and tasks, while such datasets are extremely difficult to obtain.\nInspired by the remarkable coding abilities of recent small language models\n(SLMs), we propose to convert the UI task automation problem to a code\ngeneration problem, which can be effectively solved by an on-device SLM and\nefficiently executed with an on-device code interpreter. Unlike normal coding\ntasks that can be extensively pretrained with public datasets, generating UI\nautomation code is challenging due to the diversity, complexity, and\nvariability of target apps. Therefore, we adopt a document-centered approach\nthat automatically builds fine-grained API documentation for each app and\ngenerates diverse task samples based on this documentation. By guiding the\nagent with the synthetic documents and task samples, it learns to generate\nprecise and efficient scripts to complete unseen tasks. Based on detailed\ncomparisons with state-of-the-art mobile UI agents, our approach effectively\nimproves the mobile task automation with significantly higher success rates and\nlower latency/token consumption. Code will be open-sourced."
                },
                "authors": [
                    {
                        "name": "Hao Wen"
                    },
                    {
                        "name": "Shizuo Tian"
                    },
                    {
                        "name": "Borislav Pavlov"
                    },
                    {
                        "name": "Wenjie Du"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Ge Chang"
                    },
                    {
                        "name": "Shanhui Zhao"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Ya-Qin Zhang"
                    },
                    {
                        "name": "Yuanchun Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuanchun Li"
                },
                "author": "Yuanchun Li",
                "arxiv_comment": "15 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18111v1",
                "updated": "2024-12-24T02:51:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    2,
                    51,
                    6,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T02:51:06Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    2,
                    51,
                    6,
                    1,
                    359,
                    0
                ],
                "title": "AIGT: AI Generative Table Based on Prompt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIGT: AI Generative Table Based on Prompt"
                },
                "summary": "Tabular data, which accounts for over 80% of enterprise data assets, is vital\nin various fields. With growing concerns about privacy protection and\ndata-sharing restrictions, generating high-quality synthetic tabular data has\nbecome essential. Recent advancements show that large language models (LLMs)\ncan effectively gener-ate realistic tabular data by leveraging semantic\ninformation and overcoming the challenges of high-dimensional data that arise\nfrom one-hot encoding. However, current methods do not fully utilize the rich\ninformation available in tables. To address this, we introduce AI Generative\nTable (AIGT) based on prompt enhancement, a novel approach that utilizes meta\ndata information, such as table descriptions and schemas, as prompts to\ngenerate ultra-high quality synthetic data. To overcome the token limit\nconstraints of LLMs, we propose long-token partitioning algorithms that enable\nAIGT to model tables of any scale. AIGT achieves state-of-the-art performance\non 14 out of 20 public datasets and two real industry datasets within the\nAlipay risk control system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data, which accounts for over 80% of enterprise data assets, is vital\nin various fields. With growing concerns about privacy protection and\ndata-sharing restrictions, generating high-quality synthetic tabular data has\nbecome essential. Recent advancements show that large language models (LLMs)\ncan effectively gener-ate realistic tabular data by leveraging semantic\ninformation and overcoming the challenges of high-dimensional data that arise\nfrom one-hot encoding. However, current methods do not fully utilize the rich\ninformation available in tables. To address this, we introduce AI Generative\nTable (AIGT) based on prompt enhancement, a novel approach that utilizes meta\ndata information, such as table descriptions and schemas, as prompts to\ngenerate ultra-high quality synthetic data. To overcome the token limit\nconstraints of LLMs, we propose long-token partitioning algorithms that enable\nAIGT to model tables of any scale. AIGT achieves state-of-the-art performance\non 14 out of 20 public datasets and two real industry datasets within the\nAlipay risk control system."
                },
                "authors": [
                    {
                        "name": "Mingming Zhang"
                    },
                    {
                        "name": "Zhiqing Xiao"
                    },
                    {
                        "name": "Guoshan Lu"
                    },
                    {
                        "name": "Sai Wu"
                    },
                    {
                        "name": "Weiqiang Wang"
                    },
                    {
                        "name": "Xing Fu"
                    },
                    {
                        "name": "Can Yi"
                    },
                    {
                        "name": "Junbo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Junbo Zhao"
                },
                "author": "Junbo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18110v1",
                "updated": "2024-12-24T02:49:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    2,
                    49,
                    50,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T02:49:50Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    2,
                    49,
                    50,
                    1,
                    359,
                    0
                ],
                "title": "SlimGPT: Layer-wise Structured Pruning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlimGPT: Layer-wise Structured Pruning for Large Language Models"
                },
                "summary": "Large language models (LLMs) have garnered significant attention for their\nremarkable capabilities across various domains, whose vast parameter scales\npresent challenges for practical deployment. Structured pruning is an effective\nmethod to balance model performance with efficiency, but performance\nrestoration under computational resource constraints is a principal challenge\nin pruning LLMs. Therefore, we present a low-cost and fast structured pruning\nmethod for LLMs named SlimGPT based on the Optimal Brain Surgeon framework. We\npropose Batched Greedy Pruning for rapid and near-optimal pruning, which\nenhances the accuracy of head-wise pruning error estimation through grouped\nCholesky decomposition and improves the pruning efficiency of FFN via Dynamic\nGroup Size, thereby achieving approximate local optimal pruning results within\none hour. Besides, we explore the limitations of layer-wise pruning from the\nperspective of error accumulation and propose Incremental Pruning Ratio, a\nnon-uniform pruning strategy to reduce performance degradation. Experimental\nresults on the LLaMA benchmark show that SlimGPT outperforms other methods and\nachieves state-of-the-art results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have garnered significant attention for their\nremarkable capabilities across various domains, whose vast parameter scales\npresent challenges for practical deployment. Structured pruning is an effective\nmethod to balance model performance with efficiency, but performance\nrestoration under computational resource constraints is a principal challenge\nin pruning LLMs. Therefore, we present a low-cost and fast structured pruning\nmethod for LLMs named SlimGPT based on the Optimal Brain Surgeon framework. We\npropose Batched Greedy Pruning for rapid and near-optimal pruning, which\nenhances the accuracy of head-wise pruning error estimation through grouped\nCholesky decomposition and improves the pruning efficiency of FFN via Dynamic\nGroup Size, thereby achieving approximate local optimal pruning results within\none hour. Besides, we explore the limitations of layer-wise pruning from the\nperspective of error accumulation and propose Incremental Pruning Ratio, a\nnon-uniform pruning strategy to reduce performance degradation. Experimental\nresults on the LLaMA benchmark show that SlimGPT outperforms other methods and\nachieves state-of-the-art results."
                },
                "authors": [
                    {
                        "name": "Gui Ling"
                    },
                    {
                        "name": "Ziyang Wang"
                    },
                    {
                        "name": "Yuliang Yan"
                    },
                    {
                        "name": "Qingwen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qingwen Liu"
                },
                "author": "Qingwen Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00702v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00702v3",
                "updated": "2024-12-24T02:48:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    2,
                    48,
                    26,
                    1,
                    359,
                    0
                ],
                "published": "2024-03-31T14:41:49Z",
                "published_parsed": [
                    2024,
                    3,
                    31,
                    14,
                    41,
                    49,
                    6,
                    91,
                    0
                ],
                "title": "LLMTreeRec: Unleashing the Power of Large Language Models for Cold-Start\n  Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMTreeRec: Unleashing the Power of Large Language Models for Cold-Start\n  Recommendations"
                },
                "summary": "The lack of training data gives rise to the system cold-start problem in\nrecommendation systems, making them struggle to provide effective\nrecommendations. To address this problem, Large Language Models (LLMs) can\nmodel recommendation tasks as language analysis tasks and provide zero-shot\nresults based on their vast open-world knowledge. However, the large scale of\nthe item corpus poses a challenge to LLMs, leading to substantial token\nconsumption that makes it impractical to deploy in real-world recommendation\nsystems. To tackle this challenge, we introduce a tree-based LLM recommendation\nframework LLMTreeRec, which structures all items into an item tree to improve\nthe efficiency of LLM's item retrieval. LLMTreeRec achieves state-of-the-art\nperformance under the system cold-start setting in two widely used datasets,\nwhich is even competitive with conventional deep recommendation systems that\nuse substantial training data. Furthermore, LLMTreeRec outperforms the baseline\nmodel in A/B testing on Huawei industrial systems. Consequently, LLMTreeRec\ndemonstrates its effectiveness as an industry-friendly solution that has been\nsuccessfully deployed online. Our code is available at:\nhttps://github.com/Applied-Machine-Learning-Lab/LLMTreeRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The lack of training data gives rise to the system cold-start problem in\nrecommendation systems, making them struggle to provide effective\nrecommendations. To address this problem, Large Language Models (LLMs) can\nmodel recommendation tasks as language analysis tasks and provide zero-shot\nresults based on their vast open-world knowledge. However, the large scale of\nthe item corpus poses a challenge to LLMs, leading to substantial token\nconsumption that makes it impractical to deploy in real-world recommendation\nsystems. To tackle this challenge, we introduce a tree-based LLM recommendation\nframework LLMTreeRec, which structures all items into an item tree to improve\nthe efficiency of LLM's item retrieval. LLMTreeRec achieves state-of-the-art\nperformance under the system cold-start setting in two widely used datasets,\nwhich is even competitive with conventional deep recommendation systems that\nuse substantial training data. Furthermore, LLMTreeRec outperforms the baseline\nmodel in A/B testing on Huawei industrial systems. Consequently, LLMTreeRec\ndemonstrates its effectiveness as an industry-friendly solution that has been\nsuccessfully deployed online. Our code is available at:\nhttps://github.com/Applied-Machine-Learning-Lab/LLMTreeRec."
                },
                "authors": [
                    {
                        "name": "Wenlin Zhang"
                    },
                    {
                        "name": "Chuhan Wu"
                    },
                    {
                        "name": "Xiangyang Li"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Kuicai Dong"
                    },
                    {
                        "name": "Yichao Wang"
                    },
                    {
                        "name": "Xinyi Dai"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Huifeng Guo"
                    },
                    {
                        "name": "Ruiming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruiming Tang"
                },
                "author": "Ruiming Tang",
                "arxiv_journal_ref": "COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00702v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00702v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18310v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18310v2",
                "updated": "2024-12-24T02:40:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    2,
                    40,
                    25,
                    1,
                    359,
                    0
                ],
                "published": "2024-07-25T18:02:16Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    18,
                    2,
                    16,
                    3,
                    207,
                    0
                ],
                "title": "Revolutionizing Undergraduate Learning: CourseGPT and Its Generative AI\n  Advancements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revolutionizing Undergraduate Learning: CourseGPT and Its Generative AI\n  Advancements"
                },
                "summary": "Integrating Generative AI (GenAI) into educational contexts presents a\ntransformative potential for enhancing learning experiences. This paper\nintroduces CourseGPT, a generative AI tool designed to support instructors and\nenhance the educational experiences of undergraduate students. Built on\nopen-source Large Language Models (LLMs) from Mistral AI, CourseGPT offers\ncontinuous instructor support and regular updates to course materials,\nenriching the learning environment. By utilizing course-specific content, such\nas slide decks and supplementary readings and references, CourseGPT provides\nprecise, dynamically generated responses to student inquiries. Unlike generic\nAI models, CourseGPT allows instructors to manage and control the responses,\nthus extending the course scope without overwhelming details. The paper\ndemonstrates the application of CourseGPT using the CPR E 431 - Basics of\nInformation System Security course as a pilot. This course, with its large\nenrollments and diverse curriculum, serves as an ideal testbed for CourseGPT.\nThe tool aims to enhance the learning experience, accelerate feedback\nprocesses, and streamline administrative tasks. The study evaluates CourseGPT's\nimpact on student outcomes, focusing on correctness scores, context recall, and\nfaithfulness of responses. Results indicate that the Mixtral-8x7b model, with a\nhigher parameter count, outperforms smaller models, achieving an 88.0%\ncorrectness score and a 66.6% faithfulness score. Additionally, feedback from\nformer students and teaching assistants on CourseGPT's accuracy, helpfulness,\nand overall performance was collected. The outcomes revealed that a significant\nmajority found CourseGPT to be highly accurate and beneficial in addressing\ntheir queries, with many praising its ability to provide timely and relevant\ninformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Generative AI (GenAI) into educational contexts presents a\ntransformative potential for enhancing learning experiences. This paper\nintroduces CourseGPT, a generative AI tool designed to support instructors and\nenhance the educational experiences of undergraduate students. Built on\nopen-source Large Language Models (LLMs) from Mistral AI, CourseGPT offers\ncontinuous instructor support and regular updates to course materials,\nenriching the learning environment. By utilizing course-specific content, such\nas slide decks and supplementary readings and references, CourseGPT provides\nprecise, dynamically generated responses to student inquiries. Unlike generic\nAI models, CourseGPT allows instructors to manage and control the responses,\nthus extending the course scope without overwhelming details. The paper\ndemonstrates the application of CourseGPT using the CPR E 431 - Basics of\nInformation System Security course as a pilot. This course, with its large\nenrollments and diverse curriculum, serves as an ideal testbed for CourseGPT.\nThe tool aims to enhance the learning experience, accelerate feedback\nprocesses, and streamline administrative tasks. The study evaluates CourseGPT's\nimpact on student outcomes, focusing on correctness scores, context recall, and\nfaithfulness of responses. Results indicate that the Mixtral-8x7b model, with a\nhigher parameter count, outperforms smaller models, achieving an 88.0%\ncorrectness score and a 66.6% faithfulness score. Additionally, feedback from\nformer students and teaching assistants on CourseGPT's accuracy, helpfulness,\nand overall performance was collected. The outcomes revealed that a significant\nmajority found CourseGPT to be highly accurate and beneficial in addressing\ntheir queries, with many praising its ability to provide timely and relevant\ninformation."
                },
                "authors": [
                    {
                        "name": "Ahmad M. Nazar"
                    },
                    {
                        "name": "Mohamed Y. Selim"
                    },
                    {
                        "name": "Ashraf Gaffar"
                    },
                    {
                        "name": "Shakil Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Shakil Ahmed"
                },
                "author": "Shakil Ahmed",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18310v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18310v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18108v1",
                "updated": "2024-12-24T02:31:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    2,
                    31,
                    24,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T02:31:24Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    2,
                    31,
                    24,
                    1,
                    359,
                    0
                ],
                "title": "Unveiling Visual Perception in Language Models: An Attention Head\n  Analysis Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Visual Perception in Language Models: An Attention Head\n  Analysis Approach"
                },
                "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\ndemonstrated remarkable progress in visual understanding. This impressive leap\nraises a compelling question: how can language models, initially trained solely\non linguistic data, effectively interpret and process visual content? This\npaper aims to address this question with systematic investigation across 4\nmodel families and 4 model scales, uncovering a unique class of attention heads\nthat focus specifically on visual content. Our analysis reveals a strong\ncorrelation between the behavior of these attention heads, the distribution of\nattention weights, and their concentration on visual tokens within the input.\nThese findings enhance our understanding of how LLMs adapt to multimodal tasks,\ndemonstrating their potential to bridge the gap between textual and visual\nunderstanding. This work paves the way for the development of AI systems\ncapable of engaging with diverse modalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Multimodal Large Language Models (MLLMs) have\ndemonstrated remarkable progress in visual understanding. This impressive leap\nraises a compelling question: how can language models, initially trained solely\non linguistic data, effectively interpret and process visual content? This\npaper aims to address this question with systematic investigation across 4\nmodel families and 4 model scales, uncovering a unique class of attention heads\nthat focus specifically on visual content. Our analysis reveals a strong\ncorrelation between the behavior of these attention heads, the distribution of\nattention weights, and their concentration on visual tokens within the input.\nThese findings enhance our understanding of how LLMs adapt to multimodal tasks,\ndemonstrating their potential to bridge the gap between textual and visual\nunderstanding. This work paves the way for the development of AI systems\ncapable of engaging with diverse modalities."
                },
                "authors": [
                    {
                        "name": "Jing Bi"
                    },
                    {
                        "name": "Junjia Guo"
                    },
                    {
                        "name": "Yunlong Tang"
                    },
                    {
                        "name": "Lianggong Bruce Wen"
                    },
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Chenliang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenliang Xu"
                },
                "author": "Chenliang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18106v1",
                "updated": "2024-12-24T02:27:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    2,
                    27,
                    44,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T02:27:44Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    2,
                    27,
                    44,
                    1,
                    359,
                    0
                ],
                "title": "Tackling the Dynamicity in a Production LLM Serving System with SOTA\n  Optimizations via Hybrid Prefill/Decode/Verify Scheduling on Efficient\n  Meta-kernels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tackling the Dynamicity in a Production LLM Serving System with SOTA\n  Optimizations via Hybrid Prefill/Decode/Verify Scheduling on Efficient\n  Meta-kernels"
                },
                "summary": "Meeting growing demands for low latency and cost efficiency in\nproduction-grade large language model (LLM) serving systems requires\nintegrating advanced optimization techniques. However, dynamic and\nunpredictable input-output lengths of LLM, compounded by these optimizations,\nexacerbate the issues of workload variability, making it difficult to maintain\nhigh efficiency on AI accelerators, especially DSAs with tile-based programming\nmodels. To address this challenge, we introduce XY-Serve, a versatile, Ascend\nnative, end-to-end production LLM-serving system. The core idea is an\nabstraction mechanism that smooths out the workload variability by decomposing\ncomputations into unified, hardware-friendly, fine-grained meta primitives. For\nattention, we propose a meta-kernel that computes the basic pattern of\nmatmul-softmax-matmul with architectural-aware tile sizes. For GEMM, we\nintroduce a virtual padding scheme that adapts to dynamic shape changes while\nusing highly efficient GEMM primitives with assorted fixed tile sizes. XY-Serve\nsits harmoniously with vLLM. Experimental results show up to 89% end-to-end\nthroughput improvement compared with current publicly available baselines on\nAscend NPUs. Additionally, our approach outperforms existing GEMM (average\n14.6% faster) and attention (average 21.5% faster) kernels relative to existing\nlibraries. While the work is Ascend native, we believe the approach can be\nreadily applicable to SIMT architectures as well.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meeting growing demands for low latency and cost efficiency in\nproduction-grade large language model (LLM) serving systems requires\nintegrating advanced optimization techniques. However, dynamic and\nunpredictable input-output lengths of LLM, compounded by these optimizations,\nexacerbate the issues of workload variability, making it difficult to maintain\nhigh efficiency on AI accelerators, especially DSAs with tile-based programming\nmodels. To address this challenge, we introduce XY-Serve, a versatile, Ascend\nnative, end-to-end production LLM-serving system. The core idea is an\nabstraction mechanism that smooths out the workload variability by decomposing\ncomputations into unified, hardware-friendly, fine-grained meta primitives. For\nattention, we propose a meta-kernel that computes the basic pattern of\nmatmul-softmax-matmul with architectural-aware tile sizes. For GEMM, we\nintroduce a virtual padding scheme that adapts to dynamic shape changes while\nusing highly efficient GEMM primitives with assorted fixed tile sizes. XY-Serve\nsits harmoniously with vLLM. Experimental results show up to 89% end-to-end\nthroughput improvement compared with current publicly available baselines on\nAscend NPUs. Additionally, our approach outperforms existing GEMM (average\n14.6% faster) and attention (average 21.5% faster) kernels relative to existing\nlibraries. While the work is Ascend native, we believe the approach can be\nreadily applicable to SIMT architectures as well."
                },
                "authors": [
                    {
                        "name": "Mingcong Song"
                    },
                    {
                        "name": "Xinru Tang"
                    },
                    {
                        "name": "Fengfan Hou"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yipeng Ma"
                    },
                    {
                        "name": "Runqiu Xiao"
                    },
                    {
                        "name": "Hongjie Si"
                    },
                    {
                        "name": "Dingcheng Jiang"
                    },
                    {
                        "name": "Shouyi Yin"
                    },
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Guoping Long"
                    }
                ],
                "author_detail": {
                    "name": "Guoping Long"
                },
                "author": "Guoping Long",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18100v1",
                "updated": "2024-12-24T02:21:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    2,
                    21,
                    9,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T02:21:09Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    2,
                    21,
                    9,
                    1,
                    359,
                    0
                ],
                "title": "EvoPat: A Multi-LLM-based Patents Summarization and Analysis Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvoPat: A Multi-LLM-based Patents Summarization and Analysis Agent"
                },
                "summary": "The rapid growth of scientific techniques and knowledge is reflected in the\nexponential increase in new patents filed annually. While these patents drive\ninnovation, they also present significant burden for researchers and engineers,\nespecially newcomers. To avoid the tedious work of navigating a vast and\ncomplex landscape to identify trends and breakthroughs, researchers urgently\nneed efficient tools to summarize, evaluate, and contextualize patents,\nrevealing their innovative contributions and underlying scientific\nprinciples.To address this need, we present EvoPat, a multi-LLM-based patent\nagent designed to assist users in analyzing patents through Retrieval-Augmented\nGeneration (RAG) and advanced search strategies. EvoPat leverages multiple\nLarge Language Models (LLMs), each performing specialized roles such as\nplanning, identifying innovations, and conducting comparative evaluations. The\nsystem integrates data from local databases, including patents, literature,\nproduct catalogous, and company repositories, and online searches to provide\nup-to-date insights. The ability to collect information not included in\noriginal database automatically is also implemented. Through extensive testing\nin the natural language processing (NLP) domain, we demonstrate that EvoPat\noutperforms GPT-4 in tasks such as patent summarization, comparative analysis,\nand technical evaluation. EvoPat represents a significant step toward creating\nAI-powered tools that empower researchers and engineers to efficiently navigate\nthe complexities of the patent landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of scientific techniques and knowledge is reflected in the\nexponential increase in new patents filed annually. While these patents drive\ninnovation, they also present significant burden for researchers and engineers,\nespecially newcomers. To avoid the tedious work of navigating a vast and\ncomplex landscape to identify trends and breakthroughs, researchers urgently\nneed efficient tools to summarize, evaluate, and contextualize patents,\nrevealing their innovative contributions and underlying scientific\nprinciples.To address this need, we present EvoPat, a multi-LLM-based patent\nagent designed to assist users in analyzing patents through Retrieval-Augmented\nGeneration (RAG) and advanced search strategies. EvoPat leverages multiple\nLarge Language Models (LLMs), each performing specialized roles such as\nplanning, identifying innovations, and conducting comparative evaluations. The\nsystem integrates data from local databases, including patents, literature,\nproduct catalogous, and company repositories, and online searches to provide\nup-to-date insights. The ability to collect information not included in\noriginal database automatically is also implemented. Through extensive testing\nin the natural language processing (NLP) domain, we demonstrate that EvoPat\noutperforms GPT-4 in tasks such as patent summarization, comparative analysis,\nand technical evaluation. EvoPat represents a significant step toward creating\nAI-powered tools that empower researchers and engineers to efficiently navigate\nthe complexities of the patent landscape."
                },
                "authors": [
                    {
                        "name": "Suyuan Wang"
                    },
                    {
                        "name": "Xueqian Yin"
                    },
                    {
                        "name": "Menghao Wang"
                    },
                    {
                        "name": "Ruofeng Guo"
                    },
                    {
                        "name": "Kai Nan"
                    }
                ],
                "author_detail": {
                    "name": "Kai Nan"
                },
                "author": "Kai Nan",
                "arxiv_comment": "15 pages,2 figures,8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16834v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16834v2",
                "updated": "2024-12-24T02:17:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    2,
                    17,
                    41,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-22T02:43:07Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    2,
                    43,
                    7,
                    6,
                    357,
                    0
                ],
                "title": "Online Learning from Strategic Human Feedback in LLM Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Learning from Strategic Human Feedback in LLM Fine-Tuning"
                },
                "summary": "Reinforcement learning from human feedback (RLHF) has become an essential\nstep in fine-tuning large language models (LLMs) to align them with human\npreferences. However, human labelers are selfish and have diverse preferences.\nThey may strategically misreport their online feedback to influence the\nsystem's aggregation towards their own preferences. Current practice simply\naverages labelers' feedback per time and fails to identify the most accurate\nhuman labeler, leading to linear regret $\\mathcal{O}(T)$ for $T$ time slots. To\nour best knowledge, we are the first to study online learning mechanisms\nagainst strategic human labelers in the LLM fine-tuning process. We formulate a\nnew dynamic Bayesian game and dynamically adjust human labelers' weights in the\npreference aggregation, ensuring their truthful feedback and sublinear regret\n$\\mathcal{O}(T^{1/2})$. Simulation results demonstrate our mechanism's great\nadvantages over the existing benchmark schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) has become an essential\nstep in fine-tuning large language models (LLMs) to align them with human\npreferences. However, human labelers are selfish and have diverse preferences.\nThey may strategically misreport their online feedback to influence the\nsystem's aggregation towards their own preferences. Current practice simply\naverages labelers' feedback per time and fails to identify the most accurate\nhuman labeler, leading to linear regret $\\mathcal{O}(T)$ for $T$ time slots. To\nour best knowledge, we are the first to study online learning mechanisms\nagainst strategic human labelers in the LLM fine-tuning process. We formulate a\nnew dynamic Bayesian game and dynamically adjust human labelers' weights in the\npreference aggregation, ensuring their truthful feedback and sublinear regret\n$\\mathcal{O}(T^{1/2})$. Simulation results demonstrate our mechanism's great\nadvantages over the existing benchmark schemes."
                },
                "authors": [
                    {
                        "name": "Shugang Hao"
                    },
                    {
                        "name": "Lingjie Duan"
                    }
                ],
                "author_detail": {
                    "name": "Lingjie Duan"
                },
                "author": "Lingjie Duan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16834v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16834v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18096v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18096v1",
                "updated": "2024-12-24T02:14:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    2,
                    14,
                    13,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-24T02:14:13Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    2,
                    14,
                    13,
                    1,
                    359,
                    0
                ],
                "title": "Real-world Deployment and Evaluation of PErioperative AI CHatbot (PEACH)\n  -- a Large Language Model Chatbot for Perioperative Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world Deployment and Evaluation of PErioperative AI CHatbot (PEACH)\n  -- a Large Language Model Chatbot for Perioperative Medicine"
                },
                "summary": "Large Language Models (LLMs) are emerging as powerful tools in healthcare,\nparticularly for complex, domain-specific tasks. This study describes the\ndevelopment and evaluation of the PErioperative AI CHatbot (PEACH), a secure\nLLM-based system integrated with local perioperative guidelines to support\npreoperative clinical decision-making. PEACH was embedded with 35 institutional\nperioperative protocols in the secure Claude 3.5 Sonet LLM framework within\nPair Chat (developed by Singapore Government) and tested in a silent deployment\nwith real-world data. Accuracy, safety, and usability were assessed. Deviations\nand hallucinations were categorized based on potential harm, and user feedback\nwas evaluated using the Technology Acceptance Model (TAM). Updates were made\nafter the initial silent deployment to amend one protocol.\n  In 240 real-world clinical iterations, PEACH achieved a first-generation\naccuracy of 97.5% (78/80) and an overall accuracy of 96.7% (232/240) across\nthree iterations. The updated PEACH demonstrated improved accuracy of 97.9%\n(235/240), with a statistically significant difference from the null hypothesis\nof 95% accuracy (p = 0.018, 95% CI: 0.952-0.991). Minimal hallucinations and\ndeviations were observed (both 1/240 and 2/240, respectively). Clinicians\nreported that PEACH expedited decisions in 95% of cases, and inter-rater\nreliability ranged from kappa 0.772-0.893 within PEACH and 0.610-0.784 among\nattendings.\n  PEACH is an accurate, adaptable tool that enhances consistency and efficiency\nin perioperative decision-making. Future research should explore its\nscalability across specialties and its impact on clinical outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are emerging as powerful tools in healthcare,\nparticularly for complex, domain-specific tasks. This study describes the\ndevelopment and evaluation of the PErioperative AI CHatbot (PEACH), a secure\nLLM-based system integrated with local perioperative guidelines to support\npreoperative clinical decision-making. PEACH was embedded with 35 institutional\nperioperative protocols in the secure Claude 3.5 Sonet LLM framework within\nPair Chat (developed by Singapore Government) and tested in a silent deployment\nwith real-world data. Accuracy, safety, and usability were assessed. Deviations\nand hallucinations were categorized based on potential harm, and user feedback\nwas evaluated using the Technology Acceptance Model (TAM). Updates were made\nafter the initial silent deployment to amend one protocol.\n  In 240 real-world clinical iterations, PEACH achieved a first-generation\naccuracy of 97.5% (78/80) and an overall accuracy of 96.7% (232/240) across\nthree iterations. The updated PEACH demonstrated improved accuracy of 97.9%\n(235/240), with a statistically significant difference from the null hypothesis\nof 95% accuracy (p = 0.018, 95% CI: 0.952-0.991). Minimal hallucinations and\ndeviations were observed (both 1/240 and 2/240, respectively). Clinicians\nreported that PEACH expedited decisions in 95% of cases, and inter-rater\nreliability ranged from kappa 0.772-0.893 within PEACH and 0.610-0.784 among\nattendings.\n  PEACH is an accurate, adaptable tool that enhances consistency and efficiency\nin perioperative decision-making. Future research should explore its\nscalability across specialties and its impact on clinical outcomes."
                },
                "authors": [
                    {
                        "name": "Yu He Ke"
                    },
                    {
                        "name": "Liyuan Jin"
                    },
                    {
                        "name": "Kabilan Elangovan"
                    },
                    {
                        "name": "Bryan Wen Xi Ong"
                    },
                    {
                        "name": "Chin Yang Oh"
                    },
                    {
                        "name": "Jacqueline Sim"
                    },
                    {
                        "name": "Kenny Wei-Tsen Loh"
                    },
                    {
                        "name": "Chai Rick Soh"
                    },
                    {
                        "name": "Jonathan Ming Hua Cheng"
                    },
                    {
                        "name": "Aaron Kwang Yang Lee"
                    },
                    {
                        "name": "Daniel Shu Wei Ting"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Hairil Rizal Abdullah"
                    }
                ],
                "author_detail": {
                    "name": "Hairil Rizal Abdullah"
                },
                "author": "Hairil Rizal Abdullah",
                "arxiv_comment": "21 pages, 3 figures, 1 graphical abstract",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18096v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]