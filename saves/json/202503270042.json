[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.13509v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13509v2",
                "updated": "2025-03-25T17:56:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    56,
                    1,
                    1,
                    84,
                    0
                ],
                "published": "2024-12-18T05:16:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "Visualizing the Invisible: A Generative AR System for Intuitive\n  Multi-Modal Sensor Data Presentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visualizing the Invisible: A Generative AR System for Intuitive\n  Multi-Modal Sensor Data Presentation"
                },
                "summary": "Understanding sensor data can be difficult for non-experts because of the\ncomplexity and different semantic meanings of sensor modalities. This leads to\na need for intuitive and effective methods to present sensor information.\nHowever, creating intuitive sensor data visualizations presents three key\nchallenges: the variability of sensor readings, gaps in domain comprehension,\nand the dynamic nature of sensor data. To address these issues, we propose\nVivar, a novel system that integrates multi-modal sensor data and presents 3D\nvolumetric content for AR visualization. In particular, we introduce a\ncross-modal embedding approach that maps sensor data into a pre-trained visual\nembedding space through barycentric interpolation. This approach accurately\nreflects value changes in multi-modal sensor information, ensuring that sensor\nvariations are properly shown in visualization outcomes. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation, demonstrating 11x latency reduction without compromising\nquality. A user study involving over 503 participants, including domain\nexperts, demonstrates Vivar's effectiveness in accuracy, consistency, and\nreal-world applicability, paving the way for more intuitive sensor data\nvisualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding sensor data can be difficult for non-experts because of the\ncomplexity and different semantic meanings of sensor modalities. This leads to\na need for intuitive and effective methods to present sensor information.\nHowever, creating intuitive sensor data visualizations presents three key\nchallenges: the variability of sensor readings, gaps in domain comprehension,\nand the dynamic nature of sensor data. To address these issues, we propose\nVivar, a novel system that integrates multi-modal sensor data and presents 3D\nvolumetric content for AR visualization. In particular, we introduce a\ncross-modal embedding approach that maps sensor data into a pre-trained visual\nembedding space through barycentric interpolation. This approach accurately\nreflects value changes in multi-modal sensor information, ensuring that sensor\nvariations are properly shown in visualization outcomes. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation, demonstrating 11x latency reduction without compromising\nquality. A user study involving over 503 participants, including domain\nexperts, demonstrates Vivar's effectiveness in accuracy, consistency, and\nreal-world applicability, paving the way for more intuitive sensor data\nvisualization."
                },
                "authors": [
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Kaiyuan Hou"
                    },
                    {
                        "name": "Heming Fu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13509v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13509v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19786v1",
                "updated": "2025-03-25T15:52:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    52,
                    34,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T15:52:34Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    52,
                    34,
                    1,
                    84,
                    0
                ],
                "title": "Gemma 3 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gemma 3 Technical Report"
                },
                "summary": "We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community."
                },
                "authors": [
                    {
                        "name": "Gemma Team"
                    },
                    {
                        "name": "Aishwarya Kamath"
                    },
                    {
                        "name": "Johan Ferret"
                    },
                    {
                        "name": "Shreya Pathak"
                    },
                    {
                        "name": "Nino Vieillard"
                    },
                    {
                        "name": "Ramona Merhej"
                    },
                    {
                        "name": "Sarah Perrin"
                    },
                    {
                        "name": "Tatiana Matejovicova"
                    },
                    {
                        "name": "Alexandre Ramé"
                    },
                    {
                        "name": "Morgane Rivière"
                    },
                    {
                        "name": "Louis Rouillard"
                    },
                    {
                        "name": "Thomas Mesnard"
                    },
                    {
                        "name": "Geoffrey Cideron"
                    },
                    {
                        "name": "Jean-bastien Grill"
                    },
                    {
                        "name": "Sabela Ramos"
                    },
                    {
                        "name": "Edouard Yvinec"
                    },
                    {
                        "name": "Michelle Casbon"
                    },
                    {
                        "name": "Etienne Pot"
                    },
                    {
                        "name": "Ivo Penchev"
                    },
                    {
                        "name": "Gaël Liu"
                    },
                    {
                        "name": "Francesco Visin"
                    },
                    {
                        "name": "Kathleen Kenealy"
                    },
                    {
                        "name": "Lucas Beyer"
                    },
                    {
                        "name": "Xiaohai Zhai"
                    },
                    {
                        "name": "Anton Tsitsulin"
                    },
                    {
                        "name": "Robert Busa-Fekete"
                    },
                    {
                        "name": "Alex Feng"
                    },
                    {
                        "name": "Noveen Sachdeva"
                    },
                    {
                        "name": "Benjamin Coleman"
                    },
                    {
                        "name": "Yi Gao"
                    },
                    {
                        "name": "Basil Mustafa"
                    },
                    {
                        "name": "Iain Barr"
                    },
                    {
                        "name": "Emilio Parisotto"
                    },
                    {
                        "name": "David Tian"
                    },
                    {
                        "name": "Matan Eyal"
                    },
                    {
                        "name": "Colin Cherry"
                    },
                    {
                        "name": "Jan-Thorsten Peter"
                    },
                    {
                        "name": "Danila Sinopalnikov"
                    },
                    {
                        "name": "Surya Bhupatiraju"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Mehran Kazemi"
                    },
                    {
                        "name": "Dan Malkin"
                    },
                    {
                        "name": "Ravin Kumar"
                    },
                    {
                        "name": "David Vilar"
                    },
                    {
                        "name": "Idan Brusilovsky"
                    },
                    {
                        "name": "Jiaming Luo"
                    },
                    {
                        "name": "Andreas Steiner"
                    },
                    {
                        "name": "Abe Friesen"
                    },
                    {
                        "name": "Abhanshu Sharma"
                    },
                    {
                        "name": "Abheesht Sharma"
                    },
                    {
                        "name": "Adi Mayrav Gilady"
                    },
                    {
                        "name": "Adrian Goedeckemeyer"
                    },
                    {
                        "name": "Alaa Saade"
                    },
                    {
                        "name": "Alex Feng"
                    },
                    {
                        "name": "Alexander Kolesnikov"
                    },
                    {
                        "name": "Alexei Bendebury"
                    },
                    {
                        "name": "Alvin Abdagic"
                    },
                    {
                        "name": "Amit Vadi"
                    },
                    {
                        "name": "András György"
                    },
                    {
                        "name": "André Susano Pinto"
                    },
                    {
                        "name": "Anil Das"
                    },
                    {
                        "name": "Ankur Bapna"
                    },
                    {
                        "name": "Antoine Miech"
                    },
                    {
                        "name": "Antoine Yang"
                    },
                    {
                        "name": "Antonia Paterson"
                    },
                    {
                        "name": "Ashish Shenoy"
                    },
                    {
                        "name": "Ayan Chakrabarti"
                    },
                    {
                        "name": "Bilal Piot"
                    },
                    {
                        "name": "Bo Wu"
                    },
                    {
                        "name": "Bobak Shahriari"
                    },
                    {
                        "name": "Bryce Petrini"
                    },
                    {
                        "name": "Charlie Chen"
                    },
                    {
                        "name": "Charline Le Lan"
                    },
                    {
                        "name": "Christopher A. Choquette-Choo"
                    },
                    {
                        "name": "CJ Carey"
                    },
                    {
                        "name": "Cormac Brick"
                    },
                    {
                        "name": "Daniel Deutsch"
                    },
                    {
                        "name": "Danielle Eisenbud"
                    },
                    {
                        "name": "Dee Cattle"
                    },
                    {
                        "name": "Derek Cheng"
                    },
                    {
                        "name": "Dimitris Paparas"
                    },
                    {
                        "name": "Divyashree Shivakumar Sreepathihalli"
                    },
                    {
                        "name": "Doug Reid"
                    },
                    {
                        "name": "Dustin Tran"
                    },
                    {
                        "name": "Dustin Zelle"
                    },
                    {
                        "name": "Eric Noland"
                    },
                    {
                        "name": "Erwin Huizenga"
                    },
                    {
                        "name": "Eugene Kharitonov"
                    },
                    {
                        "name": "Frederick Liu"
                    },
                    {
                        "name": "Gagik Amirkhanyan"
                    },
                    {
                        "name": "Glenn Cameron"
                    },
                    {
                        "name": "Hadi Hashemi"
                    },
                    {
                        "name": "Hanna Klimczak-Plucińska"
                    },
                    {
                        "name": "Harman Singh"
                    },
                    {
                        "name": "Harsh Mehta"
                    },
                    {
                        "name": "Harshal Tushar Lehri"
                    },
                    {
                        "name": "Hussein Hazimeh"
                    },
                    {
                        "name": "Ian Ballantyne"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Ivan Nardini"
                    },
                    {
                        "name": "Jean Pouget-Abadie"
                    },
                    {
                        "name": "Jetha Chan"
                    },
                    {
                        "name": "Joe Stanton"
                    },
                    {
                        "name": "John Wieting"
                    },
                    {
                        "name": "Jonathan Lai"
                    },
                    {
                        "name": "Jordi Orbay"
                    },
                    {
                        "name": "Joseph Fernandez"
                    },
                    {
                        "name": "Josh Newlan"
                    },
                    {
                        "name": "Ju-yeong Ji"
                    },
                    {
                        "name": "Jyotinder Singh"
                    },
                    {
                        "name": "Kat Black"
                    },
                    {
                        "name": "Kathy Yu"
                    },
                    {
                        "name": "Kevin Hui"
                    },
                    {
                        "name": "Kiran Vodrahalli"
                    },
                    {
                        "name": "Klaus Greff"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Marcella Valentine"
                    },
                    {
                        "name": "Marina Coelho"
                    },
                    {
                        "name": "Marvin Ritter"
                    },
                    {
                        "name": "Matt Hoffman"
                    },
                    {
                        "name": "Matthew Watson"
                    },
                    {
                        "name": "Mayank Chaturvedi"
                    },
                    {
                        "name": "Michael Moynihan"
                    },
                    {
                        "name": "Min Ma"
                    },
                    {
                        "name": "Nabila Babar"
                    },
                    {
                        "name": "Natasha Noy"
                    },
                    {
                        "name": "Nathan Byrd"
                    },
                    {
                        "name": "Nick Roy"
                    },
                    {
                        "name": "Nikola Momchev"
                    },
                    {
                        "name": "Nilay Chauhan"
                    },
                    {
                        "name": "Noveen Sachdeva"
                    },
                    {
                        "name": "Oskar Bunyan"
                    },
                    {
                        "name": "Pankil Botarda"
                    },
                    {
                        "name": "Paul Caron"
                    },
                    {
                        "name": "Paul Kishan Rubenstein"
                    },
                    {
                        "name": "Phil Culliton"
                    },
                    {
                        "name": "Philipp Schmid"
                    },
                    {
                        "name": "Pier Giuseppe Sessa"
                    },
                    {
                        "name": "Pingmei Xu"
                    },
                    {
                        "name": "Piotr Stanczyk"
                    },
                    {
                        "name": "Pouya Tafti"
                    },
                    {
                        "name": "Rakesh Shivanna"
                    },
                    {
                        "name": "Renjie Wu"
                    },
                    {
                        "name": "Renke Pan"
                    },
                    {
                        "name": "Reza Rokni"
                    },
                    {
                        "name": "Rob Willoughby"
                    },
                    {
                        "name": "Rohith Vallu"
                    },
                    {
                        "name": "Ryan Mullins"
                    },
                    {
                        "name": "Sammy Jerome"
                    },
                    {
                        "name": "Sara Smoot"
                    },
                    {
                        "name": "Sertan Girgin"
                    },
                    {
                        "name": "Shariq Iqbal"
                    },
                    {
                        "name": "Shashir Reddy"
                    },
                    {
                        "name": "Shruti Sheth"
                    },
                    {
                        "name": "Siim Põder"
                    },
                    {
                        "name": "Sijal Bhatnagar"
                    },
                    {
                        "name": "Sindhu Raghuram Panyam"
                    },
                    {
                        "name": "Sivan Eiger"
                    },
                    {
                        "name": "Susan Zhang"
                    },
                    {
                        "name": "Tianqi Liu"
                    },
                    {
                        "name": "Trevor Yacovone"
                    },
                    {
                        "name": "Tyler Liechty"
                    },
                    {
                        "name": "Uday Kalra"
                    },
                    {
                        "name": "Utku Evci"
                    },
                    {
                        "name": "Vedant Misra"
                    },
                    {
                        "name": "Vincent Roseberry"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Vlad Kolesnikov"
                    },
                    {
                        "name": "Woohyun Han"
                    },
                    {
                        "name": "Woosuk Kwon"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Yinlam Chow"
                    },
                    {
                        "name": "Yuvein Zhu"
                    },
                    {
                        "name": "Zichuan Wei"
                    },
                    {
                        "name": "Zoltan Egyed"
                    },
                    {
                        "name": "Victor Cotruta"
                    },
                    {
                        "name": "Minh Giang"
                    },
                    {
                        "name": "Phoebe Kirk"
                    },
                    {
                        "name": "Anand Rao"
                    },
                    {
                        "name": "Kat Black"
                    },
                    {
                        "name": "Nabila Babar"
                    },
                    {
                        "name": "Jessica Lo"
                    },
                    {
                        "name": "Erica Moreira"
                    },
                    {
                        "name": "Luiz Gustavo Martins"
                    },
                    {
                        "name": "Omar Sanseviero"
                    },
                    {
                        "name": "Lucas Gonzalez"
                    },
                    {
                        "name": "Zach Gleicher"
                    },
                    {
                        "name": "Tris Warkentin"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    },
                    {
                        "name": "Evan Senter"
                    },
                    {
                        "name": "Eli Collins"
                    },
                    {
                        "name": "Joelle Barral"
                    },
                    {
                        "name": "Zoubin Ghahramani"
                    },
                    {
                        "name": "Raia Hadsell"
                    },
                    {
                        "name": "Yossi Matias"
                    },
                    {
                        "name": "D. Sculley"
                    },
                    {
                        "name": "Slav Petrov"
                    },
                    {
                        "name": "Noah Fiedel"
                    },
                    {
                        "name": "Noam Shazeer"
                    },
                    {
                        "name": "Oriol Vinyals"
                    },
                    {
                        "name": "Jeff Dean"
                    },
                    {
                        "name": "Demis Hassabis"
                    },
                    {
                        "name": "Koray Kavukcuoglu"
                    },
                    {
                        "name": "Clement Farabet"
                    },
                    {
                        "name": "Elena Buchatskaya"
                    },
                    {
                        "name": "Jean-Baptiste Alayrac"
                    },
                    {
                        "name": "Rohan Anil"
                    },
                    {
                        "name": "Dmitry"
                    },
                    {
                        "name": "Lepikhin"
                    },
                    {
                        "name": "Sebastian Borgeaud"
                    },
                    {
                        "name": "Olivier Bachem"
                    },
                    {
                        "name": "Armand Joulin"
                    },
                    {
                        "name": "Alek Andreev"
                    },
                    {
                        "name": "Cassidy Hardin"
                    },
                    {
                        "name": "Robert Dadashi"
                    },
                    {
                        "name": "Léonard Hussenot"
                    }
                ],
                "author_detail": {
                    "name": "Léonard Hussenot"
                },
                "author": "Léonard Hussenot",
                "arxiv_affiliation": "Dima",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19390v1",
                "updated": "2025-03-25T06:45:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    45,
                    13,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T06:45:13Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    45,
                    13,
                    1,
                    84,
                    0
                ],
                "title": "Integrating Prefetcher Selection with Dynamic Request Allocation\n  Improves Prefetching Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Prefetcher Selection with Dynamic Request Allocation\n  Improves Prefetching Efficiency"
                },
                "summary": "Hardware prefetching plays a critical role in hiding the off-chip DRAM\nlatency. The complexity of applications results in a wide variety of memory\naccess patterns, prompting the development of numerous cache-prefetching\nalgorithms. Consequently, commercial processors often employ a hybrid of these\nalgorithms to enhance the overall prefetching performance. Nonetheless, since\nthese prefetchers share hardware resources, conflicts arising from competing\nprefetching requests can negate the benefits of hardware prefetching. Under\nsuch circumstances, several prefetcher selection algorithms have been proposed\nto mitigate conflicts between prefetchers. However, these prior solutions\nsuffer from two limitations. First, the input demand request allocation is\ninaccurate. Second, the prefetcher selection criteria are coarse-grained.\n  In this paper, we address both limitations by introducing an efficient and\nwidely applicable prefetcher selection algorithm--Alecto, which tailors the\ndemand requests for each prefetcher. Every demand request is first sent to\nAlecto to identify suitable prefetchers before being routed to prefetchers for\ntraining and prefetching. Our analysis shows that Alecto is adept at not only\nharmonizing prefetching accuracy, coverage, and timeliness but also\nsignificantly enhancing the utilization of the prefetcher table, which is vital\nfor temporal prefetching. Alecto outperforms the state-of-the-art RL-based\nprefetcher selection algorithm--Bandit by 2.76% in single-core, and 7.56% in\neight-core. For memory-intensive benchmarks, Alecto outperforms Bandit by\n5.25%. Alecto consistently delivers state-of-the-art performance in scheduling\nvarious types of cache prefetchers. In addition to the performance improvement,\nAlecto can reduce the energy consumption associated with accessing the\nprefetchers' table by 48%, while only adding less than 1 KB of storage\noverhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware prefetching plays a critical role in hiding the off-chip DRAM\nlatency. The complexity of applications results in a wide variety of memory\naccess patterns, prompting the development of numerous cache-prefetching\nalgorithms. Consequently, commercial processors often employ a hybrid of these\nalgorithms to enhance the overall prefetching performance. Nonetheless, since\nthese prefetchers share hardware resources, conflicts arising from competing\nprefetching requests can negate the benefits of hardware prefetching. Under\nsuch circumstances, several prefetcher selection algorithms have been proposed\nto mitigate conflicts between prefetchers. However, these prior solutions\nsuffer from two limitations. First, the input demand request allocation is\ninaccurate. Second, the prefetcher selection criteria are coarse-grained.\n  In this paper, we address both limitations by introducing an efficient and\nwidely applicable prefetcher selection algorithm--Alecto, which tailors the\ndemand requests for each prefetcher. Every demand request is first sent to\nAlecto to identify suitable prefetchers before being routed to prefetchers for\ntraining and prefetching. Our analysis shows that Alecto is adept at not only\nharmonizing prefetching accuracy, coverage, and timeliness but also\nsignificantly enhancing the utilization of the prefetcher table, which is vital\nfor temporal prefetching. Alecto outperforms the state-of-the-art RL-based\nprefetcher selection algorithm--Bandit by 2.76% in single-core, and 7.56% in\neight-core. For memory-intensive benchmarks, Alecto outperforms Bandit by\n5.25%. Alecto consistently delivers state-of-the-art performance in scheduling\nvarious types of cache prefetchers. In addition to the performance improvement,\nAlecto can reduce the energy consumption associated with accessing the\nprefetchers' table by 48%, while only adding less than 1 KB of storage\noverhead."
                },
                "authors": [
                    {
                        "name": "Mengming Li"
                    },
                    {
                        "name": "Qijun Zhang"
                    },
                    {
                        "name": "Yongqing Ren"
                    },
                    {
                        "name": "Zhiyao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyao Xie"
                },
                "author": "Zhiyao Xie",
                "arxiv_comment": "In 31th IEEE International Symposium on High-Performance Computer\n  Architecture (HPCA 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14882v2",
                "updated": "2025-03-24T23:47:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    23,
                    47,
                    51,
                    0,
                    83,
                    0
                ],
                "published": "2025-02-15T05:08:01Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    5,
                    8,
                    1,
                    5,
                    46,
                    0
                ],
                "title": "CalibQuant: 1-Bit KV Cache Quantization for Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CalibQuant: 1-Bit KV Cache Quantization for Multimodal LLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nperformance across diverse applications. However, their computational overhead\nduring deployment remains a critical bottleneck. While Key-Value (KV) caching\neffectively trades memory for computation to enhance inference efficiency, the\ngrowing memory footprint from extensive KV caches significantly reduces\nthroughput and restricts prolonged deployment on memory-constrained GPU\ndevices. To address this challenge, we propose CalibQuant, a simple yet highly\neffective visual quantization strategy that drastically reduces both memory and\ncomputational overhead. Specifically, CalibQuant introduces an extreme 1-bit\nquantization scheme, complemented by novel post-scaling and calibration\ntechniques tailored to the intrinsic patterns of KV caches, thereby ensuring\nhigh efficiency without compromising model performance. Leveraging Triton for\nruntime optimization, we achieve a 10x throughput increase on InternVL models.\nOur method is designed to be plug-and-play, seamlessly integrating with various\nexisting MLLMs without requiring architectural changes. Extensive experiments\nconfirm that our approach significantly reduces memory usage while maintaining\ncomputational efficiency and preserving multimodal capabilities. Codes are\navailable at https://github.com/insuhan/calibquant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nperformance across diverse applications. However, their computational overhead\nduring deployment remains a critical bottleneck. While Key-Value (KV) caching\neffectively trades memory for computation to enhance inference efficiency, the\ngrowing memory footprint from extensive KV caches significantly reduces\nthroughput and restricts prolonged deployment on memory-constrained GPU\ndevices. To address this challenge, we propose CalibQuant, a simple yet highly\neffective visual quantization strategy that drastically reduces both memory and\ncomputational overhead. Specifically, CalibQuant introduces an extreme 1-bit\nquantization scheme, complemented by novel post-scaling and calibration\ntechniques tailored to the intrinsic patterns of KV caches, thereby ensuring\nhigh efficiency without compromising model performance. Leveraging Triton for\nruntime optimization, we achieve a 10x throughput increase on InternVL models.\nOur method is designed to be plug-and-play, seamlessly integrating with various\nexisting MLLMs without requiring architectural changes. Extensive experiments\nconfirm that our approach significantly reduces memory usage while maintaining\ncomputational efficiency and preserving multimodal capabilities. Codes are\navailable at https://github.com/insuhan/calibquant."
                },
                "authors": [
                    {
                        "name": "Insu Han"
                    },
                    {
                        "name": "Zeliang Zhang"
                    },
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Susan Liang"
                    },
                    {
                        "name": "Jiani Liu"
                    },
                    {
                        "name": "Haiting Lin"
                    },
                    {
                        "name": "Mingjie Zhao"
                    },
                    {
                        "name": "Chenliang Xu"
                    },
                    {
                        "name": "Kun Wan"
                    },
                    {
                        "name": "Wentian Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wentian Zhao"
                },
                "author": "Wentian Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09859v2",
                "updated": "2025-03-24T21:27:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    27,
                    53,
                    0,
                    83,
                    0
                ],
                "published": "2024-11-15T00:37:31Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "title": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures"
                },
                "summary": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra, particularly in comparison to that of general and\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. This\nwork examines the factorization of a skew-symmetric matrix $X$ into its\n$LTL^\\mathrm{T}$ decomposition, where $L$ is unit lower triangular and $T$ is\ntridiagonal. This is also known as a triangular tridiagonalization. This\noperation is a means for computing the determinant of $X$ as the square of the\n(cheaply-computed) Pfaffian of the skew-symmetric tridiagonal matrix $T$ as\nwell as for solving systems of equations, across fields such as quantum\nelectronic structure and machine learning. Its application also often requires\npivoting in order to improve numerical stability. We compare and contrast\npreviously-published algorithms with those systematically derived using the\nFLAME methodology. Performant parallel CPU implementations are achieved by\nfusing operations at multiple levels in order to reduce memory traffic\noverhead. A key factor is the employment of new capabilities of the BLAS-like\nLibrary Instantiation Software (BLIS) framework, which now supports casting\nlevel-2 and level-3 BLAS-like operations by leveraging its gemm and other\nkernels, hierarchical parallelism, and cache blocking. A prototype, concise C++\nAPI facilitates the translation of correct-by-construction algorithms into\ncorrect code. Experiments verify that the resulting implementations greatly\nexceed the performance of previous work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra, particularly in comparison to that of general and\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. This\nwork examines the factorization of a skew-symmetric matrix $X$ into its\n$LTL^\\mathrm{T}$ decomposition, where $L$ is unit lower triangular and $T$ is\ntridiagonal. This is also known as a triangular tridiagonalization. This\noperation is a means for computing the determinant of $X$ as the square of the\n(cheaply-computed) Pfaffian of the skew-symmetric tridiagonal matrix $T$ as\nwell as for solving systems of equations, across fields such as quantum\nelectronic structure and machine learning. Its application also often requires\npivoting in order to improve numerical stability. We compare and contrast\npreviously-published algorithms with those systematically derived using the\nFLAME methodology. Performant parallel CPU implementations are achieved by\nfusing operations at multiple levels in order to reduce memory traffic\noverhead. A key factor is the employment of new capabilities of the BLAS-like\nLibrary Instantiation Software (BLIS) framework, which now supports casting\nlevel-2 and level-3 BLAS-like operations by leveraging its gemm and other\nkernels, hierarchical parallelism, and cache blocking. A prototype, concise C++\nAPI facilitates the translation of correct-by-construction algorithms into\ncorrect code. Experiments verify that the resulting implementations greatly\nexceed the performance of previous work."
                },
                "authors": [
                    {
                        "name": "Ishna Satyarth"
                    },
                    {
                        "name": "Chao Yin"
                    },
                    {
                        "name": "Devin A. Matthews"
                    },
                    {
                        "name": "Maggie Myers"
                    },
                    {
                        "name": "Robert van de Geijn"
                    },
                    {
                        "name": "RuQing G. Xu"
                    }
                ],
                "author_detail": {
                    "name": "RuQing G. Xu"
                },
                "author": "RuQing G. Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19145v1",
                "updated": "2025-03-24T21:00:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    0,
                    37,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T21:00:37Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    0,
                    37,
                    0,
                    83,
                    0
                ],
                "title": "Compositional Caching for Training-free Open-vocabulary Attribute\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Caching for Training-free Open-vocabulary Attribute\n  Detection"
                },
                "summary": "Attribute detection is crucial for many computer vision tasks, as it enables\nsystems to describe properties such as color, texture, and material. Current\napproaches often rely on labor-intensive annotation processes which are\ninherently limited: objects can be described at an arbitrary level of detail\n(e.g., color vs. color shades), leading to ambiguities when the annotators are\nnot instructed carefully. Furthermore, they operate within a predefined set of\nattributes, reducing scalability and adaptability to unforeseen downstream\napplications. We present Compositional Caching (ComCa), a training-free method\nfor open-vocabulary attribute detection that overcomes these constraints. ComCa\nrequires only the list of target attributes and objects as input, using them to\npopulate an auxiliary cache of images by leveraging web-scale databases and\nLarge Language Models to determine attribute-object compatibility. To account\nfor the compositional nature of attributes, cache images receive soft attribute\nlabels. Those are aggregated at inference time based on the similarity between\nthe input and cache images, refining the predictions of underlying\nVision-Language Models (VLMs). Importantly, our approach is model-agnostic,\ncompatible with various VLMs. Experiments on public datasets demonstrate that\nComCa significantly outperforms zero-shot and cache-based baselines, competing\nwith recent training-based methods, proving that a carefully designed\ntraining-free approach can successfully address open-vocabulary attribute\ndetection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attribute detection is crucial for many computer vision tasks, as it enables\nsystems to describe properties such as color, texture, and material. Current\napproaches often rely on labor-intensive annotation processes which are\ninherently limited: objects can be described at an arbitrary level of detail\n(e.g., color vs. color shades), leading to ambiguities when the annotators are\nnot instructed carefully. Furthermore, they operate within a predefined set of\nattributes, reducing scalability and adaptability to unforeseen downstream\napplications. We present Compositional Caching (ComCa), a training-free method\nfor open-vocabulary attribute detection that overcomes these constraints. ComCa\nrequires only the list of target attributes and objects as input, using them to\npopulate an auxiliary cache of images by leveraging web-scale databases and\nLarge Language Models to determine attribute-object compatibility. To account\nfor the compositional nature of attributes, cache images receive soft attribute\nlabels. Those are aggregated at inference time based on the similarity between\nthe input and cache images, refining the predictions of underlying\nVision-Language Models (VLMs). Importantly, our approach is model-agnostic,\ncompatible with various VLMs. Experiments on public datasets demonstrate that\nComCa significantly outperforms zero-shot and cache-based baselines, competing\nwith recent training-based methods, proving that a carefully designed\ntraining-free approach can successfully address open-vocabulary attribute\ndetection."
                },
                "authors": [
                    {
                        "name": "Marco Garosi"
                    },
                    {
                        "name": "Alessandro Conti"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Elisa Ricci"
                    },
                    {
                        "name": "Massimiliano Mancini"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano Mancini"
                },
                "author": "Massimiliano Mancini",
                "arxiv_comment": "CVPR 2025. Project website at https://comca-attributes.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13773v2",
                "updated": "2025-03-24T18:50:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    18,
                    50,
                    9,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-17T23:38:29Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    23,
                    38,
                    29,
                    0,
                    76,
                    0
                ],
                "title": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference"
                },
                "summary": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    },
                    {
                        "name": "Masahiro Tanaka"
                    }
                ],
                "author_detail": {
                    "name": "Masahiro Tanaka"
                },
                "author": "Masahiro Tanaka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06364v2",
                "updated": "2025-03-24T18:16:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    18,
                    16,
                    58,
                    0,
                    83,
                    0
                ],
                "published": "2024-11-10T05:12:51Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "title": "EconoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EconoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving"
                },
                "summary": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EconoServe.\nEconoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EconoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EconoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EconoServe.\nEconoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EconoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EconoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18893v1",
                "updated": "2025-03-24T17:06:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    6,
                    37,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:06:37Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    6,
                    37,
                    0,
                    83,
                    0
                ],
                "title": "xKV: Cross-Layer SVD for KV-Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xKV: Cross-Layer SVD for KV-Cache Compression"
                },
                "summary": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13064v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13064v2",
                "updated": "2025-03-24T16:47:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    47,
                    48,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-17T11:10:49Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    10,
                    49,
                    0,
                    76,
                    0
                ],
                "title": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads"
                },
                "summary": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols equipped with specialized pathways to deep-learning\naccelerators such as Gemmini. Simulation tools like Gem5 and DRAMSim2 were used\nto evaluate baseline performance and scalability under representative ML\nworkloads. The findings of this study highlight the design choices, and the\nanticipated challenges, paving the way for low-latency scalable memory\noperations for ML applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols equipped with specialized pathways to deep-learning\naccelerators such as Gemmini. Simulation tools like Gem5 and DRAMSim2 were used\nto evaluate baseline performance and scalability under representative ML\nworkloads. The findings of this study highlight the design choices, and the\nanticipated challenges, paving the way for low-latency scalable memory\noperations for ML applications."
                },
                "authors": [
                    {
                        "name": "Pranav Suryadevara"
                    }
                ],
                "author_detail": {
                    "name": "Pranav Suryadevara"
                },
                "author": "Pranav Suryadevara",
                "arxiv_comment": "5 pages, 5 figures. Individual Project",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13064v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13064v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.2; C.1.3; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v1",
                "updated": "2025-03-24T16:44:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18862v1",
                "updated": "2025-03-24T16:38:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    38,
                    31,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:38:31Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    38,
                    31,
                    0,
                    83,
                    0
                ],
                "title": "Exploring the Integration of Key-Value Attention Into Pure and Hybrid\n  Transformers for Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Integration of Key-Value Attention Into Pure and Hybrid\n  Transformers for Semantic Segmentation"
                },
                "summary": "While CNNs were long considered state of the art for image processing, the\nintroduction of Transformer architectures has challenged this position. While\nachieving excellent results in image classification and segmentation,\nTransformers remain inherently reliant on large training datasets and remain\ncomputationally expensive. A newly introduced Transformer derivative named KV\nTransformer shows promising results in synthetic, NLP, and image classification\ntasks, while reducing complexity and memory usage. This is especially conducive\nto use cases where local inference is required, such as medical screening\napplications. We endeavoured to further evaluate the merit of KV Transformers\non semantic segmentation tasks, specifically in the domain of medical imaging.\nBy directly comparing traditional and KV variants of the same base\narchitectures, we provide further insight into the practical tradeoffs of\nreduced model complexity. We observe a notable reduction in parameter count and\nmultiply accumulate operations, while achieving similar performance from most\nof the KV variant models when directly compared to their QKV implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While CNNs were long considered state of the art for image processing, the\nintroduction of Transformer architectures has challenged this position. While\nachieving excellent results in image classification and segmentation,\nTransformers remain inherently reliant on large training datasets and remain\ncomputationally expensive. A newly introduced Transformer derivative named KV\nTransformer shows promising results in synthetic, NLP, and image classification\ntasks, while reducing complexity and memory usage. This is especially conducive\nto use cases where local inference is required, such as medical screening\napplications. We endeavoured to further evaluate the merit of KV Transformers\non semantic segmentation tasks, specifically in the domain of medical imaging.\nBy directly comparing traditional and KV variants of the same base\narchitectures, we provide further insight into the practical tradeoffs of\nreduced model complexity. We observe a notable reduction in parameter count and\nmultiply accumulate operations, while achieving similar performance from most\nof the KV variant models when directly compared to their QKV implementation."
                },
                "authors": [
                    {
                        "name": "DeShin Hwa"
                    },
                    {
                        "name": "Tobias Holmes"
                    },
                    {
                        "name": "Klaus Drechsler"
                    }
                ],
                "author_detail": {
                    "name": "Klaus Drechsler"
                },
                "author": "Klaus Drechsler",
                "arxiv_doi": "10.1007/978-3-658-47422-5_71",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-658-47422-5_71",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages, 3 figures, Preprint. Final version published in:\n  Bildverarbeitung f\\\"ur die Medizin 2025, Springer. DOI:\n  https://doi.org/10.1007/978-3-658-47422-5_71",
                "arxiv_journal_ref": "Bildverarbeitung f\\\"ur die Medizin 2025. BVM 2025. Informatik\n  aktuell. Springer Vieweg, Wiesbaden, pp 305-310",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18773v1",
                "updated": "2025-03-24T15:22:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T15:22:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with\n  Low-Bit KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with\n  Low-Bit KV Cache"
                },
                "summary": "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding."
                },
                "authors": [
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05941v2",
                "updated": "2025-03-24T13:09:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    13,
                    9,
                    3,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-07T21:16:41Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "title": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions"
                },
                "summary": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example."
                },
                "authors": [
                    {
                        "name": "Avinash Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Avinash Kumar"
                },
                "author": "Avinash Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18599v1",
                "updated": "2025-03-24T11:56:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T11:56:50Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "title": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization"
                },
                "summary": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques."
                },
                "authors": [
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Seongmin Hong"
                    },
                    {
                        "name": "RyeoWook Ko"
                    },
                    {
                        "name": "Soongyu Choi"
                    },
                    {
                        "name": "Hunjong Lee"
                    },
                    {
                        "name": "Junsoo Kim"
                    },
                    {
                        "name": "Joo-Young Kim"
                    },
                    {
                        "name": "Jongse Park"
                    }
                ],
                "author_detail": {
                    "name": "Jongse Park"
                },
                "author": "Jongse Park",
                "arxiv_comment": "15 pages, 14 figures, and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17333v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17333v2",
                "updated": "2025-03-24T11:00:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    0,
                    35,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-21T17:33:03Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    33,
                    3,
                    4,
                    80,
                    0
                ],
                "title": "Register Dispersion: Reducing the Footprint of the Vector Register File\n  in Vector Engines of Low-Cost RISC-V CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Register Dispersion: Reducing the Footprint of the Vector Register File\n  in Vector Engines of Low-Cost RISC-V CPUs"
                },
                "summary": "The deployment of Machine Learning (ML) applications at the edge on\nresource-constrained devices has accentuated the need for efficient ML\nprocessing on low-cost processors. While traditional CPUs provide programming\nflexibility, their general-purpose architecture often lacks the throughput\nrequired for complex ML models. The augmentation of a RISC-V processor with a\nvector unit can provide substantial data-level parallelism. However, increasing\nthe data-level parallelism supported by vector processing would make the Vector\nRegister File (VRF) a major area consumer in ultra low-cost processors, since\n32 vector registers are required for RISC-V Vector ISA compliance. This work\nleverages the insight that many ML vectorized kernels require a small number of\nactive vector registers, and proposes the use of a physically smaller VRF that\ndynamically caches only the vector registers currently accessed by the\napplication. This approach, called Register Dispersion, maps the architectural\nvector registers to a smaller set of physical registers. The proposed\nISA-compliant VRF is significantly smaller than a full-size VRF and operates\nlike a conventional cache, i.e., it only stores the most recently accessed\nvector registers. Essential registers remain readily accessible within the\ncompact VRF, while the others are offloaded to the cache/memory sub-system. The\ncompact VRF design is demonstrated to yield substantial area and power savings,\nas compared to using a full VRF, with no or minimal impact on performance. This\neffective trade-off renders the inclusion of vector units in low-cost\nprocessors feasible and practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning (ML) applications at the edge on\nresource-constrained devices has accentuated the need for efficient ML\nprocessing on low-cost processors. While traditional CPUs provide programming\nflexibility, their general-purpose architecture often lacks the throughput\nrequired for complex ML models. The augmentation of a RISC-V processor with a\nvector unit can provide substantial data-level parallelism. However, increasing\nthe data-level parallelism supported by vector processing would make the Vector\nRegister File (VRF) a major area consumer in ultra low-cost processors, since\n32 vector registers are required for RISC-V Vector ISA compliance. This work\nleverages the insight that many ML vectorized kernels require a small number of\nactive vector registers, and proposes the use of a physically smaller VRF that\ndynamically caches only the vector registers currently accessed by the\napplication. This approach, called Register Dispersion, maps the architectural\nvector registers to a smaller set of physical registers. The proposed\nISA-compliant VRF is significantly smaller than a full-size VRF and operates\nlike a conventional cache, i.e., it only stores the most recently accessed\nvector registers. Essential registers remain readily accessible within the\ncompact VRF, while the others are offloaded to the cache/memory sub-system. The\ncompact VRF design is demonstrated to yield substantial area and power savings,\nas compared to using a full VRF, with no or minimal impact on performance. This\neffective trade-off renders the inclusion of vector units in low-cost\nprocessors feasible and practical."
                },
                "authors": [
                    {
                        "name": "Vasileios Titopoulos"
                    },
                    {
                        "name": "George Alexakis"
                    },
                    {
                        "name": "Kosmas Alexandridis"
                    },
                    {
                        "name": "Chrysostomos Nicopoulos"
                    },
                    {
                        "name": "Giorgos Dimitrakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Dimitrakopoulos"
                },
                "author": "Giorgos Dimitrakopoulos",
                "arxiv_comment": "22nd ACM International Conference on Computing Frontiers (CF' 25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17333v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17333v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17038v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17038v2",
                "updated": "2025-03-24T07:29:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    7,
                    29,
                    28,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-21T10:48:35Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    48,
                    35,
                    4,
                    80,
                    0
                ],
                "title": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation"
                },
                "summary": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference."
                },
                "authors": [
                    {
                        "name": "Ashutosh Pradhan"
                    },
                    {
                        "name": "Daniele Ottaviano"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Haozheng Huang"
                    },
                    {
                        "name": "Alexander Zuepke"
                    },
                    {
                        "name": "Andrea Bastoni"
                    },
                    {
                        "name": "Marco Caccamo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Caccamo"
                },
                "author": "Marco Caccamo",
                "arxiv_comment": "Accepted for publication in the Proceedings of the 31st IEEE\n  Real-Time and Embedded Technology and Applications Symposium (RTAS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17038v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17038v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.3; C.4; D.4.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18334v1",
                "updated": "2025-03-24T04:32:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    4,
                    32,
                    35,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T04:32:35Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    4,
                    32,
                    35,
                    0,
                    83,
                    0
                ],
                "title": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models"
                },
                "summary": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n``Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n``Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability."
                },
                "authors": [
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Tianming Sha"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICME 2025 and ICLR 2025 Workshop on Foundation Models in\n  the Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16653v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16653v2",
                "updated": "2025-03-24T03:18:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    3,
                    18,
                    49,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-20T19:10:37Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    19,
                    10,
                    37,
                    3,
                    79,
                    0
                ],
                "title": "iFlame: Interleaving Full and Linear Attention for Efficient Mesh\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iFlame: Interleaving Full and Linear Attention for Efficient Mesh\n  Generation"
                },
                "summary": "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse."
                },
                "authors": [
                    {
                        "name": "Hanxiao Wang"
                    },
                    {
                        "name": "Biao Zhang"
                    },
                    {
                        "name": "Weize Quan"
                    },
                    {
                        "name": "Dong-Ming Yan"
                    },
                    {
                        "name": "Peter Wonka"
                    }
                ],
                "author_detail": {
                    "name": "Peter Wonka"
                },
                "author": "Peter Wonka",
                "arxiv_comment": "Project website: https://wanghanxiao123.github.io/iFa/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16653v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16653v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18292v1",
                "updated": "2025-03-24T02:28:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    28,
                    4,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T02:28:04Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    28,
                    4,
                    0,
                    83,
                    0
                ],
                "title": "Jenga: Effective Memory Management for Serving LLM with Heterogeneity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jenga: Effective Memory Management for Serving LLM with Heterogeneity"
                },
                "summary": "Large language models (LLMs) are widely used but expensive to run, especially\nas inference workloads grow. To lower costs, maximizing the request batch size\nby managing GPU memory efficiently is crucial. While PagedAttention has\nrecently been proposed to improve the efficiency of memory management, we find\nthat the growing heterogeneity in the embeddings dimensions, attention, and\naccess patterns of modern LLM architectures introduces new challenges for\nmemory allocation.\n  In this paper, we present Jenga, a novel memory allocation framework for\nheterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1)\nminimizing memory fragmentation when managing embeddings of different sizes,\nand (2) enabling flexible caching and eviction policies tailored to the\nspecific token-dependency patterns of various layers. Jenga employs a two-level\nmemory allocator, leveraging the least common multiple (LCM) of embedding sizes\nto optimize memory usage and providing APIs to express layer-specific caching\nlogic to enhance memory reuse.\n  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and\nevaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations\nshow that Jenga improves GPU memory utilization by up to 79.6%, and increases\nserving throughput by up to 4.92x (1.80x on average).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used but expensive to run, especially\nas inference workloads grow. To lower costs, maximizing the request batch size\nby managing GPU memory efficiently is crucial. While PagedAttention has\nrecently been proposed to improve the efficiency of memory management, we find\nthat the growing heterogeneity in the embeddings dimensions, attention, and\naccess patterns of modern LLM architectures introduces new challenges for\nmemory allocation.\n  In this paper, we present Jenga, a novel memory allocation framework for\nheterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1)\nminimizing memory fragmentation when managing embeddings of different sizes,\nand (2) enabling flexible caching and eviction policies tailored to the\nspecific token-dependency patterns of various layers. Jenga employs a two-level\nmemory allocator, leveraging the least common multiple (LCM) of embedding sizes\nto optimize memory usage and providing APIs to express layer-specific caching\nlogic to enhance memory reuse.\n  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and\nevaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations\nshow that Jenga improves GPU memory utilization by up to 79.6%, and increases\nserving throughput by up to 4.92x (1.80x on average)."
                },
                "authors": [
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Woosuk Kwon"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Yufeng Wang"
                    },
                    {
                        "name": "Xiaoxuan Liu"
                    },
                    {
                        "name": "Kaichao You"
                    },
                    {
                        "name": "Zhuohan Li"
                    },
                    {
                        "name": "Mingsheng Long"
                    },
                    {
                        "name": "Jidong Zhai"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_comment": "16 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v5",
                "updated": "2025-03-24T02:17:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    17,
                    34,
                    0,
                    83,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe."
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Rewrite the methods section. Add more ablation studies and results in\n  LongVideoBench. Update metadata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18278v1",
                "updated": "2025-03-24T01:47:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    47,
                    26,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T01:47:26Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    47,
                    26,
                    0,
                    83,
                    0
                ],
                "title": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model"
                },
                "summary": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach."
                },
                "authors": [
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Jinqi Xiao"
                    },
                    {
                        "name": "Lingyi Huang"
                    },
                    {
                        "name": "Yu Gong"
                    },
                    {
                        "name": "Chendi Li"
                    },
                    {
                        "name": "Jinghua Yan"
                    },
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Ponnuswamy Sadayappan"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Bo Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Bo Yuan"
                },
                "author": "Bo Yuan",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18265v1",
                "updated": "2025-03-24T01:15:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    15,
                    43,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T01:15:43Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    15,
                    43,
                    0,
                    83,
                    0
                ],
                "title": "Risk Management for Distributed Arbitrage Systems: Integrating\n  Artificial Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Risk Management for Distributed Arbitrage Systems: Integrating\n  Artificial Intelligence"
                },
                "summary": "Effective risk management solutions become absolutely crucial when financial\nmarkets embrace distributed technology and decentralized financing (DeFi). This\nstudy offers a thorough survey and comparative analysis of the integration of\nartificial intelligence (AI) in risk management for distributed arbitrage\nsystems. We examine several modern caching techniques namely in memory caching,\ndistributed caching, and proxy caching and their functions in enhancing\nperformance in decentralized settings. Through literature review we examine the\nutilization of AI techniques for alleviating risks related to market\nvolatility, liquidity challenges, operational failures, regulatory compliance,\nand security threats. This comparison research evaluates various case studies\nfrom prominent DeFi technologies, emphasizing critical performance metrics like\nlatency reduction, load balancing, and system resilience. Additionally, we\nexamine the problems and trade offs associated with these technologies,\nemphasizing their effects on consistency, scalability, and fault tolerance. By\nmeticulously analyzing real world applications, specifically centering on the\nAave platform as our principal case study, we illustrate how the purposeful\namalgamation of AI with contemporary caching methodologies has revolutionized\nrisk management in distributed arbitrage systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective risk management solutions become absolutely crucial when financial\nmarkets embrace distributed technology and decentralized financing (DeFi). This\nstudy offers a thorough survey and comparative analysis of the integration of\nartificial intelligence (AI) in risk management for distributed arbitrage\nsystems. We examine several modern caching techniques namely in memory caching,\ndistributed caching, and proxy caching and their functions in enhancing\nperformance in decentralized settings. Through literature review we examine the\nutilization of AI techniques for alleviating risks related to market\nvolatility, liquidity challenges, operational failures, regulatory compliance,\nand security threats. This comparison research evaluates various case studies\nfrom prominent DeFi technologies, emphasizing critical performance metrics like\nlatency reduction, load balancing, and system resilience. Additionally, we\nexamine the problems and trade offs associated with these technologies,\nemphasizing their effects on consistency, scalability, and fault tolerance. By\nmeticulously analyzing real world applications, specifically centering on the\nAave platform as our principal case study, we illustrate how the purposeful\namalgamation of AI with contemporary caching methodologies has revolutionized\nrisk management in distributed arbitrage systems."
                },
                "authors": [
                    {
                        "name": "Akaash Vishal Hazarika"
                    },
                    {
                        "name": "Mahak Shah"
                    },
                    {
                        "name": "Swapnil Patil"
                    },
                    {
                        "name": "Pradyumna Shukla"
                    }
                ],
                "author_detail": {
                    "name": "Pradyumna Shukla"
                },
                "author": "Pradyumna Shukla",
                "arxiv_comment": "International Conference on AI and Financial Innovation AIFI-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18191v1",
                "updated": "2025-03-23T20:18:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T20:18:16Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "title": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems"
                },
                "summary": "The large-scale, multi-tenant nature of cloud computing requires distributed\nfile systems that offer stability, adaptability, and compatibility. FUSE-based\ndistributed file systems have emerged as a popular solution for the cloud,\noffering fast deployment, fault isolation, and POSIX compliance. However,\nFUSE's performance limitations, particularly its inability to reconcile page\ncaching with strong consistency in distributed environments, remain a\npersistent problem. Existing approaches either sacrifice consistency for\nperformance or rely on inefficient caching, limiting their practicality.\n  To this end, we present DistFUSE, the first FUSE-based distributed file\nsystem that relies on a write-back kernel-based page cache for performance and\nprovides strong consistency. DistFUSE achieves this by offloading userspace\nlock management to the kernel driver, allowing coordinated access to the\nkernel's page cache across nodes. This design eliminates blind local cache\nupdates and ensures cluster-wide consistency without compromising performance.\nOur evaluation shows DistFUSE improves throughput by up to 75% compared to\nbaseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The large-scale, multi-tenant nature of cloud computing requires distributed\nfile systems that offer stability, adaptability, and compatibility. FUSE-based\ndistributed file systems have emerged as a popular solution for the cloud,\noffering fast deployment, fault isolation, and POSIX compliance. However,\nFUSE's performance limitations, particularly its inability to reconcile page\ncaching with strong consistency in distributed environments, remain a\npersistent problem. Existing approaches either sacrifice consistency for\nperformance or rely on inefficient caching, limiting their practicality.\n  To this end, we present DistFUSE, the first FUSE-based distributed file\nsystem that relies on a write-back kernel-based page cache for performance and\nprovides strong consistency. DistFUSE achieves this by offloading userspace\nlock management to the kernel driver, allowing coordinated access to the\nkernel's page cache across nodes. This design eliminates blind local cache\nupdates and ensures cluster-wide consistency without compromising performance.\nOur evaluation shows DistFUSE improves throughput by up to 75% compared to\nbaseline approaches."
                },
                "authors": [
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Jingkai Fu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Windsor Hsu"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18030v1",
                "updated": "2025-03-23T11:07:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    11,
                    7,
                    24,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T11:07:24Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    11,
                    7,
                    24,
                    6,
                    82,
                    0
                ],
                "title": "Formal Verification of Parameterized Systems based on Induction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal Verification of Parameterized Systems based on Induction"
                },
                "summary": "Parameterized systems play a crucial role in the computer field, and their\nsecurity is of great significance. Formal verification of parameterized\nprotocols is especially challenging due to its \"parameterized\" feature, which\nbrings complexity and undecidability. Existing automated parameterized\nverification methods have limitations, such as facing difficulties in\nautomatically deriving parameterized invariants constrained by mixed Forall and\nExists quantifiers, or having challenges in completing the parameterized\nverification of large and complex protocols. This paper proposes a formal\nverification framework for parameterized systems based on induction, named\nwiseParaverifier. It starts from small concretizations of protocols, analyzes\ninductive counterexamples, and constructs counterexample formulas to guide the\nentire process of parameterized verification. It also presents a heuristic\nGeneralize method to quickly find auxiliary invariants, a method for promoting\ncomplex mixed quantifiers and merging parameterized invariants, and uses\nsymmetric reduction ideas to accelerate the verification process. Experimental\nresults show that wiseParaverifier can successfully complete automatic\ninductive verification on 7 cache coherence protocols and 10 distributed\nprotocols. It has strong verification capabilities and migration capabilities,\nand can provide concise and readable verification results, which is helpful for\nlearners to understand protocol behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameterized systems play a crucial role in the computer field, and their\nsecurity is of great significance. Formal verification of parameterized\nprotocols is especially challenging due to its \"parameterized\" feature, which\nbrings complexity and undecidability. Existing automated parameterized\nverification methods have limitations, such as facing difficulties in\nautomatically deriving parameterized invariants constrained by mixed Forall and\nExists quantifiers, or having challenges in completing the parameterized\nverification of large and complex protocols. This paper proposes a formal\nverification framework for parameterized systems based on induction, named\nwiseParaverifier. It starts from small concretizations of protocols, analyzes\ninductive counterexamples, and constructs counterexample formulas to guide the\nentire process of parameterized verification. It also presents a heuristic\nGeneralize method to quickly find auxiliary invariants, a method for promoting\ncomplex mixed quantifiers and merging parameterized invariants, and uses\nsymmetric reduction ideas to accelerate the verification process. Experimental\nresults show that wiseParaverifier can successfully complete automatic\ninductive verification on 7 cache coherence protocols and 10 distributed\nprotocols. It has strong verification capabilities and migration capabilities,\nand can provide concise and readable verification results, which is helpful for\nlearners to understand protocol behaviors."
                },
                "authors": [
                    {
                        "name": "Jiaqi Xiu"
                    },
                    {
                        "name": "Yongjian Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongjian Li"
                },
                "author": "Yongjian Li",
                "arxiv_comment": "9 pages,2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.10425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.10425v2",
                "updated": "2025-03-23T06:14:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    6,
                    14,
                    35,
                    6,
                    82,
                    0
                ],
                "published": "2023-12-16T11:40:49Z",
                "published_parsed": [
                    2023,
                    12,
                    16,
                    11,
                    40,
                    49,
                    5,
                    350,
                    0
                ],
                "title": "Knowledge Rumination for Client Utility Evaluation in Heterogeneous\n  Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Rumination for Client Utility Evaluation in Heterogeneous\n  Federated Learning"
                },
                "summary": "Federated Learning (FL) allows several clients to cooperatively train machine\nlearning models without disclosing the raw data. In practical applications,\nasynchronous FL (AFL) can address the straggler effect compared to synchronous\nFL. However, Non-IID data and stale models pose significant challenges to AFL,\nas they can diminish the practicality of the global model and even lead to\ntraining failures. In this work, we propose a novel AFL framework called\nFederated Historical Learning (FedHist), which effectively addresses the\nchallenges posed by both Non-IID data and gradient staleness based on the\nconcept of knowledge rumination. FedHist enhances the stability of local\ngradients by performing weighted fusion with historical global gradients cached\non the server. Relying on hindsight, it assigns aggregation weights to each\nparticipant in a multi-dimensional manner during each communication round. To\nfurther enhance the efficiency and stability of the training process, we\nintroduce an intelligent $\\ell_2$-norm amplification scheme, which dynamically\nregulates the learning progress based on the $\\ell_2$-norms of the submitted\ngradients. Extensive experiments indicate FedHist outperforms state-of-the-art\nmethods in terms of convergence performance and test accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) allows several clients to cooperatively train machine\nlearning models without disclosing the raw data. In practical applications,\nasynchronous FL (AFL) can address the straggler effect compared to synchronous\nFL. However, Non-IID data and stale models pose significant challenges to AFL,\nas they can diminish the practicality of the global model and even lead to\ntraining failures. In this work, we propose a novel AFL framework called\nFederated Historical Learning (FedHist), which effectively addresses the\nchallenges posed by both Non-IID data and gradient staleness based on the\nconcept of knowledge rumination. FedHist enhances the stability of local\ngradients by performing weighted fusion with historical global gradients cached\non the server. Relying on hindsight, it assigns aggregation weights to each\nparticipant in a multi-dimensional manner during each communication round. To\nfurther enhance the efficiency and stability of the training process, we\nintroduce an intelligent $\\ell_2$-norm amplification scheme, which dynamically\nregulates the learning progress based on the $\\ell_2$-norms of the submitted\ngradients. Extensive experiments indicate FedHist outperforms state-of-the-art\nmethods in terms of convergence performance and test accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaorui Jiang"
                    },
                    {
                        "name": "Yu Gao"
                    },
                    {
                        "name": "Hengwei Xu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Yong Liao"
                    },
                    {
                        "name": "Pengyuan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Pengyuan Zhou"
                },
                "author": "Pengyuan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.10425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.10425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17922v1",
                "updated": "2025-03-23T03:36:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    36,
                    52,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T03:36:52Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    36,
                    52,
                    6,
                    82,
                    0
                ],
                "title": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference"
                },
                "summary": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness."
                },
                "authors": [
                    {
                        "name": "Youhui Zuo"
                    },
                    {
                        "name": "Sibo Wei"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhuorui Liu"
                    },
                    {
                        "name": "Wenpeng Lu"
                    },
                    {
                        "name": "Dawei Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Song"
                },
                "author": "Dawei Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17913v1",
                "updated": "2025-03-23T03:20:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    20,
                    25,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T03:20:25Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    20,
                    25,
                    6,
                    82,
                    0
                ],
                "title": "Cache-Aware Cooperative Multicast Beamforming in Dynamic\n  Satellite-Terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aware Cooperative Multicast Beamforming in Dynamic\n  Satellite-Terrestrial Networks"
                },
                "summary": "With the burgeoning demand for data-intensive services, satellite-terrestrial\nnetworks (STNs) face increasing backhaul link congestion, deteriorating user\nquality of service (QoS), and escalating power consumption. Cache-aided STNs\nare acknowledged as a promising paradigm for accelerating content delivery to\nusers and alleviating the load of backhaul links. However, the dynamic nature\nof low earth orbit (LEO) satellites and the complex interference among\nsatellite beams and terrestrial base stations pose challenges in effectively\nmanaging limited edge resources. To address these issues, this paper proposes a\nmethod for dynamically scheduling caching and communication resources, aiming\nto reduce network costs in terms of transmission power consumption and backhaul\ntraffic, while meeting user QoS demands and resource constraints. We formulate\na mixed timescale problem to jointly optimize cache placement, LEO satellite\nbeam direction, and cooperative multicast beamforming among satellite beams and\nbase stations. To tackle this intricate problem, we propose a two-stage\nsolution framework, where the primary problem is decoupled into a short-term\ncontent delivery subproblem and a long-term cache placement subproblem. The\nformer subproblem is solved by designing an alternating optimization approach\nwith whale optimization and successive convex approximation methods according\nto the cache placement state, while cache content in STNs is updated using an\niterative algorithm that utilizes historical information. Simulation results\ndemonstrate the effectiveness of our proposed algorithms, showcasing their\nconvergence and significantly reducing transmission power consumption and\nbackhaul traffic by up to 52%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the burgeoning demand for data-intensive services, satellite-terrestrial\nnetworks (STNs) face increasing backhaul link congestion, deteriorating user\nquality of service (QoS), and escalating power consumption. Cache-aided STNs\nare acknowledged as a promising paradigm for accelerating content delivery to\nusers and alleviating the load of backhaul links. However, the dynamic nature\nof low earth orbit (LEO) satellites and the complex interference among\nsatellite beams and terrestrial base stations pose challenges in effectively\nmanaging limited edge resources. To address these issues, this paper proposes a\nmethod for dynamically scheduling caching and communication resources, aiming\nto reduce network costs in terms of transmission power consumption and backhaul\ntraffic, while meeting user QoS demands and resource constraints. We formulate\na mixed timescale problem to jointly optimize cache placement, LEO satellite\nbeam direction, and cooperative multicast beamforming among satellite beams and\nbase stations. To tackle this intricate problem, we propose a two-stage\nsolution framework, where the primary problem is decoupled into a short-term\ncontent delivery subproblem and a long-term cache placement subproblem. The\nformer subproblem is solved by designing an alternating optimization approach\nwith whale optimization and successive convex approximation methods according\nto the cache placement state, while cache content in STNs is updated using an\niterative algorithm that utilizes historical information. Simulation results\ndemonstrate the effectiveness of our proposed algorithms, showcasing their\nconvergence and significantly reducing transmission power consumption and\nbackhaul traffic by up to 52%."
                },
                "authors": [
                    {
                        "name": "Shuo Yuan"
                    },
                    {
                        "name": "Yaohua Sun"
                    },
                    {
                        "name": "Mugen Peng"
                    }
                ],
                "author_detail": {
                    "name": "Mugen Peng"
                },
                "author": "Mugen Peng",
                "arxiv_doi": "10.1109/TVT.2024.3463548",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVT.2024.3463548",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.17913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by IEEE Transactions on Vehicular Technology",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17911v1",
                "updated": "2025-03-23T03:16:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T03:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "title": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaoyao Zhong"
                    },
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Deming Chu"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "George Gu"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Heng Tao Shen"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "arxiv_comment": "16 pages, the report of open-source library VSAG\n  (https://github.com/antgroup/vsag)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17895v1",
                "updated": "2025-03-23T01:17:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    1,
                    17,
                    8,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T01:17:08Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    1,
                    17,
                    8,
                    6,
                    82,
                    0
                ],
                "title": "Orientation-Dependent \\b{eta}-Ga2O3 Heterojunction Diode with Atomic\n  Layer Deposition (ALD) Grown NiO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orientation-Dependent \\b{eta}-Ga2O3 Heterojunction Diode with Atomic\n  Layer Deposition (ALD) Grown NiO"
                },
                "summary": "This work reports the demonstration of ALD-deposited NiO/\\b{eta}-Ga2O3\nheterojunction diodes (HJDs) on low doped drift layer and highly doped (001) &\n(100) n+ substrates with experimental observation of a parallel-plane junction\nelectric field as high as 7.5 MV/cm, revealing a crystal orientation dependence\nin \\b{eta}-Ga2O3. We use a novel metalorganic precursor\nbis(1,4-di-tert-butyl-1,3-diazadienyl) (nickel Ni(tBu2DAD)2) with ozone (O3) to\ndeposit NiO. The NiO/\\b{eta}-Ga2O3 HJD on 7.7 {\\mu}m-thick HVPE-grown drift\nregion exhibited an on-state current density of ~20 A/cm2 at 5 V, ~10-8 A/cm2\nreverse leakage at low reverse bias(-5 V), and a rectifying ratio(Jon/Joff) of\n~109. The HJD broke down at ~2.2 kV reverse bias, corresponding to a ~3.4 MV/cm\nparallel-plane junction electric field, with a noise floor reverse leakage\n(10-8~10-6 A/cm2, nA) at 80% of the device catastrophic breakdown voltage. The\nNiO/\\b{eta}-Ga2O3 HJDs on n+ (001) & (100) highly-doped substrates exhibited\nbreakdown voltages at 12.5-16.0 V and 28.5-70.5 V, respectively, with extracted\ncritical electric field (EC) at 2.30-2.76 MV/cm, and 4.33-7.50 MV/cm, revealing\na substrate crystal orientation dependence on breakdown electric field for\n\\b{eta}-Ga2O3. The 7.5 MV/cm EC reported here is one of the highest\nparallel-plane junction electric fields reported in literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work reports the demonstration of ALD-deposited NiO/\\b{eta}-Ga2O3\nheterojunction diodes (HJDs) on low doped drift layer and highly doped (001) &\n(100) n+ substrates with experimental observation of a parallel-plane junction\nelectric field as high as 7.5 MV/cm, revealing a crystal orientation dependence\nin \\b{eta}-Ga2O3. We use a novel metalorganic precursor\nbis(1,4-di-tert-butyl-1,3-diazadienyl) (nickel Ni(tBu2DAD)2) with ozone (O3) to\ndeposit NiO. The NiO/\\b{eta}-Ga2O3 HJD on 7.7 {\\mu}m-thick HVPE-grown drift\nregion exhibited an on-state current density of ~20 A/cm2 at 5 V, ~10-8 A/cm2\nreverse leakage at low reverse bias(-5 V), and a rectifying ratio(Jon/Joff) of\n~109. The HJD broke down at ~2.2 kV reverse bias, corresponding to a ~3.4 MV/cm\nparallel-plane junction electric field, with a noise floor reverse leakage\n(10-8~10-6 A/cm2, nA) at 80% of the device catastrophic breakdown voltage. The\nNiO/\\b{eta}-Ga2O3 HJDs on n+ (001) & (100) highly-doped substrates exhibited\nbreakdown voltages at 12.5-16.0 V and 28.5-70.5 V, respectively, with extracted\ncritical electric field (EC) at 2.30-2.76 MV/cm, and 4.33-7.50 MV/cm, revealing\na substrate crystal orientation dependence on breakdown electric field for\n\\b{eta}-Ga2O3. The 7.5 MV/cm EC reported here is one of the highest\nparallel-plane junction electric fields reported in literature."
                },
                "authors": [
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Shane M. W. Witsell"
                    },
                    {
                        "name": "John F. Conley"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17603v1",
                "updated": "2025-03-22T01:17:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    17,
                    56,
                    5,
                    81,
                    0
                ],
                "published": "2025-03-22T01:17:56Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    17,
                    56,
                    5,
                    81,
                    0
                ],
                "title": "A Generative Caching System for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generative Caching System for Large Language Models"
                },
                "summary": "Caching has the potential to be of significant benefit for accessing large\nlanguage models (LLMs) due to their high latencies which typically range from a\nsmall number of seconds to well over a minute. Furthermore, many LLMs charge\nmoney for queries; caching thus has a clear monetary benefit. This paper\npresents a new caching system for improving user experiences with LLMs. In\naddition to reducing both latencies and monetary costs for accessing LLMs, our\nsystem also provides important features that go beyond the performance benefits\ntypically associated with caches. A key feature we provide is generative\ncaching, wherein multiple cached responses can be synthesized to provide\nanswers to queries which have never been seen before. Our generative caches\nfunction as repositories of valuable information which can be mined and\nanalyzed. We also improve upon past semantic caching techniques by tailoring\nthe caching algorithms to optimally balance cost and latency reduction with the\nquality of responses provided. Performance tests indicate that our caches are\nconsiderably faster than GPTcache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching has the potential to be of significant benefit for accessing large\nlanguage models (LLMs) due to their high latencies which typically range from a\nsmall number of seconds to well over a minute. Furthermore, many LLMs charge\nmoney for queries; caching thus has a clear monetary benefit. This paper\npresents a new caching system for improving user experiences with LLMs. In\naddition to reducing both latencies and monetary costs for accessing LLMs, our\nsystem also provides important features that go beyond the performance benefits\ntypically associated with caches. A key feature we provide is generative\ncaching, wherein multiple cached responses can be synthesized to provide\nanswers to queries which have never been seen before. Our generative caches\nfunction as repositories of valuable information which can be mined and\nanalyzed. We also improve upon past semantic caching techniques by tailoring\nthe caching algorithms to optimally balance cost and latency reduction with the\nquality of responses provided. Performance tests indicate that our caches are\nconsiderably faster than GPTcache."
                },
                "authors": [
                    {
                        "name": "Arun Iyengar"
                    },
                    {
                        "name": "Ashish Kundu"
                    },
                    {
                        "name": "Ramana Kompella"
                    },
                    {
                        "name": "Sai Nandan Mamidi"
                    }
                ],
                "author_detail": {
                    "name": "Sai Nandan Mamidi"
                },
                "author": "Sai Nandan Mamidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17602v1",
                "updated": "2025-03-22T01:16:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    16,
                    24,
                    5,
                    81,
                    0
                ],
                "published": "2025-03-22T01:16:24Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    16,
                    24,
                    5,
                    81,
                    0
                ],
                "title": "Multiport Support for Vortex OpenGPU Memory Hierarchy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiport Support for Vortex OpenGPU Memory Hierarchy"
                },
                "summary": "Modern day applications have grown in size and require more computational\npower. The rise of machine learning and AI increased the need for parallel\ncomputation, which has increased the need for GPGPUs. With the increasing\ndemand for computational power, GPGPUs' SIMT architecture has solved this with\nan increase in the number of threads and the number of cores in a GPU,\nincreasing the throughput of these processors to match the demand of the\napplications. However, this created a larger demand for the memory, making the\nmemory bandwidth a bottleneck. The introduction of High-Bandwidth Memory (HBM)\nwith its increased number of memory ports offers a potential solution for the\nGPU to exploit its memory parallelism to increase the memory bandwidth.\nHowever, effectively leveraging HBM's memory parallelism to maximize bandwidth\npresents a unique and complex challenge for GPU architectures on how to\ndistribute those ports among the streaming multiprocessors in the GPGPU. In\nthis work, we extend the Vortex OpenGPU microarchitecture to incorporate a\nmultiport memory hierarchy, spanning from the L1 cache to the last-level cache\n(LLC). In addition, we propose various arbitration strategies to optimize\nmemory transfers across the cache hierarchy. The results have shown that an\nincrease in memory ports increases IPC, achieving an average speedup of 2.34x\nwith 8 memory ports in the tested configuration while showing relatively small\narea overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern day applications have grown in size and require more computational\npower. The rise of machine learning and AI increased the need for parallel\ncomputation, which has increased the need for GPGPUs. With the increasing\ndemand for computational power, GPGPUs' SIMT architecture has solved this with\nan increase in the number of threads and the number of cores in a GPU,\nincreasing the throughput of these processors to match the demand of the\napplications. However, this created a larger demand for the memory, making the\nmemory bandwidth a bottleneck. The introduction of High-Bandwidth Memory (HBM)\nwith its increased number of memory ports offers a potential solution for the\nGPU to exploit its memory parallelism to increase the memory bandwidth.\nHowever, effectively leveraging HBM's memory parallelism to maximize bandwidth\npresents a unique and complex challenge for GPU architectures on how to\ndistribute those ports among the streaming multiprocessors in the GPGPU. In\nthis work, we extend the Vortex OpenGPU microarchitecture to incorporate a\nmultiport memory hierarchy, spanning from the L1 cache to the last-level cache\n(LLC). In addition, we propose various arbitration strategies to optimize\nmemory transfers across the cache hierarchy. The results have shown that an\nincrease in memory ports increases IPC, achieving an average speedup of 2.34x\nwith 8 memory ports in the tested configuration while showing relatively small\narea overhead."
                },
                "authors": [
                    {
                        "name": "Injae Shin"
                    },
                    {
                        "name": "Blaise Tine"
                    }
                ],
                "author_detail": {
                    "name": "Blaise Tine"
                },
                "author": "Blaise Tine",
                "arxiv_comment": "OSSMPIC2025, 1st workshop on Open Source Solutions for Massively\n  Parallel Integrated Circuits",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v2",
                "updated": "2025-03-21T21:10:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    21,
                    10,
                    2,
                    4,
                    80,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09003v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09003v2",
                "updated": "2025-03-21T19:26:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    19,
                    26,
                    12,
                    4,
                    80,
                    0
                ],
                "published": "2025-02-13T06:44:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models"
                },
                "summary": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia, Qwen and\nLlama models of different sizes demonstrate the effectiveness of RoSTE.\nCompared to existing post-SFT quantization baselines, our method consistently\nachieves superior performances across various tasks and different LLM\narchitectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia, Qwen and\nLlama models of different sizes demonstrate the effectiveness of RoSTE.\nCompared to existing post-SFT quantization baselines, our method consistently\nachieves superior performances across various tasks and different LLM\narchitectures."
                },
                "authors": [
                    {
                        "name": "Quan Wei"
                    },
                    {
                        "name": "Chung-Yiu Yau"
                    },
                    {
                        "name": "Hoi-To Wai"
                    },
                    {
                        "name": "Yang Katie Zhao"
                    },
                    {
                        "name": "Dongyeop Kang"
                    },
                    {
                        "name": "Youngsuk Park"
                    },
                    {
                        "name": "Mingyi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Mingyi Hong"
                },
                "author": "Mingyi Hong",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09003v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12444v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12444v3",
                "updated": "2025-03-21T15:52:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    52,
                    39,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-17T01:12:35Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "title": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers"
                },
                "summary": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency. Code: https://github.com/shawnricecake/lazydit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency. Code: https://github.com/shawnricecake/lazydit"
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yanyu Li"
                    },
                    {
                        "name": "Yifan Gong"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Zhihao Shu"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12444v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12444v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v3",
                "updated": "2025-03-21T15:47:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    47,
                    53,
                    4,
                    80,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "24 pages, 11 figures, ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v4",
                "updated": "2025-03-21T13:30:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    13,
                    30,
                    33,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Accepted to ICLR 2025. Code is available at\n  https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v3",
                "updated": "2025-03-21T12:51:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    12,
                    51,
                    15,
                    4,
                    80,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16870v1",
                "updated": "2025-03-21T05:58:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T05:58:18Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs"
                },
                "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B."
                },
                "authors": [
                    {
                        "name": "Anshumann"
                    },
                    {
                        "name": "Mohd Abbas Zaidi"
                    },
                    {
                        "name": "Akhil Kedia"
                    },
                    {
                        "name": "Jinwoo Ahn"
                    },
                    {
                        "name": "Taehwak Kwon"
                    },
                    {
                        "name": "Kangwook Lee"
                    },
                    {
                        "name": "Haejun Lee"
                    },
                    {
                        "name": "Joohyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Joohyung Lee"
                },
                "author": "Joohyung Lee",
                "arxiv_comment": "Anshumann, Mohd Abbas Zaidi and Akhil Kedia have Equal Contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16131v2",
                "updated": "2025-03-21T01:59:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    1,
                    59,
                    12,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-20T13:25:03Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    25,
                    3,
                    3,
                    79,
                    0
                ],
                "title": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for\n  Multilingual Medical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for\n  Multilingual Medical Question Answering"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 35.03% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 35.03% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds."
                },
                "authors": [
                    {
                        "name": "Feiyang Li"
                    },
                    {
                        "name": "Yingjian Chen"
                    },
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Han Yuan"
                    },
                    {
                        "name": "Yuang Jiang"
                    },
                    {
                        "name": "Tianxiao Li"
                    },
                    {
                        "name": "Edison Marrese Taylor"
                    },
                    {
                        "name": "Hossein Rouhizadeh"
                    },
                    {
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "name": "Douglas Teodoro"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "name": "Irene Li"
                    }
                ],
                "author_detail": {
                    "name": "Irene Li"
                },
                "author": "Irene Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02430v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02430v3",
                "updated": "2025-03-20T21:49:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    21,
                    49,
                    15,
                    3,
                    79,
                    0
                ],
                "published": "2025-02-04T15:55:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals"
                },
                "summary": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach."
                },
                "authors": [
                    {
                        "name": "Róbert Busa-Fekete"
                    },
                    {
                        "name": "Julian Zimmert"
                    },
                    {
                        "name": "András György"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Tzu-Wei Sung"
                    },
                    {
                        "name": "Hao Shen"
                    },
                    {
                        "name": "Hyomin Choi"
                    },
                    {
                        "name": "Sharmila Subramaniam"
                    },
                    {
                        "name": "Li Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiao"
                },
                "author": "Li Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02430v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02430v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v1",
                "updated": "2025-03-20T17:37:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to\nblock-wise competitiveness and systematically analyze the competitiveness and\nblock competitiveness of FIFO and MRU relative to LRU for arbitrary\nassociativities. We show how competitiveness and block competitiveness can be\nexploited in state-of-the-art WCET analysis based on the results of existing\npersistence analyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to\nblock-wise competitiveness and systematically analyze the competitiveness and\nblock competitiveness of FIFO and MRU relative to LRU for arbitrary\nassociativities. We show how competitiveness and block competitiveness can be\nexploited in state-of-the-art WCET analysis based on the results of existing\npersistence analyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16302v1",
                "updated": "2025-03-20T16:23:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    23,
                    44,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T16:23:44Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    23,
                    44,
                    3,
                    79,
                    0
                ],
                "title": "Unleashing Vecset Diffusion Model for Fast Shape Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Vecset Diffusion Model for Fast Shape Generation"
                },
                "summary": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM."
                },
                "authors": [
                    {
                        "name": "Zeqiang Lai"
                    },
                    {
                        "name": "Yunfei Zhao"
                    },
                    {
                        "name": "Zibo Zhao"
                    },
                    {
                        "name": "Haolin Liu"
                    },
                    {
                        "name": "Fuyun Wang"
                    },
                    {
                        "name": "Huiwen Shi"
                    },
                    {
                        "name": "Xianghui Yang"
                    },
                    {
                        "name": "Qinxiang Lin"
                    },
                    {
                        "name": "Jinwei Huang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Chunchao Guo"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16257v1",
                "updated": "2025-03-20T15:52:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T15:52:43Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models"
                },
                "summary": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16163v1",
                "updated": "2025-03-20T14:01:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    1,
                    56,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T14:01:56Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    1,
                    56,
                    3,
                    79,
                    0
                ],
                "title": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs"
                },
                "summary": "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio."
                },
                "authors": [
                    {
                        "name": "Shibo Jie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Zhi-Hong Deng"
                    },
                    {
                        "name": "Jing Han"
                    }
                ],
                "author_detail": {
                    "name": "Jing Han"
                },
                "author": "Jing Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16112v1",
                "updated": "2025-03-20T13:00:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    0,
                    36,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T13:00:36Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    0,
                    36,
                    3,
                    79,
                    0
                ],
                "title": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming"
                },
                "summary": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6\\%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60\\% of severely distorted\nframes (compared to VQGAN).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6\\%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60\\% of severely distorted\nframes (compared to VQGAN)."
                },
                "authors": [
                    {
                        "name": "Liming Liu"
                    },
                    {
                        "name": "Jiangkai Wu"
                    },
                    {
                        "name": "Haoyang Wang"
                    },
                    {
                        "name": "Peiheng Wang"
                    },
                    {
                        "name": "Xinggong Zhang"
                    },
                    {
                        "name": "Zongming Guo"
                    }
                ],
                "author_detail": {
                    "name": "Zongming Guo"
                },
                "author": "Zongming Guo",
                "arxiv_comment": "7 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15927v1",
                "updated": "2025-03-20T08:07:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    7,
                    31,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T08:07:31Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    7,
                    31,
                    3,
                    79,
                    0
                ],
                "title": "BlockDance: Reuse Structurally Similar Spatio-Temporal Features to\n  Accelerate Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockDance: Reuse Structurally Similar Spatio-Temporal Features to\n  Accelerate Diffusion Transformers"
                },
                "summary": "Diffusion models have demonstrated impressive generation capabilities,\nparticularly with recent advancements leveraging transformer architectures to\nimprove both visual and artistic quality. However, Diffusion Transformers\n(DiTs) continue to encounter challenges related to low inference speed,\nprimarily due to the iterative denoising process. To address this issue, we\npropose BlockDance, a training-free approach that explores feature similarities\nat adjacent time steps to accelerate DiTs. Unlike previous feature-reuse\nmethods that lack tailored reuse strategies for features at different scales,\nBlockDance prioritizes the identification of the most structurally similar\nfeatures, referred to as Structurally Similar Spatio-Temporal (STSS) features.\nThese features are primarily located within the structure-focused blocks of the\ntransformer during the later stages of denoising. BlockDance caches and reuses\nthese highly similar features to mitigate redundant computation, thereby\naccelerating DiTs while maximizing consistency with the generated results of\nthe original model. Furthermore, considering the diversity of generated content\nand the varying distributions of redundant features, we introduce\nBlockDance-Ada, a lightweight decision-making network tailored for\ninstance-specific acceleration. BlockDance-Ada dynamically allocates resources\nand provides superior content quality. Both BlockDance and BlockDance-Ada have\nproven effective across various generation tasks and models, achieving\naccelerations between 25% and 50% while maintaining generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated impressive generation capabilities,\nparticularly with recent advancements leveraging transformer architectures to\nimprove both visual and artistic quality. However, Diffusion Transformers\n(DiTs) continue to encounter challenges related to low inference speed,\nprimarily due to the iterative denoising process. To address this issue, we\npropose BlockDance, a training-free approach that explores feature similarities\nat adjacent time steps to accelerate DiTs. Unlike previous feature-reuse\nmethods that lack tailored reuse strategies for features at different scales,\nBlockDance prioritizes the identification of the most structurally similar\nfeatures, referred to as Structurally Similar Spatio-Temporal (STSS) features.\nThese features are primarily located within the structure-focused blocks of the\ntransformer during the later stages of denoising. BlockDance caches and reuses\nthese highly similar features to mitigate redundant computation, thereby\naccelerating DiTs while maximizing consistency with the generated results of\nthe original model. Furthermore, considering the diversity of generated content\nand the varying distributions of redundant features, we introduce\nBlockDance-Ada, a lightweight decision-making network tailored for\ninstance-specific acceleration. BlockDance-Ada dynamically allocates resources\nand provides superior content quality. Both BlockDance and BlockDance-Ada have\nproven effective across various generation tasks and models, achieving\naccelerations between 25% and 50% while maintaining generation quality."
                },
                "authors": [
                    {
                        "name": "Hui Zhang"
                    },
                    {
                        "name": "Tingwei Gao"
                    },
                    {
                        "name": "Jie Shao"
                    },
                    {
                        "name": "Zuxuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zuxuan Wu"
                },
                "author": "Zuxuan Wu",
                "arxiv_comment": "Accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18921v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18921v2",
                "updated": "2025-03-20T05:23:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    5,
                    23,
                    42,
                    3,
                    79,
                    0
                ],
                "published": "2024-07-09T13:47:05Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    13,
                    47,
                    5,
                    1,
                    191,
                    0
                ],
                "title": "Mobile Edge Intelligence for Large Language Models: A Contemporary\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Edge Intelligence for Large Language Models: A Contemporary\n  Survey"
                },
                "summary": "On-device large language models (LLMs), referring to running LLMs on edge\ndevices, have raised considerable interest since they are more cost-effective,\nlatency-efficient, and privacy-preserving compared with the cloud paradigm.\nNonetheless, the performance of on-device LLMs is intrinsically constrained by\nresource limitations on edge devices. Sitting between cloud and on-device AI,\nmobile edge intelligence (MEI) presents a viable solution by provisioning AI\ncapabilities at the edge of mobile networks, enabling end users to offload\nheavy AI computation to capable edge servers nearby. This article provides a\ncontemporary survey on harnessing MEI for LLMs. We begin by illustrating\nseveral killer applications to demonstrate the urgent need for deploying LLMs\nat the network edge. Next, we present the preliminaries of LLMs and MEI,\nfollowed by resource-efficient LLM techniques. We then present an architectural\noverview of MEI for LLMs (MEI4LLM), outlining its core components and how it\nsupports the deployment of LLMs. Subsequently, we delve into various aspects of\nMEI4LLM, extensively covering edge LLM caching and delivery, edge LLM training,\nand edge LLM inference. Finally, we identify future research opportunities. We\nhope this article inspires researchers in the field to leverage mobile edge\ncomputing to facilitate LLM deployment, thereby unleashing the potential of\nLLMs across various privacy- and delay-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-device large language models (LLMs), referring to running LLMs on edge\ndevices, have raised considerable interest since they are more cost-effective,\nlatency-efficient, and privacy-preserving compared with the cloud paradigm.\nNonetheless, the performance of on-device LLMs is intrinsically constrained by\nresource limitations on edge devices. Sitting between cloud and on-device AI,\nmobile edge intelligence (MEI) presents a viable solution by provisioning AI\ncapabilities at the edge of mobile networks, enabling end users to offload\nheavy AI computation to capable edge servers nearby. This article provides a\ncontemporary survey on harnessing MEI for LLMs. We begin by illustrating\nseveral killer applications to demonstrate the urgent need for deploying LLMs\nat the network edge. Next, we present the preliminaries of LLMs and MEI,\nfollowed by resource-efficient LLM techniques. We then present an architectural\noverview of MEI for LLMs (MEI4LLM), outlining its core components and how it\nsupports the deployment of LLMs. Subsequently, we delve into various aspects of\nMEI4LLM, extensively covering edge LLM caching and delivery, edge LLM training,\nand edge LLM inference. Finally, we identify future research opportunities. We\nhope this article inspires researchers in the field to leverage mobile edge\ncomputing to facilitate LLM deployment, thereby unleashing the potential of\nLLMs across various privacy- and delay-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Guanqiao Qu"
                    },
                    {
                        "name": "Qiyuan Chen"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "arxiv_comment": "42 pages, 17 figures. This paper has been accepted by IEEE\n  Communications Surveys & Tutorials",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18921v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18921v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15908v2",
                "updated": "2025-03-19T10:19:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    19,
                    30,
                    2,
                    78,
                    0
                ],
                "published": "2024-10-21T11:29:49Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "title": "Formalising CXL Cache Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formalising CXL Cache Coherence"
                },
                "summary": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs."
                },
                "authors": [
                    {
                        "name": "Chengsong Tan"
                    },
                    {
                        "name": "Alastair F. Donaldson"
                    },
                    {
                        "name": "John Wickerson"
                    }
                ],
                "author_detail": {
                    "name": "John Wickerson"
                },
                "author": "John Wickerson",
                "arxiv_doi": "10.1145/3676641.3715999",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3715999",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.15908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages",
                "arxiv_journal_ref": "Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, ASPLOS\n  2025, Rotterdam, Netherlands",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68 (Primary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1; F.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14881v1",
                "updated": "2025-03-19T04:18:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    4,
                    18,
                    57,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T04:18:57Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    4,
                    18,
                    57,
                    2,
                    78,
                    0
                ],
                "title": "Exploring the Limits of KV Cache Compression in Visual Autoregressive\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Limits of KV Cache Compression in Visual Autoregressive\n  Transformers"
                },
                "summary": "A fundamental challenge in Visual Autoregressive models is the substantial\nmemory overhead required during inference to store previously generated\nrepresentations. Despite various attempts to mitigate this issue through\ncompression techniques, prior works have not explicitly formalized the problem\nof KV-cache compression in this context. In this work, we take the first step\nin formally defining the KV-cache compression problem for Visual Autoregressive\ntransformers. We then establish a fundamental negative result, proving that any\nmechanism for sequential visual token generation under attention-based\narchitectures must use at least $\\Omega(n^2 d)$ memory, when $d = \\Omega(\\log\nn)$, where $n$ is the number of tokens generated and $d$ is the embedding\ndimensionality. This result demonstrates that achieving truly sub-quadratic\nmemory usage is impossible without additional structural constraints. Our proof\nis constructed via a reduction from a computational lower bound problem,\nleveraging randomized embedding techniques inspired by dimensionality reduction\nprinciples. Finally, we discuss how sparsity priors on visual representations\ncan influence memory efficiency, presenting both impossibility results and\npotential directions for mitigating memory overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fundamental challenge in Visual Autoregressive models is the substantial\nmemory overhead required during inference to store previously generated\nrepresentations. Despite various attempts to mitigate this issue through\ncompression techniques, prior works have not explicitly formalized the problem\nof KV-cache compression in this context. In this work, we take the first step\nin formally defining the KV-cache compression problem for Visual Autoregressive\ntransformers. We then establish a fundamental negative result, proving that any\nmechanism for sequential visual token generation under attention-based\narchitectures must use at least $\\Omega(n^2 d)$ memory, when $d = \\Omega(\\log\nn)$, where $n$ is the number of tokens generated and $d$ is the embedding\ndimensionality. This result demonstrates that achieving truly sub-quadratic\nmemory usage is impossible without additional structural constraints. Our proof\nis constructed via a reduction from a computational lower bound problem,\nleveraging randomized embedding techniques inspired by dimensionality reduction\nprinciples. Finally, we discuss how sparsity priors on visual representations\ncan influence memory efficiency, presenting both impossibility results and\npotential directions for mitigating memory overhead."
                },
                "authors": [
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yekun Ke"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Song"
                },
                "author": "Zhao Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14805v1",
                "updated": "2025-03-19T00:30:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    0,
                    30,
                    43,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T00:30:43Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    0,
                    30,
                    43,
                    2,
                    78,
                    0
                ],
                "title": "Degradation of 2.4-kV $Ga_{2}O_{3}$ Schottky Barrier Diode at High\n  Temperatures up to 500 °C",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Degradation of 2.4-kV $Ga_{2}O_{3}$ Schottky Barrier Diode at High\n  Temperatures up to 500 °C"
                },
                "summary": "Ga2O3 Schottky barrier diodes featuring a field plate and a composite\nSiO2/SiNx dielectric layer beneath the field plate were fabricated, achieving a\nbreakdown voltage of 2.4 kV at room temperature. Electrical performance and\ndegradation were analyzed via I-V and C-V measurements from 25 {\\deg}C to 500\n{\\deg}C, revealing temperature-dependent transport, interface stability, and\ndevice stability. Upon returning to room temperature, the diodes exhibited\nnearly unchanged forward characteristics, while the breakdown voltage declined\nsignificantly from 2.4 kV to 700 V. This behavior indicates a\ntemperature-induced reduction in the barrier height. Detailed analysis revealed\nthat variable range hopping (VRH) dominated the leakage mechanism at moderate\ntemperatures, while thermal emission (TE) became increasingly significant at\ntemperatures exceeding 400 {\\deg}C.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ga2O3 Schottky barrier diodes featuring a field plate and a composite\nSiO2/SiNx dielectric layer beneath the field plate were fabricated, achieving a\nbreakdown voltage of 2.4 kV at room temperature. Electrical performance and\ndegradation were analyzed via I-V and C-V measurements from 25 {\\deg}C to 500\n{\\deg}C, revealing temperature-dependent transport, interface stability, and\ndevice stability. Upon returning to room temperature, the diodes exhibited\nnearly unchanged forward characteristics, while the breakdown voltage declined\nsignificantly from 2.4 kV to 700 V. This behavior indicates a\ntemperature-induced reduction in the barrier height. Detailed analysis revealed\nthat variable range hopping (VRH) dominated the leakage mechanism at moderate\ntemperatures, while thermal emission (TE) became increasingly significant at\ntemperatures exceeding 400 {\\deg}C."
                },
                "authors": [
                    {
                        "name": "Hunter Ellis"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "Imteaz Rahaman"
                    },
                    {
                        "name": "Apostoli Hillas"
                    },
                    {
                        "name": "Botong Li"
                    },
                    {
                        "name": "Michael A. Scarpulla"
                    },
                    {
                        "name": "Berardi Sensale Rodriguez"
                    },
                    {
                        "name": "Kai Fu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Fu"
                },
                "author": "Kai Fu",
                "arxiv_comment": "7 Pages, 9 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14708v1",
                "updated": "2025-03-18T20:16:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    20,
                    16,
                    50,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T20:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    20,
                    16,
                    50,
                    1,
                    77,
                    0
                ],
                "title": "NeCTAr: A Heterogeneous RISC-V SoC for Language Model Inference in Intel\n  16",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeCTAr: A Heterogeneous RISC-V SoC for Language Model Inference in Intel\n  16"
                },
                "summary": "This paper introduces NeCTAr (Near-Cache Transformer Accelerator), a 16nm\nheterogeneous multicore RISC-V SoC for sparse and dense machine learning\nkernels with both near-core and near-memory accelerators. A prototype chip runs\nat 400MHz at 0.85V and performs matrix-vector multiplications with 109 GOPs/W.\nThe effectiveness of the design is demonstrated by running inference on a\nsparse language model, ReLU-Llama.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces NeCTAr (Near-Cache Transformer Accelerator), a 16nm\nheterogeneous multicore RISC-V SoC for sparse and dense machine learning\nkernels with both near-core and near-memory accelerators. A prototype chip runs\nat 400MHz at 0.85V and performs matrix-vector multiplications with 109 GOPs/W.\nThe effectiveness of the design is demonstrated by running inference on a\nsparse language model, ReLU-Llama."
                },
                "authors": [
                    {
                        "name": "Viansa Schmulbach"
                    },
                    {
                        "name": "Jason Kim"
                    },
                    {
                        "name": "Ethan Gao"
                    },
                    {
                        "name": "Lucy Revina"
                    },
                    {
                        "name": "Nikhil Jha"
                    },
                    {
                        "name": "Ethan Wu"
                    },
                    {
                        "name": "Borivoje Nikolic"
                    }
                ],
                "author_detail": {
                    "name": "Borivoje Nikolic"
                },
                "author": "Borivoje Nikolic",
                "arxiv_doi": "10.1109/HCS61935.2024.10665203",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/HCS61935.2024.10665203",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.14708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14647v1",
                "updated": "2025-03-18T18:52:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    18,
                    52,
                    3,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T18:52:03Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    18,
                    52,
                    3,
                    1,
                    77,
                    0
                ],
                "title": "Towards More Economical Context-Augmented LLM Generation by Reusing\n  Stored KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards More Economical Context-Augmented LLM Generation by Reusing\n  Stored KV Cache"
                },
                "summary": "Across large language model (LLM) applications, we observe an emerging trend\nfor reusing KV caches to save the prefill delays of processing repeated input\ntexts in different LLM inputs. This has led to a broad design space, including\ncolocating stored KV caches with (or close to) GPUs to various KV cache\ncompression. However, a key question remains unanswered: can these delay\nreductions also be economically favorable? Specifically, we ask whether a\ndeveloper can use public cloud services to store precomputed KV caches and\nreuse them to save delay without incurring more costs in terms of compute,\nstorage, and network. To answer this question, we propose an validated\nanalytical model for the cloud cost (in compute, storage, and network) of\nstoring and reusing KV caches based on various workload parameters, such as\nreuse frequency, generated text lengths, model sizes, etc. Preliminary results\nshow that KV cache reusing is able to save both delay and cloud cost across a\nrange of workloads with long context. And we call more efforts on building more\neconomical context augmented LLM by KV cache reusing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Across large language model (LLM) applications, we observe an emerging trend\nfor reusing KV caches to save the prefill delays of processing repeated input\ntexts in different LLM inputs. This has led to a broad design space, including\ncolocating stored KV caches with (or close to) GPUs to various KV cache\ncompression. However, a key question remains unanswered: can these delay\nreductions also be economically favorable? Specifically, we ask whether a\ndeveloper can use public cloud services to store precomputed KV caches and\nreuse them to save delay without incurring more costs in terms of compute,\nstorage, and network. To answer this question, we propose an validated\nanalytical model for the cloud cost (in compute, storage, and network) of\nstoring and reusing KV caches based on various workload parameters, such as\nreuse frequency, generated text lengths, model sizes, etc. Preliminary results\nshow that KV cache reusing is able to save both delay and cloud cost across a\nrange of workloads with long context. And we call more efforts on building more\neconomical context augmented LLM by KV cache reusing."
                },
                "authors": [
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08640v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08640v2",
                "updated": "2025-03-18T17:13:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    13,
                    42,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-11T17:30:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    30,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention"
                },
                "summary": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale."
                },
                "authors": [
                    {
                        "name": "Emily Xiao"
                    },
                    {
                        "name": "Chin-Jou Li"
                    },
                    {
                        "name": "Yilin Zhang"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Amanda Bertsch"
                    }
                ],
                "author_detail": {
                    "name": "Amanda Bertsch"
                },
                "author": "Amanda Bertsch",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08640v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08640v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09573v2",
                "updated": "2025-03-18T15:58:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    58,
                    18,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-12T17:43:40Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    43,
                    40,
                    2,
                    71,
                    0
                ],
                "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models"
                },
                "summary": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/"
                },
                "authors": [
                    {
                        "name": "Marianne Arriola"
                    },
                    {
                        "name": "Aaron Gokaslan"
                    },
                    {
                        "name": "Justin T Chiu"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Zhixuan Qi"
                    },
                    {
                        "name": "Jiaqi Han"
                    },
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Volodymyr Kuleshov"
                    }
                ],
                "author_detail": {
                    "name": "Volodymyr Kuleshov"
                },
                "author": "Volodymyr Kuleshov",
                "arxiv_comment": "ICLR 2025 Oral. We provide the code at\n  https://github.com/kuleshov-group/bd3lms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18753v2",
                "updated": "2025-03-18T09:43:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    9,
                    43,
                    33,
                    1,
                    77,
                    0
                ],
                "published": "2024-07-26T14:08:53Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    14,
                    8,
                    53,
                    4,
                    208,
                    0
                ],
                "title": "Suffixient Arrays: a New Efficient Suffix Array Compression Technique",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Suffixient Arrays: a New Efficient Suffix Array Compression Technique"
                },
                "summary": "The Suffix Array is a classic text index enabling on-line pattern matching\nqueries via simple binary search. The main drawback of the Suffix Array is that\nit takes linear space in the text's length, even if the text itself is\nextremely compressible. Several works in the literature showed that the Suffix\nArray can be compressed, but they all rely on complex succinct data structures\nwhich in practice tend to exhibit poor cache locality and thus significantly\nslow down queries. In this paper, we propose a new simple and very efficient\nsolution to this problem by presenting the \\emph{Suffixient Array}: a tiny\nsubset of the Suffix Array \\emph{sufficient} to locate on-line one pattern\noccurrence (in general, all its Maximal Exact Matches) via binary search,\nprovided that random access to the text is available. We prove that: (i) the\nSuffixient Array length $\\chi$ is a strong repetitiveness measure, (ii) unlike\nmost existing repetition-aware indexes such as the $r$-index, our new index is\nefficient in the I/O model, and (iii) Suffixient Arrays can be computed in\nlinear time and compressed working space. We show experimentally that, when\nusing well-established compressed random access data structures on repetitive\ncollections, the Suffixient Array $\\SuA$ is \\emph{simultaneously} (i) faster\nand orders of magnitude smaller than the Suffix Array $\\SA$ and (ii) smaller\nand \\emph{one to two orders of magnitude faster} than the $r$-index. With an\naverage pattern matching query time as low as 3.5 ns per character, our new\nindex gets very close to the ultimate lower bound: the RAM throughput of our\nworkstation (1.18 ns per character).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Suffix Array is a classic text index enabling on-line pattern matching\nqueries via simple binary search. The main drawback of the Suffix Array is that\nit takes linear space in the text's length, even if the text itself is\nextremely compressible. Several works in the literature showed that the Suffix\nArray can be compressed, but they all rely on complex succinct data structures\nwhich in practice tend to exhibit poor cache locality and thus significantly\nslow down queries. In this paper, we propose a new simple and very efficient\nsolution to this problem by presenting the \\emph{Suffixient Array}: a tiny\nsubset of the Suffix Array \\emph{sufficient} to locate on-line one pattern\noccurrence (in general, all its Maximal Exact Matches) via binary search,\nprovided that random access to the text is available. We prove that: (i) the\nSuffixient Array length $\\chi$ is a strong repetitiveness measure, (ii) unlike\nmost existing repetition-aware indexes such as the $r$-index, our new index is\nefficient in the I/O model, and (iii) Suffixient Arrays can be computed in\nlinear time and compressed working space. We show experimentally that, when\nusing well-established compressed random access data structures on repetitive\ncollections, the Suffixient Array $\\SuA$ is \\emph{simultaneously} (i) faster\nand orders of magnitude smaller than the Suffix Array $\\SA$ and (ii) smaller\nand \\emph{one to two orders of magnitude faster} than the $r$-index. With an\naverage pattern matching query time as low as 3.5 ns per character, our new\nindex gets very close to the ultimate lower bound: the RAM throughput of our\nworkstation (1.18 ns per character)."
                },
                "authors": [
                    {
                        "name": "Davide Cenzato"
                    },
                    {
                        "name": "Lore Depuydt"
                    },
                    {
                        "name": "Travis Gagie"
                    },
                    {
                        "name": "Sung-Hwan Kim"
                    },
                    {
                        "name": "Giovanni Manzini"
                    },
                    {
                        "name": "Francisco Olivares"
                    },
                    {
                        "name": "Nicola Prezza"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Prezza"
                },
                "author": "Nicola Prezza",
                "arxiv_comment": "40 pages, 7 figure, 1 table and 7 pseudocodes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13145v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13145v2",
                "updated": "2025-03-18T07:02:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    7,
                    2,
                    33,
                    1,
                    77,
                    0
                ],
                "published": "2025-02-18T18:59:57Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba"
                },
                "authors": [
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Tianheng Cheng"
                    },
                    {
                        "name": "Yingyue Li"
                    },
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "Code and model are available at https://github.com/hustvl/mmMamba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13145v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13145v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19108v2",
                "updated": "2025-03-18T04:49:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    4,
                    49,
                    23,
                    1,
                    77,
                    0
                ],
                "published": "2024-11-28T12:50:05Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model"
                },
                "summary": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality."
                },
                "authors": [
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Yujie Wei"
                    },
                    {
                        "name": "Haonan Qiu"
                    },
                    {
                        "name": "Yuzhong Zhao"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Qixiang Ye"
                    },
                    {
                        "name": "Fang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Fang Wan"
                },
                "author": "Fang Wan",
                "arxiv_comment": "Accepted in CVPR 2025. Project: https://liewfeng.github.io/TeaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10511v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10511v3",
                "updated": "2025-03-18T01:58:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    1,
                    58,
                    36,
                    1,
                    77,
                    0
                ],
                "published": "2024-06-15T05:28:55Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    5,
                    28,
                    55,
                    5,
                    167,
                    0
                ],
                "title": "Efficient Hardware Accelerator Based on Medium Granularity Dataflow for\n  SpTRSV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Hardware Accelerator Based on Medium Granularity Dataflow for\n  SpTRSV"
                },
                "summary": "Sparse triangular solve (SpTRSV) is widely used in various domains. Numerous\nstudies have been conducted using CPUs, GPUs, and specific hardware\naccelerators, where dataflows can be categorized into coarse and fine\ngranularity. Coarse dataflows offer good spatial locality but suffer from low\nparallelism, while fine dataflows provide high parallelism but disrupt the\nspatial structure, leading to increased nodes and poor data reuse. This paper\nproposes a novel hardware accelerator for SpTRSV or SpTRSV-like DAGs. The\naccelerator implements a medium granularity dataflow through hardware-software\ncodesign and achieves both excellent spatial locality and high parallelism.\nAdditionally, a partial sum caching mechanism is introduced to reduce the\nblocking frequency of processing elements (PEs), and a reordering algorithm of\nintra-node edges computation is developed to enhance data reuse. Experimental\nresults on 245 benchmarks with node counts reaching up to 85,392 demonstrate\nthat this work achieves average performance improvements of 7.0$\\times$ (up to\n27.8$\\times$) over CPUs and 5.8$\\times$ (up to 98.8$\\times$) over GPUs.\nCompared to the state-of-the-art technique (DPU-v2), this work shows a\n2.5$\\times$ (up to 5.9$\\times$) average performance improvement and 1.7$\\times$\n(up to 4.1$\\times$) average energy efficiency enhancement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse triangular solve (SpTRSV) is widely used in various domains. Numerous\nstudies have been conducted using CPUs, GPUs, and specific hardware\naccelerators, where dataflows can be categorized into coarse and fine\ngranularity. Coarse dataflows offer good spatial locality but suffer from low\nparallelism, while fine dataflows provide high parallelism but disrupt the\nspatial structure, leading to increased nodes and poor data reuse. This paper\nproposes a novel hardware accelerator for SpTRSV or SpTRSV-like DAGs. The\naccelerator implements a medium granularity dataflow through hardware-software\ncodesign and achieves both excellent spatial locality and high parallelism.\nAdditionally, a partial sum caching mechanism is introduced to reduce the\nblocking frequency of processing elements (PEs), and a reordering algorithm of\nintra-node edges computation is developed to enhance data reuse. Experimental\nresults on 245 benchmarks with node counts reaching up to 85,392 demonstrate\nthat this work achieves average performance improvements of 7.0$\\times$ (up to\n27.8$\\times$) over CPUs and 5.8$\\times$ (up to 98.8$\\times$) over GPUs.\nCompared to the state-of-the-art technique (DPU-v2), this work shows a\n2.5$\\times$ (up to 5.9$\\times$) average performance improvement and 1.7$\\times$\n(up to 4.1$\\times$) average energy efficiency enhancement."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    },
                    {
                        "name": "Shengli Lu"
                    }
                ],
                "author_detail": {
                    "name": "Shengli Lu"
                },
                "author": "Shengli Lu",
                "arxiv_doi": "10.1109/TVLSI.2024.3497166",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVLSI.2024.3497166",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.10511v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10511v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Trans. Very Large Scale Integr. (VLSI) Syst. 33 (2025)\n  807-820",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13737v1",
                "updated": "2025-03-17T21:47:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    47,
                    43,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T21:47:43Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    47,
                    43,
                    0,
                    76,
                    0
                ],
                "title": "AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference\n  Serving for Diverse Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference\n  Serving for Diverse Applications"
                },
                "summary": "In this paper, we consider a mixed-prompt scenario for a large language model\n(LLM) inference serving system that supports diverse applications with both\nshort prompts and long prompts and heterogeneous SLOs for iteration time. To\nimprove throughput when handling long prompts, previous research introduces a\nchunking method, but has not addressed heterogeneous SLOs. To address the\nlimitation, we propose AccelGen, a high-throughput LLM inference serving system\nwith heterogeneous SLO guarantees for diverse applications. AccelGen introduces\nfour core components: (1) SLO-guaranteed dynamic chunking, which dynamically\nadjusts chunk sizes to maximize GPU compute utilization while meeting\niteration-level SLOs; (2) Iteration-level SLO-based task prioritization, which\nprioritizes tight-SLO requests and batches requests with similar SLOs; (3)\nMulti-resource-aware batching, which selects queued requests to maximize the\nutilizations of both GPU compute resource and key-value cache (KVC).\nTrace-driven real experiments demonstrate that AccelGen achieves 1.42-11.21X\nhigher throughput, 1.43-13.71X higher goodput, 37-90% higher SLO attainment,\nand 1.61-12.22X lower response latency compared to the state-of-the-art\napproaches. It achieves performance near the Oracle, which optimally maximizes\ngoodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we consider a mixed-prompt scenario for a large language model\n(LLM) inference serving system that supports diverse applications with both\nshort prompts and long prompts and heterogeneous SLOs for iteration time. To\nimprove throughput when handling long prompts, previous research introduces a\nchunking method, but has not addressed heterogeneous SLOs. To address the\nlimitation, we propose AccelGen, a high-throughput LLM inference serving system\nwith heterogeneous SLO guarantees for diverse applications. AccelGen introduces\nfour core components: (1) SLO-guaranteed dynamic chunking, which dynamically\nadjusts chunk sizes to maximize GPU compute utilization while meeting\niteration-level SLOs; (2) Iteration-level SLO-based task prioritization, which\nprioritizes tight-SLO requests and batches requests with similar SLOs; (3)\nMulti-resource-aware batching, which selects queued requests to maximize the\nutilizations of both GPU compute resource and key-value cache (KVC).\nTrace-driven real experiments demonstrate that AccelGen achieves 1.42-11.21X\nhigher throughput, 1.43-13.71X higher goodput, 37-90% higher SLO attainment,\nand 1.61-12.22X lower response latency compared to the state-of-the-art\napproaches. It achieves performance near the Oracle, which optimally maximizes\ngoodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13723v1",
                "updated": "2025-03-17T21:11:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    11,
                    30,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T21:11:30Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    11,
                    30,
                    0,
                    76,
                    0
                ],
                "title": "Fast Maximum Likelihood Positioning for a Staggered Layer Scintillation\n  PET Detector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Maximum Likelihood Positioning for a Staggered Layer Scintillation\n  PET Detector"
                },
                "summary": "In this study, we propose a fast implementation of a Maximum Likelihood\nPositioning (MLP) algorithm to estimate the energy and identify the active\nscintillator pixel in staggered layer scintillation detectors for PET. The\nstaggered layer design with pixelated scintillators enables the determination\nof the gamma's depth of interaction and facilitates an iteration-free\nformulation of the MLP algorithm. The efficacy of the algorithm optimization\nwas tested on a scintillation detector block designed for an ultra-high field\nBrainPET 7T, comprising three scintillator pixel layers. The three layers\ncontain 24 x 24, 24 x 23 and 23 x 22 scintillator pixels, respectively, with a\npixel pitch of 2 mm in both directions and layer thicknesses of 9, 8 and 7 mm.\nCalibration measurements, in combination with an automated calibration script,\nwere used to obtain the expected counts of scintillation photons required in\nthe MLP algorithm. Using Single-Instruction-Multiple-Data parallelization,\nmulti-threading and optimized cache lines, a maximum processing speed of\napproximately 22.5 million singles per second was achieved on a platform with\nfour Intel Xeon Platinum 8168 CPUs and 60 threads, encompassing all required\nprocessing steps. The automatic calibration failed for 1 to 15 individual\nscintillator pixels in approximately 10 per cent of the 120 scintillation\ndetector blocks, necessitating manual correction. After applying the energy\ncorrection to the positioned single events, an energy resolution of of 12 +/- 2\nper cent FWHM was obtained for the entire scintillation block. This value is\nvery close to the energy resolutions measured for the individual scintillator\npixels, proving that the MLP accurately identifies the scintillating pixel and\nthat the energy correction method effectively compensates for the light\ncollection variations of the SiPM array.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we propose a fast implementation of a Maximum Likelihood\nPositioning (MLP) algorithm to estimate the energy and identify the active\nscintillator pixel in staggered layer scintillation detectors for PET. The\nstaggered layer design with pixelated scintillators enables the determination\nof the gamma's depth of interaction and facilitates an iteration-free\nformulation of the MLP algorithm. The efficacy of the algorithm optimization\nwas tested on a scintillation detector block designed for an ultra-high field\nBrainPET 7T, comprising three scintillator pixel layers. The three layers\ncontain 24 x 24, 24 x 23 and 23 x 22 scintillator pixels, respectively, with a\npixel pitch of 2 mm in both directions and layer thicknesses of 9, 8 and 7 mm.\nCalibration measurements, in combination with an automated calibration script,\nwere used to obtain the expected counts of scintillation photons required in\nthe MLP algorithm. Using Single-Instruction-Multiple-Data parallelization,\nmulti-threading and optimized cache lines, a maximum processing speed of\napproximately 22.5 million singles per second was achieved on a platform with\nfour Intel Xeon Platinum 8168 CPUs and 60 threads, encompassing all required\nprocessing steps. The automatic calibration failed for 1 to 15 individual\nscintillator pixels in approximately 10 per cent of the 120 scintillation\ndetector blocks, necessitating manual correction. After applying the energy\ncorrection to the positioned single events, an energy resolution of of 12 +/- 2\nper cent FWHM was obtained for the entire scintillation block. This value is\nvery close to the energy resolutions measured for the individual scintillator\npixels, proving that the MLP accurately identifies the scintillating pixel and\nthat the energy correction method effectively compensates for the light\ncollection variations of the SiPM array."
                },
                "authors": [
                    {
                        "name": "Christoph W. Lerche"
                    },
                    {
                        "name": "Wenwei Bi"
                    },
                    {
                        "name": "Mirjam Schoeneck"
                    },
                    {
                        "name": "Debora Niekaemper"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Elisabeth Pfaehler"
                    },
                    {
                        "name": "Lutz Tellmann"
                    },
                    {
                        "name": "Juergen J. Scheins"
                    },
                    {
                        "name": "N. Jon Shah"
                    }
                ],
                "author_detail": {
                    "name": "N. Jon Shah"
                },
                "arxiv_affiliation": "Department of Neurology RWTH Aachen University Aachen Germany",
                "author": "N. Jon Shah",
                "arxiv_comment": "20 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92C55 (Primary) 94A08 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13873v2",
                "updated": "2025-03-17T20:31:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    20,
                    31,
                    46,
                    0,
                    76,
                    0
                ],
                "published": "2025-02-19T16:54:58Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "title": "NVR: Vector Runahead on NPUs for Sparse Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVR: Vector Runahead on NPUs for Sparse Memory Access"
                },
                "summary": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount."
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Zhengpeng Zhao"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Bing Guo"
                    },
                    {
                        "name": "He Xiao"
                    },
                    {
                        "name": "Chenhao Ma"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13679v1",
                "updated": "2025-03-17T19:32:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    19,
                    32,
                    26,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T19:32:26Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    19,
                    32,
                    26,
                    0,
                    76,
                    0
                ],
                "title": "PrETi: Predicting Execution Time in Early Stage with LLVM and Machine\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrETi: Predicting Execution Time in Early Stage with LLVM and Machine\n  Learning"
                },
                "summary": "We introduce preti, a novel framework for predicting software execution time\nduring the early stages of development. preti leverages an LLVM-based\nsimulation environment to extract timing-related runtime information, such as\nthe count of executed LLVM IR instructions. This information, combined with\nhistorical execution time data, is utilized to train machine learning models\nfor accurate time prediction. To further enhance prediction accuracy, our\napproach incorporates simulations of cache accesses and branch prediction. The\nevaluations on public benchmarks demonstrate that preti achieves an average\nAbsolute Percentage Error (APE) of 11.98\\%, surpassing state-of-the-art\nmethods. These results underscore the effectiveness and efficiency of preti as\na robust solution for early-stage timing analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce preti, a novel framework for predicting software execution time\nduring the early stages of development. preti leverages an LLVM-based\nsimulation environment to extract timing-related runtime information, such as\nthe count of executed LLVM IR instructions. This information, combined with\nhistorical execution time data, is utilized to train machine learning models\nfor accurate time prediction. To further enhance prediction accuracy, our\napproach incorporates simulations of cache accesses and branch prediction. The\nevaluations on public benchmarks demonstrate that preti achieves an average\nAbsolute Percentage Error (APE) of 11.98\\%, surpassing state-of-the-art\nmethods. These results underscore the effectiveness and efficiency of preti as\na robust solution for early-stage timing analysis."
                },
                "authors": [
                    {
                        "name": "Risheng Xu"
                    },
                    {
                        "name": "Philipp Sieweck"
                    },
                    {
                        "name": "Hermann von Hasseln"
                    },
                    {
                        "name": "Dirk Nowotka"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Nowotka"
                },
                "author": "Dirk Nowotka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16525v1",
                "updated": "2025-03-17T16:43:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    43,
                    35,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T16:43:35Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    43,
                    35,
                    0,
                    76,
                    0
                ],
                "title": "KVShare: Semantic-Aware Key-Value Cache Sharing for Efficient Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVShare: Semantic-Aware Key-Value Cache Sharing for Efficient Large\n  Language Model Inference"
                },
                "summary": "This paper presents KVShare, a multi-user Key-Value (KV) Cache sharing\ntechnology based on semantic similarity, designed to enhance the inference\nefficiency of Large Language Models (LLMs) and Multimodal Large Language Models\n(MLLMs). Addressing the limitations of existing prefix caching (strict text\nprefix matching) and semantic caching (loss of response diversity), KVShare\nachieves fine-grained KV cache reuse through semantic alignment algorithms and\ndifferential editing operations. Experiments on real-world user conversation\ndatasets demonstrate that KVShare improves KV cache hit rates by over 60%,\nwhile maintaining output quality comparable to full computation (no significant\ndegradation in BLEU and Rouge-L metrics). This approach effectively reduces GPU\nresource consumption and is applicable to scenarios with repetitive queries,\nsuch as healthcare and education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents KVShare, a multi-user Key-Value (KV) Cache sharing\ntechnology based on semantic similarity, designed to enhance the inference\nefficiency of Large Language Models (LLMs) and Multimodal Large Language Models\n(MLLMs). Addressing the limitations of existing prefix caching (strict text\nprefix matching) and semantic caching (loss of response diversity), KVShare\nachieves fine-grained KV cache reuse through semantic alignment algorithms and\ndifferential editing operations. Experiments on real-world user conversation\ndatasets demonstrate that KVShare improves KV cache hit rates by over 60%,\nwhile maintaining output quality comparable to full computation (no significant\ndegradation in BLEU and Rouge-L metrics). This approach effectively reduces GPU\nresource consumption and is applicable to scenarios with repetitive queries,\nsuch as healthcare and education."
                },
                "authors": [
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Renji Zhang"
                    },
                    {
                        "name": "Deyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhang"
                },
                "author": "Deyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13275v1",
                "updated": "2025-03-17T15:27:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:27:02Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "title": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems"
                },
                "summary": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability."
                },
                "authors": [
                    {
                        "name": "Seyoung Song"
                    }
                ],
                "author_detail": {
                    "name": "Seyoung Song"
                },
                "author": "Seyoung Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; I.2.11; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12991v1",
                "updated": "2025-03-17T09:46:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    46,
                    35,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T09:46:35Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    46,
                    35,
                    0,
                    76,
                    0
                ],
                "title": "Tuning the CMS Coffea-casa facility for 200 Gbps Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tuning the CMS Coffea-casa facility for 200 Gbps Challenge"
                },
                "summary": "As a part of the IRIS-HEP \"Analysis Grand Challenge\" activities, the\nCoffea-casa AF team executed a \"200 Gbps Challenge\". One of the goals of this\nchallenge was to provide a setup for execution of a test notebook-style\nanalysis on the facility that could process a 200 TB CMS NanoAOD dataset in 20\nminutes.\n  We describe the solutions we deployed at the facility to execute the\nchallenge tasks. The facility was configured to provide 2000+ cores for quick\nturn-around, low-latency analysis. To reach the highest event processing rates\nwe tested different scaling backends, both scaling over HTCondor and Kubernetes\nresources and using Dask and Taskvine schedulers. This configuration also\nallowed us to compare two different services for managing Dask clusters, Dask\nlabextention, and Dask Gateway server, under extreme conditions.\n  A robust set of XCache servers with a redirector were deployed in Kubernetes\nto cache the dataset to minimize wide-area network traffic. The XCache servers\nwere backed with solid-state NVME drives deployed within the Kubernetes cluster\nnodes. All data access was authenticated using scitokens and was transparent to\nthe user. To ensure we could track and measure data throughput precisely, we\nused our existing Prometheus monitoring stack to monitor the XCache pod\nthroughput on the Kubernetes network layer. Using the rate query across all of\nthe 8 XCache pods we were able to view a stacked cumulative graph of the total\nthroughput for each XCache. This monitoring setup allowed us to ensure uniform\ndata rates across all nodes while verifying we had reached the 200 Gbps\nbenchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a part of the IRIS-HEP \"Analysis Grand Challenge\" activities, the\nCoffea-casa AF team executed a \"200 Gbps Challenge\". One of the goals of this\nchallenge was to provide a setup for execution of a test notebook-style\nanalysis on the facility that could process a 200 TB CMS NanoAOD dataset in 20\nminutes.\n  We describe the solutions we deployed at the facility to execute the\nchallenge tasks. The facility was configured to provide 2000+ cores for quick\nturn-around, low-latency analysis. To reach the highest event processing rates\nwe tested different scaling backends, both scaling over HTCondor and Kubernetes\nresources and using Dask and Taskvine schedulers. This configuration also\nallowed us to compare two different services for managing Dask clusters, Dask\nlabextention, and Dask Gateway server, under extreme conditions.\n  A robust set of XCache servers with a redirector were deployed in Kubernetes\nto cache the dataset to minimize wide-area network traffic. The XCache servers\nwere backed with solid-state NVME drives deployed within the Kubernetes cluster\nnodes. All data access was authenticated using scitokens and was transparent to\nthe user. To ensure we could track and measure data throughput precisely, we\nused our existing Prometheus monitoring stack to monitor the XCache pod\nthroughput on the Kubernetes network layer. Using the rate query across all of\nthe 8 XCache pods we were able to view a stacked cumulative graph of the total\nthroughput for each XCache. This monitoring setup allowed us to ensure uniform\ndata rates across all nodes while verifying we had reached the 200 Gbps\nbenchmark."
                },
                "authors": [
                    {
                        "name": "Sam Albin"
                    },
                    {
                        "name": "Garhan Attebury"
                    },
                    {
                        "name": "Kenneth Bloom"
                    },
                    {
                        "name": "Brian Paul Bockelman"
                    },
                    {
                        "name": "Benjamin Tovar Lopez"
                    },
                    {
                        "name": "Carl Lundstedt"
                    },
                    {
                        "name": "Oksana Shadura"
                    },
                    {
                        "name": "John Thiltges"
                    },
                    {
                        "name": "Derek Weitzel"
                    },
                    {
                        "name": "Andrew Wightman"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Wightman"
                },
                "arxiv_affiliation": "University of Nebraska-Lincoln",
                "author": "Andrew Wightman",
                "arxiv_comment": "Draft submitted to EPJ journal (CHEP 2024 conference proceedings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12988v1",
                "updated": "2025-03-17T09:44:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    44,
                    17,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T09:44:17Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    44,
                    17,
                    0,
                    76,
                    0
                ],
                "title": "ROMA: a Read-Only-Memory-based Accelerator for QLoRA-based On-Device LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ROMA: a Read-Only-Memory-based Accelerator for QLoRA-based On-Device LLM"
                },
                "summary": "As large language models (LLMs) demonstrate powerful capabilities, deploying\nthem on edge devices has become increasingly crucial, offering advantages in\nprivacy and real-time interaction. QLoRA has emerged as the standard approach\nfor on-device LLMs, leveraging quantized models to reduce memory and\ncomputational costs while utilizing LoRA for task-specific adaptability. In\nthis work, we propose ROMA, a QLoRA accelerator with a hybrid storage\narchitecture that uses ROM for quantized base models and SRAM for LoRA weights\nand KV cache. Our insight is that the quantized base model is stable and\nconverged, making it well-suited for ROM storage. Meanwhile, LoRA modules offer\nthe flexibility to adapt to new data without requiring updates to the base\nmodel. To further reduce the area cost of ROM, we introduce a novel B-ROM\ndesign and integrate it with the compute unit to form a fused cell for\nefficient use of chip resources. ROMA can effectively store both a 4-bit 3B and\na 2-bit 8B LLaMA model entirely on-chip, achieving a notable generation speed\nexceeding 20,000 tokens/s without requiring external memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) demonstrate powerful capabilities, deploying\nthem on edge devices has become increasingly crucial, offering advantages in\nprivacy and real-time interaction. QLoRA has emerged as the standard approach\nfor on-device LLMs, leveraging quantized models to reduce memory and\ncomputational costs while utilizing LoRA for task-specific adaptability. In\nthis work, we propose ROMA, a QLoRA accelerator with a hybrid storage\narchitecture that uses ROM for quantized base models and SRAM for LoRA weights\nand KV cache. Our insight is that the quantized base model is stable and\nconverged, making it well-suited for ROM storage. Meanwhile, LoRA modules offer\nthe flexibility to adapt to new data without requiring updates to the base\nmodel. To further reduce the area cost of ROM, we introduce a novel B-ROM\ndesign and integrate it with the compute unit to form a fused cell for\nefficient use of chip resources. ROMA can effectively store both a 4-bit 3B and\na 2-bit 8B LLaMA model entirely on-chip, achieving a notable generation speed\nexceeding 20,000 tokens/s without requiring external memory."
                },
                "authors": [
                    {
                        "name": "Wenqiang Wang"
                    },
                    {
                        "name": "Yijia Zhang"
                    },
                    {
                        "name": "Zikai Zhang"
                    },
                    {
                        "name": "Guanting Huo"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Ningyi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ningyi Xu"
                },
                "author": "Ningyi Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08407v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08407v2",
                "updated": "2025-03-17T03:30:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    3,
                    30,
                    29,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-11T13:10:41Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    10,
                    41,
                    1,
                    70,
                    0
                ],
                "title": "WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images"
                },
                "summary": "Recent advances in interactive 3D segmentation from 2D images have\ndemonstrated impressive performance. However, current models typically require\nextensive scene-specific training to accurately reconstruct and segment\nobjects, which limits their applicability in real-time scenarios. In this\npaper, we introduce WildSeg3D, an efficient approach that enables the\nsegmentation of arbitrary 3D objects across diverse environments using a\nfeed-forward mechanism. A key challenge of this feed-forward approach lies in\nthe accumulation of 3D alignment errors across multiple 2D views, which can\nlead to inaccurate 3D segmentation results. To address this issue, we propose\nDynamic Global Aligning (DGA), a technique that improves the accuracy of global\nmulti-view alignment by focusing on difficult-to-match 3D points across images,\nusing a dynamic adjustment function. Additionally, for real-time interactive\nsegmentation, we introduce Multi-view Group Mapping (MGM), a method that\nutilizes an object mask cache to integrate multi-view segmentations and respond\nrapidly to user prompts. WildSeg3D demonstrates robust generalization across\narbitrary scenes, thereby eliminating the need for scene-specific training.\nSpecifically, WildSeg3D not only attains the accuracy of state-of-the-art\n(SOTA) methods but also achieves a $40\\times$ speedup compared to existing SOTA\nmodels. Our code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in interactive 3D segmentation from 2D images have\ndemonstrated impressive performance. However, current models typically require\nextensive scene-specific training to accurately reconstruct and segment\nobjects, which limits their applicability in real-time scenarios. In this\npaper, we introduce WildSeg3D, an efficient approach that enables the\nsegmentation of arbitrary 3D objects across diverse environments using a\nfeed-forward mechanism. A key challenge of this feed-forward approach lies in\nthe accumulation of 3D alignment errors across multiple 2D views, which can\nlead to inaccurate 3D segmentation results. To address this issue, we propose\nDynamic Global Aligning (DGA), a technique that improves the accuracy of global\nmulti-view alignment by focusing on difficult-to-match 3D points across images,\nusing a dynamic adjustment function. Additionally, for real-time interactive\nsegmentation, we introduce Multi-view Group Mapping (MGM), a method that\nutilizes an object mask cache to integrate multi-view segmentations and respond\nrapidly to user prompts. WildSeg3D demonstrates robust generalization across\narbitrary scenes, thereby eliminating the need for scene-specific training.\nSpecifically, WildSeg3D not only attains the accuracy of state-of-the-art\n(SOTA) methods but also achieves a $40\\times$ speedup compared to existing SOTA\nmodels. Our code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Yansong Guo"
                    },
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yansong Qu"
                    },
                    {
                        "name": "Liujuan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Liujuan Cao"
                },
                "author": "Liujuan Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08407v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08407v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12491v1",
                "updated": "2025-03-16T12:49:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    16,
                    12,
                    49,
                    44,
                    6,
                    75,
                    0
                ],
                "published": "2025-03-16T12:49:44Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    12,
                    49,
                    44,
                    6,
                    75,
                    0
                ],
                "title": "CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences"
                },
                "summary": "Large language models (LLMs) excel at processing long sequences, boosting\ndemand for key-value (KV) caching. While recent efforts to evict KV cache have\nalleviated the inference burden, they often fail to allocate resources\nrationally across layers with different attention patterns. In this paper, we\nintroduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach\nthat frames KV cache eviction as a \"cake-slicing problem.\" CAKE assesses\nlayer-specific preferences by considering attention dynamics in both spatial\nand temporal dimensions, allocates rational cache size for layers accordingly,\nand manages memory constraints in a cascading manner. This approach enables a\nglobal view of cache allocation, adaptively distributing resources across\ndiverse attention mechanisms while maintaining memory budgets. CAKE also\nemploys a new eviction indicator that considers the shifting importance of\ntokens over time, addressing limitations in existing methods that overlook\ntemporal dynamics. Comprehensive experiments on LongBench and NeedleBench show\nthat CAKE maintains model performance with only 3.2% of the KV cache and\nconsistently outperforms current baselines across various models and memory\nconstraints, particularly in low-memory settings. Additionally, CAKE achieves\nover 10x speedup in decoding latency compared to full cache when processing\ncontexts of 128K tokens with FlashAttention-2. Our code is available at\nhttps://github.com/antgroup/cakekv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at processing long sequences, boosting\ndemand for key-value (KV) caching. While recent efforts to evict KV cache have\nalleviated the inference burden, they often fail to allocate resources\nrationally across layers with different attention patterns. In this paper, we\nintroduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach\nthat frames KV cache eviction as a \"cake-slicing problem.\" CAKE assesses\nlayer-specific preferences by considering attention dynamics in both spatial\nand temporal dimensions, allocates rational cache size for layers accordingly,\nand manages memory constraints in a cascading manner. This approach enables a\nglobal view of cache allocation, adaptively distributing resources across\ndiverse attention mechanisms while maintaining memory budgets. CAKE also\nemploys a new eviction indicator that considers the shifting importance of\ntokens over time, addressing limitations in existing methods that overlook\ntemporal dynamics. Comprehensive experiments on LongBench and NeedleBench show\nthat CAKE maintains model performance with only 3.2% of the KV cache and\nconsistently outperforms current baselines across various models and memory\nconstraints, particularly in low-memory settings. Additionally, CAKE achieves\nover 10x speedup in decoding latency compared to full cache when processing\ncontexts of 128K tokens with FlashAttention-2. Our code is available at\nhttps://github.com/antgroup/cakekv."
                },
                "authors": [
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Yuchen Cao"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Wen Hu"
                    },
                    {
                        "name": "Shixuan Fan"
                    },
                    {
                        "name": "Ke Cheng"
                    },
                    {
                        "name": "Weiyao Lin"
                    },
                    {
                        "name": "Jianguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianguo Li"
                },
                "author": "Jianguo Li",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12450v1",
                "updated": "2025-03-16T10:54:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    16,
                    10,
                    54,
                    59,
                    6,
                    75,
                    0
                ],
                "published": "2025-03-16T10:54:59Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    10,
                    54,
                    59,
                    6,
                    75,
                    0
                ],
                "title": "LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching"
                },
                "summary": "Masked Autoregressive (MAR) models have emerged as a promising approach in\nimage generation, expected to surpass traditional autoregressive models in\ncomputational efficiency by leveraging the capability of parallel decoding.\nHowever, their dependence on bidirectional self-attention inherently conflicts\nwith conventional KV caching mechanisms, creating unexpected computational\nbottlenecks that undermine their expected efficiency. To address this problem,\nthis paper studies the caching mechanism for MAR by leveraging two types of\nredundancy: Token Redundancy indicates that a large portion of tokens have very\nsimilar representations in the adjacent decoding steps, which allows us to\nfirst cache them in previous steps and then reuse them in the later steps.\nCondition Redundancy indicates that the difference between conditional and\nunconditional output in classifier-free guidance exhibits very similar values\nin adjacent steps. Based on these two redundancies, we propose LazyMAR, which\nintroduces two caching mechanisms to handle them one by one. LazyMAR is\ntraining-free and plug-and-play for all MAR models. Experimental results\ndemonstrate that our method achieves 2.83 times acceleration with almost no\ndrop in generation quality. Our codes will be released in\nhttps://github.com/feihongyan1/LazyMAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Autoregressive (MAR) models have emerged as a promising approach in\nimage generation, expected to surpass traditional autoregressive models in\ncomputational efficiency by leveraging the capability of parallel decoding.\nHowever, their dependence on bidirectional self-attention inherently conflicts\nwith conventional KV caching mechanisms, creating unexpected computational\nbottlenecks that undermine their expected efficiency. To address this problem,\nthis paper studies the caching mechanism for MAR by leveraging two types of\nredundancy: Token Redundancy indicates that a large portion of tokens have very\nsimilar representations in the adjacent decoding steps, which allows us to\nfirst cache them in previous steps and then reuse them in the later steps.\nCondition Redundancy indicates that the difference between conditional and\nunconditional output in classifier-free guidance exhibits very similar values\nin adjacent steps. Based on these two redundancies, we propose LazyMAR, which\nintroduces two caching mechanisms to handle them one by one. LazyMAR is\ntraining-free and plug-and-play for all MAR models. Experimental results\ndemonstrate that our method achieves 2.83 times acceleration with almost no\ndrop in generation quality. Our codes will be released in\nhttps://github.com/feihongyan1/LazyMAR."
                },
                "authors": [
                    {
                        "name": "Feihong Yan"
                    },
                    {
                        "name": "Qingyan Wei"
                    },
                    {
                        "name": "Jiayi Tang"
                    },
                    {
                        "name": "Jiajun Li"
                    },
                    {
                        "name": "Yulin Wang"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Huiqi Li"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v1",
                "updated": "2025-03-15T14:13:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data-often inaccessible during online inference-and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\nPoint-Cache, a hierarchical cache model that captures essential clues of online\ntest samples, particularly focusing on the global structure of point clouds and\ntheir local-part details. Point-Cache, which serves as a rich 3D knowledge\nbase, is dynamically managed to prioritize the inclusion of high-quality\nsamples. Designed as a plug-and-play module, our method can be flexibly\nintegrated into large multimodal 3D models to support open-vocabulary point\ncloud recognition. Notably, our solution operates with efficiency comparable to\nzero-shot inference, as it is entirely training-free. Point-Cache demonstrates\nsubstantial gains across 8 challenging benchmarks and 4 representative large 3D\nmodels, highlighting its effectiveness. Code is available at\nhttps://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data-often inaccessible during online inference-and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\nPoint-Cache, a hierarchical cache model that captures essential clues of online\ntest samples, particularly focusing on the global structure of point clouds and\ntheir local-part details. Point-Cache, which serves as a rich 3D knowledge\nbase, is dynamically managed to prioritize the inclusion of high-quality\nsamples. Designed as a plug-and-play module, our method can be flexibly\nintegrated into large multimodal 3D models to support open-vocabulary point\ncloud recognition. Notably, our solution operates with efficiency comparable to\nzero-shot inference, as it is entirely training-free. Point-Cache demonstrates\nsubstantial gains across 8 challenging benchmarks and 4 representative large 3D\nmodels, highlighting its effectiveness. Code is available at\nhttps://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11972v1",
                "updated": "2025-03-15T02:48:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    2,
                    48,
                    27,
                    5,
                    74,
                    0
                ],
                "published": "2025-03-15T02:48:27Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    2,
                    48,
                    27,
                    5,
                    74,
                    0
                ],
                "title": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models"
                },
                "summary": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment."
                },
                "authors": [
                    {
                        "name": "Yuchen Xia"
                    },
                    {
                        "name": "Divyam Sharma"
                    },
                    {
                        "name": "Yichao Yuan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Nishil Talati"
                    }
                ],
                "author_detail": {
                    "name": "Nishil Talati"
                },
                "author": "Nishil Talati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11946v1",
                "updated": "2025-03-15T01:35:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    1,
                    35,
                    53,
                    5,
                    74,
                    0
                ],
                "published": "2025-03-15T01:35:53Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    1,
                    35,
                    53,
                    5,
                    74,
                    0
                ],
                "title": "CCRSat: A Collaborative Computation Reuse Framework for Satellite Edge\n  Computing Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CCRSat: A Collaborative Computation Reuse Framework for Satellite Edge\n  Computing Networks"
                },
                "summary": "In satellite computing applications, such as remote sensing, tasks often\ninvolve similar or identical input data, leading to the same processing\nresults. Computation reuse is an emerging paradigm that leverages the execution\nresults of previous tasks to enhance the utilization of computational\nresources. While this paradigm has been extensively studied in terrestrial\nnetworks with abundant computing and caching resources, such as named data\nnetworking (NDN), it is essential to develop a framework appropriate for\nresource-constrained satellite networks, which are expected to have longer task\ncompletion times. In this paper, we propose CCRSat, a collaborative computation\nreuse framework for satellite edge computing networks. CCRSat initially\nimplements local computation reuse on an independent satellite, utilizing a\nsatellite reuse state (SRS) to assess the efficiency of computation reuse.\nAdditionally, an inter-satellite computation reuse algorithm is introduced,\nwhich utilizes the collaborative sharing of similarity in previously processed\ndata among multiple satellites. The evaluation results tested on real-world\ndatasets demonstrate that, compared to comparative scenarios, our proposed\nCCRSat can significantly reduce task completion time by up to 62.1% and\ncomputational resource consumption by up to 28.8%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In satellite computing applications, such as remote sensing, tasks often\ninvolve similar or identical input data, leading to the same processing\nresults. Computation reuse is an emerging paradigm that leverages the execution\nresults of previous tasks to enhance the utilization of computational\nresources. While this paradigm has been extensively studied in terrestrial\nnetworks with abundant computing and caching resources, such as named data\nnetworking (NDN), it is essential to develop a framework appropriate for\nresource-constrained satellite networks, which are expected to have longer task\ncompletion times. In this paper, we propose CCRSat, a collaborative computation\nreuse framework for satellite edge computing networks. CCRSat initially\nimplements local computation reuse on an independent satellite, utilizing a\nsatellite reuse state (SRS) to assess the efficiency of computation reuse.\nAdditionally, an inter-satellite computation reuse algorithm is introduced,\nwhich utilizes the collaborative sharing of similarity in previously processed\ndata among multiple satellites. The evaluation results tested on real-world\ndatasets demonstrate that, compared to comparative scenarios, our proposed\nCCRSat can significantly reduce task completion time by up to 62.1% and\ncomputational resource consumption by up to 28.8%."
                },
                "authors": [
                    {
                        "name": "Ye Zhang"
                    },
                    {
                        "name": "Zhishu Shen"
                    },
                    {
                        "name": "Dawen Jiang"
                    },
                    {
                        "name": "Xiangrui Liu"
                    },
                    {
                        "name": "Qiushi Zheng"
                    },
                    {
                        "name": "Jiong Jin"
                    }
                ],
                "author_detail": {
                    "name": "Jiong Jin"
                },
                "author": "Jiong Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06348v2",
                "updated": "2025-03-15T00:49:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    0,
                    49,
                    55,
                    5,
                    74,
                    0
                ],
                "published": "2024-03-11T00:30:25Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    0,
                    30,
                    25,
                    0,
                    71,
                    0
                ],
                "title": "Accelerating Sparse Tensor Decomposition Using Adaptive Linearized\n  Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Sparse Tensor Decomposition Using Adaptive Linearized\n  Representation"
                },
                "summary": "High-dimensional sparse data emerge in many critical application domains such\nas healthcare and cybersecurity. To extract meaningful insights from massive\nvolumes of these multi-dimensional data, scientists employ unsupervised\nanalysis tools based on tensor decomposition (TD) methods. However, real-world\nsparse tensors exhibit highly irregular shapes and data distributions, which\npose significant challenges for making efficient use of modern parallel\nprocessors. This study breaks the prevailing assumption that compressing sparse\ntensors into coarse-grained structures or along a particular dimension/mode is\nmore efficient than keeping them in a fine-grained, mode-agnostic form. Our\nnovel sparse tensor representation, Adaptive Linearized Tensor Order (ALTO),\nencodes tensors in a compact format that can be easily streamed from memory and\nis amenable to both caching and parallel execution. In contrast to existing\ncompressed tensor formats, ALTO constructs one tensor copy that is agnostic to\nboth the mode orientation and the irregular distribution of nonzero elements.\nTo demonstrate the efficacy of ALTO, we propose a set of parallel TD algorithms\nthat exploit the inherent data reuse of tensor computations to substantially\nreduce synchronization overhead, decrease memory footprint, and improve\nparallel performance. Additionally, we characterize the major execution\nbottlenecks of TD methods on the latest Intel Xeon Scalable processors and\nintroduce dynamic adaptation heuristics to automatically select the best\nalgorithm based on the sparse tensor characteristics. Across a diverse set of\nreal-world data sets, ALTO outperforms the state-of-the-art approaches,\nachieving more than an order-of-magnitude speedup over the best mode-agnostic\nformats. Compared to the best mode-specific formats, ALTO achieves 5.1X\ngeometric mean speedup at a fraction (25%) of their storage costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-dimensional sparse data emerge in many critical application domains such\nas healthcare and cybersecurity. To extract meaningful insights from massive\nvolumes of these multi-dimensional data, scientists employ unsupervised\nanalysis tools based on tensor decomposition (TD) methods. However, real-world\nsparse tensors exhibit highly irregular shapes and data distributions, which\npose significant challenges for making efficient use of modern parallel\nprocessors. This study breaks the prevailing assumption that compressing sparse\ntensors into coarse-grained structures or along a particular dimension/mode is\nmore efficient than keeping them in a fine-grained, mode-agnostic form. Our\nnovel sparse tensor representation, Adaptive Linearized Tensor Order (ALTO),\nencodes tensors in a compact format that can be easily streamed from memory and\nis amenable to both caching and parallel execution. In contrast to existing\ncompressed tensor formats, ALTO constructs one tensor copy that is agnostic to\nboth the mode orientation and the irregular distribution of nonzero elements.\nTo demonstrate the efficacy of ALTO, we propose a set of parallel TD algorithms\nthat exploit the inherent data reuse of tensor computations to substantially\nreduce synchronization overhead, decrease memory footprint, and improve\nparallel performance. Additionally, we characterize the major execution\nbottlenecks of TD methods on the latest Intel Xeon Scalable processors and\nintroduce dynamic adaptation heuristics to automatically select the best\nalgorithm based on the sparse tensor characteristics. Across a diverse set of\nreal-world data sets, ALTO outperforms the state-of-the-art approaches,\nachieving more than an order-of-magnitude speedup over the best mode-agnostic\nformats. Compared to the best mode-specific formats, ALTO achieves 5.1X\ngeometric mean speedup at a fraction (25%) of their storage costs."
                },
                "authors": [
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Ahmed E. Helal"
                    },
                    {
                        "name": "S. Isaac Geronimo Anderson"
                    },
                    {
                        "name": "Fabio Checconi"
                    },
                    {
                        "name": "Yongseok Soh"
                    },
                    {
                        "name": "Jesmin Jahan Tithi"
                    },
                    {
                        "name": "Teresa Ranadive"
                    },
                    {
                        "name": "Brian J Gravelle"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    },
                    {
                        "name": "Jee Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jee Choi"
                },
                "author": "Jee Choi",
                "arxiv_comment": "Accepted to TPDS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v1",
                "updated": "2025-03-14T19:02:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Invited paper to IEEE Custom Integrated Circuits Conference (CICC)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11750v1",
                "updated": "2025-03-14T17:57:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    57,
                    42,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T17:57:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    57,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Making Every Step Effective: Jailbreaking Large Vision-Language Models\n  Through Hierarchical KV Equalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Every Step Effective: Jailbreaking Large Vision-Language Models\n  Through Hierarchical KV Equalization"
                },
                "summary": "In the realm of large vision-language models (LVLMs), adversarial jailbreak\nattacks serve as a red-teaming approach to identify safety vulnerabilities of\nthese models and their associated defense mechanisms. However, we identify a\ncritical limitation: not every adversarial optimization step leads to a\npositive outcome, and indiscriminately accepting optimization results at each\nstep may reduce the overall attack success rate. To address this challenge, we\nintroduce HKVE (Hierarchical Key-Value Equalization), an innovative\njailbreaking framework that selectively accepts gradient optimization results\nbased on the distribution of attention scores across different layers, ensuring\nthat every optimization step positively contributes to the attack. Extensive\nexperiments demonstrate HKVE's significant effectiveness, achieving attack\nsuccess rates of 75.08% on MiniGPT4, 85.84% on LLaVA and 81.00% on Qwen-VL,\nsubstantially outperforming existing methods by margins of 20.43\\%, 21.01\\% and\n26.43\\% respectively. Furthermore, making every step effective not only leads\nto an increase in attack success rate but also allows for a reduction in the\nnumber of iterations, thereby lowering computational costs. Warning: This paper\ncontains potentially harmful example data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of large vision-language models (LVLMs), adversarial jailbreak\nattacks serve as a red-teaming approach to identify safety vulnerabilities of\nthese models and their associated defense mechanisms. However, we identify a\ncritical limitation: not every adversarial optimization step leads to a\npositive outcome, and indiscriminately accepting optimization results at each\nstep may reduce the overall attack success rate. To address this challenge, we\nintroduce HKVE (Hierarchical Key-Value Equalization), an innovative\njailbreaking framework that selectively accepts gradient optimization results\nbased on the distribution of attention scores across different layers, ensuring\nthat every optimization step positively contributes to the attack. Extensive\nexperiments demonstrate HKVE's significant effectiveness, achieving attack\nsuccess rates of 75.08% on MiniGPT4, 85.84% on LLaVA and 81.00% on Qwen-VL,\nsubstantially outperforming existing methods by margins of 20.43\\%, 21.01\\% and\n26.43\\% respectively. Furthermore, making every step effective not only leads\nto an increase in attack success rate but also allows for a reduction in the\nnumber of iterations, thereby lowering computational costs. Warning: This paper\ncontains potentially harmful example data."
                },
                "authors": [
                    {
                        "name": "Shuyang Hao"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Muhao Chen"
                    },
                    {
                        "name": "Zi Huang"
                    },
                    {
                        "name": "Yujun Cai"
                    }
                ],
                "author_detail": {
                    "name": "Yujun Cai"
                },
                "author": "Yujun Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01066v2",
                "updated": "2025-03-14T16:57:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    57,
                    12,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-03T00:14:34Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    0,
                    14,
                    34,
                    0,
                    62,
                    0
                ],
                "title": "Alchemist: Towards the Design of Efficient Online Continual Learning\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alchemist: Towards the Design of Efficient Online Continual Learning\n  System"
                },
                "summary": "Continual learning has become a promising solution to refine large language\nmodels incrementally by leveraging user feedback. In particular, online\ncontinual learning - iteratively training the model with small batches of user\nfeedback - has demonstrated notable performance improvements. However, the\nexisting practice of separating training and serving processes forces the\nonline trainer to recompute the intermediate results already done during\nserving. Such redundant computations can account for 30%-42% of total training\ntime.\n  In this paper, we propose Alchemist, to the best of our knowledge, the first\nonline continual learning system that efficiently reuses serving activations to\nincrease training throughput. Alchemist introduces two key techniques: (1)\nrecording and storing activations and KV cache only during the prefill phase to\nminimize latency and memory overhead; and (2) smart activation offloading and\nhedging. Evaluations with inputs of varied token length sampled from ShareGPT\ndataset show that compared with a separate training cluster, Alchemist\nsignificantly increases training throughput by up to 1.72x, reduces up to 47%\nmemory usage during training, and supports up to 2x more training tokens - all\nwhile maintaining negligible impact on serving latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning has become a promising solution to refine large language\nmodels incrementally by leveraging user feedback. In particular, online\ncontinual learning - iteratively training the model with small batches of user\nfeedback - has demonstrated notable performance improvements. However, the\nexisting practice of separating training and serving processes forces the\nonline trainer to recompute the intermediate results already done during\nserving. Such redundant computations can account for 30%-42% of total training\ntime.\n  In this paper, we propose Alchemist, to the best of our knowledge, the first\nonline continual learning system that efficiently reuses serving activations to\nincrease training throughput. Alchemist introduces two key techniques: (1)\nrecording and storing activations and KV cache only during the prefill phase to\nminimize latency and memory overhead; and (2) smart activation offloading and\nhedging. Evaluations with inputs of varied token length sampled from ShareGPT\ndataset show that compared with a separate training cluster, Alchemist\nsignificantly increases training throughput by up to 1.72x, reduces up to 47%\nmemory usage during training, and supports up to 2x more training tokens - all\nwhile maintaining negligible impact on serving latency."
                },
                "authors": [
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Haryadi S. Gunawi"
                    },
                    {
                        "name": "Beibin Li"
                    },
                    {
                        "name": "Changho Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Changho Hwang"
                },
                "author": "Changho Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11460v1",
                "updated": "2025-03-14T14:47:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    47,
                    55,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:47:55Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    47,
                    55,
                    4,
                    73,
                    0
                ],
                "title": "ARCAS: Adaptive Runtime System for Chiplet-Aware Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCAS: Adaptive Runtime System for Chiplet-Aware Scheduling"
                },
                "summary": "The growing disparity between CPU core counts and available memory bandwidth\nhas intensified memory contention in servers. This particularly affects highly\nparallelizable applications, which must achieve efficient cache utilization to\nmaintain performance as CPU core counts grow. Optimizing cache utilization,\nhowever, is complex for recent chiplet-based CPUs, whose partitioned L3 caches\nlead to varying latencies and bandwidths, even within a single NUMA domain.\nClassical NUMA optimizations and task scheduling approaches unfortunately fail\nto address the performance issues of chiplet-based CPUs.\n  We describe Adaptive Runtime system for Chiplet-Aware Scheduling (ARCAS), a\nnew runtime system designed for chiplet-based CPUs. ARCAS combines\nchiplet-aware task scheduling heuristics, hardware-aware memory allocation, and\nfine-grained performance monitoring to optimize workload execution. It\nimplements a lightweight concurrency model that combines user-level thread\nfeatures-such as individual stacks, per-task scheduling, and state\nmanagement-with coroutine-like behavior, allowing tasks to suspend and resume\nexecution at defined points while efficiently managing task migration across\nchiplets. Our evaluation across diverse scenarios shows ARCAS's effectiveness\nfor optimizing the performance of memory-intensive parallel applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing disparity between CPU core counts and available memory bandwidth\nhas intensified memory contention in servers. This particularly affects highly\nparallelizable applications, which must achieve efficient cache utilization to\nmaintain performance as CPU core counts grow. Optimizing cache utilization,\nhowever, is complex for recent chiplet-based CPUs, whose partitioned L3 caches\nlead to varying latencies and bandwidths, even within a single NUMA domain.\nClassical NUMA optimizations and task scheduling approaches unfortunately fail\nto address the performance issues of chiplet-based CPUs.\n  We describe Adaptive Runtime system for Chiplet-Aware Scheduling (ARCAS), a\nnew runtime system designed for chiplet-based CPUs. ARCAS combines\nchiplet-aware task scheduling heuristics, hardware-aware memory allocation, and\nfine-grained performance monitoring to optimize workload execution. It\nimplements a lightweight concurrency model that combines user-level thread\nfeatures-such as individual stacks, per-task scheduling, and state\nmanagement-with coroutine-like behavior, allowing tasks to suspend and resume\nexecution at defined points while efficiently managing task migration across\nchiplets. Our evaluation across diverse scenarios shows ARCAS's effectiveness\nfor optimizing the performance of memory-intensive parallel applications."
                },
                "authors": [
                    {
                        "name": "Alessandro Fogli"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Peter Pietzuch"
                    },
                    {
                        "name": "Jana Giceva"
                    }
                ],
                "author_detail": {
                    "name": "Jana Giceva"
                },
                "author": "Jana Giceva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11426v1",
                "updated": "2025-03-14T14:14:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    14,
                    5,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:14:05Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    14,
                    5,
                    4,
                    73,
                    0
                ],
                "title": "Text Compression for Efficient Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Compression for Efficient Language Generation"
                },
                "summary": "We challenge the prevailing assumption that LLMs must rely fully on sub-word\ntokens for high-quality text generation. To this end, we propose the\n\"Generative Pretrained Thoughtformer\" (GPTHF), a hierarchical transformer\nlanguage model capable of text generation by compressing text into sentence\nembeddings and employing a sentence attention mechanism. GPTHF retains GPT's\narchitecture, modifying only token interactions via dynamic sparse attention\nmasks.\n  Our experiments show that GPTHF achieves an up to an order of magnitude\nimprovement in FLOPs efficiency and a threefold increase in runtime speed\ncompared to equally-sized GPT models in the low-size regime. This is achieved\nthrough a unique generation method that caches and reuses sentence embeddings,\nallowing significant portions of the input to bypass large parts of the\nnetwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We challenge the prevailing assumption that LLMs must rely fully on sub-word\ntokens for high-quality text generation. To this end, we propose the\n\"Generative Pretrained Thoughtformer\" (GPTHF), a hierarchical transformer\nlanguage model capable of text generation by compressing text into sentence\nembeddings and employing a sentence attention mechanism. GPTHF retains GPT's\narchitecture, modifying only token interactions via dynamic sparse attention\nmasks.\n  Our experiments show that GPTHF achieves an up to an order of magnitude\nimprovement in FLOPs efficiency and a threefold increase in runtime speed\ncompared to equally-sized GPT models in the low-size regime. This is achieved\nthrough a unique generation method that caches and reuses sentence embeddings,\nallowing significant portions of the input to bypass large parts of the\nnetwork."
                },
                "authors": [
                    {
                        "name": "David Gu"
                    },
                    {
                        "name": "Peter Belcak"
                    },
                    {
                        "name": "Roger Wattenhofer"
                    }
                ],
                "author_detail": {
                    "name": "Roger Wattenhofer"
                },
                "author": "Roger Wattenhofer",
                "arxiv_comment": "accepted to NAACL SRW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v1",
                "updated": "2025-03-14T06:49:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid (i.e., combination of regular attention and MLA\nlayers) or full MLA variant through lightweight post-training adaptation,\nbypassing the need for extensive pre-training. We demonstrate that leveraging\nthe dark knowledge of a well-trained model can enhance training accuracy and\nenable extreme KV cache compression in MLA without compromising model\nperformance. Our results show that using an 8B teacher model allows us to\ncompress the KV cache size of the Llama3.2-1B-Inst baseline by 6.4x while\npreserving 100% of its average score across multiple tasks on the LM Harness\nEvaluation benchmark. This is achieved with only 3.6B training tokens and about\n70 GPU hours on AMD MI300 GPUs, compared to the 370K GPU hours required for\npre-training the Llama3.2-1B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid (i.e., combination of regular attention and MLA\nlayers) or full MLA variant through lightweight post-training adaptation,\nbypassing the need for extensive pre-training. We demonstrate that leveraging\nthe dark knowledge of a well-trained model can enhance training accuracy and\nenable extreme KV cache compression in MLA without compromising model\nperformance. Our results show that using an 8B teacher model allows us to\ncompress the KV cache size of the Llama3.2-1B-Inst baseline by 6.4x while\npreserving 100% of its average score across multiple tasks on the LM Harness\nEvaluation benchmark. This is achieved with only 3.6B training tokens and about\n70 GPU hours on AMD MI300 GPUs, compared to the 370K GPU hours required for\npre-training the Llama3.2-1B model."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11108v1",
                "updated": "2025-03-14T06:01:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    1,
                    42,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T06:01:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    1,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Limits of KV Cache Compression for Tensor Attention based Autoregressive\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limits of KV Cache Compression for Tensor Attention based Autoregressive\n  Transformers"
                },
                "summary": "The key-value (KV) cache in autoregressive transformers presents a\nsignificant bottleneck during inference, which restricts the context length\ncapabilities of large language models (LLMs). While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanism [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a novel\nreduction from communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. In the low\ndimensional regime where $d = o(\\log n)$, we analyze the theoretical bounds of\nthe space complexity as well. Overall, our work provides a theoretical\nfoundation for us to understand the compression-expressivity tradeoff in tensor\nattention mechanisms and offers more perspectives in developing more\nmemory-efficient transformer architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache in autoregressive transformers presents a\nsignificant bottleneck during inference, which restricts the context length\ncapabilities of large language models (LLMs). While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanism [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a novel\nreduction from communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. In the low\ndimensional regime where $d = o(\\log n)$, we analyze the theoretical bounds of\nthe space complexity as well. Overall, our work provides a theoretical\nfoundation for us to understand the compression-expressivity tradeoff in tensor\nattention mechanisms and offers more perspectives in developing more\nmemory-efficient transformer architectures."
                },
                "authors": [
                    {
                        "name": "Yifang Chen"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yu Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tian"
                },
                "author": "Yu Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10589v1",
                "updated": "2025-03-13T17:40:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    40,
                    7,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:40:07Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    40,
                    7,
                    3,
                    72,
                    0
                ],
                "title": "Long Context Tuning for Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Context Tuning for Video Generation"
                },
                "summary": "Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details."
                },
                "authors": [
                    {
                        "name": "Yuwei Guo"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Ziyan Yang"
                    },
                    {
                        "name": "Zhibei Ma"
                    },
                    {
                        "name": "Zhijie Lin"
                    },
                    {
                        "name": "Zhenheng Yang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Lu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Jiang"
                },
                "author": "Lu Jiang",
                "arxiv_comment": "Project Page: https://guoyww.github.io/projects/long-context-video/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10568v1",
                "updated": "2025-03-13T17:19:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:19:51Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Randomized Parallel Decoding"
                },
                "summary": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale."
                },
                "authors": [
                    {
                        "name": "Haopeng Li"
                    },
                    {
                        "name": "Jinyue Yang"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07720v2",
                "updated": "2025-03-13T16:29:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    29,
                    17,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-10T18:13:20Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer"
                },
                "summary": "We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion\nTransformer, that innovatively combines autoregressive and diffusion paradigms\nfor modeling continuous visual information. By introducing a block-wise\nautoregressive unit, ACDiT offers a flexible interpolation between token-wise\nautoregression and full-sequence diffusion, bypassing the limitations of\ndiscrete tokenization. The generation of each block is formulated as a\nconditional diffusion process, conditioned on prior blocks. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) on\nstandard diffusion transformer during training. During inference, the process\niterates between diffusion denoising and autoregressive decoding that can make\nfull use of KV-Cache. We show that ACDiT performs best among all autoregressive\nbaselines under similar model scales on image and video generation tasks. We\nalso demonstrate that benefiting from autoregressive modeling, pretrained ACDiT\ncan be transferred in visual understanding tasks despite being trained with the\ndiffusion objective. The analysis of the trade-off between autoregressive\nmodeling and diffusion demonstrates the potential of ACDiT to be used in\nlong-horizon visual generation tasks. We hope that ACDiT offers a novel\nperspective on visual autoregressive generation and unlocks new avenues for\nunified models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion\nTransformer, that innovatively combines autoregressive and diffusion paradigms\nfor modeling continuous visual information. By introducing a block-wise\nautoregressive unit, ACDiT offers a flexible interpolation between token-wise\nautoregression and full-sequence diffusion, bypassing the limitations of\ndiscrete tokenization. The generation of each block is formulated as a\nconditional diffusion process, conditioned on prior blocks. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) on\nstandard diffusion transformer during training. During inference, the process\niterates between diffusion denoising and autoregressive decoding that can make\nfull use of KV-Cache. We show that ACDiT performs best among all autoregressive\nbaselines under similar model scales on image and video generation tasks. We\nalso demonstrate that benefiting from autoregressive modeling, pretrained ACDiT\ncan be transferred in visual understanding tasks despite being trained with the\ndiffusion objective. The analysis of the trade-off between autoregressive\nmodeling and diffusion demonstrates the potential of ACDiT to be used in\nlong-horizon visual generation tasks. We hope that ACDiT offers a novel\nperspective on visual autoregressive generation and unlocks new avenues for\nunified models."
                },
                "authors": [
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10501v1",
                "updated": "2025-03-13T16:04:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    4,
                    31,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T16:04:31Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    4,
                    31,
                    3,
                    72,
                    0
                ],
                "title": "TokenCarve: Information-Preserving Visual Token Compression in\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenCarve: Information-Preserving Visual Token Compression in\n  Multimodal Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are becoming increasingly popular,\nwhile the high computational cost associated with multimodal data input,\nparticularly from visual tokens, poses a significant challenge. Existing\ntraining-based token compression methods improve inference efficiency but\nrequire costly retraining, while training-free methods struggle to maintain\nperformance when aggressively reducing token counts. In this study, we reveal\nthat the performance degradation of MLLM closely correlates with the\naccelerated loss of information in the attention output matrix. This insight\nintroduces a novel information-preserving perspective, making it possible to\nmaintain performance even under extreme token compression. Based on this\nfinding, we propose TokenCarve, a training-free, plug-and-play, two-stage token\ncompression framework. The first stage employs an\nInformation-Preservation-Guided Selection (IPGS) strategy to prune\nlow-information tokens, while the second stage further leverages IPGS to guide\ntoken merging, minimizing information loss. Extensive experiments on 11\ndatasets and 2 model variants demonstrate the effectiveness of TokenCarve. It\ncan even reduce the number of visual tokens to 22.2% of the original count,\nachieving a 1.23x speedup in inference, a 64% reduction in KV cache storage,\nand only a 1.54% drop in accuracy. Our code is available at\nhttps://github.com/ShawnTan86/TokenCarve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are becoming increasingly popular,\nwhile the high computational cost associated with multimodal data input,\nparticularly from visual tokens, poses a significant challenge. Existing\ntraining-based token compression methods improve inference efficiency but\nrequire costly retraining, while training-free methods struggle to maintain\nperformance when aggressively reducing token counts. In this study, we reveal\nthat the performance degradation of MLLM closely correlates with the\naccelerated loss of information in the attention output matrix. This insight\nintroduces a novel information-preserving perspective, making it possible to\nmaintain performance even under extreme token compression. Based on this\nfinding, we propose TokenCarve, a training-free, plug-and-play, two-stage token\ncompression framework. The first stage employs an\nInformation-Preservation-Guided Selection (IPGS) strategy to prune\nlow-information tokens, while the second stage further leverages IPGS to guide\ntoken merging, minimizing information loss. Extensive experiments on 11\ndatasets and 2 model variants demonstrate the effectiveness of TokenCarve. It\ncan even reduce the number of visual tokens to 22.2% of the original count,\nachieving a 1.23x speedup in inference, a 64% reduction in KV cache storage,\nand only a 1.54% drop in accuracy. Our code is available at\nhttps://github.com/ShawnTan86/TokenCarve."
                },
                "authors": [
                    {
                        "name": "Xudong Tan"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Chongjun Tu"
                    },
                    {
                        "name": "Jianjian Cao"
                    },
                    {
                        "name": "Yaoxin Yang"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10494v1",
                "updated": "2025-03-13T15:57:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    57,
                    50,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T15:57:50Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    57,
                    50,
                    3,
                    72,
                    0
                ],
                "title": "Source-primed Multi-turn Conversation Helps Large Language Models\n  Translate Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source-primed Multi-turn Conversation Helps Large Language Models\n  Translate Documents"
                },
                "summary": "LLMs have paved the way for truly simple document-level machine translation,\nbut challenges such as omission errors remain. In this paper, we study a simple\nmethod for handling document-level machine translation, by leveraging previous\ncontexts in a multi-turn conversational manner. Specifically, by decomposing\ndocuments into segments and iteratively translating them while maintaining\nprevious turns, this method ensures coherent translations without additional\ntraining, and can fully re-use the KV cache of previous turns thus minimizing\ncomputational overhead. We further propose a `source-primed' method that first\nprovides the whole source document before multi-turn translation. We\nempirically show this multi-turn method outperforms both translating entire\ndocuments in a single turn and translating each segment independently according\nto multiple automatic metrics in representative LLMs, establishing a strong\nbaseline for document-level translation using LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have paved the way for truly simple document-level machine translation,\nbut challenges such as omission errors remain. In this paper, we study a simple\nmethod for handling document-level machine translation, by leveraging previous\ncontexts in a multi-turn conversational manner. Specifically, by decomposing\ndocuments into segments and iteratively translating them while maintaining\nprevious turns, this method ensures coherent translations without additional\ntraining, and can fully re-use the KV cache of previous turns thus minimizing\ncomputational overhead. We further propose a `source-primed' method that first\nprovides the whole source document before multi-turn translation. We\nempirically show this multi-turn method outperforms both translating entire\ndocuments in a single turn and translating each segment independently according\nto multiple automatic metrics in representative LLMs, establishing a strong\nbaseline for document-level translation using LLMs."
                },
                "authors": [
                    {
                        "name": "Hanxu Hu"
                    },
                    {
                        "name": "Jannis Vamvas"
                    },
                    {
                        "name": "Rico Sennrich"
                    }
                ],
                "author_detail": {
                    "name": "Rico Sennrich"
                },
                "author": "Rico Sennrich",
                "arxiv_comment": "9 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10337v1",
                "updated": "2025-03-13T13:15:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    15,
                    28,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T13:15:28Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    15,
                    28,
                    3,
                    72,
                    0
                ],
                "title": "KV-Distill: Nearly Lossless Learnable Context Compression for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Distill: Nearly Lossless Learnable Context Compression for LLMs"
                },
                "summary": "Sequence-to-sequence tasks often benefit from long contexts, but the\nquadratic complexity of self-attention in standard Transformers renders this\nnon-trivial. During generation, temporary representations -stored in the\nso-called KV cache-account for a large portion of GPU memory usage and scale\nlinearly with context length. We introduce KV-Distill, a Transformer\ncompression framework that distills long context KV caches into significantly\nshorter representations in a question-independent fashion. KV-Distill can be\ntrained as a parameter-efficient adaptor for pretrained models, and enables the\ncompression of arbitrary spans of a context while preserving pre-trained model\ncapabilities. We treat a compressed-uncompressed cache as a student-teacher\npairing and apply a KL-type divergence to match the generated outputs.\nKV-Distill outperforms other compression techniques in worst-case extractive\ntasks and approaches uncompressed performance in long context question\nanswering and summarization, and it can be fine-tuned on domain-specific\ncontexts to reduce lengths by up to 99% while preserving downstream\nperformance. We demonstrate the generalizability of KV-Distill across various\nmodel sizes and architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence-to-sequence tasks often benefit from long contexts, but the\nquadratic complexity of self-attention in standard Transformers renders this\nnon-trivial. During generation, temporary representations -stored in the\nso-called KV cache-account for a large portion of GPU memory usage and scale\nlinearly with context length. We introduce KV-Distill, a Transformer\ncompression framework that distills long context KV caches into significantly\nshorter representations in a question-independent fashion. KV-Distill can be\ntrained as a parameter-efficient adaptor for pretrained models, and enables the\ncompression of arbitrary spans of a context while preserving pre-trained model\ncapabilities. We treat a compressed-uncompressed cache as a student-teacher\npairing and apply a KL-type divergence to match the generated outputs.\nKV-Distill outperforms other compression techniques in worst-case extractive\ntasks and approaches uncompressed performance in long context question\nanswering and summarization, and it can be fine-tuned on domain-specific\ncontexts to reduce lengths by up to 99% while preserving downstream\nperformance. We demonstrate the generalizability of KV-Distill across various\nmodel sizes and architectures."
                },
                "authors": [
                    {
                        "name": "Vivek Chari"
                    },
                    {
                        "name": "Guanghui Qin"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v1",
                "updated": "2025-03-13T11:26:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit : Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit : Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v3",
                "updated": "2025-03-13T11:14:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    14,
                    49,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: https://github.com/NX-AI/flashrnn",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: https://github.com/NX-AI/flashrnn"
                },
                "authors": [
                    {
                        "name": "Korbinian Pöppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10074v1",
                "updated": "2025-03-13T05:43:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T05:43:14Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "title": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension"
                },
                "summary": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions."
                },
                "authors": [
                    {
                        "name": "Taehun Kim"
                    },
                    {
                        "name": "Hyerean Jang"
                    },
                    {
                        "name": "Youngjoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Youngjoo Shin"
                },
                "author": "Youngjoo Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17599v2",
                "updated": "2025-03-13T04:04:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    4,
                    4,
                    8,
                    3,
                    72,
                    0
                ],
                "published": "2025-02-24T19:34:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference"
                },
                "summary": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Hui Shen"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Zheda Mai"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10714v1",
                "updated": "2025-03-13T03:36:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T03:36:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "ZeroMerge: Parameter-Free KV Cache Compression for Memory-Efficient\n  Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZeroMerge: Parameter-Free KV Cache Compression for Memory-Efficient\n  Long-Context LLMs"
                },
                "summary": "The linear growth of key-value (KV) cache memory and quadratic computational\ncomplexity pose significant bottlenecks for large language models (LLMs) in\nlong-context processing. While existing KV cache optimization methods address\nthese challenges through token pruning or feature merging, they often suffer\nfrom irreversible information loss or require costly parameter retraining. We\npropose ZeroMerge, a dynamic zero-shot compression framework that achieves\nefficient cache management through three key innovations: (1) Fine-grained\nmemory allocation guided by multi-dimensional token importance metrics at\nhead-level granularity, (2) A residual merging mechanism that preserves\ncritical context through compensated attention scoring, and (3) Parameter-free\nadaptation compatible with diverse LLM architectures without retraining.\nComprehensive evaluations across LLaMA-2 model demonstrate that ZeroMerge\nmaintains full-cache performance at 5\\% compression ratios while doubling\ninference throughput at 40K token lengths. The method effectively balances\nmemory efficiency, generation quality, and deployment flexibility, advancing\npractical long-context LLM applications. The code is available at\nhttps://github.com/SusCom-Lab/ZeroMerge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The linear growth of key-value (KV) cache memory and quadratic computational\ncomplexity pose significant bottlenecks for large language models (LLMs) in\nlong-context processing. While existing KV cache optimization methods address\nthese challenges through token pruning or feature merging, they often suffer\nfrom irreversible information loss or require costly parameter retraining. We\npropose ZeroMerge, a dynamic zero-shot compression framework that achieves\nefficient cache management through three key innovations: (1) Fine-grained\nmemory allocation guided by multi-dimensional token importance metrics at\nhead-level granularity, (2) A residual merging mechanism that preserves\ncritical context through compensated attention scoring, and (3) Parameter-free\nadaptation compatible with diverse LLM architectures without retraining.\nComprehensive evaluations across LLaMA-2 model demonstrate that ZeroMerge\nmaintains full-cache performance at 5\\% compression ratios while doubling\ninference throughput at 40K token lengths. The method effectively balances\nmemory efficiency, generation quality, and deployment flexibility, advancing\npractical long-context LLM applications. The code is available at\nhttps://github.com/SusCom-Lab/ZeroMerge."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13035v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13035v3",
                "updated": "2025-03-13T03:16:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    16,
                    43,
                    3,
                    72,
                    0
                ],
                "published": "2024-06-18T20:01:51Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    20,
                    1,
                    51,
                    1,
                    170,
                    0
                ],
                "title": "D2O: Dynamic Discriminative Operations for Efficient Long-Context\n  Inference of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2O: Dynamic Discriminative Operations for Efficient Long-Context\n  Inference of Large Language Models"
                },
                "summary": "Generative inference in Large Language Models (LLMs) is impeded by the\ngrowing memory demands of Key-Value (KV) cache, especially for longer\nsequences. Traditional KV cache eviction strategies, which discard less\ncritical KV pairs based on attention scores, often degrade generation quality,\nleading to issues such as context loss or hallucinations. In this work, we\nintroduce Dynamic Discriminative Operations (D2O), a KV cache compression\nmethod that optimizes KV cache size dynamically and discriminatively at two\nlevels without fine-tuning, while preserving essential context. At layer level,\nD2O leverages the varying densities of attention weights between shallow and\ndeep layers to dynamically determine which layers should avoid excessive\neviction via a novel dynamic allocation strategy to minimize information loss.\nAt token level, D2O incorporates a compensation mechanism that maintains a\nsimilarity threshold to re-discriminate the importance of currently discarded\ntokens, determining whether they should be recalled and merged with similar\ntokens. We conduct experiments on various benchmarks and LLM architectures. Our\nresults show that D2O not only achieves significant memory savings and enhances\ninference throughput by more than 3$\\times$ but also maintains high-quality\nlong-text generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative inference in Large Language Models (LLMs) is impeded by the\ngrowing memory demands of Key-Value (KV) cache, especially for longer\nsequences. Traditional KV cache eviction strategies, which discard less\ncritical KV pairs based on attention scores, often degrade generation quality,\nleading to issues such as context loss or hallucinations. In this work, we\nintroduce Dynamic Discriminative Operations (D2O), a KV cache compression\nmethod that optimizes KV cache size dynamically and discriminatively at two\nlevels without fine-tuning, while preserving essential context. At layer level,\nD2O leverages the varying densities of attention weights between shallow and\ndeep layers to dynamically determine which layers should avoid excessive\neviction via a novel dynamic allocation strategy to minimize information loss.\nAt token level, D2O incorporates a compensation mechanism that maintains a\nsimilarity threshold to re-discriminate the importance of currently discarded\ntokens, determining whether they should be recalled and merged with similar\ntokens. We conduct experiments on various benchmarks and LLM architectures. Our\nresults show that D2O not only achieves significant memory savings and enhances\ninference throughput by more than 3$\\times$ but also maintains high-quality\nlong-text generation."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Xinjian Wu"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zhihong Zhu"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Siqi Luo"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13035v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13035v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v3",
                "updated": "2025-03-12T18:14:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    18,
                    14,
                    21,
                    2,
                    71,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Efficient MoE Inference on Personal Machines with\n  Sparsity-Aware Expert Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Efficient MoE Inference on Personal Machines with\n  Sparsity-Aware Expert Cache"
                },
                "summary": "This paper presents MoE-Infinity, an efficient MoE inference system designed\nfor personal machines with limited GPU memory capacity. The key idea for\nMoE-Infinity is that on personal machines, which are often single-user\nenvironments, MoE-based LLMs typically operate with a batch size of one. In\nthis setting, MoE models exhibit a high degree of activation sparsity, meaning\na small number of experts are frequently reused in generating tokens during the\ndecode phase. Leveraging this idea, we design a sparsity-aware expert cache,\nwhich can trace the sparse activation of experts during inference and carefully\nselect the trace that represents the sparsity pattern. By analyzing these\nselected traces, MoE-Infinity guides the replacement and prefetching of the\nexpert cache, providing 3.1-16.7x per-token latency improvements over numerous\nstate-of-the-art systems, including vLLM, Ollama, DeepSpeed and BrainStorm\nacross various MoE models (DeepSeek and Mixtral) when handling different LLM\ntasks. MoE-Infinity's source code is publicly available at\nhttps://github.com/EfficientMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an efficient MoE inference system designed\nfor personal machines with limited GPU memory capacity. The key idea for\nMoE-Infinity is that on personal machines, which are often single-user\nenvironments, MoE-based LLMs typically operate with a batch size of one. In\nthis setting, MoE models exhibit a high degree of activation sparsity, meaning\na small number of experts are frequently reused in generating tokens during the\ndecode phase. Leveraging this idea, we design a sparsity-aware expert cache,\nwhich can trace the sparse activation of experts during inference and carefully\nselect the trace that represents the sparsity pattern. By analyzing these\nselected traces, MoE-Infinity guides the replacement and prefetching of the\nexpert cache, providing 3.1-16.7x per-token latency improvements over numerous\nstate-of-the-art systems, including vLLM, Ollama, DeepSpeed and BrainStorm\nacross various MoE models (DeepSeek and Mixtral) when handling different LLM\ntasks. MoE-Infinity's source code is publicly available at\nhttps://github.com/EfficientMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v2",
                "updated": "2025-03-12T17:59:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    59,
                    18,
                    2,
                    71,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs"
                },
                "summary": "Long-range tasks demand reasoning over long inputs. Current solutions require\nlarge compute budgets, training data, model weight access, or complex\ntask-specific designs. We introduce PRISM, which processes information as a\nstream of chunks while maintaining a structured in-context memory specified\nwith a typed hierarchical schema. PRISM outperforms baselines on diverse tasks\nwhile using at least 4x shorter contexts than long-context models. This\napproach is token-efficient, producing concise outputs and efficiently\nleveraging key-value (KV) caches to reduce costs by up to 54% compared to\nalternative short-context methods. PRISM scales down to tiny chunks (<500\ntokens) without increasing encoding costs or sacrificing quality, and\ngeneralizes to new tasks with minimal effort by automatically generating\nschemas from task descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks demand reasoning over long inputs. Current solutions require\nlarge compute budgets, training data, model weight access, or complex\ntask-specific designs. We introduce PRISM, which processes information as a\nstream of chunks while maintaining a structured in-context memory specified\nwith a typed hierarchical schema. PRISM outperforms baselines on diverse tasks\nwhile using at least 4x shorter contexts than long-context models. This\napproach is token-efficient, producing concise outputs and efficiently\nleveraging key-value (KV) caches to reduce costs by up to 54% compared to\nalternative short-context methods. PRISM scales down to tiny chunks (<500\ntokens) without increasing encoding costs or sacrificing quality, and\ngeneralizes to new tasks with minimal effort by automatically generating\nschemas from task descriptions."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "28 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09218v1",
                "updated": "2025-03-12T10:05:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    5,
                    5,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T10:05:05Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    5,
                    5,
                    2,
                    71,
                    0
                ],
                "title": "N2C2: Nearest Neighbor Enhanced Confidence Calibration for Cross-Lingual\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "N2C2: Nearest Neighbor Enhanced Confidence Calibration for Cross-Lingual\n  In-Context Learning"
                },
                "summary": "Recent advancements of in-context learning (ICL) show language models can\nsignificantly improve their performance when demonstrations are provided.\nHowever, little attention has been paid to model calibration and prediction\nconfidence of ICL in cross-lingual scenarios. To bridge this gap, we conduct a\nthorough analysis of ICL for cross-lingual sentiment classification. Our\nfindings suggest that ICL performs poorly in cross-lingual scenarios,\nexhibiting low accuracy and presenting high calibration errors. In response, we\npropose a novel approach, N2C2, which employs a -nearest neighbors augmented\nclassifier for prediction confidence calibration. N2C2 narrows the prediction\ngap by leveraging a datastore of cached few-shot instances. Specifically, N2C2\nintegrates the predictions from the datastore and incorporates confidence-aware\ndistribution, semantically consistent retrieval representation, and adaptive\nneighbor combination modules to effectively utilize the limited number of\nsupporting instances. Evaluation on two multilingual sentiment classification\ndatasets demonstrates that N2C2 outperforms traditional ICL. It surpasses fine\ntuning, prompt tuning and recent state-of-the-art methods in terms of accuracy\nand calibration errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements of in-context learning (ICL) show language models can\nsignificantly improve their performance when demonstrations are provided.\nHowever, little attention has been paid to model calibration and prediction\nconfidence of ICL in cross-lingual scenarios. To bridge this gap, we conduct a\nthorough analysis of ICL for cross-lingual sentiment classification. Our\nfindings suggest that ICL performs poorly in cross-lingual scenarios,\nexhibiting low accuracy and presenting high calibration errors. In response, we\npropose a novel approach, N2C2, which employs a -nearest neighbors augmented\nclassifier for prediction confidence calibration. N2C2 narrows the prediction\ngap by leveraging a datastore of cached few-shot instances. Specifically, N2C2\nintegrates the predictions from the datastore and incorporates confidence-aware\ndistribution, semantically consistent retrieval representation, and adaptive\nneighbor combination modules to effectively utilize the limited number of\nsupporting instances. Evaluation on two multilingual sentiment classification\ndatasets demonstrates that N2C2 outperforms traditional ICL. It surpasses fine\ntuning, prompt tuning and recent state-of-the-art methods in terms of accuracy\nand calibration errors."
                },
                "authors": [
                    {
                        "name": "Jie He"
                    },
                    {
                        "name": "Simon Yu"
                    },
                    {
                        "name": "Deyi Xiong"
                    },
                    {
                        "name": "Víctor Gutiérrez-Basulto"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17363v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17363v3",
                "updated": "2025-03-12T07:23:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    7,
                    23,
                    32,
                    2,
                    71,
                    0
                ],
                "published": "2025-02-24T17:40:09Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    40,
                    9,
                    0,
                    55,
                    0
                ],
                "title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Edit: Training-Free Image Editing for Precise Background Preservation"
                },
                "summary": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit"
                },
                "authors": [
                    {
                        "name": "Tianrui Zhu"
                    },
                    {
                        "name": "Shiyi Zhang"
                    },
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Yansong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Tang"
                },
                "author": "Yansong Tang",
                "arxiv_comment": "Project webpage is available at\n  https://xilluill.github.io/projectpages/KV-Edit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17363v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17363v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19355v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19355v2",
                "updated": "2025-03-12T03:40:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    3,
                    40,
                    38,
                    2,
                    71,
                    0
                ],
                "published": "2024-10-25T07:24:38Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality"
                },
                "summary": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality."
                },
                "authors": [
                    {
                        "name": "Zhengyao Lv"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Junhao Song"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Kwan-Yee K. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Yee K. Wong"
                },
                "author": "Kwan-Yee K. Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19355v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19355v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08966v1",
                "updated": "2025-03-12T00:12:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    0,
                    12,
                    39,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T00:12:39Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    0,
                    12,
                    39,
                    2,
                    71,
                    0
                ],
                "title": "Performance Models for a Two-tiered Storage System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Models for a Two-tiered Storage System"
                },
                "summary": "This work describes the design, implementation and performance analysis of a\ndistributed two-tiered storage software. The first tier functions as a\ndistributed software cache implemented using solid-state devices~(NVMes) and\nthe second tier consists of multiple hard disks~(HDDs). We describe an online\nlearning algorithm that manages data movement between the tiers. The software\nis hybrid, i.e. both distributed and multi-threaded. The end-to-end performance\nmodel of the two-tier system was developed using queuing networks and\nbehavioral models of storage devices. We identified significant parameters that\naffect the performance of storage devices and created behavioral models for\neach device. The performance of the software was evaluated on a many-core\ncluster using non-trivial read/write workloads. The paper provides examples to\nillustrate the use of these models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work describes the design, implementation and performance analysis of a\ndistributed two-tiered storage software. The first tier functions as a\ndistributed software cache implemented using solid-state devices~(NVMes) and\nthe second tier consists of multiple hard disks~(HDDs). We describe an online\nlearning algorithm that manages data movement between the tiers. The software\nis hybrid, i.e. both distributed and multi-threaded. The end-to-end performance\nmodel of the two-tier system was developed using queuing networks and\nbehavioral models of storage devices. We identified significant parameters that\naffect the performance of storage devices and created behavioral models for\neach device. The performance of the software was evaluated on a many-core\ncluster using non-trivial read/write workloads. The paper provides examples to\nillustrate the use of these models."
                },
                "authors": [
                    {
                        "name": "Aparna Sasidharan"
                    },
                    {
                        "name": "Xian-He"
                    },
                    {
                        "name": "Jay Lofstead"
                    },
                    {
                        "name": "Scott Klasky"
                    }
                ],
                "author_detail": {
                    "name": "Scott Klasky"
                },
                "author": "Scott Klasky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08941v1",
                "updated": "2025-03-11T22:44:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    22,
                    44,
                    38,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T22:44:38Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    22,
                    44,
                    38,
                    1,
                    70,
                    0
                ],
                "title": "BCZT/LSMO/BCZT multilayer films for high temperature energy storage\n  capacitors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BCZT/LSMO/BCZT multilayer films for high temperature energy storage\n  capacitors"
                },
                "summary": "Ba0.85Ca0.15Zr0.1Ti0.9O3/La0.8Sr0.2MnO3/Ba0.85Ca0.15Zr0.1Ti0.9O3\n(BCZT/LSMO/BCZT) sandwich films were elaborated using the sol-gel spin coating\nprocess. The dielectric properties displayed excellent thermal stability with\nthe temperature coefficient of capacitance, TCC, remaining within 10% between\n-50 C and 300 C. The high energy storage density, Wrec, of 11.8 J/cm3 observed\nin this sandwich films, is nearly twice as high as that of the BCZT films, with\nan efficiency, n, of 77% under a weak electric field of 800 kV/cm. Furthermore,\nthe stability of Wrec and n was observed along the studied temperature interval\nmaking them promising candidates for high-temperature energy storage\ncapacitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ba0.85Ca0.15Zr0.1Ti0.9O3/La0.8Sr0.2MnO3/Ba0.85Ca0.15Zr0.1Ti0.9O3\n(BCZT/LSMO/BCZT) sandwich films were elaborated using the sol-gel spin coating\nprocess. The dielectric properties displayed excellent thermal stability with\nthe temperature coefficient of capacitance, TCC, remaining within 10% between\n-50 C and 300 C. The high energy storage density, Wrec, of 11.8 J/cm3 observed\nin this sandwich films, is nearly twice as high as that of the BCZT films, with\nan efficiency, n, of 77% under a weak electric field of 800 kV/cm. Furthermore,\nthe stability of Wrec and n was observed along the studied temperature interval\nmaking them promising candidates for high-temperature energy storage\ncapacitors."
                },
                "authors": [
                    {
                        "name": "Afaak Lakouader"
                    },
                    {
                        "name": "Abdelilah Lahmar"
                    },
                    {
                        "name": "Spela Kunej"
                    },
                    {
                        "name": "Daoud Mezzane"
                    },
                    {
                        "name": "Jamal Belhadi"
                    },
                    {
                        "name": "El Hassan Choukri"
                    },
                    {
                        "name": "Lahoucine Hajji"
                    },
                    {
                        "name": "Mbarek Amjoud"
                    },
                    {
                        "name": "Zdravko Kutnjak"
                    },
                    {
                        "name": "Igor A. Lukyanchuk"
                    },
                    {
                        "name": "Mimoun El Marssi"
                    }
                ],
                "author_detail": {
                    "name": "Mimoun El Marssi"
                },
                "author": "Mimoun El Marssi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.19910v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19910v1",
                "updated": "2025-03-25T17:59:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    59,
                    50,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T17:59:50Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    59,
                    50,
                    1,
                    84,
                    0
                ],
                "title": "CoLLM: A Large Language Model for Composed Image Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoLLM: A Large Language Model for Composed Image Retrieval"
                },
                "summary": "Composed Image Retrieval (CIR) is a complex task that aims to retrieve images\nbased on a multimodal query. Typical training data consists of triplets\ncontaining a reference image, a textual description of desired modifications,\nand the target image, which are expensive and time-consuming to acquire. The\nscarcity of CIR datasets has led to zero-shot approaches utilizing synthetic\ntriplets or leveraging vision-language models (VLMs) with ubiquitous\nweb-crawled image-caption pairs. However, these methods have significant\nlimitations: synthetic triplets suffer from limited scale, lack of diversity,\nand unnatural modification text, while image-caption pairs hinder joint\nembedding learning of the multimodal query due to the absence of triplet data.\nMoreover, existing approaches struggle with complex and nuanced modification\ntexts that demand sophisticated fusion and understanding of vision and language\nmodalities. We present CoLLM, a one-stop framework that effectively addresses\nthese limitations. Our approach generates triplets on-the-fly from\nimage-caption pairs, enabling supervised training without manual annotation. We\nleverage Large Language Models (LLMs) to generate joint embeddings of reference\nimages and modification texts, facilitating deeper multimodal fusion.\nAdditionally, we introduce Multi-Text CIR (MTCIR), a large-scale dataset\ncomprising 3.4M samples, and refine existing CIR benchmarks (CIRR and\nFashion-IQ) to enhance evaluation reliability. Experimental results demonstrate\nthat CoLLM achieves state-of-the-art performance across multiple CIR benchmarks\nand settings. MTCIR yields competitive results, with up to 15% performance\nimprovement. Our refined benchmarks provide more reliable evaluation metrics\nfor CIR models, contributing to the advancement of this important field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Composed Image Retrieval (CIR) is a complex task that aims to retrieve images\nbased on a multimodal query. Typical training data consists of triplets\ncontaining a reference image, a textual description of desired modifications,\nand the target image, which are expensive and time-consuming to acquire. The\nscarcity of CIR datasets has led to zero-shot approaches utilizing synthetic\ntriplets or leveraging vision-language models (VLMs) with ubiquitous\nweb-crawled image-caption pairs. However, these methods have significant\nlimitations: synthetic triplets suffer from limited scale, lack of diversity,\nand unnatural modification text, while image-caption pairs hinder joint\nembedding learning of the multimodal query due to the absence of triplet data.\nMoreover, existing approaches struggle with complex and nuanced modification\ntexts that demand sophisticated fusion and understanding of vision and language\nmodalities. We present CoLLM, a one-stop framework that effectively addresses\nthese limitations. Our approach generates triplets on-the-fly from\nimage-caption pairs, enabling supervised training without manual annotation. We\nleverage Large Language Models (LLMs) to generate joint embeddings of reference\nimages and modification texts, facilitating deeper multimodal fusion.\nAdditionally, we introduce Multi-Text CIR (MTCIR), a large-scale dataset\ncomprising 3.4M samples, and refine existing CIR benchmarks (CIRR and\nFashion-IQ) to enhance evaluation reliability. Experimental results demonstrate\nthat CoLLM achieves state-of-the-art performance across multiple CIR benchmarks\nand settings. MTCIR yields competitive results, with up to 15% performance\nimprovement. Our refined benchmarks provide more reliable evaluation metrics\nfor CIR models, contributing to the advancement of this important field."
                },
                "authors": [
                    {
                        "name": "Chuong Huynh"
                    },
                    {
                        "name": "Jinyu Yang"
                    },
                    {
                        "name": "Ashish Tawari"
                    },
                    {
                        "name": "Mubarak Shah"
                    },
                    {
                        "name": "Son Tran"
                    },
                    {
                        "name": "Raffay Hamid"
                    },
                    {
                        "name": "Trishul Chilimbi"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Shrivastava"
                },
                "author": "Abhinav Shrivastava",
                "arxiv_comment": "CVPR 2025. Project page: https://collm-cvpr25.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19910v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19910v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19903v1",
                "updated": "2025-03-25T17:58:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    58,
                    37,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T17:58:37Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    58,
                    37,
                    1,
                    84,
                    0
                ],
                "title": "Scaling Vision Pre-Training to 4K Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Vision Pre-Training to 4K Resolution"
                },
                "summary": "High-resolution perception of visual details is crucial for daily tasks.\nCurrent vision pre-training, however, is still limited to low resolutions\n(e.g., 378 x 378 pixels) due to the quadratic cost of processing larger images.\nWe introduce PS3 that scales CLIP-style vision pre-training to 4K resolution\nwith a near-constant cost. Instead of contrastive learning on global image\nrepresentation, PS3 is pre-trained by selectively processing local regions and\ncontrasting them with local detailed captions, enabling high-resolution\nrepresentation learning with greatly reduced computational overhead. The\npre-trained PS3 is able to both encode the global image at low resolution and\nselectively process local high-resolution regions based on their saliency or\nrelevance to a text prompt. When applying PS3 to multi-modal LLM (MLLM), the\nresulting model, named VILA-HD, significantly improves high-resolution visual\nperception compared to baselines without high-resolution vision pre-training\nsuch as AnyRes and S^2 while using up to 4.3x fewer tokens. PS3 also unlocks\nappealing scaling properties of VILA-HD, including scaling up resolution for\nfree and scaling up test-time compute for better performance. Compared to state\nof the arts, VILA-HD outperforms previous MLLMs such as NVILA and Qwen2-VL\nacross multiple benchmarks and achieves better efficiency than latest token\npruning approaches. Finally, we find current benchmarks do not require\n4K-resolution perception, which motivates us to propose 4KPro, a new benchmark\nof image QA at 4K resolution, on which VILA-HD outperforms all previous MLLMs,\nincluding a 14.5% improvement over GPT-4o, and a 3.2% improvement and 2.96x\nspeedup over Qwen2-VL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-resolution perception of visual details is crucial for daily tasks.\nCurrent vision pre-training, however, is still limited to low resolutions\n(e.g., 378 x 378 pixels) due to the quadratic cost of processing larger images.\nWe introduce PS3 that scales CLIP-style vision pre-training to 4K resolution\nwith a near-constant cost. Instead of contrastive learning on global image\nrepresentation, PS3 is pre-trained by selectively processing local regions and\ncontrasting them with local detailed captions, enabling high-resolution\nrepresentation learning with greatly reduced computational overhead. The\npre-trained PS3 is able to both encode the global image at low resolution and\nselectively process local high-resolution regions based on their saliency or\nrelevance to a text prompt. When applying PS3 to multi-modal LLM (MLLM), the\nresulting model, named VILA-HD, significantly improves high-resolution visual\nperception compared to baselines without high-resolution vision pre-training\nsuch as AnyRes and S^2 while using up to 4.3x fewer tokens. PS3 also unlocks\nappealing scaling properties of VILA-HD, including scaling up resolution for\nfree and scaling up test-time compute for better performance. Compared to state\nof the arts, VILA-HD outperforms previous MLLMs such as NVILA and Qwen2-VL\nacross multiple benchmarks and achieves better efficiency than latest token\npruning approaches. Finally, we find current benchmarks do not require\n4K-resolution perception, which motivates us to propose 4KPro, a new benchmark\nof image QA at 4K resolution, on which VILA-HD outperforms all previous MLLMs,\nincluding a 14.5% improvement over GPT-4o, and a 3.2% improvement and 2.96x\nspeedup over Qwen2-VL."
                },
                "authors": [
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Boyi Li"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Marco Pavone"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Trevor Darrell"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Hongxu Yin"
                    }
                ],
                "author_detail": {
                    "name": "Hongxu Yin"
                },
                "author": "Hongxu Yin",
                "arxiv_comment": "CVPR 2025. Project Page: https://nvlabs.github.io/PS3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19898v1",
                "updated": "2025-03-25T17:56:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    56,
                    31,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T17:56:31Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    56,
                    31,
                    1,
                    84,
                    0
                ],
                "title": "Non-minimally coupled gravity constraints from DESI DR2 data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-minimally coupled gravity constraints from DESI DR2 data"
                },
                "summary": "It has been observed that the hint about phantom crossing in the DESI BAO\nobservation might point to non-minimally coupled gravity. We analyzed DESI DR2\nBAO together with CMB and Type Ia supernova data to constrain non-minimal\ncoupling of gravity in the effective field theory (EFT) approach. Using a\nnon-parametric method to infer the EFT functions, we found that with DESI BAO,\nDESY5 SN, and Planck CMB data the signal for non-minimal coupling reaches\n$\\sim3\\sigma$. It is found that current data can constrain up to the quadratic\norder $(n=2)$ if the EFT function representing non-minimal coupling is Taylor\nexpanded as a general function of the dark energy fraction $\\Omega_{\\rm{DE}}$,\ni.e. $\\Omega^{\\text{EFT}}(a)=\\sum_{i=0}^{n} c^{\\rm{EFT}}_i\n\\Omega^i_{\\text{DE}}(a)$. This calls for a more flexible parametrization of the\nEFT functions than commonly used ones in literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It has been observed that the hint about phantom crossing in the DESI BAO\nobservation might point to non-minimally coupled gravity. We analyzed DESI DR2\nBAO together with CMB and Type Ia supernova data to constrain non-minimal\ncoupling of gravity in the effective field theory (EFT) approach. Using a\nnon-parametric method to infer the EFT functions, we found that with DESI BAO,\nDESY5 SN, and Planck CMB data the signal for non-minimal coupling reaches\n$\\sim3\\sigma$. It is found that current data can constrain up to the quadratic\norder $(n=2)$ if the EFT function representing non-minimal coupling is Taylor\nexpanded as a general function of the dark energy fraction $\\Omega_{\\rm{DE}}$,\ni.e. $\\Omega^{\\text{EFT}}(a)=\\sum_{i=0}^{n} c^{\\rm{EFT}}_i\n\\Omega^i_{\\text{DE}}(a)$. This calls for a more flexible parametrization of the\nEFT functions than commonly used ones in literature."
                },
                "authors": [
                    {
                        "name": "Jiaming Pan"
                    },
                    {
                        "name": "Gen Ye"
                    }
                ],
                "author_detail": {
                    "name": "Gen Ye"
                },
                "author": "Gen Ye",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.01390v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.01390v3",
                "updated": "2025-03-25T17:44:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    44,
                    19,
                    1,
                    84,
                    0
                ],
                "published": "2024-03-03T04:22:13Z",
                "published_parsed": [
                    2024,
                    3,
                    3,
                    4,
                    22,
                    13,
                    6,
                    63,
                    0
                ],
                "title": "Right for Right Reasons: Large Language Models for Verifiable\n  Commonsense Knowledge Graph Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Right for Right Reasons: Large Language Models for Verifiable\n  Commonsense Knowledge Graph Question Answering"
                },
                "summary": "Knowledge Graph Question Answering (KGQA) methods seek to answer Natural\nLanguage questions using the relational information stored in Knowledge Graphs\n(KGs). With the recent advancements of Large Language Models (LLMs) and their\nremarkable reasoning abilities, there is a growing trend to leverage them for\nKGQA. However, existing methodologies have only focused on answering factual\nquestions, e.g., \"In which city was Silvio Berlusconi's first wife born?\",\nleaving questions involving commonsense reasoning that real-world users may\npose more often, e.g., \"Do I need separate visas to see the Venus of Willendorf\nand attend the Olympics this summer?\" unaddressed. In this work, we first\nobserve that existing LLM-based methods for KGQA struggle with hallucination on\nsuch questions, especially on queries targeting long-tail entities (e.g.,\nnon-mainstream and recent entities), thus hindering their applicability in\nreal-world applications especially since their reasoning processes are not\neasily verifiable. In response, we propose Right for Right Reasons (R3), a\ncommonsense KGQA methodology that allows for a verifiable reasoning procedure\nby axiomatically surfacing intrinsic commonsense knowledge of LLMs and\ngrounding every factual reasoning step on KG triples. Through experimental\nevaluations across three different tasks--question answering, claim\nverification, and preference matching--our findings showcase R3 as a superior\napproach, outperforming existing methodologies and notably reducing instances\nof hallucination and reasoning errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph Question Answering (KGQA) methods seek to answer Natural\nLanguage questions using the relational information stored in Knowledge Graphs\n(KGs). With the recent advancements of Large Language Models (LLMs) and their\nremarkable reasoning abilities, there is a growing trend to leverage them for\nKGQA. However, existing methodologies have only focused on answering factual\nquestions, e.g., \"In which city was Silvio Berlusconi's first wife born?\",\nleaving questions involving commonsense reasoning that real-world users may\npose more often, e.g., \"Do I need separate visas to see the Venus of Willendorf\nand attend the Olympics this summer?\" unaddressed. In this work, we first\nobserve that existing LLM-based methods for KGQA struggle with hallucination on\nsuch questions, especially on queries targeting long-tail entities (e.g.,\nnon-mainstream and recent entities), thus hindering their applicability in\nreal-world applications especially since their reasoning processes are not\neasily verifiable. In response, we propose Right for Right Reasons (R3), a\ncommonsense KGQA methodology that allows for a verifiable reasoning procedure\nby axiomatically surfacing intrinsic commonsense knowledge of LLMs and\ngrounding every factual reasoning step on KG triples. Through experimental\nevaluations across three different tasks--question answering, claim\nverification, and preference matching--our findings showcase R3 as a superior\napproach, outperforming existing methodologies and notably reducing instances\nof hallucination and reasoning errors."
                },
                "authors": [
                    {
                        "name": "Armin Toroghi"
                    },
                    {
                        "name": "Willis Guo"
                    },
                    {
                        "name": "Mohammad Mahdi Abdollah Pour"
                    },
                    {
                        "name": "Scott Sanner"
                    }
                ],
                "author_detail": {
                    "name": "Scott Sanner"
                },
                "author": "Scott Sanner",
                "arxiv_comment": "33 pages, EMNLP24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.01390v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.01390v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19878v1",
                "updated": "2025-03-25T17:43:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    43,
                    8,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T17:43:08Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    43,
                    8,
                    1,
                    84,
                    0
                ],
                "title": "CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation"
                },
                "summary": "Large language models (LLMs) have revolutionized natural language processing\n(NLP), particularly through Retrieval-Augmented Generation (RAG), which\nenhances LLM capabilities by integrating external knowledge. However,\ntraditional RAG systems face critical limitations, including disrupted\ncontextual integrity due to text chunking, and over-reliance on semantic\nsimilarity for retrieval. To address these issues, we propose CausalRAG, a\nnovel framework that incorporates causal graphs into the retrieval process. By\nconstructing and tracing causal relationships, CausalRAG preserves contextual\ncontinuity and improves retrieval precision, leading to more accurate and\ninterpretable responses. We evaluate CausalRAG against regular RAG and\ngraph-based RAG approaches, demonstrating its superiority across several\nmetrics. Our findings suggest that grounding retrieval in causal reasoning\nprovides a promising approach to knowledge-intensive tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized natural language processing\n(NLP), particularly through Retrieval-Augmented Generation (RAG), which\nenhances LLM capabilities by integrating external knowledge. However,\ntraditional RAG systems face critical limitations, including disrupted\ncontextual integrity due to text chunking, and over-reliance on semantic\nsimilarity for retrieval. To address these issues, we propose CausalRAG, a\nnovel framework that incorporates causal graphs into the retrieval process. By\nconstructing and tracing causal relationships, CausalRAG preserves contextual\ncontinuity and improves retrieval precision, leading to more accurate and\ninterpretable responses. We evaluate CausalRAG against regular RAG and\ngraph-based RAG approaches, demonstrating its superiority across several\nmetrics. Our findings suggest that grounding retrieval in causal reasoning\nprovides a promising approach to knowledge-intensive tasks."
                },
                "authors": [
                    {
                        "name": "Nengbo Wang"
                    },
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Jagdip Singh"
                    },
                    {
                        "name": "Jing Ma"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    }
                ],
                "author_detail": {
                    "name": "Vipin Chaudhary"
                },
                "author": "Vipin Chaudhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19876v1",
                "updated": "2025-03-25T17:38:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    38,
                    28,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T17:38:28Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    38,
                    28,
                    1,
                    84,
                    0
                ],
                "title": "SLA-Awareness for AI-assisted coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLA-Awareness for AI-assisted coding"
                },
                "summary": "The integration of AI-assisted coding tools within development environments\ndrastically reduces development time, and allows developers to focus more on\ncreative and critical aspects of software engineering through the use of Code\nLarge Language Models (CodeLLMs). These coding assistants automate repetitive\nand time-consuming coding tasks such as code generation, code completion, code\nsummarization, and code translation. Responsiveness is a crucial requirement of\nthese coding assistants to maintain real-time interactivity, such that their\nuse does not impede the developers' workflows. Different coding tasks have\nunique characteristics and latency requirements: Time-To-First-Token (TTFT)\nlatency is essential for code completion tasks, while End-To-End (E2E) latency\nis crucial for code translation tasks. Managing these varying requirements\nsimultaneously while optimizing resource usage poses significant challenges.\nExisting work adopts the Model-as-a-Service paradigm for serving individual\nCodeLLMs, but cannot effectively manage latency requirements of concurrent\ncoding tasks and sequences of CodeLLM inference calls, due to a lack of\nend-to-end latency awareness. Another challenge is keeping resource utilization\nhigh, when the serving system is deployed on a shared cluster environment. To\naddress these challenges, we propose Coding Assistant Task Orchestrator (CATO),\na runtime system designed to serve a diverse assortment of coding tasks while\nmeeting latency requirements and maximizing resource utilization. Our\nexperiments demonstrate that when all types of coding tasks were served\nsimultaneously, for TTFT-critical tasks, CATO improves overall Goodput rate and\nresource utilization by up to 10% and 41.1%, respectively. P95 E2E latency was\nalso reduced by 18% for code summarization tasks, and P95 TTFT for code\ngeneration tasks were reduced by 14% compared against state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of AI-assisted coding tools within development environments\ndrastically reduces development time, and allows developers to focus more on\ncreative and critical aspects of software engineering through the use of Code\nLarge Language Models (CodeLLMs). These coding assistants automate repetitive\nand time-consuming coding tasks such as code generation, code completion, code\nsummarization, and code translation. Responsiveness is a crucial requirement of\nthese coding assistants to maintain real-time interactivity, such that their\nuse does not impede the developers' workflows. Different coding tasks have\nunique characteristics and latency requirements: Time-To-First-Token (TTFT)\nlatency is essential for code completion tasks, while End-To-End (E2E) latency\nis crucial for code translation tasks. Managing these varying requirements\nsimultaneously while optimizing resource usage poses significant challenges.\nExisting work adopts the Model-as-a-Service paradigm for serving individual\nCodeLLMs, but cannot effectively manage latency requirements of concurrent\ncoding tasks and sequences of CodeLLM inference calls, due to a lack of\nend-to-end latency awareness. Another challenge is keeping resource utilization\nhigh, when the serving system is deployed on a shared cluster environment. To\naddress these challenges, we propose Coding Assistant Task Orchestrator (CATO),\na runtime system designed to serve a diverse assortment of coding tasks while\nmeeting latency requirements and maximizing resource utilization. Our\nexperiments demonstrate that when all types of coding tasks were served\nsimultaneously, for TTFT-critical tasks, CATO improves overall Goodput rate and\nresource utilization by up to 10% and 41.1%, respectively. P95 E2E latency was\nalso reduced by 18% for code summarization tasks, and P95 TTFT for code\ngeneration tasks were reduced by 14% compared against state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Kishanthan Thangarajah"
                    },
                    {
                        "name": "Arthur Leung"
                    },
                    {
                        "name": "Boyuan Chen"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16034v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16034v2",
                "updated": "2025-03-25T17:29:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    29,
                    12,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-20T11:00:56Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    0,
                    56,
                    3,
                    79,
                    0
                ],
                "title": "Verification and External Parameter Inference for Stochastic World\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verification and External Parameter Inference for Stochastic World\n  Models"
                },
                "summary": "Given its ability to analyse stochastic models ranging from discrete and\ncontinuous-time Markov chains to Markov decision processes and stochastic\ngames, probabilistic model checking (PMC) is widely used to verify system\ndependability and performance properties. However, modelling the behaviour of,\nand verifying these properties for many software-intensive systems requires the\njoint analysis of multiple interdependent stochastic models of different types,\nwhich existing PMC techniques and tools cannot handle. To address this\nlimitation, we introduce a tool-supported UniversaL stochasTIc Modelling,\nverificAtion and synThEsis (ULTIMATE) framework that supports the\nrepresentation, verification and synthesis of heterogeneous multi-model\nstochastic systems with complex model interdependencies. Through its unique\nintegration of multiple PMC paradigms, and underpinned by a novel verification\nmethod for handling model interdependencies, ULTIMATE unifies-for the first\ntime-the modelling of probabilistic and nondeterministic uncertainty, discrete\nand continuous time, partial observability, and the use of both Bayesian and\nfrequentist inference to exploit domain knowledge and data about the modelled\nsystem and its context. A comprehensive suite of case studies and experiments\nconfirm the generality and effectiveness of our novel verification framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given its ability to analyse stochastic models ranging from discrete and\ncontinuous-time Markov chains to Markov decision processes and stochastic\ngames, probabilistic model checking (PMC) is widely used to verify system\ndependability and performance properties. However, modelling the behaviour of,\nand verifying these properties for many software-intensive systems requires the\njoint analysis of multiple interdependent stochastic models of different types,\nwhich existing PMC techniques and tools cannot handle. To address this\nlimitation, we introduce a tool-supported UniversaL stochasTIc Modelling,\nverificAtion and synThEsis (ULTIMATE) framework that supports the\nrepresentation, verification and synthesis of heterogeneous multi-model\nstochastic systems with complex model interdependencies. Through its unique\nintegration of multiple PMC paradigms, and underpinned by a novel verification\nmethod for handling model interdependencies, ULTIMATE unifies-for the first\ntime-the modelling of probabilistic and nondeterministic uncertainty, discrete\nand continuous time, partial observability, and the use of both Bayesian and\nfrequentist inference to exploit domain knowledge and data about the modelled\nsystem and its context. A comprehensive suite of case studies and experiments\nconfirm the generality and effectiveness of our novel verification framework."
                },
                "authors": [
                    {
                        "name": "Radu Calinescu"
                    },
                    {
                        "name": "Sinem Getir Yaman"
                    },
                    {
                        "name": "Simos Gerasimou"
                    },
                    {
                        "name": "Gricel Vázquez"
                    },
                    {
                        "name": "Micah Bassett"
                    }
                ],
                "author_detail": {
                    "name": "Micah Bassett"
                },
                "author": "Micah Bassett",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16034v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16034v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19855v1",
                "updated": "2025-03-25T17:19:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    19,
                    38,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T17:19:38Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    19,
                    38,
                    1,
                    84,
                    0
                ],
                "title": "Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time\n  Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time\n  Thinking"
                },
                "summary": "Recent advances in large language models (LLMs), such as OpenAI-o1 and\nDeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where\nextended reasoning processes substantially enhance model performance. Despite\nthis, current models are constrained by limitations in handling long texts and\nreinforcement learning (RL) training efficiency. To address these issues, we\npropose a simple yet effective test-time scaling approach Multi-round Thinking.\nThis method iteratively refines model reasoning by leveraging previous answers\nas prompts for subsequent rounds. Extensive experiments across multiple models,\nincluding QwQ-32B and DeepSeek-R1, consistently show performance improvements\non various benchmarks such as AIME 2024, MATH-500, GPQA-diamond, and\nLiveCodeBench. For instance, the accuracy of QwQ-32B improved from 80.3% (Round\n1) to 82.1% (Round 2) on the AIME 2024 dataset, while DeepSeek-R1 showed a\nsimilar increase from 79.7% to 82.0%. These results confirm that Multi-round\nThinking is a broadly applicable, straightforward approach to achieving stable\nenhancements in model performance, underscoring its potential for future\ndevelopments in test-time scaling techniques. The key prompt: {Original\nquestion prompt} The assistant's previous answer is: <answer> {last round\nanswer} </answer>, and please re-answer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs), such as OpenAI-o1 and\nDeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where\nextended reasoning processes substantially enhance model performance. Despite\nthis, current models are constrained by limitations in handling long texts and\nreinforcement learning (RL) training efficiency. To address these issues, we\npropose a simple yet effective test-time scaling approach Multi-round Thinking.\nThis method iteratively refines model reasoning by leveraging previous answers\nas prompts for subsequent rounds. Extensive experiments across multiple models,\nincluding QwQ-32B and DeepSeek-R1, consistently show performance improvements\non various benchmarks such as AIME 2024, MATH-500, GPQA-diamond, and\nLiveCodeBench. For instance, the accuracy of QwQ-32B improved from 80.3% (Round\n1) to 82.1% (Round 2) on the AIME 2024 dataset, while DeepSeek-R1 showed a\nsimilar increase from 79.7% to 82.0%. These results confirm that Multi-round\nThinking is a broadly applicable, straightforward approach to achieving stable\nenhancements in model performance, underscoring its potential for future\ndevelopments in test-time scaling techniques. The key prompt: {Original\nquestion prompt} The assistant's previous answer is: <answer> {last round\nanswer} </answer>, and please re-answer."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Tian"
                    },
                    {
                        "name": "Sitong Zhao"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Shuaiting Chen"
                    },
                    {
                        "name": "Yunjie Ji"
                    },
                    {
                        "name": "Yiping Peng"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Xiangang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangang Li"
                },
                "author": "Xiangang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19850v1",
                "updated": "2025-03-25T17:17:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    17,
                    19,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T17:17:19Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    17,
                    19,
                    1,
                    84,
                    0
                ],
                "title": "FALCONEye: Finding Answers and Localizing Content in ONE-hour-long\n  videos with multi-modal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FALCONEye: Finding Answers and Localizing Content in ONE-hour-long\n  videos with multi-modal LLMs"
                },
                "summary": "Information retrieval in hour-long videos presents a significant challenge,\neven for state-of-the-art Vision-Language Models (VLMs), particularly when the\ndesired information is localized within a small subset of frames. Long video\ndata presents challenges for VLMs due to context window limitations and the\ndifficulty of pinpointing frames containing the answer. Our novel video agent,\nFALCONEye, combines a VLM and a Large Language Model (LLM) to search relevant\ninformation along the video, and locate the frames with the answer. FALCONEye\nnovelty relies on 1) the proposed meta-architecture, which is better suited to\ntackle hour-long videos compared to short video approaches in the\nstate-of-the-art; 2) a new efficient exploration algorithm to locate the\ninformation using short clips, captions and answer confidence; and 3) our\nstate-of-the-art VLMs calibration analysis for the answer confidence. Our agent\nis built over a small-size VLM and a medium-size LLM being accessible to run on\nstandard computational resources. We also release FALCON-Bench, a benchmark to\nevaluate long (average > 1 hour) Video Answer Search challenges, highlighting\nthe need for open-ended question evaluation. Our experiments show FALCONEye's\nsuperior performance than the state-of-the-art in FALCON-Bench, and similar or\nbetter performance in related benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information retrieval in hour-long videos presents a significant challenge,\neven for state-of-the-art Vision-Language Models (VLMs), particularly when the\ndesired information is localized within a small subset of frames. Long video\ndata presents challenges for VLMs due to context window limitations and the\ndifficulty of pinpointing frames containing the answer. Our novel video agent,\nFALCONEye, combines a VLM and a Large Language Model (LLM) to search relevant\ninformation along the video, and locate the frames with the answer. FALCONEye\nnovelty relies on 1) the proposed meta-architecture, which is better suited to\ntackle hour-long videos compared to short video approaches in the\nstate-of-the-art; 2) a new efficient exploration algorithm to locate the\ninformation using short clips, captions and answer confidence; and 3) our\nstate-of-the-art VLMs calibration analysis for the answer confidence. Our agent\nis built over a small-size VLM and a medium-size LLM being accessible to run on\nstandard computational resources. We also release FALCON-Bench, a benchmark to\nevaluate long (average > 1 hour) Video Answer Search challenges, highlighting\nthe need for open-ended question evaluation. Our experiments show FALCONEye's\nsuperior performance than the state-of-the-art in FALCON-Bench, and similar or\nbetter performance in related benchmarks."
                },
                "authors": [
                    {
                        "name": "Carlos Plou"
                    },
                    {
                        "name": "Cesar Borja"
                    },
                    {
                        "name": "Ruben Martinez-Cantin"
                    },
                    {
                        "name": "Ana C. Murillo"
                    }
                ],
                "author_detail": {
                    "name": "Ana C. Murillo"
                },
                "author": "Ana C. Murillo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19787v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19787v4",
                "updated": "2025-03-25T17:16:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    16,
                    42,
                    1,
                    84,
                    0
                ],
                "published": "2024-06-28T09:49:26Z",
                "published_parsed": [
                    2024,
                    6,
                    28,
                    9,
                    49,
                    26,
                    4,
                    180,
                    0
                ],
                "title": "Approximate solutions of a general stochastic velocity-jump model\n  subject to discrete-time noisy observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate solutions of a general stochastic velocity-jump model\n  subject to discrete-time noisy observations"
                },
                "summary": "Advances in experimental techniques allow the collection of high-resolution\nspatio-temporal data that track individual motile entities over time. These\ntracking data motivate the use of mathematical models to characterise the\nmotion observed. In this paper, we aim to describe the solutions of\nvelocity-jump models for single-agent motion in one spatial dimension,\ncharacterised by successive Markovian transitions within a finite network of n\nstates, each with a specified velocity and a fixed rate of switching to every\nother state. In particular, we focus on obtaining the solutions of the model\nsubject to noisy, discrete-time, observations, with no direct access to the\nagent state. The lack of direct observation of the hidden state makes the\nproblem of finding the exact distributions generally intractable. Therefore, we\nderive a series of approximations for the data distributions. We verify the\naccuracy of these approximations by comparing them to the empirical\ndistributions generated through simulations of four example model structures.\nThese comparisons confirm that the approximations are accurate given\nsufficiently infrequent state switching relative to the imaging frequency. The\napproximate distributions computed can be used to obtain fast forwards\npredictions, to give guidelines on experimental design, and as likelihoods for\ninference and model selection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in experimental techniques allow the collection of high-resolution\nspatio-temporal data that track individual motile entities over time. These\ntracking data motivate the use of mathematical models to characterise the\nmotion observed. In this paper, we aim to describe the solutions of\nvelocity-jump models for single-agent motion in one spatial dimension,\ncharacterised by successive Markovian transitions within a finite network of n\nstates, each with a specified velocity and a fixed rate of switching to every\nother state. In particular, we focus on obtaining the solutions of the model\nsubject to noisy, discrete-time, observations, with no direct access to the\nagent state. The lack of direct observation of the hidden state makes the\nproblem of finding the exact distributions generally intractable. Therefore, we\nderive a series of approximations for the data distributions. We verify the\naccuracy of these approximations by comparing them to the empirical\ndistributions generated through simulations of four example model structures.\nThese comparisons confirm that the approximations are accurate given\nsufficiently infrequent state switching relative to the imaging frequency. The\napproximate distributions computed can be used to obtain fast forwards\npredictions, to give guidelines on experimental design, and as likelihoods for\ninference and model selection."
                },
                "authors": [
                    {
                        "name": "Arianna Ceccarelli"
                    },
                    {
                        "name": "Alexander P. Browning"
                    },
                    {
                        "name": "Ruth E. Baker"
                    }
                ],
                "author_detail": {
                    "name": "Ruth E. Baker"
                },
                "author": "Ruth E. Baker",
                "arxiv_doi": "10.1007/s11538-025-01437-x",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s11538-025-01437-x",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.19787v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19787v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Main: 35 pages, 9 figures. Supplementary Information: 25 pages, 5\n  figures",
                "arxiv_journal_ref": "Ceccarelli, A., Browning, A.P. & Baker, R.E. Approximate Solutions\n  of a General Stochastic Velocity-Jump Model Subject to Discrete-Time Noisy\n  Observations. Bull Math Biol 87, 57 (2025)",
                "arxiv_primary_category": {
                    "term": "physics.data-an",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92-08",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19844v1",
                "updated": "2025-03-25T17:07:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    7,
                    21,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T17:07:21Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    7,
                    21,
                    1,
                    84,
                    0
                ],
                "title": "A Comparative Analysis of Word Segmentation, Part-of-Speech Tagging, and\n  Named Entity Recognition for Historical Chinese Sources, 1900-1950",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative Analysis of Word Segmentation, Part-of-Speech Tagging, and\n  Named Entity Recognition for Historical Chinese Sources, 1900-1950"
                },
                "summary": "This paper compares large language models (LLMs) and traditional natural\nlanguage processing (NLP) tools for performing word segmentation,\npart-of-speech (POS) tagging, and named entity recognition (NER) on Chinese\ntexts from 1900 to 1950. Historical Chinese documents pose challenges for text\nanalysis due to their logographic script, the absence of natural word\nboundaries, and significant linguistic changes. Using a sample dataset from the\nShanghai Library Republican Journal corpus, traditional tools such as Jieba and\nspaCy are compared to LLMs, including GPT-4o, Claude 3.5, and the GLM series.\nThe results show that LLMs outperform traditional methods in all metrics,\nalbeit at considerably higher computational costs, highlighting a trade-off\nbetween accuracy and efficiency. Additionally, LLMs better handle\ngenre-specific challenges such as poetry and temporal variations (i.e.,\npre-1920 versus post-1920 texts), demonstrating that their contextual learning\ncapabilities can advance NLP approaches to historical texts by reducing the\nneed for domain-specific training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper compares large language models (LLMs) and traditional natural\nlanguage processing (NLP) tools for performing word segmentation,\npart-of-speech (POS) tagging, and named entity recognition (NER) on Chinese\ntexts from 1900 to 1950. Historical Chinese documents pose challenges for text\nanalysis due to their logographic script, the absence of natural word\nboundaries, and significant linguistic changes. Using a sample dataset from the\nShanghai Library Republican Journal corpus, traditional tools such as Jieba and\nspaCy are compared to LLMs, including GPT-4o, Claude 3.5, and the GLM series.\nThe results show that LLMs outperform traditional methods in all metrics,\nalbeit at considerably higher computational costs, highlighting a trade-off\nbetween accuracy and efficiency. Additionally, LLMs better handle\ngenre-specific challenges such as poetry and temporal variations (i.e.,\npre-1920 versus post-1920 texts), demonstrating that their contextual learning\ncapabilities can advance NLP approaches to historical texts by reducing the\nneed for domain-specific training data."
                },
                "authors": [
                    {
                        "name": "Zhao Fang"
                    },
                    {
                        "name": "Liang-Chun Wu"
                    },
                    {
                        "name": "Xuening Kong"
                    },
                    {
                        "name": "Spencer Dean Stewart"
                    }
                ],
                "author_detail": {
                    "name": "Spencer Dean Stewart"
                },
                "author": "Spencer Dean Stewart",
                "arxiv_comment": "Accepted to NLP4DH 2025 at NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14670v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14670v2",
                "updated": "2025-03-25T17:00:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    0,
                    2,
                    1,
                    84,
                    0
                ],
                "published": "2024-10-18T17:58:53Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    58,
                    53,
                    4,
                    292,
                    0
                ],
                "title": "Decomposing The Dark Matter of Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decomposing The Dark Matter of Sparse Autoencoders"
                },
                "summary": "Sparse autoencoders (SAEs) are a promising technique for decomposing language\nmodel activations into interpretable linear features. However, current SAEs\nfall short of completely explaining model performance, resulting in \"dark\nmatter\": unexplained variance in activations. This work investigates dark\nmatter as an object of study in its own right. Surprisingly, we find that much\nof SAE dark matter -- about half of the error vector itself and >90% of its\nnorm -- can be linearly predicted from the initial activation vector.\nAdditionally, we find that the scaling behavior of SAE error norms at a per\ntoken level is remarkably predictable: larger SAEs mostly struggle to\nreconstruct the same contexts as smaller SAEs. We build on the linear\nrepresentation hypothesis to propose models of activations that might lead to\nthese observations. These insights imply that the part of the SAE error vector\nthat cannot be linearly predicted (\"nonlinear\" error) might be fundamentally\ndifferent from the linearly predictable component. To validate this hypothesis,\nwe empirically analyze nonlinear SAE error and show that 1) it contains fewer\nnot yet learned features, 2) SAEs trained on it are quantitatively worse, and\n3) it is responsible for a proportional amount of the downstream increase in\ncross entropy loss when SAE activations are inserted into the model. Finally,\nwe examine two methods to reduce nonlinear SAE error: inference time gradient\npursuit, which leads to a very slight decrease in nonlinear error, and linear\ntransformations from earlier layer SAE outputs, which leads to a larger\nreduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse autoencoders (SAEs) are a promising technique for decomposing language\nmodel activations into interpretable linear features. However, current SAEs\nfall short of completely explaining model performance, resulting in \"dark\nmatter\": unexplained variance in activations. This work investigates dark\nmatter as an object of study in its own right. Surprisingly, we find that much\nof SAE dark matter -- about half of the error vector itself and >90% of its\nnorm -- can be linearly predicted from the initial activation vector.\nAdditionally, we find that the scaling behavior of SAE error norms at a per\ntoken level is remarkably predictable: larger SAEs mostly struggle to\nreconstruct the same contexts as smaller SAEs. We build on the linear\nrepresentation hypothesis to propose models of activations that might lead to\nthese observations. These insights imply that the part of the SAE error vector\nthat cannot be linearly predicted (\"nonlinear\" error) might be fundamentally\ndifferent from the linearly predictable component. To validate this hypothesis,\nwe empirically analyze nonlinear SAE error and show that 1) it contains fewer\nnot yet learned features, 2) SAEs trained on it are quantitatively worse, and\n3) it is responsible for a proportional amount of the downstream increase in\ncross entropy loss when SAE activations are inserted into the model. Finally,\nwe examine two methods to reduce nonlinear SAE error: inference time gradient\npursuit, which leads to a very slight decrease in nonlinear error, and linear\ntransformations from earlier layer SAE outputs, which leads to a larger\nreduction."
                },
                "authors": [
                    {
                        "name": "Joshua Engels"
                    },
                    {
                        "name": "Logan Riggs"
                    },
                    {
                        "name": "Max Tegmark"
                    }
                ],
                "author_detail": {
                    "name": "Max Tegmark"
                },
                "author": "Max Tegmark",
                "arxiv_comment": "Published in TMLR. Code at\n  https://github.com/JoshEngels/SAE-Dark-Matter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14670v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14670v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19839v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19839v1",
                "updated": "2025-03-25T16:59:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    59,
                    42,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T16:59:42Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    59,
                    42,
                    1,
                    84,
                    0
                ],
                "title": "FireEdit: Fine-grained Instruction-based Image Editing via Region-aware\n  Vision Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FireEdit: Fine-grained Instruction-based Image Editing via Region-aware\n  Vision Language Model"
                },
                "summary": "Currently, instruction-based image editing methods have made significant\nprogress by leveraging the powerful cross-modal understanding capabilities of\nvision language models (VLMs). However, they still face challenges in three key\nareas: 1) complex scenarios; 2) semantic consistency; and 3) fine-grained\nediting. To address these issues, we propose FireEdit, an innovative\nFine-grained Instruction-based image editing framework that exploits a\nREgion-aware VLM. FireEdit is designed to accurately comprehend user\ninstructions and ensure effective control over the editing process.\nSpecifically, we enhance the fine-grained visual perception capabilities of the\nVLM by introducing additional region tokens. Relying solely on the output of\nthe LLM to guide the diffusion model may lead to suboptimal editing results.\nTherefore, we propose a Time-Aware Target Injection module and a Hybrid Visual\nCross Attention module. The former dynamically adjusts the guidance strength at\nvarious denoising stages by integrating timestep embeddings with the text\nembeddings. The latter enhances visual details for image editing, thereby\npreserving semantic consistency between the edited result and the source image.\nBy combining the VLM enhanced with fine-grained region tokens and the\ntime-dependent diffusion model, FireEdit demonstrates significant advantages in\ncomprehending editing instructions and maintaining high semantic consistency.\nExtensive experiments indicate that our approach surpasses the state-of-the-art\ninstruction-based image editing methods. Our project is available at\nhttps://zjgans.github.io/fireedit.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Currently, instruction-based image editing methods have made significant\nprogress by leveraging the powerful cross-modal understanding capabilities of\nvision language models (VLMs). However, they still face challenges in three key\nareas: 1) complex scenarios; 2) semantic consistency; and 3) fine-grained\nediting. To address these issues, we propose FireEdit, an innovative\nFine-grained Instruction-based image editing framework that exploits a\nREgion-aware VLM. FireEdit is designed to accurately comprehend user\ninstructions and ensure effective control over the editing process.\nSpecifically, we enhance the fine-grained visual perception capabilities of the\nVLM by introducing additional region tokens. Relying solely on the output of\nthe LLM to guide the diffusion model may lead to suboptimal editing results.\nTherefore, we propose a Time-Aware Target Injection module and a Hybrid Visual\nCross Attention module. The former dynamically adjusts the guidance strength at\nvarious denoising stages by integrating timestep embeddings with the text\nembeddings. The latter enhances visual details for image editing, thereby\npreserving semantic consistency between the edited result and the source image.\nBy combining the VLM enhanced with fine-grained region tokens and the\ntime-dependent diffusion model, FireEdit demonstrates significant advantages in\ncomprehending editing instructions and maintaining high semantic consistency.\nExtensive experiments indicate that our approach surpasses the state-of-the-art\ninstruction-based image editing methods. Our project is available at\nhttps://zjgans.github.io/fireedit.github.io."
                },
                "authors": [
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Jiahao Li"
                    },
                    {
                        "name": "Zunnan Xu"
                    },
                    {
                        "name": "Hanhui Li"
                    },
                    {
                        "name": "Yiji Cheng"
                    },
                    {
                        "name": "Fa-Ting Hong"
                    },
                    {
                        "name": "Qin Lin"
                    },
                    {
                        "name": "Qinglin Lu"
                    },
                    {
                        "name": "Xiaodan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Liang"
                },
                "author": "Xiaodan Liang",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19839v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19839v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.12693v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.12693v5",
                "updated": "2025-03-25T16:44:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    44,
                    19,
                    1,
                    84,
                    0
                ],
                "published": "2023-04-25T09:54:35Z",
                "published_parsed": [
                    2023,
                    4,
                    25,
                    9,
                    54,
                    35,
                    1,
                    115,
                    0
                ],
                "title": "Phylo2Vec: a vector representation for binary trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phylo2Vec: a vector representation for binary trees"
                },
                "summary": "Binary phylogenetic trees inferred from biological data are central to\nunderstanding the shared history among evolutionary units. However, inferring\nthe placement of latent nodes in a tree is computationally expensive.\nState-of-the-art methods rely on carefully designed heuristics for tree search,\nusing different data structures for easy manipulation (e.g., classes in\nobject-oriented programming languages) and readable representation of trees\n(e.g., Newick-format strings). Here, we present Phylo2Vec, a parsimonious\nencoding for phylogenetic trees that serves as a unified approach for both\nmanipulating and representing phylogenetic trees. Phylo2Vec maps any binary\ntree with $n$ leaves to a unique integer vector of length $n-1$. The advantages\nof Phylo2Vec are fourfold: i) fast tree sampling, (ii) compressed tree\nrepresentation compared to a Newick string, iii) quick and unambiguous\nverification if two binary trees are identical topologically, and iv)\nsystematic ability to traverse tree space in very large or small jumps. As a\nproof of concept, we use Phylo2Vec for maximum likelihood inference on five\nreal-world datasets and show that a simple hill-climbing-based optimisation\nscheme can efficiently traverse the vastness of tree space from a random to an\noptimal tree.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary phylogenetic trees inferred from biological data are central to\nunderstanding the shared history among evolutionary units. However, inferring\nthe placement of latent nodes in a tree is computationally expensive.\nState-of-the-art methods rely on carefully designed heuristics for tree search,\nusing different data structures for easy manipulation (e.g., classes in\nobject-oriented programming languages) and readable representation of trees\n(e.g., Newick-format strings). Here, we present Phylo2Vec, a parsimonious\nencoding for phylogenetic trees that serves as a unified approach for both\nmanipulating and representing phylogenetic trees. Phylo2Vec maps any binary\ntree with $n$ leaves to a unique integer vector of length $n-1$. The advantages\nof Phylo2Vec are fourfold: i) fast tree sampling, (ii) compressed tree\nrepresentation compared to a Newick string, iii) quick and unambiguous\nverification if two binary trees are identical topologically, and iv)\nsystematic ability to traverse tree space in very large or small jumps. As a\nproof of concept, we use Phylo2Vec for maximum likelihood inference on five\nreal-world datasets and show that a simple hill-climbing-based optimisation\nscheme can efficiently traverse the vastness of tree space from a random to an\noptimal tree."
                },
                "authors": [
                    {
                        "name": "Matthew J Penn"
                    },
                    {
                        "name": "Neil Scheidwasser"
                    },
                    {
                        "name": "Mark P Khurana"
                    },
                    {
                        "name": "David A Duchêne"
                    },
                    {
                        "name": "Christl A Donnelly"
                    },
                    {
                        "name": "Samir Bhatt"
                    }
                ],
                "author_detail": {
                    "name": "Samir Bhatt"
                },
                "author": "Samir Bhatt",
                "arxiv_doi": "10.1093/sysbio/syae030",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/sysbio/syae030",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2304.12693v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.12693v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "38 pages, 9 figures, 1 table, 2 supplementary figures",
                "arxiv_journal_ref": "Systematic Biology, 2024, syae030",
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01136v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01136v2",
                "updated": "2025-03-25T16:32:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    32,
                    52,
                    1,
                    84,
                    0
                ],
                "published": "2024-10-02T00:15:59Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    0,
                    15,
                    59,
                    2,
                    276,
                    0
                ],
                "title": "$\\texttt{synax}$: A Differentiable and GPU-accelerated Synchrotron\n  Simulation Package",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\texttt{synax}$: A Differentiable and GPU-accelerated Synchrotron\n  Simulation Package"
                },
                "summary": "We introduce synax, a novel library for automatically differentiable\nsimulation of Galactic synchrotron emission. Built on the JAX framework, synax\nleverages JAX's capabilities, including batch acceleration, just-in-time\ncompilation, and hardware-specific optimizations (CPU, GPU, TPU). Crucially,\nsynax uses JAX's automatic differentiation (AD) mechanism, enabling precise\ncomputation of derivatives with respect to any model parameters. This feature\nfacilitates powerful inference algorithms, such as Hamiltonian Monte Carlo\n(HMC) and gradient-based optimization, which enables inference over models that\nwould otherwise be computationally prohibitive. In its initial release, synax\nsupports synchrotron intensity and polarization calculations down to GHz\nfrequencies, alongside several models of the Galactic magnetic field (GMF),\ncosmic ray (CR) spectra, and thermal electron density fields. We demonstrate\nthe transformative potential of AD for tasks involving full posterior inference\nusing gradient-based techniques or Maximum Likelihood Estimation (MLE)\noptimization. Notably, we show that GPU acceleration brings a twenty-fold\nenhancement in efficiency, while HMC achieves a two-fold improvement over\nstandard random walk Metropolis-Hastings (RWMH) when performing inference over\na four-parameter test model. HMC still works on a more complex, 16-parameter\nmodel while RWMH fails to converge. Additionally, we showcase the application\nof synax in optimizing the GMF based on the Haslam 408 MHz map, achieving\nresiduals with a standard deviation below 1 K.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce synax, a novel library for automatically differentiable\nsimulation of Galactic synchrotron emission. Built on the JAX framework, synax\nleverages JAX's capabilities, including batch acceleration, just-in-time\ncompilation, and hardware-specific optimizations (CPU, GPU, TPU). Crucially,\nsynax uses JAX's automatic differentiation (AD) mechanism, enabling precise\ncomputation of derivatives with respect to any model parameters. This feature\nfacilitates powerful inference algorithms, such as Hamiltonian Monte Carlo\n(HMC) and gradient-based optimization, which enables inference over models that\nwould otherwise be computationally prohibitive. In its initial release, synax\nsupports synchrotron intensity and polarization calculations down to GHz\nfrequencies, alongside several models of the Galactic magnetic field (GMF),\ncosmic ray (CR) spectra, and thermal electron density fields. We demonstrate\nthe transformative potential of AD for tasks involving full posterior inference\nusing gradient-based techniques or Maximum Likelihood Estimation (MLE)\noptimization. Notably, we show that GPU acceleration brings a twenty-fold\nenhancement in efficiency, while HMC achieves a two-fold improvement over\nstandard random walk Metropolis-Hastings (RWMH) when performing inference over\na four-parameter test model. HMC still works on a more complex, 16-parameter\nmodel while RWMH fails to converge. Additionally, we showcase the application\nof synax in optimizing the GMF based on the Haslam 408 MHz map, achieving\nresiduals with a standard deviation below 1 K."
                },
                "authors": [
                    {
                        "name": "Kangning Diao"
                    },
                    {
                        "name": "Zack Li"
                    },
                    {
                        "name": "Richard D. P. Grumitt"
                    },
                    {
                        "name": "Yi Mao"
                    }
                ],
                "author_detail": {
                    "name": "Yi Mao"
                },
                "author": "Yi Mao",
                "arxiv_comment": "Matched the version accepted by ApJS, 11 pages, 8 figures, comments\n  welcome. Code at https://github.com/dkn16/Synax",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01136v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01136v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.04535v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.04535v2",
                "updated": "2025-03-25T16:32:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    32,
                    46,
                    1,
                    84,
                    0
                ],
                "published": "2023-10-06T19:02:04Z",
                "published_parsed": [
                    2023,
                    10,
                    6,
                    19,
                    2,
                    4,
                    4,
                    279,
                    0
                ],
                "title": "LLM4DV: Using Large Language Models for Hardware Test Stimuli Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4DV: Using Large Language Models for Hardware Test Stimuli Generation"
                },
                "summary": "Hardware design verification (DV) is a process that checks the functional\nequivalence of a hardware design against its specifications, improving hardware\nreliability and robustness. A key task in the DV process is the test stimuli\ngeneration, which creates a set of conditions or inputs for testing. These test\nconditions are often complex and specific to the given hardware design,\nrequiring substantial human engineering effort to optimize. We seek a solution\nof automated and efficient testing for arbitrary hardware designs that takes\nadvantage of large language models (LLMs). LLMs have already shown promising\nresults for improving hardware design automation, but remain under-explored for\nhardware DV. In this paper, we propose an open-source benchmarking framework\nnamed LLM4DV that efficiently orchestrates LLMs for automated hardware test\nstimuli generation. Our analysis evaluates six different LLMs involving six\nprompting improvements over eight hardware designs and provides insight for\nfuture work on LLMs development for efficient automated DV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware design verification (DV) is a process that checks the functional\nequivalence of a hardware design against its specifications, improving hardware\nreliability and robustness. A key task in the DV process is the test stimuli\ngeneration, which creates a set of conditions or inputs for testing. These test\nconditions are often complex and specific to the given hardware design,\nrequiring substantial human engineering effort to optimize. We seek a solution\nof automated and efficient testing for arbitrary hardware designs that takes\nadvantage of large language models (LLMs). LLMs have already shown promising\nresults for improving hardware design automation, but remain under-explored for\nhardware DV. In this paper, we propose an open-source benchmarking framework\nnamed LLM4DV that efficiently orchestrates LLMs for automated hardware test\nstimuli generation. Our analysis evaluates six different LLMs involving six\nprompting improvements over eight hardware designs and provides insight for\nfuture work on LLMs development for efficient automated DV."
                },
                "authors": [
                    {
                        "name": "Zixi Zhang"
                    },
                    {
                        "name": "Balint Szekely"
                    },
                    {
                        "name": "Pedro Gimenes"
                    },
                    {
                        "name": "Greg Chadwick"
                    },
                    {
                        "name": "Hugo McNally"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Robert Mullins"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.04535v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.04535v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19819v1",
                "updated": "2025-03-25T16:30:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    30,
                    58,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T16:30:58Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    30,
                    58,
                    1,
                    84,
                    0
                ],
                "title": "Domain-incremental White Blood Cell Classification with Privacy-aware\n  Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain-incremental White Blood Cell Classification with Privacy-aware\n  Continual Learning"
                },
                "summary": "White blood cell (WBC) classification plays a vital role in hematology for\ndiagnosing various medical conditions. However, it faces significant challenges\ndue to domain shifts caused by variations in sample sources (e.g., blood or\nbone marrow) and differing imaging conditions across hospitals. Traditional\ndeep learning models often suffer from catastrophic forgetting in such dynamic\nenvironments, while foundation models, though generally robust, experience\nperformance degradation when the distribution of inference data differs from\nthat of the training data. To address these challenges, we propose a generative\nreplay-based Continual Learning (CL) strategy designed to prevent forgetting in\nfoundation models for WBC classification. Our method employs lightweight\ngenerators to mimic past data with a synthetic latent representation to enable\nprivacy-preserving replay. To showcase the effectiveness, we carry out\nextensive experiments with a total of four datasets with different task\nordering and four backbone models including ResNet50, RetCCL, CTransPath, and\nUNI. Experimental results demonstrate that conventional fine-tuning methods\ndegrade performance on previously learned tasks and struggle with domain\nshifts. In contrast, our continual learning strategy effectively mitigates\ncatastrophic forgetting, preserving model performance across varying domains.\nThis work presents a practical solution for maintaining reliable WBC\nclassification in real-world clinical settings, where data distributions\nfrequently evolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "White blood cell (WBC) classification plays a vital role in hematology for\ndiagnosing various medical conditions. However, it faces significant challenges\ndue to domain shifts caused by variations in sample sources (e.g., blood or\nbone marrow) and differing imaging conditions across hospitals. Traditional\ndeep learning models often suffer from catastrophic forgetting in such dynamic\nenvironments, while foundation models, though generally robust, experience\nperformance degradation when the distribution of inference data differs from\nthat of the training data. To address these challenges, we propose a generative\nreplay-based Continual Learning (CL) strategy designed to prevent forgetting in\nfoundation models for WBC classification. Our method employs lightweight\ngenerators to mimic past data with a synthetic latent representation to enable\nprivacy-preserving replay. To showcase the effectiveness, we carry out\nextensive experiments with a total of four datasets with different task\nordering and four backbone models including ResNet50, RetCCL, CTransPath, and\nUNI. Experimental results demonstrate that conventional fine-tuning methods\ndegrade performance on previously learned tasks and struggle with domain\nshifts. In contrast, our continual learning strategy effectively mitigates\ncatastrophic forgetting, preserving model performance across varying domains.\nThis work presents a practical solution for maintaining reliable WBC\nclassification in real-world clinical settings, where data distributions\nfrequently evolve."
                },
                "authors": [
                    {
                        "name": "Pratibha Kumari"
                    },
                    {
                        "name": "Afshin Bozorgpour"
                    },
                    {
                        "name": "Daniel Reisenbüchler"
                    },
                    {
                        "name": "Edgar Jost"
                    },
                    {
                        "name": "Martina Crysandt"
                    },
                    {
                        "name": "Christian Matek"
                    },
                    {
                        "name": "Dorit Merhof"
                    }
                ],
                "author_detail": {
                    "name": "Dorit Merhof"
                },
                "author": "Dorit Merhof",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16815v2",
                "updated": "2025-03-25T16:19:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    19,
                    52,
                    1,
                    84,
                    0
                ],
                "published": "2024-11-25T15:35:01Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    35,
                    1,
                    0,
                    330,
                    0
                ],
                "title": "FREE-Merging: Fourier Transform for Efficient Model Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FREE-Merging: Fourier Transform for Efficient Model Merging"
                },
                "summary": "With the rapid growth of deep learning, there is an increasing availability\nof open-source models for various tasks. However, single fine-tuned models\noften fall short of meeting the diverse needs of users. Model merging has thus\nemerged as an efficient method to integrate the capabilities of existing models\ninto a unified model. Nevertheless, existing model merging methods face\nchallenging trade-offs between performance and deployment costs, primarily due\nto task interference. For the first time, we reveal that task interference is\nevident in the frequency domain of model parameters, yet current efforts only\nfocus on spatial domain solutions, which are largely ineffective in addressing\nfrequency domain interference. To mitigate the impact of frequency domain\ninterference, we propose FR-Merging, an innovative method that effectively\nfilters harmful frequency domain interference on the backbone with minimal\ncomputational overhead. Since performance loss is inevitable with cost-free\nmethods, we propose a lightweight task-specific expert module that dynamically\ncompensates for information loss during merging. This proposed framework,\nFREE-Merging (FR-Merging with experts), strikes a balanced trade-off between\ntraining cost, inference latency, storage requirements, and performance. We\ndemonstrate the effectiveness of both FR-Merging and FREE-Merging on multiple\ntasks across CV, NLP, and Multi-Modal domains and show that they can be\nflexibly adapted to specific needs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of deep learning, there is an increasing availability\nof open-source models for various tasks. However, single fine-tuned models\noften fall short of meeting the diverse needs of users. Model merging has thus\nemerged as an efficient method to integrate the capabilities of existing models\ninto a unified model. Nevertheless, existing model merging methods face\nchallenging trade-offs between performance and deployment costs, primarily due\nto task interference. For the first time, we reveal that task interference is\nevident in the frequency domain of model parameters, yet current efforts only\nfocus on spatial domain solutions, which are largely ineffective in addressing\nfrequency domain interference. To mitigate the impact of frequency domain\ninterference, we propose FR-Merging, an innovative method that effectively\nfilters harmful frequency domain interference on the backbone with minimal\ncomputational overhead. Since performance loss is inevitable with cost-free\nmethods, we propose a lightweight task-specific expert module that dynamically\ncompensates for information loss during merging. This proposed framework,\nFREE-Merging (FR-Merging with experts), strikes a balanced trade-off between\ntraining cost, inference latency, storage requirements, and performance. We\ndemonstrate the effectiveness of both FR-Merging and FREE-Merging on multiple\ntasks across CV, NLP, and Multi-Modal domains and show that they can be\nflexibly adapted to specific needs."
                },
                "authors": [
                    {
                        "name": "Shenghe Zheng"
                    },
                    {
                        "name": "Hongzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Wang"
                },
                "author": "Hongzhi Wang",
                "arxiv_comment": "20 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17435v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17435v2",
                "updated": "2025-03-25T16:17:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    17,
                    47,
                    1,
                    84,
                    0
                ],
                "published": "2025-02-24T18:59:54Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    59,
                    54,
                    0,
                    55,
                    0
                ],
                "title": "GCC: Generative Color Constancy via Diffusing a Color Checker",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GCC: Generative Color Constancy via Diffusing a Color Checker"
                },
                "summary": "Color constancy methods often struggle to generalize across different camera\nsensors due to varying spectral sensitivities. We present GCC, which leverages\ndiffusion models to inpaint color checkers into images for illumination\nestimation. Our key innovations include (1) a single-step deterministic\ninference approach that inpaints color checkers reflecting scene illumination,\n(2) a Laplacian decomposition technique that preserves checker structure while\nallowing illumination-dependent color adaptation, and (3) a mask-based data\naugmentation strategy for handling imprecise color checker annotations. By\nharnessing rich priors from pre-trained diffusion models, GCC demonstrates\nstrong robustness in challenging cross-camera scenarios. These results\nhighlight our method's effective generalization capability across different\ncamera characteristics without requiring sensor-specific training, making it a\nversatile and practical solution for real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Color constancy methods often struggle to generalize across different camera\nsensors due to varying spectral sensitivities. We present GCC, which leverages\ndiffusion models to inpaint color checkers into images for illumination\nestimation. Our key innovations include (1) a single-step deterministic\ninference approach that inpaints color checkers reflecting scene illumination,\n(2) a Laplacian decomposition technique that preserves checker structure while\nallowing illumination-dependent color adaptation, and (3) a mask-based data\naugmentation strategy for handling imprecise color checker annotations. By\nharnessing rich priors from pre-trained diffusion models, GCC demonstrates\nstrong robustness in challenging cross-camera scenarios. These results\nhighlight our method's effective generalization capability across different\ncamera characteristics without requiring sensor-specific training, making it a\nversatile and practical solution for real-world applications."
                },
                "authors": [
                    {
                        "name": "Chen-Wei Chang"
                    },
                    {
                        "name": "Cheng-De Fan"
                    },
                    {
                        "name": "Chia-Che Chang"
                    },
                    {
                        "name": "Yi-Chen Lo"
                    },
                    {
                        "name": "Yu-Chee Tseng"
                    },
                    {
                        "name": "Jiun-Long Huang"
                    },
                    {
                        "name": "Yu-Lun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Lun Liu"
                },
                "author": "Yu-Lun Liu",
                "arxiv_comment": "Paper accepted to CVPR 2025. Project page:\n  https://chenwei891213.github.io/GCC/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17435v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17435v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19794v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19794v1",
                "updated": "2025-03-25T16:02:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    2,
                    37,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T16:02:37Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    2,
                    37,
                    1,
                    84,
                    0
                ],
                "title": "PAVE: Patching and Adapting Video Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAVE: Patching and Adapting Video Large Language Models"
                },
                "summary": "Pre-trained video large language models (Video LLMs) exhibit remarkable\nreasoning capabilities, yet adapting these models to new tasks involving\nadditional modalities or data types (e.g., audio or 3D information) remains\nchallenging. In this paper, we present PAVE, a flexible framework for adapting\npre-trained Video LLMs to downstream tasks with side-channel signals, such as\naudio, 3D cues, or multi-view videos. PAVE introduces lightweight adapters,\nreferred to as \"patches,\" which add a small number of parameters and operations\nto a base model without modifying its architecture or pre-trained weights. In\ndoing so, PAVE can effectively adapt the pre-trained base model to support\ndiverse downstream tasks, including audio-visual question answering, 3D\nreasoning, multi-view video recognition, and high frame rate video\nunderstanding. Across these tasks, PAVE significantly enhances the performance\nof the base model, surpassing state-of-the-art task-specific models while\nincurring a minor cost of ~0.1% additional FLOPs and parameters. Further, PAVE\nsupports multi-task learning and generalizes well across different Video LLMs.\nOur code is available at https://github.com/dragonlzm/PAVE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained video large language models (Video LLMs) exhibit remarkable\nreasoning capabilities, yet adapting these models to new tasks involving\nadditional modalities or data types (e.g., audio or 3D information) remains\nchallenging. In this paper, we present PAVE, a flexible framework for adapting\npre-trained Video LLMs to downstream tasks with side-channel signals, such as\naudio, 3D cues, or multi-view videos. PAVE introduces lightweight adapters,\nreferred to as \"patches,\" which add a small number of parameters and operations\nto a base model without modifying its architecture or pre-trained weights. In\ndoing so, PAVE can effectively adapt the pre-trained base model to support\ndiverse downstream tasks, including audio-visual question answering, 3D\nreasoning, multi-view video recognition, and high frame rate video\nunderstanding. Across these tasks, PAVE significantly enhances the performance\nof the base model, surpassing state-of-the-art task-specific models while\nincurring a minor cost of ~0.1% additional FLOPs and parameters. Further, PAVE\nsupports multi-task learning and generalizes well across different Video LLMs.\nOur code is available at https://github.com/dragonlzm/PAVE."
                },
                "authors": [
                    {
                        "name": "Zhuoming Liu"
                    },
                    {
                        "name": "Yiquan Li"
                    },
                    {
                        "name": "Khoi Duc Nguyen"
                    },
                    {
                        "name": "Yiwu Zhong"
                    },
                    {
                        "name": "Yin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yin Li"
                },
                "author": "Yin Li",
                "arxiv_comment": "CVPR2025 Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19794v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19794v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18155v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18155v2",
                "updated": "2025-03-25T15:58:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    58,
                    36,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-23T17:48:44Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    17,
                    48,
                    44,
                    6,
                    82,
                    0
                ],
                "title": "Decorum: A Language-Based Approach For Style-Conditioned Synthesis of\n  Indoor 3D Scenes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decorum: A Language-Based Approach For Style-Conditioned Synthesis of\n  Indoor 3D Scenes"
                },
                "summary": "3D indoor scene generation is an important problem for the design of digital\nand real-world environments. To automate this process, a scene generation model\nshould be able to not only generate plausible scene layouts, but also take into\nconsideration visual features and style preferences. Existing methods for this\ntask exhibit very limited control over these attributes, only allowing text\ninputs in the form of simple object-level descriptions or pairwise spatial\nrelationships. Our proposed method Decorum enables users to control the scene\ngeneration process with natural language by adopting language-based\nrepresentations at each stage. This enables us to harness recent advancements\nin Large Language Models (LLMs) to model language-to-language mappings. In\naddition, we show that using a text-based representation allows us to select\nfurniture for our scenes using a novel object retrieval method based on\nmultimodal LLMs. Evaluations on the benchmark 3D-FRONT dataset show that our\nmethods achieve improvements over existing work in text-conditioned scene\nsynthesis and object retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D indoor scene generation is an important problem for the design of digital\nand real-world environments. To automate this process, a scene generation model\nshould be able to not only generate plausible scene layouts, but also take into\nconsideration visual features and style preferences. Existing methods for this\ntask exhibit very limited control over these attributes, only allowing text\ninputs in the form of simple object-level descriptions or pairwise spatial\nrelationships. Our proposed method Decorum enables users to control the scene\ngeneration process with natural language by adopting language-based\nrepresentations at each stage. This enables us to harness recent advancements\nin Large Language Models (LLMs) to model language-to-language mappings. In\naddition, we show that using a text-based representation allows us to select\nfurniture for our scenes using a novel object retrieval method based on\nmultimodal LLMs. Evaluations on the benchmark 3D-FRONT dataset show that our\nmethods achieve improvements over existing work in text-conditioned scene\nsynthesis and object retrieval."
                },
                "authors": [
                    {
                        "name": "Kelly O. Marshall"
                    },
                    {
                        "name": "Omid Poursaeed"
                    },
                    {
                        "name": "Sergiu Oprea"
                    },
                    {
                        "name": "Amit Kumar"
                    },
                    {
                        "name": "Anushrut Jignasu"
                    },
                    {
                        "name": "Chinmay Hegde"
                    },
                    {
                        "name": "Yilei Li"
                    },
                    {
                        "name": "Rakesh Ranjan"
                    }
                ],
                "author_detail": {
                    "name": "Rakesh Ranjan"
                },
                "author": "Rakesh Ranjan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18155v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18155v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02341v2",
                "updated": "2025-03-25T15:55:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    55,
                    33,
                    1,
                    84,
                    0
                ],
                "published": "2025-01-04T17:32:12Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    17,
                    32,
                    12,
                    5,
                    4,
                    0
                ],
                "title": "UAVs Meet LLMs: Overviews and Perspectives Toward Agentic Low-Altitude\n  Mobility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UAVs Meet LLMs: Overviews and Perspectives Toward Agentic Low-Altitude\n  Mobility"
                },
                "summary": "Low-altitude mobility, exemplified by unmanned aerial vehicles (UAVs), has\nintroduced transformative advancements across various domains, like\ntransportation, logistics, and agriculture. Leveraging flexible perspectives\nand rapid maneuverability, UAVs extend traditional systems' perception and\naction capabilities, garnering widespread attention from academia and industry.\nHowever, current UAV operations primarily depend on human control, with only\nlimited autonomy in simple scenarios, and lack the intelligence and\nadaptability needed for more complex environments and tasks. The emergence of\nlarge language models (LLMs) demonstrates remarkable problem-solving and\ngeneralization capabilities, offering a promising pathway for advancing UAV\nintelligence. This paper explores the integration of LLMs and UAVs, beginning\nwith an overview of UAV systems' fundamental components and functionalities,\nfollowed by an overview of the state-of-the-art in LLM technology.\nSubsequently, it systematically highlights the multimodal data resources\navailable for UAVs, which provide critical support for training and evaluation.\nFurthermore, it categorizes and analyzes key tasks and application scenarios\nwhere UAVs and LLMs converge. Finally, a reference roadmap towards agentic UAVs\nis proposed, aiming to enable UAVs to achieve agentic intelligence through\nautonomous perception, memory, reasoning, and tool utilization. Related\nresources are available at https://github.com/Hub-Tian/UAVs_Meet_LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-altitude mobility, exemplified by unmanned aerial vehicles (UAVs), has\nintroduced transformative advancements across various domains, like\ntransportation, logistics, and agriculture. Leveraging flexible perspectives\nand rapid maneuverability, UAVs extend traditional systems' perception and\naction capabilities, garnering widespread attention from academia and industry.\nHowever, current UAV operations primarily depend on human control, with only\nlimited autonomy in simple scenarios, and lack the intelligence and\nadaptability needed for more complex environments and tasks. The emergence of\nlarge language models (LLMs) demonstrates remarkable problem-solving and\ngeneralization capabilities, offering a promising pathway for advancing UAV\nintelligence. This paper explores the integration of LLMs and UAVs, beginning\nwith an overview of UAV systems' fundamental components and functionalities,\nfollowed by an overview of the state-of-the-art in LLM technology.\nSubsequently, it systematically highlights the multimodal data resources\navailable for UAVs, which provide critical support for training and evaluation.\nFurthermore, it categorizes and analyzes key tasks and application scenarios\nwhere UAVs and LLMs converge. Finally, a reference roadmap towards agentic UAVs\nis proposed, aiming to enable UAVs to achieve agentic intelligence through\nautonomous perception, memory, reasoning, and tool utilization. Related\nresources are available at https://github.com/Hub-Tian/UAVs_Meet_LLMs."
                },
                "authors": [
                    {
                        "name": "Yonglin Tian"
                    },
                    {
                        "name": "Fei Lin"
                    },
                    {
                        "name": "Yiduo Li"
                    },
                    {
                        "name": "Tengchao Zhang"
                    },
                    {
                        "name": "Qiyao Zhang"
                    },
                    {
                        "name": "Xuan Fu"
                    },
                    {
                        "name": "Jun Huang"
                    },
                    {
                        "name": "Xingyuan Dai"
                    },
                    {
                        "name": "Yutong Wang"
                    },
                    {
                        "name": "Chunwei Tian"
                    },
                    {
                        "name": "Bai Li"
                    },
                    {
                        "name": "Yisheng Lv"
                    },
                    {
                        "name": "Levente Kovács"
                    },
                    {
                        "name": "Fei-Yue Wang"
                    }
                ],
                "author_detail": {
                    "name": "Fei-Yue Wang"
                },
                "author": "Fei-Yue Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19777v1",
                "updated": "2025-03-25T15:47:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    47,
                    13,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T15:47:13Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    47,
                    13,
                    1,
                    84,
                    0
                ],
                "title": "LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary\n  Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary\n  Semantic Segmentation"
                },
                "summary": "We propose a training-free method for open-vocabulary semantic segmentation\nusing Vision-and-Language Models (VLMs). Our approach enhances the initial\nper-patch predictions of VLMs through label propagation, which jointly\noptimizes predictions by incorporating patch-to-patch relationships. Since VLMs\nare primarily optimized for cross-modal alignment and not for intra-modal\nsimilarity, we use a Vision Model (VM) that is observed to better capture these\nrelationships. We address resolution limitations inherent to patch-based\nencoders by applying label propagation at the pixel level as a refinement step,\nsignificantly improving segmentation accuracy near class boundaries. Our\nmethod, called LPOSS+, performs inference over the entire image, avoiding\nwindow-based processing and thereby capturing contextual interactions across\nthe full image. LPOSS+ achieves state-of-the-art performance among\ntraining-free methods, across a diverse set of datasets. Code:\nhttps://github.com/vladan-stojnic/LPOSS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a training-free method for open-vocabulary semantic segmentation\nusing Vision-and-Language Models (VLMs). Our approach enhances the initial\nper-patch predictions of VLMs through label propagation, which jointly\noptimizes predictions by incorporating patch-to-patch relationships. Since VLMs\nare primarily optimized for cross-modal alignment and not for intra-modal\nsimilarity, we use a Vision Model (VM) that is observed to better capture these\nrelationships. We address resolution limitations inherent to patch-based\nencoders by applying label propagation at the pixel level as a refinement step,\nsignificantly improving segmentation accuracy near class boundaries. Our\nmethod, called LPOSS+, performs inference over the entire image, avoiding\nwindow-based processing and thereby capturing contextual interactions across\nthe full image. LPOSS+ achieves state-of-the-art performance among\ntraining-free methods, across a diverse set of datasets. Code:\nhttps://github.com/vladan-stojnic/LPOSS"
                },
                "authors": [
                    {
                        "name": "Vladan Stojnić"
                    },
                    {
                        "name": "Yannis Kalantidis"
                    },
                    {
                        "name": "Jiří Matas"
                    },
                    {
                        "name": "Giorgos Tolias"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Tolias"
                },
                "author": "Giorgos Tolias",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11102v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11102v3",
                "updated": "2025-03-25T15:35:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    35,
                    29,
                    1,
                    84,
                    0
                ],
                "published": "2024-12-15T07:49:31Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    7,
                    49,
                    31,
                    6,
                    350,
                    0
                ],
                "title": "Empowering LLMs to Understand and Generate Complex Vector Graphics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering LLMs to Understand and Generate Complex Vector Graphics"
                },
                "summary": "The unprecedented advancements in Large Language Models (LLMs) have\nprofoundly impacted natural language processing but have yet to fully embrace\nthe realm of scalable vector graphics (SVG) generation. While LLMs encode\npartial knowledge of SVG data from web pages during training, recent findings\nsuggest that semantically ambiguous and tokenized representations within LLMs\nmay result in hallucinations in vector primitive predictions. Additionally, LLM\ntraining typically lacks modeling and understanding of the rendering sequence\nof vector paths, which can lead to occlusion between output vector primitives.\nIn this paper, we present LLM4SVG, an initial yet substantial step toward\nbridging this gap by enabling LLMs to better understand and generate vector\ngraphics. LLM4SVG facilitates a deeper understanding of SVG components through\nlearnable semantic tokens, which precisely encode these tokens and their\ncorresponding properties to generate semantically aligned SVG outputs. Using a\nseries of learnable semantic tokens, a structured dataset for instruction\nfollowing is developed to support comprehension and generation across two\nprimary tasks. Our method introduces a modular architecture to existing large\nlanguage models, integrating semantic tags, vector instruction encoders,\nfine-tuned commands, and powerful LLMs to tightly combine geometric,\nappearance, and language information. To overcome the scarcity of SVG-text\ninstruction data, we developed an automated data generation pipeline that\ncollected our SVGX-SFT Dataset, consisting of high-quality human-designed SVGs\nand 580k SVG instruction following data specifically crafted for LLM training,\nwhich facilitated the adoption of the supervised fine-tuning strategy popular\nin LLM development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The unprecedented advancements in Large Language Models (LLMs) have\nprofoundly impacted natural language processing but have yet to fully embrace\nthe realm of scalable vector graphics (SVG) generation. While LLMs encode\npartial knowledge of SVG data from web pages during training, recent findings\nsuggest that semantically ambiguous and tokenized representations within LLMs\nmay result in hallucinations in vector primitive predictions. Additionally, LLM\ntraining typically lacks modeling and understanding of the rendering sequence\nof vector paths, which can lead to occlusion between output vector primitives.\nIn this paper, we present LLM4SVG, an initial yet substantial step toward\nbridging this gap by enabling LLMs to better understand and generate vector\ngraphics. LLM4SVG facilitates a deeper understanding of SVG components through\nlearnable semantic tokens, which precisely encode these tokens and their\ncorresponding properties to generate semantically aligned SVG outputs. Using a\nseries of learnable semantic tokens, a structured dataset for instruction\nfollowing is developed to support comprehension and generation across two\nprimary tasks. Our method introduces a modular architecture to existing large\nlanguage models, integrating semantic tags, vector instruction encoders,\nfine-tuned commands, and powerful LLMs to tightly combine geometric,\nappearance, and language information. To overcome the scarcity of SVG-text\ninstruction data, we developed an automated data generation pipeline that\ncollected our SVGX-SFT Dataset, consisting of high-quality human-designed SVGs\nand 580k SVG instruction following data specifically crafted for LLM training,\nwhich facilitated the adoption of the supervised fine-tuning strategy popular\nin LLM development."
                },
                "authors": [
                    {
                        "name": "Ximing Xing"
                    },
                    {
                        "name": "Juncheng Hu"
                    },
                    {
                        "name": "Guotao Liang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Dong Xu"
                    },
                    {
                        "name": "Qian Yu"
                    }
                ],
                "author_detail": {
                    "name": "Qian Yu"
                },
                "author": "Qian Yu",
                "arxiv_comment": "Accepted by CVPR 2025. Project Page:\n  https://ximinng.github.io/LLM4SVGProject/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11102v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11102v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19755v1",
                "updated": "2025-03-25T15:18:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    18,
                    43,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T15:18:43Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    18,
                    43,
                    1,
                    84,
                    0
                ],
                "title": "ORION: A Holistic End-to-End Autonomous Driving Framework by\n  Vision-Language Instructed Action Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORION: A Holistic End-to-End Autonomous Driving Framework by\n  Vision-Language Instructed Action Generation"
                },
                "summary": "End-to-end (E2E) autonomous driving methods still struggle to make correct\ndecisions in interactive closed-loop evaluation due to limited causal reasoning\ncapability. Current methods attempt to leverage the powerful understanding and\nreasoning abilities of Vision-Language Models (VLMs) to resolve this dilemma.\nHowever, the problem is still open that few VLMs for E2E methods perform well\nin the closed-loop evaluation due to the gap between the semantic reasoning\nspace and the purely numerical trajectory output in the action space. To tackle\nthis issue, we propose ORION, a holistic E2E autonomous driving framework by\nvision-language instructed action generation. ORION uniquely combines a\nQT-Former to aggregate long-term history context, a Large Language Model (LLM)\nfor driving scenario reasoning, and a generative planner for precision\ntrajectory prediction. ORION further aligns the reasoning space and the action\nspace to implement a unified E2E optimization for both visual\nquestion-answering (VQA) and planning tasks. Our method achieves an impressive\nclosed-loop performance of 77.74 Driving Score (DS) and 54.62% Success Rate\n(SR) on the challenge Bench2Drive datasets, which outperforms state-of-the-art\n(SOTA) methods by a large margin of 14.28 DS and 19.61% SR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end (E2E) autonomous driving methods still struggle to make correct\ndecisions in interactive closed-loop evaluation due to limited causal reasoning\ncapability. Current methods attempt to leverage the powerful understanding and\nreasoning abilities of Vision-Language Models (VLMs) to resolve this dilemma.\nHowever, the problem is still open that few VLMs for E2E methods perform well\nin the closed-loop evaluation due to the gap between the semantic reasoning\nspace and the purely numerical trajectory output in the action space. To tackle\nthis issue, we propose ORION, a holistic E2E autonomous driving framework by\nvision-language instructed action generation. ORION uniquely combines a\nQT-Former to aggregate long-term history context, a Large Language Model (LLM)\nfor driving scenario reasoning, and a generative planner for precision\ntrajectory prediction. ORION further aligns the reasoning space and the action\nspace to implement a unified E2E optimization for both visual\nquestion-answering (VQA) and planning tasks. Our method achieves an impressive\nclosed-loop performance of 77.74 Driving Score (DS) and 54.62% Success Rate\n(SR) on the challenge Bench2Drive datasets, which outperforms state-of-the-art\n(SOTA) methods by a large margin of 14.28 DS and 19.61% SR."
                },
                "authors": [
                    {
                        "name": "Haoyu Fu"
                    },
                    {
                        "name": "Diankun Zhang"
                    },
                    {
                        "name": "Zongchuang Zhao"
                    },
                    {
                        "name": "Jianfeng Cui"
                    },
                    {
                        "name": "Dingkang Liang"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Dingyuan Zhang"
                    },
                    {
                        "name": "Hongwei Xie"
                    },
                    {
                        "name": "Bing Wang"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19752v1",
                "updated": "2025-03-25T15:16:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    16,
                    35,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T15:16:35Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    16,
                    35,
                    1,
                    84,
                    0
                ],
                "title": "Inducing Personality in LLM-Based Honeypot Agents: Measuring the Effect\n  on Human-Like Agenda Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inducing Personality in LLM-Based Honeypot Agents: Measuring the Effect\n  on Human-Like Agenda Generation"
                },
                "summary": "This paper presents SANDMAN, an architecture for cyber deception that\nleverages Language Agents to emulate convincing human simulacra. Our 'Deceptive\nAgents' serve as advanced cyber decoys, designed for high-fidelity engagement\nwith attackers by extending the observation period of attack behaviours.\nThrough experimentation, measurement, and analysis, we demonstrate how a prompt\nschema based on the five-factor model of personality systematically induces\ndistinct 'personalities' in Large Language Models. Our results highlight the\nfeasibility of persona-driven Language Agents for generating diverse, realistic\nbehaviours, ultimately improving cyber deception strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents SANDMAN, an architecture for cyber deception that\nleverages Language Agents to emulate convincing human simulacra. Our 'Deceptive\nAgents' serve as advanced cyber decoys, designed for high-fidelity engagement\nwith attackers by extending the observation period of attack behaviours.\nThrough experimentation, measurement, and analysis, we demonstrate how a prompt\nschema based on the five-factor model of personality systematically induces\ndistinct 'personalities' in Large Language Models. Our results highlight the\nfeasibility of persona-driven Language Agents for generating diverse, realistic\nbehaviours, ultimately improving cyber deception strategies."
                },
                "authors": [
                    {
                        "name": "Lewis Newsham"
                    },
                    {
                        "name": "Ryan Hyland"
                    },
                    {
                        "name": "Daniel Prince"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Prince"
                },
                "author": "Daniel Prince",
                "arxiv_comment": "11 pages, 1 figure, 6 tables. Accepted to NLPAICS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19748v1",
                "updated": "2025-03-25T15:11:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    11,
                    14,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T15:11:14Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    11,
                    14,
                    1,
                    84,
                    0
                ],
                "title": "No-prior Bayesian inference reIMagined: probabilistic approximations of\n  inferential models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No-prior Bayesian inference reIMagined: probabilistic approximations of\n  inferential models"
                },
                "summary": "When prior information is lacking, the go-to strategy for probabilistic\ninference is to combine a \"default prior\" and the likelihood via Bayes's\ntheorem. Objective Bayes, (generalized) fiducial inference, etc. fall under\nthis umbrella. This construction is natural, but the corresponding posterior\ndistributions generally only offer limited, approximately valid uncertainty\nquantification. The present paper takes a reimagined approach offering\nposterior distributions with stronger reliability properties. The proposed\nconstruction starts with an inferential model (IM), one that takes the\nmathematical form of a data-driven possibility measure and features exactly\nvalid uncertainty quantification, and then returns a so-called inner\nprobabilistic approximation thereof. This inner probabilistic approximation\ninherits many of the original IM's desirable properties, including credible\nsets with exact coverage and asymptotic efficiency. The approximation also\nagrees with the familiar Bayes/fiducial solution obtained in applications where\nthe model has a group transformation structure. A Monte Carlo method for\nevaluating the probabilistic approximation is presented, along with numerical\nillustrations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When prior information is lacking, the go-to strategy for probabilistic\ninference is to combine a \"default prior\" and the likelihood via Bayes's\ntheorem. Objective Bayes, (generalized) fiducial inference, etc. fall under\nthis umbrella. This construction is natural, but the corresponding posterior\ndistributions generally only offer limited, approximately valid uncertainty\nquantification. The present paper takes a reimagined approach offering\nposterior distributions with stronger reliability properties. The proposed\nconstruction starts with an inferential model (IM), one that takes the\nmathematical form of a data-driven possibility measure and features exactly\nvalid uncertainty quantification, and then returns a so-called inner\nprobabilistic approximation thereof. This inner probabilistic approximation\ninherits many of the original IM's desirable properties, including credible\nsets with exact coverage and asymptotic efficiency. The approximation also\nagrees with the familiar Bayes/fiducial solution obtained in applications where\nthe model has a group transformation structure. A Monte Carlo method for\nevaluating the probabilistic approximation is presented, along with numerical\nillustrations."
                },
                "authors": [
                    {
                        "name": "Ryan Martin"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Martin"
                },
                "author": "Ryan Martin",
                "arxiv_comment": "16 pages + appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09360v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09360v2",
                "updated": "2025-03-25T15:10:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    10,
                    24,
                    1,
                    84,
                    0
                ],
                "published": "2024-12-12T15:27:47Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    27,
                    47,
                    3,
                    347,
                    0
                ],
                "title": "Doc2OracLL: Investigating the Impact of Documentation on LLM-based Test\n  Oracle Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doc2OracLL: Investigating the Impact of Documentation on LLM-based Test\n  Oracle Generation"
                },
                "summary": "Code documentation is a critical aspect of software development, serving as a\nbridge between human understanding and machine-readable code. Beyond assisting\ndevelopers in understanding and maintaining code, documentation also plays a\ncritical role in automating various software engineering tasks, such as test\noracle generation (TOG). In Java, Javadoc comments provide structured, natural\nlanguage documentation embedded directly in the source code, typically\ndetailing functionality, usage, parameters, return values, and exceptions.\nWhile prior research has utilized Javadoc comments in test oracle generation\n(TOG), there has not been a thorough investigation into their impact when\ncombined with other contextual information, nor into identifying the most\nrelevant components for generating correct and strong test oracles, or\nunderstanding their role in detecting real bugs. In this study, we dive deep\ninto investigating the impact of Javadoc comments on TOG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code documentation is a critical aspect of software development, serving as a\nbridge between human understanding and machine-readable code. Beyond assisting\ndevelopers in understanding and maintaining code, documentation also plays a\ncritical role in automating various software engineering tasks, such as test\noracle generation (TOG). In Java, Javadoc comments provide structured, natural\nlanguage documentation embedded directly in the source code, typically\ndetailing functionality, usage, parameters, return values, and exceptions.\nWhile prior research has utilized Javadoc comments in test oracle generation\n(TOG), there has not been a thorough investigation into their impact when\ncombined with other contextual information, nor into identifying the most\nrelevant components for generating correct and strong test oracles, or\nunderstanding their role in detecting real bugs. In this study, we dive deep\ninto investigating the impact of Javadoc comments on TOG."
                },
                "authors": [
                    {
                        "name": "Soneya Binta Hossain"
                    },
                    {
                        "name": "Raygan Taylor"
                    },
                    {
                        "name": "Matthew Dwyer"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Dwyer"
                },
                "author": "Matthew Dwyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09360v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09360v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19742v1",
                "updated": "2025-03-25T15:05:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    5,
                    25,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T15:05:25Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    5,
                    25,
                    1,
                    84,
                    0
                ],
                "title": "Optimizing Photonic Structures with Large Language Model Driven\n  Algorithm Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Photonic Structures with Large Language Model Driven\n  Algorithm Discovery"
                },
                "summary": "We study how large language models can be used in combination with\nevolutionary computation techniques to automatically discover optimization\nalgorithms for the design of photonic structures. Building on the Large\nLanguage Model Evolutionary Algorithm (LLaMEA) framework, we introduce\nstructured prompt engineering tailored to multilayer photonic problems such as\nBragg mirror, ellipsometry inverse analysis, and solar cell antireflection\ncoatings. We systematically explore multiple evolutionary strategies, including\n(1+1), (1+5), (2+10), and others, to balance exploration and exploitation. Our\nexperiments show that LLM-generated algorithms, generated using small-scale\nproblem instances, can match or surpass established methods like\nquasi-oppositional differential evolution on large-scale realistic real-world\nproblem instances. Notably, LLaMEA's self-debugging mutation loop, augmented by\nautomatically extracted problem-specific insights, achieves strong anytime\nperformance and reliable convergence across diverse problem scales. This work\ndemonstrates the feasibility of domain-focused LLM prompts and evolutionary\napproaches in solving optical design tasks, paving the way for rapid, automated\nphotonic inverse design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study how large language models can be used in combination with\nevolutionary computation techniques to automatically discover optimization\nalgorithms for the design of photonic structures. Building on the Large\nLanguage Model Evolutionary Algorithm (LLaMEA) framework, we introduce\nstructured prompt engineering tailored to multilayer photonic problems such as\nBragg mirror, ellipsometry inverse analysis, and solar cell antireflection\ncoatings. We systematically explore multiple evolutionary strategies, including\n(1+1), (1+5), (2+10), and others, to balance exploration and exploitation. Our\nexperiments show that LLM-generated algorithms, generated using small-scale\nproblem instances, can match or surpass established methods like\nquasi-oppositional differential evolution on large-scale realistic real-world\nproblem instances. Notably, LLaMEA's self-debugging mutation loop, augmented by\nautomatically extracted problem-specific insights, achieves strong anytime\nperformance and reliable convergence across diverse problem scales. This work\ndemonstrates the feasibility of domain-focused LLM prompts and evolutionary\napproaches in solving optical design tasks, paving the way for rapid, automated\nphotonic inverse design."
                },
                "authors": [
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Anna V. Kononova"
                    },
                    {
                        "name": "Thomas Bäck"
                    },
                    {
                        "name": "Niki van Stein"
                    }
                ],
                "author_detail": {
                    "name": "Niki van Stein"
                },
                "author": "Niki van Stein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12893v3",
                "updated": "2025-03-25T15:02:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    2,
                    17,
                    1,
                    84,
                    0
                ],
                "published": "2024-10-16T12:24:42Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    24,
                    42,
                    2,
                    290,
                    0
                ],
                "title": "MIRROR: A Novel Approach for the Automated Evaluation of Open-Ended\n  Question Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIRROR: A Novel Approach for the Automated Evaluation of Open-Ended\n  Question Generation"
                },
                "summary": "Automatic question generation is a critical task that involves evaluating\nquestion quality by considering factors such as engagement, pedagogical value,\nand the ability to stimulate critical thinking. These aspects require\nhuman-like understanding and judgment, which automated systems currently lack.\nHowever, human evaluations are costly and impractical for large-scale samples\nof generated questions. Therefore, we propose a novel system, MIRROR (Multi-LLM\nIterative Review and Response for Optimized Rating), which leverages large\nlanguage models (LLMs) to automate the evaluation process for questions\ngenerated by automated question generation systems. We experimented with\nseveral state-of-the-art LLMs, such as GPT-4, Gemini, and Llama2-70b. We\nobserved that the scores of human evaluation metrics, namely relevance,\nappropriateness, novelty, complexity, and grammaticality, improved when using\nthe feedback-based approach called MIRROR, tending to be closer to the human\nbaseline scores. Furthermore, we observed that Pearson's correlation\ncoefficient between GPT-4 and human experts improved when using our proposed\nfeedback-based approach, MIRROR, compared to direct prompting for evaluation.\nError analysis shows that our proposed approach, MIRROR, significantly helps to\nimprove relevance and appropriateness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic question generation is a critical task that involves evaluating\nquestion quality by considering factors such as engagement, pedagogical value,\nand the ability to stimulate critical thinking. These aspects require\nhuman-like understanding and judgment, which automated systems currently lack.\nHowever, human evaluations are costly and impractical for large-scale samples\nof generated questions. Therefore, we propose a novel system, MIRROR (Multi-LLM\nIterative Review and Response for Optimized Rating), which leverages large\nlanguage models (LLMs) to automate the evaluation process for questions\ngenerated by automated question generation systems. We experimented with\nseveral state-of-the-art LLMs, such as GPT-4, Gemini, and Llama2-70b. We\nobserved that the scores of human evaluation metrics, namely relevance,\nappropriateness, novelty, complexity, and grammaticality, improved when using\nthe feedback-based approach called MIRROR, tending to be closer to the human\nbaseline scores. Furthermore, we observed that Pearson's correlation\ncoefficient between GPT-4 and human experts improved when using our proposed\nfeedback-based approach, MIRROR, compared to direct prompting for evaluation.\nError analysis shows that our proposed approach, MIRROR, significantly helps to\nimprove relevance and appropriateness."
                },
                "authors": [
                    {
                        "name": "Aniket Deroy"
                    },
                    {
                        "name": "Subhankar Maity"
                    },
                    {
                        "name": "Sudeshna Sarkar"
                    }
                ],
                "author_detail": {
                    "name": "Sudeshna Sarkar"
                },
                "author": "Sudeshna Sarkar",
                "arxiv_comment": "Updated Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05215v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05215v3",
                "updated": "2025-03-25T14:48:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    14,
                    48,
                    1,
                    1,
                    84,
                    0
                ],
                "published": "2023-12-08T18:07:05Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    18,
                    7,
                    5,
                    4,
                    342,
                    0
                ],
                "title": "DeltaZip: Efficient Serving of Multiple Full-Model-Tuned LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeltaZip: Efficient Serving of Multiple Full-Model-Tuned LLMs"
                },
                "summary": "Fine-tuning large language models (LLMs) greatly improves model quality for\ndownstream tasks. However, serving many fine-tuned LLMs concurrently is\nchallenging due to the sporadic, bursty, and varying request patterns of\ndifferent LLMs. To bridge this gap, we present DeltaZip, an LLM serving system\nthat efficiently serves multiple full-parameter fine-tuned models concurrently\nby aggressively compressing model deltas by up to 10x while maintaining high\nmodel quality. The key insight behind this design is that fine-tuning results\nin small-magnitude changes to the pre-trained model. By co-designing the\nserving system with the compression algorithm, DeltaZip achieves 2x to 12x\nimprovement in throughput compared to the state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) greatly improves model quality for\ndownstream tasks. However, serving many fine-tuned LLMs concurrently is\nchallenging due to the sporadic, bursty, and varying request patterns of\ndifferent LLMs. To bridge this gap, we present DeltaZip, an LLM serving system\nthat efficiently serves multiple full-parameter fine-tuned models concurrently\nby aggressively compressing model deltas by up to 10x while maintaining high\nmodel quality. The key insight behind this design is that fine-tuning results\nin small-magnitude changes to the pre-trained model. By co-designing the\nserving system with the compression algorithm, DeltaZip achieves 2x to 12x\nimprovement in throughput compared to the state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Xiaozhe Yao"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Ana Klimovic"
                    }
                ],
                "author_detail": {
                    "name": "Ana Klimovic"
                },
                "author": "Ana Klimovic",
                "arxiv_comment": "EuroSys 2025'",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05215v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05215v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17662v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17662v2",
                "updated": "2025-03-25T14:43:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    14,
                    43,
                    35,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-22T06:12:34Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    6,
                    12,
                    34,
                    5,
                    81,
                    0
                ],
                "title": "Enhancing Persona Consistency for LLMs' Role-Playing using Persona-Aware\n  Contrastive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Persona Consistency for LLMs' Role-Playing using Persona-Aware\n  Contrastive Learning"
                },
                "summary": "In recent years, large language models (LLMs) have achieved breakthrough\nprogress in many dialogue generation tasks. However, their lack of emotion and\nfine-grained role awareness limits the model's ability to provide personalized\nand diverse interactions further. Current methods face high costs in collecting\nhigh-quality annotated data for scenarios such as role-playing, and traditional\nhuman alignment methods are difficult to deploy due to the inherent diversity\nof model behavior in role-playing scenarios. Inspired by the alignment of\nmodels for safety behaviors through RLHF (Reinforcement Learning from Human\nFeedback), in this paper, we revisit model role-playing behavior from the\nperspective of persona alignment and propose a novel annotation-free framework\nnamed \\textbf{\\underline{P}}ersona-Aware \\textbf{\\underline{C}}ontrastive\n\\textbf{\\underline{L}}earning (PCL) to align LLMs' behavior during\nrole-playing, enhancing the model's role consistency. Specifically, we first\ndesign a role chain method to encourage the model to self-question based on the\nrole characteristics and dialogue context to adjust personality consistency.\nThen, we further enhance the model's role-playing strategy through iterative\ncontrastive learning between the use of role characteristics and not.\nExperiments on both black-box and white-box LLMs show that LLMs equipped with\nPCL significantly outperform vanilla LLMs under automatic evaluation methods\n(CharEval \\& GPT-4) and human expert evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have achieved breakthrough\nprogress in many dialogue generation tasks. However, their lack of emotion and\nfine-grained role awareness limits the model's ability to provide personalized\nand diverse interactions further. Current methods face high costs in collecting\nhigh-quality annotated data for scenarios such as role-playing, and traditional\nhuman alignment methods are difficult to deploy due to the inherent diversity\nof model behavior in role-playing scenarios. Inspired by the alignment of\nmodels for safety behaviors through RLHF (Reinforcement Learning from Human\nFeedback), in this paper, we revisit model role-playing behavior from the\nperspective of persona alignment and propose a novel annotation-free framework\nnamed \\textbf{\\underline{P}}ersona-Aware \\textbf{\\underline{C}}ontrastive\n\\textbf{\\underline{L}}earning (PCL) to align LLMs' behavior during\nrole-playing, enhancing the model's role consistency. Specifically, we first\ndesign a role chain method to encourage the model to self-question based on the\nrole characteristics and dialogue context to adjust personality consistency.\nThen, we further enhance the model's role-playing strategy through iterative\ncontrastive learning between the use of role characteristics and not.\nExperiments on both black-box and white-box LLMs show that LLMs equipped with\nPCL significantly outperform vanilla LLMs under automatic evaluation methods\n(CharEval \\& GPT-4) and human expert evaluation."
                },
                "authors": [
                    {
                        "name": "Ke Ji"
                    },
                    {
                        "name": "Yixin Lian"
                    },
                    {
                        "name": "Linxu Li"
                    },
                    {
                        "name": "Jingsheng Gao"
                    },
                    {
                        "name": "Weiyuan Li"
                    },
                    {
                        "name": "Bin Dai"
                    }
                ],
                "author_detail": {
                    "name": "Bin Dai"
                },
                "author": "Bin Dai",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17662v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17662v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06914v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06914v2",
                "updated": "2025-03-25T14:42:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    14,
                    42,
                    14,
                    1,
                    84,
                    0
                ],
                "published": "2024-12-09T19:03:14Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    19,
                    3,
                    14,
                    0,
                    344,
                    0
                ],
                "title": "A Multiwavelength Autopsy of the Interacting IIn Supernova 2020ywx:\n  Tracing its Progenitor Mass-Loss History for 100 Years before Death",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multiwavelength Autopsy of the Interacting IIn Supernova 2020ywx:\n  Tracing its Progenitor Mass-Loss History for 100 Years before Death"
                },
                "summary": "While the subclass of interacting supernovae with narrow hydrogen emission\nlines (SNe IIn) consists of some of the longest-lasting and brightest SNe ever\ndiscovered, their progenitors are still not well understood. Investigating SNe\nIIn as they emit across the electromagnetic spectrum is the most robust way to\nunderstand the progenitor evolution before the explosion. This work presents\nX-Ray, optical, infrared, and radio observations of the strongly interacting\nType IIn SN 2020ywx covering a period $>1200$ days after discovery. Through\nmultiwavelength modeling, we find that the progenitor of 2020ywx was losing\nmass at $\\sim10^{-2}$--$10^{-3} \\mathrm{\\,M_{\\odot}\\,yr^{-1}}$ for at least 100\nyrs pre-explosion using the circumstellar medium (CSM) speed of 120 km/s\nmeasured from our optical and NIR spectra. Despite the similar magnitude of\nmass loss measured in different wavelength ranges, we find discrepancies\nbetween the X-ray and optical/radio-derived mass-loss evolution, which suggest\nasymmetries in the CSM. Furthermore, we find evidence for dust formation due to\nthe combination of a growing blueshift in optical emission lines and\nnear-infrared continuum emission which we fit with blackbodies at $\\sim$ 1000\nK. Based on the observed elevated mass loss over more than 100 years and the\nconfiguration of the CSM inferred from the multiwavelength observations, we\ninvoke binary interaction as the most plausible mechanism to explain the\noverall mass-loss evolution. SN 2020ywx is thus a case that may support the\ngrowing observational consensus that SNe IIn mass loss is explained by binary\ninteraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the subclass of interacting supernovae with narrow hydrogen emission\nlines (SNe IIn) consists of some of the longest-lasting and brightest SNe ever\ndiscovered, their progenitors are still not well understood. Investigating SNe\nIIn as they emit across the electromagnetic spectrum is the most robust way to\nunderstand the progenitor evolution before the explosion. This work presents\nX-Ray, optical, infrared, and radio observations of the strongly interacting\nType IIn SN 2020ywx covering a period $>1200$ days after discovery. Through\nmultiwavelength modeling, we find that the progenitor of 2020ywx was losing\nmass at $\\sim10^{-2}$--$10^{-3} \\mathrm{\\,M_{\\odot}\\,yr^{-1}}$ for at least 100\nyrs pre-explosion using the circumstellar medium (CSM) speed of 120 km/s\nmeasured from our optical and NIR spectra. Despite the similar magnitude of\nmass loss measured in different wavelength ranges, we find discrepancies\nbetween the X-ray and optical/radio-derived mass-loss evolution, which suggest\nasymmetries in the CSM. Furthermore, we find evidence for dust formation due to\nthe combination of a growing blueshift in optical emission lines and\nnear-infrared continuum emission which we fit with blackbodies at $\\sim$ 1000\nK. Based on the observed elevated mass loss over more than 100 years and the\nconfiguration of the CSM inferred from the multiwavelength observations, we\ninvoke binary interaction as the most plausible mechanism to explain the\noverall mass-loss evolution. SN 2020ywx is thus a case that may support the\ngrowing observational consensus that SNe IIn mass loss is explained by binary\ninteraction."
                },
                "authors": [
                    {
                        "name": "Raphael Baer-Way"
                    },
                    {
                        "name": "Poonam Chandra"
                    },
                    {
                        "name": "Maryam Modjaz"
                    },
                    {
                        "name": "Sahana Kumar"
                    },
                    {
                        "name": "Craig Pellegrino"
                    },
                    {
                        "name": "Roger Chevalier"
                    },
                    {
                        "name": "Adrian Crawford"
                    },
                    {
                        "name": "Arkaprabha Sarangi"
                    },
                    {
                        "name": "Nathan Smith"
                    },
                    {
                        "name": "Keiichi Maeda"
                    },
                    {
                        "name": "A. J. Nayana"
                    },
                    {
                        "name": "Alexei V. Filippenko"
                    },
                    {
                        "name": "Jennifer E. Andrews"
                    },
                    {
                        "name": "Iair Arcavi"
                    },
                    {
                        "name": "K. Azalee Bostroem"
                    },
                    {
                        "name": "Thomas G. Brink"
                    },
                    {
                        "name": "Yize Dong"
                    },
                    {
                        "name": "Vikram Dwarkadas"
                    },
                    {
                        "name": "Joseph R. Farah"
                    },
                    {
                        "name": "D. Andrew Howell"
                    },
                    {
                        "name": "Daichi Hiramatsu"
                    },
                    {
                        "name": "Griffin Hosseinzadeh"
                    },
                    {
                        "name": "Curtis McCully"
                    },
                    {
                        "name": "Nicolas Meza"
                    },
                    {
                        "name": "Megan Newsome"
                    },
                    {
                        "name": "Estefania Padilla Gonzalez"
                    },
                    {
                        "name": "Jeniveve Pearson"
                    },
                    {
                        "name": "David J. Sand"
                    },
                    {
                        "name": "Manisha Shrestha"
                    },
                    {
                        "name": "Giacomo Terreran"
                    },
                    {
                        "name": "Stefano Valenti"
                    },
                    {
                        "name": "Samuel Wyatt"
                    },
                    {
                        "name": "Yi Yang"
                    },
                    {
                        "name": "WeiKang Zheng"
                    }
                ],
                "author_detail": {
                    "name": "WeiKang Zheng"
                },
                "author": "WeiKang Zheng",
                "arxiv_comment": "Now accepted to ApJ, 33 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06914v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06914v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19711v1",
                "updated": "2025-03-25T14:38:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    14,
                    38,
                    36,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T14:38:36Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    14,
                    38,
                    36,
                    1,
                    84,
                    0
                ],
                "title": "Writing as a testbed for open ended agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing as a testbed for open ended agents"
                },
                "summary": "Open-ended tasks are particularly challenging for LLMs due to the vast\nsolution space, demanding both expansive exploration and adaptable strategies,\nespecially when success lacks a clear, objective definition. Writing, with its\nvast solution space and subjective evaluation criteria, provides a compelling\ntestbed for studying such problems. In this paper, we investigate the potential\nof LLMs to act as collaborative co-writers, capable of suggesting and\nimplementing text improvements autonomously. We analyse three prominent LLMs -\nGemini 1.5 Pro, Claude 3.5 Sonnet, and GPT-4o - focusing on how their action\ndiversity, human alignment, and iterative improvement capabilities impact\noverall performance. This work establishes a framework for benchmarking\nautonomous writing agents and, more broadly, highlights fundamental challenges\nand potential solutions for building systems capable of excelling in diverse\nopen-ended domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-ended tasks are particularly challenging for LLMs due to the vast\nsolution space, demanding both expansive exploration and adaptable strategies,\nespecially when success lacks a clear, objective definition. Writing, with its\nvast solution space and subjective evaluation criteria, provides a compelling\ntestbed for studying such problems. In this paper, we investigate the potential\nof LLMs to act as collaborative co-writers, capable of suggesting and\nimplementing text improvements autonomously. We analyse three prominent LLMs -\nGemini 1.5 Pro, Claude 3.5 Sonnet, and GPT-4o - focusing on how their action\ndiversity, human alignment, and iterative improvement capabilities impact\noverall performance. This work establishes a framework for benchmarking\nautonomous writing agents and, more broadly, highlights fundamental challenges\nand potential solutions for building systems capable of excelling in diverse\nopen-ended domains."
                },
                "authors": [
                    {
                        "name": "Sian Gooding"
                    },
                    {
                        "name": "Lucia Lopez-Rivilla"
                    },
                    {
                        "name": "Edward Grefenstette"
                    }
                ],
                "author_detail": {
                    "name": "Edward Grefenstette"
                },
                "author": "Edward Grefenstette",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19697v1",
                "updated": "2025-03-25T14:25:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    14,
                    25,
                    32,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T14:25:32Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    14,
                    25,
                    32,
                    1,
                    84,
                    0
                ],
                "title": "Spectral classification of young stars using conditional invertible\n  neural networks II. Application to Trumpler 14 in Carina",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectral classification of young stars using conditional invertible\n  neural networks II. Application to Trumpler 14 in Carina"
                },
                "summary": "We introduce an updated version of our deep learning tool that predicts\nstellar parameters from the optical spectra of young low-mass stars with\nintermediate spectral resolution. We adopt a conditional invertible neural\nnetwork (cINN) architecture to infer the posterior distribution of stellar\nparameters and train our cINN on two Phoenix stellar atmosphere model libraries\n(Settl and Dusty). Compared to the cINNs presented in our first study, the\nupdated cINN considers the influence of the relative flux error on the\nparameter estimation and predicts an additional fourth parameter, veiling. We\ntest the performance of cINN on synthetic test models to quantify the intrinsic\nerror of the cINN as a function of relative flux error and on 36 class III\ntemplate stars to validate the performance on real spectra. Using our cINN, we\nestimate the stellar parameters of young stars in Trumpler 14 (Tr14) in the\nCarina Nebula Complex, observed with VLT-MUSE, and compare them with those\nderived using the classical template fitting method. We provide Teff, log g,\nAv, and veiling values measured by our cINN as well as stellar ages and masses\nderived from the HR diagram. Our parameter estimates generally agree well with\nthose measured by template fitting. However, for K- and G-type stars, the Teff\nderived from template fitting is, on average, 2-3 subclasses hotter than the\ncINN estimates, while the corresponding veiling values from template fitting\nappear to be underestimated compared to the cINN predictions. We obtain an\naverage age of 0.7(+3.2)(-0.6) Myr for the Tr14 stars. By examining the impact\nof veiling on the equivalent width-based classification, we demonstrate that\nthe main cause of temperature overestimation for K- and G-type stars in the\nprevious study is that veiling and effective temperature are not considered\nsimultaneously in their process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an updated version of our deep learning tool that predicts\nstellar parameters from the optical spectra of young low-mass stars with\nintermediate spectral resolution. We adopt a conditional invertible neural\nnetwork (cINN) architecture to infer the posterior distribution of stellar\nparameters and train our cINN on two Phoenix stellar atmosphere model libraries\n(Settl and Dusty). Compared to the cINNs presented in our first study, the\nupdated cINN considers the influence of the relative flux error on the\nparameter estimation and predicts an additional fourth parameter, veiling. We\ntest the performance of cINN on synthetic test models to quantify the intrinsic\nerror of the cINN as a function of relative flux error and on 36 class III\ntemplate stars to validate the performance on real spectra. Using our cINN, we\nestimate the stellar parameters of young stars in Trumpler 14 (Tr14) in the\nCarina Nebula Complex, observed with VLT-MUSE, and compare them with those\nderived using the classical template fitting method. We provide Teff, log g,\nAv, and veiling values measured by our cINN as well as stellar ages and masses\nderived from the HR diagram. Our parameter estimates generally agree well with\nthose measured by template fitting. However, for K- and G-type stars, the Teff\nderived from template fitting is, on average, 2-3 subclasses hotter than the\ncINN estimates, while the corresponding veiling values from template fitting\nappear to be underestimated compared to the cINN predictions. We obtain an\naverage age of 0.7(+3.2)(-0.6) Myr for the Tr14 stars. By examining the impact\nof veiling on the equivalent width-based classification, we demonstrate that\nthe main cause of temperature overestimation for K- and G-type stars in the\nprevious study is that veiling and effective temperature are not considered\nsimultaneously in their process."
                },
                "authors": [
                    {
                        "name": "Da Eun Kang"
                    },
                    {
                        "name": "Dominika Itrich"
                    },
                    {
                        "name": "Victor F. Ksoll"
                    },
                    {
                        "name": "Leonardo Testi"
                    },
                    {
                        "name": "Ralf S. Klessen"
                    },
                    {
                        "name": "Sergio Molinari"
                    }
                ],
                "author_detail": {
                    "name": "Sergio Molinari"
                },
                "author": "Sergio Molinari",
                "arxiv_comment": "33 pages, 24 figures, Accepted for publication by Astronomy &\n  Astrophysics on 21 March",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18865v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18865v2",
                "updated": "2025-03-25T14:21:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    14,
                    21,
                    15,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-24T16:41:17Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    41,
                    17,
                    0,
                    83,
                    0
                ],
                "title": "Structuring Scientific Innovation: A Framework for Modeling and\n  Discovering Impactful Knowledge Combinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structuring Scientific Innovation: A Framework for Modeling and\n  Discovering Impactful Knowledge Combinations"
                },
                "summary": "The emergence of large language models offers new possibilities for\nstructured exploration of scientific knowledge. Rather than viewing scientific\ndiscovery as isolated ideas or content, we propose a structured approach that\nemphasizes the role of method combinations in shaping disruptive insights.\nSpecifically, we investigate how knowledge unit--especially those tied to\nmethodological design--can be modeled and recombined to yield research\nbreakthroughs. Our proposed framework addresses two key challenges. First, we\nintroduce a contrastive learning-based mechanism to identify distinguishing\nfeatures of historically disruptive method combinations within problem-driven\ncontexts. Second, we propose a reasoning-guided Monte Carlo search algorithm\nthat leverages the chain-of-thought capability of LLMs to identify promising\nknowledge recombinations for new problem statements.Empirical studies across\nmultiple domains show that the framework is capable of modeling the structural\ndynamics of innovation and successfully highlights combinations with high\ndisruptive potential. This research provides a new path for computationally\nguided scientific ideation grounded in structured reasoning and historical data\nmodeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models offers new possibilities for\nstructured exploration of scientific knowledge. Rather than viewing scientific\ndiscovery as isolated ideas or content, we propose a structured approach that\nemphasizes the role of method combinations in shaping disruptive insights.\nSpecifically, we investigate how knowledge unit--especially those tied to\nmethodological design--can be modeled and recombined to yield research\nbreakthroughs. Our proposed framework addresses two key challenges. First, we\nintroduce a contrastive learning-based mechanism to identify distinguishing\nfeatures of historically disruptive method combinations within problem-driven\ncontexts. Second, we propose a reasoning-guided Monte Carlo search algorithm\nthat leverages the chain-of-thought capability of LLMs to identify promising\nknowledge recombinations for new problem statements.Empirical studies across\nmultiple domains show that the framework is capable of modeling the structural\ndynamics of innovation and successfully highlights combinations with high\ndisruptive potential. This research provides a new path for computationally\nguided scientific ideation grounded in structured reasoning and historical data\nmodeling."
                },
                "authors": [
                    {
                        "name": "Junlan Chen"
                    },
                    {
                        "name": "Kexin Zhang"
                    },
                    {
                        "name": "Daifeng Li"
                    },
                    {
                        "name": "Yangyang Feng"
                    },
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Bowen Deng"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Deng"
                },
                "author": "Bowen Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18865v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18865v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16870v2",
                "updated": "2025-03-25T14:18:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    14,
                    18,
                    33,
                    1,
                    84,
                    0
                ],
                "published": "2024-10-22T10:19:17Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    10,
                    19,
                    17,
                    1,
                    296,
                    0
                ],
                "title": "Federated Causal Inference: Multi-Study ATE Estimation beyond\n  Meta-Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Causal Inference: Multi-Study ATE Estimation beyond\n  Meta-Analysis"
                },
                "summary": "We study Federated Causal Inference, an approach to estimate treatment\neffects from decentralized data across centers. We compare three classes of\nAverage Treatment Effect (ATE) estimators derived from the Plug-in G-Formula,\nranging from simple meta-analysis to one-shot and multi-shot federated\nlearning, the latter leveraging the full data to learn the outcome model\n(albeit requiring more communication). Focusing on Randomized Controlled Trials\n(RCTs), we derive the asymptotic variance of these estimators for linear\nmodels. Our results provide practical guidance on selecting the appropriate\nestimator for various scenarios, including heterogeneity in sample sizes,\ncovariate distributions, treatment assignment schemes, and center effects. We\nvalidate these findings with a simulation study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study Federated Causal Inference, an approach to estimate treatment\neffects from decentralized data across centers. We compare three classes of\nAverage Treatment Effect (ATE) estimators derived from the Plug-in G-Formula,\nranging from simple meta-analysis to one-shot and multi-shot federated\nlearning, the latter leveraging the full data to learn the outcome model\n(albeit requiring more communication). Focusing on Randomized Controlled Trials\n(RCTs), we derive the asymptotic variance of these estimators for linear\nmodels. Our results provide practical guidance on selecting the appropriate\nestimator for various scenarios, including heterogeneity in sample sizes,\ncovariate distributions, treatment assignment schemes, and center effects. We\nvalidate these findings with a simulation study."
                },
                "authors": [
                    {
                        "name": "Rémi Khellaf"
                    },
                    {
                        "name": "Aurélien Bellet"
                    },
                    {
                        "name": "Julie Josse"
                    }
                ],
                "author_detail": {
                    "name": "Julie Josse"
                },
                "author": "Julie Josse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19693v1",
                "updated": "2025-03-25T14:18:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    14,
                    18,
                    21,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T14:18:21Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    14,
                    18,
                    21,
                    1,
                    84,
                    0
                ],
                "title": "AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through\n  Lightweight Vocabulary Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through\n  Lightweight Vocabulary Adaptation"
                },
                "summary": "Large Language Models (LLMs) have shown impressive versatility as general\npurpose models. However, their broad applicability comes at a high-cost\ncomputational overhead, particularly in auto-regressive decoding where each\nstep requires a forward pass. In domain-specific settings, general-purpose\ncapabilities are unnecessary and can be exchanged for efficiency. In this work,\nwe take a novel perspective on domain adaptation, reducing latency and\ncomputational costs by adapting the vocabulary to focused domains of interest.\nWe introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation,\ndesigned to enhance LLM efficiency in low-resource domains. AdaptiVocab can be\napplied to any tokenizer and architecture, modifying the vocabulary by\nreplacing tokens with domain-specific n-gram-based tokens, thereby reducing the\nnumber of tokens required for both input processing and output generation.\nAdaptiVocab initializes new n-token embeddings using an exponentially weighted\ncombination of existing embeddings and employs a lightweight fine-tuning phase\nthat can be efficiently performed on a single GPU. We evaluate two 7B LLMs\nacross three niche domains, assessing efficiency, generation quality, and\nend-task performance. Our results show that AdaptiVocab reduces token usage by\nover 25% without compromising performance",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive versatility as general\npurpose models. However, their broad applicability comes at a high-cost\ncomputational overhead, particularly in auto-regressive decoding where each\nstep requires a forward pass. In domain-specific settings, general-purpose\ncapabilities are unnecessary and can be exchanged for efficiency. In this work,\nwe take a novel perspective on domain adaptation, reducing latency and\ncomputational costs by adapting the vocabulary to focused domains of interest.\nWe introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation,\ndesigned to enhance LLM efficiency in low-resource domains. AdaptiVocab can be\napplied to any tokenizer and architecture, modifying the vocabulary by\nreplacing tokens with domain-specific n-gram-based tokens, thereby reducing the\nnumber of tokens required for both input processing and output generation.\nAdaptiVocab initializes new n-token embeddings using an exponentially weighted\ncombination of existing embeddings and employs a lightweight fine-tuning phase\nthat can be efficiently performed on a single GPU. We evaluate two 7B LLMs\nacross three niche domains, assessing efficiency, generation quality, and\nend-task performance. Our results show that AdaptiVocab reduces token usage by\nover 25% without compromising performance"
                },
                "authors": [
                    {
                        "name": "Itay Nakash"
                    },
                    {
                        "name": "Nitay Calderon"
                    },
                    {
                        "name": "Eyal Ben David"
                    },
                    {
                        "name": "Elad Hoffer"
                    },
                    {
                        "name": "Roi Reichart"
                    }
                ],
                "author_detail": {
                    "name": "Roi Reichart"
                },
                "author": "Roi Reichart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17546v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17546v2",
                "updated": "2025-03-25T14:02:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    14,
                    2,
                    42,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-21T21:41:48Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    21,
                    41,
                    48,
                    4,
                    80,
                    0
                ],
                "title": "Communities in the Kuramoto Model: Dynamics and Detection via Path\n  Signatures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communities in the Kuramoto Model: Dynamics and Detection via Path\n  Signatures"
                },
                "summary": "The behavior of multivariate dynamical processes is often governed by\nunderlying structural connections that relate the components of the system. For\nexample, brain activity which is often measured via time series is determined\nby an underlying structural graph, where nodes represent neurons or brain\nregions and edges represent cortical connectivity. Existing methods for\ninferring structural connections from observed dynamics, such as\ncorrelation-based or spectral techniques, may fail to fully capture complex\nrelationships in high-dimensional time series in an interpretable way. Here, we\npropose the use of path signatures a mathematical framework that encodes\ngeometric and temporal properties of continuous paths to address this problem.\nPath signatures provide a reparametrization-invariant characterization of\ndynamical data and, in particular, can be used to compute the lead matrix which\nreveals lead-lag phenomena. We showcase our approach on time series from\ncoupled oscillators in the Kuramoto model defined on a stochastic block model\ngraph, termed the Kuramoto stochastic block model (KSBM). Using mean-field\ntheory and Gaussian approximations, we analytically derive reduced models of\nKSBM dynamics in different temporal regimes and theoretically characterize the\nlead matrix in these settings. Leveraging these insights, we propose a novel\nsignature-based community detection algorithm, achieving exact recovery of\nstructural communities from observed time series in multiple KSBM instances.\nOur results demonstrate that path signatures provide a novel perspective on\nanalyzing complex neural data and other high-dimensional systems, explicitly\nexploiting temporal functional relationships to infer underlying structure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The behavior of multivariate dynamical processes is often governed by\nunderlying structural connections that relate the components of the system. For\nexample, brain activity which is often measured via time series is determined\nby an underlying structural graph, where nodes represent neurons or brain\nregions and edges represent cortical connectivity. Existing methods for\ninferring structural connections from observed dynamics, such as\ncorrelation-based or spectral techniques, may fail to fully capture complex\nrelationships in high-dimensional time series in an interpretable way. Here, we\npropose the use of path signatures a mathematical framework that encodes\ngeometric and temporal properties of continuous paths to address this problem.\nPath signatures provide a reparametrization-invariant characterization of\ndynamical data and, in particular, can be used to compute the lead matrix which\nreveals lead-lag phenomena. We showcase our approach on time series from\ncoupled oscillators in the Kuramoto model defined on a stochastic block model\ngraph, termed the Kuramoto stochastic block model (KSBM). Using mean-field\ntheory and Gaussian approximations, we analytically derive reduced models of\nKSBM dynamics in different temporal regimes and theoretically characterize the\nlead matrix in these settings. Leveraging these insights, we propose a novel\nsignature-based community detection algorithm, achieving exact recovery of\nstructural communities from observed time series in multiple KSBM instances.\nOur results demonstrate that path signatures provide a novel perspective on\nanalyzing complex neural data and other high-dimensional systems, explicitly\nexploiting temporal functional relationships to infer underlying structure."
                },
                "authors": [
                    {
                        "name": "Tâm Johan Nguyên"
                    },
                    {
                        "name": "Darrick Lee"
                    },
                    {
                        "name": "Bernadette Jana Stolz"
                    }
                ],
                "author_detail": {
                    "name": "Bernadette Jana Stolz"
                },
                "author": "Bernadette Jana Stolz",
                "arxiv_comment": "46 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17546v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17546v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.AO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19666v2",
                "updated": "2025-03-26T10:39:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    10,
                    39,
                    33,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-25T13:52:26Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    52,
                    26,
                    1,
                    84,
                    0
                ],
                "title": "Towards Efficient Training of Graph Neural Networks: A Multiscale\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient Training of Graph Neural Networks: A Multiscale\n  Approach"
                },
                "summary": "Graph Neural Networks (GNNs) have emerged as a powerful tool for learning and\ninferring from graph-structured data, and are widely used in a variety of\napplications, often considering large amounts of data and large graphs.\nHowever, training on such data requires large memory and extensive\ncomputations. In this paper, we introduce a novel framework for efficient\nmultiscale training of GNNs, designed to integrate information across\nmultiscale representations of a graph. Our approach leverages a hierarchical\ngraph representation, taking advantage of coarse graph scales in the training\nprocess, where each coarse scale graph has fewer nodes and edges. Based on this\napproach, we propose a suite of GNN training methods: such as coarse-to-fine,\nsub-to-full, and multiscale gradient computation. We demonstrate the\neffectiveness of our methods on various datasets and learning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have emerged as a powerful tool for learning and\ninferring from graph-structured data, and are widely used in a variety of\napplications, often considering large amounts of data and large graphs.\nHowever, training on such data requires large memory and extensive\ncomputations. In this paper, we introduce a novel framework for efficient\nmultiscale training of GNNs, designed to integrate information across\nmultiscale representations of a graph. Our approach leverages a hierarchical\ngraph representation, taking advantage of coarse graph scales in the training\nprocess, where each coarse scale graph has fewer nodes and edges. Based on this\napproach, we propose a suite of GNN training methods: such as coarse-to-fine,\nsub-to-full, and multiscale gradient computation. We demonstrate the\neffectiveness of our methods on various datasets and learning tasks."
                },
                "authors": [
                    {
                        "name": "Eshed Gal"
                    },
                    {
                        "name": "Moshe Eliasof"
                    },
                    {
                        "name": "Carola-Bibiane Schönlieb"
                    },
                    {
                        "name": "Eldad Haber"
                    },
                    {
                        "name": "Eran Treister"
                    }
                ],
                "author_detail": {
                    "name": "Eran Treister"
                },
                "author": "Eran Treister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18854v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18854v2",
                "updated": "2025-03-25T13:50:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    50,
                    20,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-24T16:32:17Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    32,
                    17,
                    0,
                    83,
                    0
                ],
                "title": "MC-LLaVA: Multi-Concept Personalized Vision-Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC-LLaVA: Multi-Concept Personalized Vision-Language Model"
                },
                "summary": "Current vision-language models (VLMs) show exceptional abilities across\ndiverse tasks, such as visual question answering. To enhance user experience,\nrecent studies investigate VLM personalization to understand user-provided\nconcepts. However, they mainly focus on single-concept personalization,\nneglecting the existence and interplay of multiple concepts, which limits\nreal-world applicability. This paper proposes the first multi-concept\npersonalization paradigm, MC-LLaVA. Specifically, MC-LLaVA employs a\nmulti-concept instruction tuning strategy, effectively integrating multiple\nconcepts in a single training step. To reduce the costs related to joint\ntraining, we propose a personalized textual prompt that uses visual token\ninformation to initialize concept tokens. Additionally, we introduce a\npersonalized visual prompt during inference, aggregating location confidence\nmaps for enhanced recognition and grounding capabilities. To advance\nmulti-concept personalization research, we further contribute a high-quality\ninstruction tuning dataset. We carefully collect images with multiple\ncharacters and objects from movies and manually generate question-answer\nsamples for multi-concept scenarios, featuring superior diversity.\nComprehensive qualitative and quantitative experiments demonstrate that\nMC-LLaVA can achieve impressive multi-concept personalized responses, paving\nthe way for VLMs to become better user-specific assistants. The code and\ndataset will be publicly available at https://github.com/arctanxarc/MC-LLaVA}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current vision-language models (VLMs) show exceptional abilities across\ndiverse tasks, such as visual question answering. To enhance user experience,\nrecent studies investigate VLM personalization to understand user-provided\nconcepts. However, they mainly focus on single-concept personalization,\nneglecting the existence and interplay of multiple concepts, which limits\nreal-world applicability. This paper proposes the first multi-concept\npersonalization paradigm, MC-LLaVA. Specifically, MC-LLaVA employs a\nmulti-concept instruction tuning strategy, effectively integrating multiple\nconcepts in a single training step. To reduce the costs related to joint\ntraining, we propose a personalized textual prompt that uses visual token\ninformation to initialize concept tokens. Additionally, we introduce a\npersonalized visual prompt during inference, aggregating location confidence\nmaps for enhanced recognition and grounding capabilities. To advance\nmulti-concept personalization research, we further contribute a high-quality\ninstruction tuning dataset. We carefully collect images with multiple\ncharacters and objects from movies and manually generate question-answer\nsamples for multi-concept scenarios, featuring superior diversity.\nComprehensive qualitative and quantitative experiments demonstrate that\nMC-LLaVA can achieve impressive multi-concept personalized responses, paving\nthe way for VLMs to become better user-specific assistants. The code and\ndataset will be publicly available at https://github.com/arctanxarc/MC-LLaVA}."
                },
                "authors": [
                    {
                        "name": "Ruichuan An"
                    },
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Ming Lu"
                    },
                    {
                        "name": "Renrui Zhang"
                    },
                    {
                        "name": "Kai Zeng"
                    },
                    {
                        "name": "Yulin Luo"
                    },
                    {
                        "name": "Jiajun Cao"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Ying Chen"
                    },
                    {
                        "name": "Qi She"
                    },
                    {
                        "name": "Shanghang Zhang"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_comment": "I sincerely apologize for any inconvenience caused. We actually\n  uploaded this paper to arXiv in November 2024, as arXiv:2411.11706. During\n  this update, we did not consider the replacement operation of arXiv, which\n  led to duplicate submissions. We have made modifications at the original\n  address arXiv:2411.11706",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18854v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18854v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19651v1",
                "updated": "2025-03-25T13:40:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    40,
                    59,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T13:40:59Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    40,
                    59,
                    1,
                    84,
                    0
                ],
                "title": "Enhancing Graphical Lasso: A Robust Scheme for Non-Stationary Mean Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Graphical Lasso: A Robust Scheme for Non-Stationary Mean Data"
                },
                "summary": "This work addresses the problem of graph learning from data following a\nGaussian Graphical Model (GGM) with a time-varying mean. Graphical Lasso (GL),\nthe standard method for estimating sparse precision matrices, assumes that the\nobserved data follows a zero-mean Gaussian distribution. However, this\nassumption is often violated in real-world scenarios where the mean evolves\nover time due to external influences, trends, or regime shifts. When the mean\nis not properly accounted for, applying GL directly can lead to estimating a\nbiased precision matrix, hence hindering the graph learning task. To overcome\nthis limitation, we propose Graphical Lasso with Adaptive Targeted Adaptive\nImportance Sampling (GL-ATAIS), an iterative method that jointly estimates the\ntime-varying mean and the precision matrix. Our approach integrates Bayesian\ninference with frequentist estimation, leveraging importance sampling to obtain\nan estimate of the mean while using a regularized maximum likelihood estimator\nto infer the precision matrix. By iteratively refining both estimates, GL-ATAIS\nmitigates the bias introduced by time-varying means, leading to more accurate\ngraph recovery. Our numerical evaluation demonstrates the impact of properly\naccounting for time-dependent means and highlights the advantages of GL-ATAIS\nover standard GL in recovering the true graph structure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work addresses the problem of graph learning from data following a\nGaussian Graphical Model (GGM) with a time-varying mean. Graphical Lasso (GL),\nthe standard method for estimating sparse precision matrices, assumes that the\nobserved data follows a zero-mean Gaussian distribution. However, this\nassumption is often violated in real-world scenarios where the mean evolves\nover time due to external influences, trends, or regime shifts. When the mean\nis not properly accounted for, applying GL directly can lead to estimating a\nbiased precision matrix, hence hindering the graph learning task. To overcome\nthis limitation, we propose Graphical Lasso with Adaptive Targeted Adaptive\nImportance Sampling (GL-ATAIS), an iterative method that jointly estimates the\ntime-varying mean and the precision matrix. Our approach integrates Bayesian\ninference with frequentist estimation, leveraging importance sampling to obtain\nan estimate of the mean while using a regularized maximum likelihood estimator\nto infer the precision matrix. By iteratively refining both estimates, GL-ATAIS\nmitigates the bias introduced by time-varying means, leading to more accurate\ngraph recovery. Our numerical evaluation demonstrates the impact of properly\naccounting for time-dependent means and highlights the advantages of GL-ATAIS\nover standard GL in recovering the true graph structure."
                },
                "authors": [
                    {
                        "name": "Samuel Rey"
                    },
                    {
                        "name": "Ernesto Curbelo"
                    },
                    {
                        "name": "Luca Martino"
                    },
                    {
                        "name": "Fernando Llorente"
                    },
                    {
                        "name": "Antonio G. Marques"
                    }
                ],
                "author_detail": {
                    "name": "Antonio G. Marques"
                },
                "author": "Antonio G. Marques",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19650v1",
                "updated": "2025-03-25T13:40:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    40,
                    22,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T13:40:22Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    40,
                    22,
                    1,
                    84,
                    0
                ],
                "title": "HausaNLP at SemEval-2025 Task 3: Towards a Fine-Grained Model-Aware\n  Hallucination Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HausaNLP at SemEval-2025 Task 3: Towards a Fine-Grained Model-Aware\n  Hallucination Detection"
                },
                "summary": "This paper presents our findings of the Multilingual Shared Task on\nHallucinations and Related Observable Overgeneration Mistakes, MU-SHROOM, which\nfocuses on identifying hallucinations and related overgeneration errors in\nlarge language models (LLMs). The shared task involves detecting specific text\nspans that constitute hallucinations in the outputs generated by LLMs in 14\nlanguages. To address this task, we aim to provide a nuanced, model-aware\nunderstanding of hallucination occurrences and severity in English. We used\nnatural language inference and fine-tuned a ModernBERT model using a synthetic\ndataset of 400 samples, achieving an Intersection over Union (IoU) score of\n0.032 and a correlation score of 0.422. These results indicate a moderately\npositive correlation between the model's confidence scores and the actual\npresence of hallucinations. The IoU score indicates that our model has a\nrelatively low overlap between the predicted hallucination span and the truth\nannotation. The performance is unsurprising, given the intricate nature of\nhallucination detection. Hallucinations often manifest subtly, relying on\ncontext, making pinpointing their exact boundaries formidable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents our findings of the Multilingual Shared Task on\nHallucinations and Related Observable Overgeneration Mistakes, MU-SHROOM, which\nfocuses on identifying hallucinations and related overgeneration errors in\nlarge language models (LLMs). The shared task involves detecting specific text\nspans that constitute hallucinations in the outputs generated by LLMs in 14\nlanguages. To address this task, we aim to provide a nuanced, model-aware\nunderstanding of hallucination occurrences and severity in English. We used\nnatural language inference and fine-tuned a ModernBERT model using a synthetic\ndataset of 400 samples, achieving an Intersection over Union (IoU) score of\n0.032 and a correlation score of 0.422. These results indicate a moderately\npositive correlation between the model's confidence scores and the actual\npresence of hallucinations. The IoU score indicates that our model has a\nrelatively low overlap between the predicted hallucination span and the truth\nannotation. The performance is unsurprising, given the intricate nature of\nhallucination detection. Hallucinations often manifest subtly, relying on\ncontext, making pinpointing their exact boundaries formidable."
                },
                "authors": [
                    {
                        "name": "Maryam Bala"
                    },
                    {
                        "name": "Amina Imam Abubakar"
                    },
                    {
                        "name": "Abdulhamid Abubakar"
                    },
                    {
                        "name": "Abdulkadir Shehu Bichi"
                    },
                    {
                        "name": "Hafsa Kabir Ahmad"
                    },
                    {
                        "name": "Sani Abdullahi Sani"
                    },
                    {
                        "name": "Idris Abdulmumin"
                    },
                    {
                        "name": "Shamsuddeen Hassan Muhamad"
                    },
                    {
                        "name": "Ibrahim Said Ahmad"
                    }
                ],
                "author_detail": {
                    "name": "Ibrahim Said Ahmad"
                },
                "author": "Ibrahim Said Ahmad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18227v2",
                "updated": "2025-03-25T13:25:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    25,
                    6,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-23T22:06:07Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    22,
                    6,
                    7,
                    6,
                    82,
                    0
                ],
                "title": "PG-SAM: Prior-Guided SAM with Medical for Multi-organ Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PG-SAM: Prior-Guided SAM with Medical for Multi-organ Segmentation"
                },
                "summary": "Segment Anything Model (SAM) demonstrates powerful zero-shot capabilities;\nhowever, its accuracy and robustness significantly decrease when applied to\nmedical image segmentation. Existing methods address this issue through\nmodality fusion, integrating textual and image information to provide more\ndetailed priors. In this study, we argue that the granularity of text and the\ndomain gap affect the accuracy of the priors. Furthermore, the discrepancy\nbetween high-level abstract semantics and pixel-level boundary details in\nimages can introduce noise into the fusion process. To address this, we propose\nPrior-Guided SAM (PG-SAM), which employs a fine-grained modality prior aligner\nto leverage specialized medical knowledge for better modality alignment. The\ncore of our method lies in efficiently addressing the domain gap with\nfine-grained text from a medical LLM. Meanwhile, it also enhances the priors'\nquality after modality alignment, ensuring more accurate segmentation. In\naddition, our decoder enhances the model's expressive capabilities through\nmulti-level feature fusion and iterative mask optimizer operations, supporting\nunprompted learning. We also propose a unified pipeline that effectively\nsupplies high-quality semantic information to SAM. Extensive experiments on the\nSynapse dataset demonstrate that the proposed PG-SAM achieves state-of-the-art\nperformance. Our anonymous code is released at\nhttps://github.com/logan-0623/PG-SAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Segment Anything Model (SAM) demonstrates powerful zero-shot capabilities;\nhowever, its accuracy and robustness significantly decrease when applied to\nmedical image segmentation. Existing methods address this issue through\nmodality fusion, integrating textual and image information to provide more\ndetailed priors. In this study, we argue that the granularity of text and the\ndomain gap affect the accuracy of the priors. Furthermore, the discrepancy\nbetween high-level abstract semantics and pixel-level boundary details in\nimages can introduce noise into the fusion process. To address this, we propose\nPrior-Guided SAM (PG-SAM), which employs a fine-grained modality prior aligner\nto leverage specialized medical knowledge for better modality alignment. The\ncore of our method lies in efficiently addressing the domain gap with\nfine-grained text from a medical LLM. Meanwhile, it also enhances the priors'\nquality after modality alignment, ensuring more accurate segmentation. In\naddition, our decoder enhances the model's expressive capabilities through\nmulti-level feature fusion and iterative mask optimizer operations, supporting\nunprompted learning. We also propose a unified pipeline that effectively\nsupplies high-quality semantic information to SAM. Extensive experiments on the\nSynapse dataset demonstrate that the proposed PG-SAM achieves state-of-the-art\nperformance. Our anonymous code is released at\nhttps://github.com/logan-0623/PG-SAM."
                },
                "authors": [
                    {
                        "name": "Yiheng Zhong"
                    },
                    {
                        "name": "Zihong Luo"
                    },
                    {
                        "name": "Chengzhi Liu"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Zelin Peng"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Yingzhen Hu"
                    },
                    {
                        "name": "Jionglong Su"
                    },
                    {
                        "name": "Zongyuan Geand"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19633v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19633v1",
                "updated": "2025-03-25T13:19:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    19,
                    46,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T13:19:46Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    19,
                    46,
                    1,
                    84,
                    0
                ],
                "title": "1.4 Million Open-Source Distilled Reasoning Dataset to Empower Large\n  Language Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "1.4 Million Open-Source Distilled Reasoning Dataset to Empower Large\n  Language Model Training"
                },
                "summary": "The AM-DeepSeek-R1-Distilled is a large-scale dataset with thinking traces\nfor general reasoning tasks, composed of high-quality and challenging reasoning\nproblems. These problems are collected from a multitude of open-source\ndatasets, subjected to semantic deduplication and meticulous cleaning to\neliminate test set contamination. All responses within the dataset are\ndistilled from reasoning models (predominantly DeepSeek-R1) and have undergone\nrigorous verification procedures. Mathematical problems are validated by\nchecking against reference answers, code problems are verified using test\ncases, and other tasks are evaluated with the aid of a reward model. The\nAM-Distill-Qwen-32B model, which was trained through only simple Supervised\nFine-Tuning (SFT) using this batch of data, outperformed the\nDeepSeek-R1-Distill-Qwen-32B model on four benchmarks: AIME2024, MATH-500,\nGPQA-Diamond, and LiveCodeBench. Additionally, the AM-Distill-Qwen-72B model\nsurpassed the DeepSeek-R1-Distill-Llama-70B model on all benchmarks as well. We\nare releasing these 1.4 million problems and their corresponding responses to\nthe research community with the objective of fostering the development of\npowerful reasoning-oriented Large Language Models (LLMs). The dataset was\npublished in\n\\href{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M}{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The AM-DeepSeek-R1-Distilled is a large-scale dataset with thinking traces\nfor general reasoning tasks, composed of high-quality and challenging reasoning\nproblems. These problems are collected from a multitude of open-source\ndatasets, subjected to semantic deduplication and meticulous cleaning to\neliminate test set contamination. All responses within the dataset are\ndistilled from reasoning models (predominantly DeepSeek-R1) and have undergone\nrigorous verification procedures. Mathematical problems are validated by\nchecking against reference answers, code problems are verified using test\ncases, and other tasks are evaluated with the aid of a reward model. The\nAM-Distill-Qwen-32B model, which was trained through only simple Supervised\nFine-Tuning (SFT) using this batch of data, outperformed the\nDeepSeek-R1-Distill-Qwen-32B model on four benchmarks: AIME2024, MATH-500,\nGPQA-Diamond, and LiveCodeBench. Additionally, the AM-Distill-Qwen-72B model\nsurpassed the DeepSeek-R1-Distill-Llama-70B model on all benchmarks as well. We\nare releasing these 1.4 million problems and their corresponding responses to\nthe research community with the objective of fostering the development of\npowerful reasoning-oriented Large Language Models (LLMs). The dataset was\npublished in\n\\href{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M}{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M}."
                },
                "authors": [
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yiping Peng"
                    },
                    {
                        "name": "Sitong Zhao"
                    },
                    {
                        "name": "Xiaoyu Tian"
                    },
                    {
                        "name": "Shuaiting Chen"
                    },
                    {
                        "name": "Yunjie Ji"
                    },
                    {
                        "name": "Xiangang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangang Li"
                },
                "author": "Xiangang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19633v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19633v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03766v2",
                "updated": "2025-03-25T13:13:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    13,
                    51,
                    1,
                    84,
                    0
                ],
                "published": "2025-02-06T04:01:27Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    1,
                    27,
                    3,
                    37,
                    0
                ],
                "title": "Hierarchical Contextual Manifold Alignment for Structuring Latent\n  Representations in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Contextual Manifold Alignment for Structuring Latent\n  Representations in Large Language Models"
                },
                "summary": "The organization of latent token representations plays a crucial role in\ndetermining the stability, generalization, and contextual consistency of\nlanguage models, yet conventional approaches to embedding refinement often rely\non parameter modifications that introduce additional computational overhead. A\nhierarchical alignment method was introduced to restructure token embeddings\nwithout altering core model weights, ensuring that representational\ndistributions maintained coherence across different linguistic contexts.\nExperimental evaluations demonstrated improvements in rare token retrieval,\nadversarial robustness, and long-range dependency tracking, highlighting the\nadvantages of hierarchical structuring in mitigating inconsistencies in latent\nspace organization. The comparative analysis against conventional fine-tuning\nand embedding perturbation methods revealed that hierarchical restructuring\nmaintained computational efficiency while achieving measurable gains in\nrepresentation quality. Structural refinements introduced through the alignment\nprocess resulted in improved contextual stability across varied linguistic\ntasks, reducing inconsistencies in token proximity relationships and enhancing\ninterpretability in language generation. A detailed computational assessment\nconfirmed that the realignment process introduced minimal inference overhead,\nensuring that representational improvements did not compromise model\nefficiency. The findings reinforced the broader significance of structured\nrepresentation learning, illustrating that hierarchical embedding modifications\ncould serve as an effective strategy for refining latent space distributions\nwhile preserving pre-learned semantic associations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The organization of latent token representations plays a crucial role in\ndetermining the stability, generalization, and contextual consistency of\nlanguage models, yet conventional approaches to embedding refinement often rely\non parameter modifications that introduce additional computational overhead. A\nhierarchical alignment method was introduced to restructure token embeddings\nwithout altering core model weights, ensuring that representational\ndistributions maintained coherence across different linguistic contexts.\nExperimental evaluations demonstrated improvements in rare token retrieval,\nadversarial robustness, and long-range dependency tracking, highlighting the\nadvantages of hierarchical structuring in mitigating inconsistencies in latent\nspace organization. The comparative analysis against conventional fine-tuning\nand embedding perturbation methods revealed that hierarchical restructuring\nmaintained computational efficiency while achieving measurable gains in\nrepresentation quality. Structural refinements introduced through the alignment\nprocess resulted in improved contextual stability across varied linguistic\ntasks, reducing inconsistencies in token proximity relationships and enhancing\ninterpretability in language generation. A detailed computational assessment\nconfirmed that the realignment process introduced minimal inference overhead,\nensuring that representational improvements did not compromise model\nefficiency. The findings reinforced the broader significance of structured\nrepresentation learning, illustrating that hierarchical embedding modifications\ncould serve as an effective strategy for refining latent space distributions\nwhile preserving pre-learned semantic associations."
                },
                "authors": [
                    {
                        "name": "Meiquan Dong"
                    },
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Yan Huang"
                    },
                    {
                        "name": "Zixuan Feng"
                    },
                    {
                        "name": "Jianhong Tang"
                    },
                    {
                        "name": "Ruoxi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ruoxi Wang"
                },
                "author": "Ruoxi Wang",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03102v2",
                "updated": "2025-03-25T13:12:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    12,
                    11,
                    1,
                    84,
                    0
                ],
                "published": "2025-02-05T11:59:22Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    11,
                    59,
                    22,
                    2,
                    36,
                    0
                ],
                "title": "Structured Token Retention and Computational Memory Paths in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured Token Retention and Computational Memory Paths in Large\n  Language Models"
                },
                "summary": "Memory retention mechanisms play a central role in determining the efficiency\nof computational architectures designed for processing extended sequences.\nConventional methods for token management often impose fixed retention\nthresholds or rely on uniform attention weight distributions, leading to\ninefficient memory utilization and premature information loss in extended\nsequence modeling. Structured Token Retention (STR) introduces a probabilistic\nselection framework that dynamically adjusts token persistence based on\ncontextual significance, ensuring that computational resources are allocated to\nsemantically relevant elements. Computational Memory Paths (CMP) extend this\nframework through hierarchical memory allocation, refining retention efficiency\nthrough structured reallocation of token embeddings. Comparative assessments\nagainst baseline models demonstrate that STR and CMP improve token survival\nrates across long input sequences while reducing cumulative error propagation\nacross processing layers. Experimental results further indicate reductions in\ncomputational overhead, improving inference speed without degrading contextual\ncoherence. Token distribution analyses reveal that structured memory allocation\nprevents excessive redundancy in attention weight calculations, optimizing\ninformation retrieval efficiency in large-scale generative architectures. The\nintegration of STR and CMP into an open-source model illustrates the\nadaptability of structured memory retention methodologies, highlighting their\napplicability in generative text processing, long-context comprehension, and\nscalable sequence modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory retention mechanisms play a central role in determining the efficiency\nof computational architectures designed for processing extended sequences.\nConventional methods for token management often impose fixed retention\nthresholds or rely on uniform attention weight distributions, leading to\ninefficient memory utilization and premature information loss in extended\nsequence modeling. Structured Token Retention (STR) introduces a probabilistic\nselection framework that dynamically adjusts token persistence based on\ncontextual significance, ensuring that computational resources are allocated to\nsemantically relevant elements. Computational Memory Paths (CMP) extend this\nframework through hierarchical memory allocation, refining retention efficiency\nthrough structured reallocation of token embeddings. Comparative assessments\nagainst baseline models demonstrate that STR and CMP improve token survival\nrates across long input sequences while reducing cumulative error propagation\nacross processing layers. Experimental results further indicate reductions in\ncomputational overhead, improving inference speed without degrading contextual\ncoherence. Token distribution analyses reveal that structured memory allocation\nprevents excessive redundancy in attention weight calculations, optimizing\ninformation retrieval efficiency in large-scale generative architectures. The\nintegration of STR and CMP into an open-source model illustrates the\nadaptability of structured memory retention methodologies, highlighting their\napplicability in generative text processing, long-context comprehension, and\nscalable sequence modeling."
                },
                "authors": [
                    {
                        "name": "Jonathan Delena"
                    },
                    {
                        "name": "Augustin Moreau"
                    },
                    {
                        "name": "Dominic Ravensdale"
                    },
                    {
                        "name": "Frederick Chatterton"
                    }
                ],
                "author_detail": {
                    "name": "Frederick Chatterton"
                },
                "author": "Frederick Chatterton",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02730v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02730v2",
                "updated": "2025-03-25T13:11:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    11,
                    30,
                    1,
                    84,
                    0
                ],
                "published": "2025-02-04T21:27:58Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    21,
                    27,
                    58,
                    1,
                    35,
                    0
                ],
                "title": "Semantic Entanglement-Based Ransomware Detection via Probabilistic\n  Latent Encryption Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Entanglement-Based Ransomware Detection via Probabilistic\n  Latent Encryption Mapping"
                },
                "summary": "Encryption-based attacks have introduced significant challenges for detection\nmechanisms that rely on predefined signatures, heuristic indicators, or static\nrule-based classifications. Probabilistic Latent Encryption Mapping presents an\nalternative detection framework that models ransomware-induced encryption\nbehaviors through statistical representations of entropy deviations and\nprobabilistic dependencies in execution traces. Unlike conventional approaches\nthat depend on explicit bytecode analysis or predefined cryptographic function\ncall monitoring, probabilistic inference techniques classify encryption\nanomalies based on their underlying statistical characteristics, ensuring\ngreater adaptability to polymorphic attack strategies. Evaluations demonstrate\nthat entropy-driven classification reduces false positive rates while\nmaintaining high detection accuracy across diverse ransomware families and\nencryption methodologies. Experimental results further highlight the\nframework's ability to differentiate between benign encryption workflows and\nadversarial cryptographic manipulations, ensuring that classification\nperformance remains effective across cloud-based and localized execution\nenvironments. Benchmark comparisons illustrate that probabilistic modeling\nexhibits advantages over heuristic and machine learning-based detection\napproaches, particularly in handling previously unseen encryption techniques\nand adversarial obfuscation strategies. Computational efficiency analysis\nconfirms that detection latency remains within operational feasibility\nconstraints, reinforcing the viability of probabilistic encryption\nclassification for real-time security infrastructures. The ability to\nsystematically infer encryption-induced deviations without requiring static\nattack signatures strengthens detection robustness against adversarial evasion\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Encryption-based attacks have introduced significant challenges for detection\nmechanisms that rely on predefined signatures, heuristic indicators, or static\nrule-based classifications. Probabilistic Latent Encryption Mapping presents an\nalternative detection framework that models ransomware-induced encryption\nbehaviors through statistical representations of entropy deviations and\nprobabilistic dependencies in execution traces. Unlike conventional approaches\nthat depend on explicit bytecode analysis or predefined cryptographic function\ncall monitoring, probabilistic inference techniques classify encryption\nanomalies based on their underlying statistical characteristics, ensuring\ngreater adaptability to polymorphic attack strategies. Evaluations demonstrate\nthat entropy-driven classification reduces false positive rates while\nmaintaining high detection accuracy across diverse ransomware families and\nencryption methodologies. Experimental results further highlight the\nframework's ability to differentiate between benign encryption workflows and\nadversarial cryptographic manipulations, ensuring that classification\nperformance remains effective across cloud-based and localized execution\nenvironments. Benchmark comparisons illustrate that probabilistic modeling\nexhibits advantages over heuristic and machine learning-based detection\napproaches, particularly in handling previously unseen encryption techniques\nand adversarial obfuscation strategies. Computational efficiency analysis\nconfirms that detection latency remains within operational feasibility\nconstraints, reinforcing the viability of probabilistic encryption\nclassification for real-time security infrastructures. The ability to\nsystematically infer encryption-induced deviations without requiring static\nattack signatures strengthens detection robustness against adversarial evasion\ntechniques."
                },
                "authors": [
                    {
                        "name": "Mohammad Eisa"
                    },
                    {
                        "name": "Quentin Yardley"
                    },
                    {
                        "name": "Rafael Witherspoon"
                    },
                    {
                        "name": "Harriet Pendlebury"
                    },
                    {
                        "name": "Clement Rutherford"
                    }
                ],
                "author_detail": {
                    "name": "Clement Rutherford"
                },
                "author": "Clement Rutherford",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02730v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02730v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19620v1",
                "updated": "2025-03-25T13:08:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    8,
                    46,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T13:08:46Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    8,
                    46,
                    1,
                    84,
                    0
                ],
                "title": "Optimization through In-Context Learning and Iterative LLM Prompting for\n  Nuclear Engineering Design Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization through In-Context Learning and Iterative LLM Prompting for\n  Nuclear Engineering Design Problems"
                },
                "summary": "The optimization of nuclear engineering designs, such as nuclear fuel\nassembly configurations, involves managing competing objectives like reactivity\ncontrol and power distribution. This study explores the use of Optimization by\nPrompting, an iterative approach utilizing large language models (LLMs), to\naddress these challenges. The method is straightforward to implement, requiring\nno hyperparameter tuning or complex mathematical formulations. Optimization\nproblems can be described in plain English, with only an evaluator and a\nparsing script needed for execution. The in-context learning capabilities of\nLLMs enable them to understand problem nuances, therefore, they have the\npotential to surpass traditional metaheuristic optimization methods. This study\ndemonstrates the application of LLMs as optimizers to Boiling Water Reactor\n(BWR) fuel lattice design, showing the capability of commercial LLMs to achieve\nsuperior optimization results compared to traditional methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The optimization of nuclear engineering designs, such as nuclear fuel\nassembly configurations, involves managing competing objectives like reactivity\ncontrol and power distribution. This study explores the use of Optimization by\nPrompting, an iterative approach utilizing large language models (LLMs), to\naddress these challenges. The method is straightforward to implement, requiring\nno hyperparameter tuning or complex mathematical formulations. Optimization\nproblems can be described in plain English, with only an evaluator and a\nparsing script needed for execution. The in-context learning capabilities of\nLLMs enable them to understand problem nuances, therefore, they have the\npotential to surpass traditional metaheuristic optimization methods. This study\ndemonstrates the application of LLMs as optimizers to Boiling Water Reactor\n(BWR) fuel lattice design, showing the capability of commercial LLMs to achieve\nsuperior optimization results compared to traditional methods."
                },
                "authors": [
                    {
                        "name": "M. Rizki Oktavian"
                    },
                    {
                        "name": "Anirudh Tunga"
                    },
                    {
                        "name": "Amandeep Bakshi"
                    },
                    {
                        "name": "Michael J. Mueterthies"
                    },
                    {
                        "name": "J. Thomas Gruenwald"
                    },
                    {
                        "name": "Jonathan Nistor"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Nistor"
                },
                "author": "Jonathan Nistor",
                "arxiv_comment": "Codes and data are available upon request",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19619v1",
                "updated": "2025-03-25T13:08:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    8,
                    26,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T13:08:26Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    8,
                    26,
                    1,
                    84,
                    0
                ],
                "title": "Exploring Next Token Prediction For Optimizing Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Next Token Prediction For Optimizing Databases"
                },
                "summary": "The Next Token Prediction paradigm (NTP, for short) lies at the forefront of\nmodern large foundational models that are pre-trained on diverse and large\ndatasets. These models generalize effectively and have proven to be very\nsuccessful in Natural Language Processing (NLP). Inspired by the generalization\ncapabilities of Large Language Models (LLMs), we investigate whether the same\nNTP paradigm can also be applied to DBMS design and optimization tasks.\nAdopting NTP directly for database optimization is non-trivial due to the\nfundamental differences between the domains. In this paper, we present a\nframework termed Probe and Learn (PoLe) for applying NTP to optimize database\nsystems. PoLe leverages Decision Transformers and hardware-generated tokens to\neffectively incorporate NTP into database systems. Preliminary results from the\nmain-memory index scheduling task demonstrate that adopting NTP can improve\nboth performance and generalizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Next Token Prediction paradigm (NTP, for short) lies at the forefront of\nmodern large foundational models that are pre-trained on diverse and large\ndatasets. These models generalize effectively and have proven to be very\nsuccessful in Natural Language Processing (NLP). Inspired by the generalization\ncapabilities of Large Language Models (LLMs), we investigate whether the same\nNTP paradigm can also be applied to DBMS design and optimization tasks.\nAdopting NTP directly for database optimization is non-trivial due to the\nfundamental differences between the domains. In this paper, we present a\nframework termed Probe and Learn (PoLe) for applying NTP to optimize database\nsystems. PoLe leverages Decision Transformers and hardware-generated tokens to\neffectively incorporate NTP into database systems. Preliminary results from the\nmain-memory index scheduling task demonstrate that adopting NTP can improve\nboth performance and generalizability."
                },
                "authors": [
                    {
                        "name": "Yeasir Rayhan"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19618v1",
                "updated": "2025-03-25T13:03:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    3,
                    9,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T13:03:09Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    3,
                    9,
                    1,
                    84,
                    0
                ],
                "title": "Learning to chain-of-thought with Jensen's evidence lower bound",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to chain-of-thought with Jensen's evidence lower bound"
                },
                "summary": "We propose a way to optimize chain-of-thought with reinforcement learning,\nbut without external reward function. Our algorithm relies on viewing\nchain-of-thought as latent variable as part of a probabilistic inference\nproblem. Contrary to the full evidence lower bound, we propose to apply a much\nsimpler Jensen's lower bound, which derives tractable objectives with simple\nalgorithmic components (e.g., without the need for parametric approximate\nposterior), making it more conducive to modern large-scale training. The lower\nbound approach naturally interpolates other methods such as supervised\nfine-tuning and online reinforcement learning, whose practical trade-offs we\nwill illustrate. Finally, we show that on mathematical reasoning problems,\noptimizing with Jensen's lower bound is as effective as policy gradient with\nexternal reward. Taken together, our results showcase as a proof of concept to\nthis new algorithmic paradigm's potential to more generic applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a way to optimize chain-of-thought with reinforcement learning,\nbut without external reward function. Our algorithm relies on viewing\nchain-of-thought as latent variable as part of a probabilistic inference\nproblem. Contrary to the full evidence lower bound, we propose to apply a much\nsimpler Jensen's lower bound, which derives tractable objectives with simple\nalgorithmic components (e.g., without the need for parametric approximate\nposterior), making it more conducive to modern large-scale training. The lower\nbound approach naturally interpolates other methods such as supervised\nfine-tuning and online reinforcement learning, whose practical trade-offs we\nwill illustrate. Finally, we show that on mathematical reasoning problems,\noptimizing with Jensen's lower bound is as effective as policy gradient with\nexternal reward. Taken together, our results showcase as a proof of concept to\nthis new algorithmic paradigm's potential to more generic applications."
                },
                "authors": [
                    {
                        "name": "Yunhao Tang"
                    },
                    {
                        "name": "Sid Wang"
                    },
                    {
                        "name": "Rémi Munos"
                    }
                ],
                "author_detail": {
                    "name": "Rémi Munos"
                },
                "author": "Rémi Munos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00758v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00758v2",
                "updated": "2025-03-25T13:00:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    0,
                    6,
                    1,
                    84,
                    0
                ],
                "published": "2025-02-02T11:45:35Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    11,
                    45,
                    35,
                    6,
                    33,
                    0
                ],
                "title": "Structural Latency Perturbation in Large Language Models Through\n  Recursive State Induction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structural Latency Perturbation in Large Language Models Through\n  Recursive State Induction"
                },
                "summary": "Computational efficiency has remained a critical consideration in scaling\nhigh-capacity language models, with inference latency and resource consumption\npresenting significant constraints on real-time applications. The study has\nintroduced a structured latency perturbation mechanism that modifies\ncomputational pathways through recursive state induction, enabling dynamic\nsuppression of redundant activations while preserving generative fidelity. A\nformal mathematical framework has been established to describe recursive\nperturbations, ensuring that modifications remain adaptive rather than\nstatically imposed. Experiments have demonstrated that applying recursive state\nadjustments reduces inference latency across varying sequence lengths, with\nlonger text generations benefiting from cumulative efficiency improvements.\nComparative evaluations against structured pruning and quantization have\nindicated that latency gains can be achieved without compromising token\nretention or memory utilization. The analysis of computational overhead has\nsuggested that selectively suppressing redundant activations contributes to\nimproved power efficiency, particularly in scenarios requiring extended text\ngeneration. An assessment of linguistic stability has shown that token-level\nconsistency remains largely intact under controlled perturbation thresholds,\nreinforcing the viability of structural latency modifications as an alternative\nto weight-centric optimization techniques. The results have supported the\nhypothesis that recursive state induction offers an effective method for\nreducing computational complexity without requiring architectural modifications\nor external augmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational efficiency has remained a critical consideration in scaling\nhigh-capacity language models, with inference latency and resource consumption\npresenting significant constraints on real-time applications. The study has\nintroduced a structured latency perturbation mechanism that modifies\ncomputational pathways through recursive state induction, enabling dynamic\nsuppression of redundant activations while preserving generative fidelity. A\nformal mathematical framework has been established to describe recursive\nperturbations, ensuring that modifications remain adaptive rather than\nstatically imposed. Experiments have demonstrated that applying recursive state\nadjustments reduces inference latency across varying sequence lengths, with\nlonger text generations benefiting from cumulative efficiency improvements.\nComparative evaluations against structured pruning and quantization have\nindicated that latency gains can be achieved without compromising token\nretention or memory utilization. The analysis of computational overhead has\nsuggested that selectively suppressing redundant activations contributes to\nimproved power efficiency, particularly in scenarios requiring extended text\ngeneration. An assessment of linguistic stability has shown that token-level\nconsistency remains largely intact under controlled perturbation thresholds,\nreinforcing the viability of structural latency modifications as an alternative\nto weight-centric optimization techniques. The results have supported the\nhypothesis that recursive state induction offers an effective method for\nreducing computational complexity without requiring architectural modifications\nor external augmentation."
                },
                "authors": [
                    {
                        "name": "Michael Mangrum"
                    },
                    {
                        "name": "Jonathan Pemberton"
                    },
                    {
                        "name": "Benedict Wetherby"
                    },
                    {
                        "name": "Philip Montague"
                    }
                ],
                "author_detail": {
                    "name": "Philip Montague"
                },
                "author": "Philip Montague",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00758v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00758v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18957v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18957v2",
                "updated": "2025-03-25T12:59:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    59,
                    28,
                    1,
                    84,
                    0
                ],
                "published": "2025-01-31T08:32:32Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    8,
                    32,
                    32,
                    4,
                    31,
                    0
                ],
                "title": "Intrinsic Tensor Field Propagation in Large Language Models: A Novel\n  Approach to Contextual Information Flow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intrinsic Tensor Field Propagation in Large Language Models: A Novel\n  Approach to Contextual Information Flow"
                },
                "summary": "Context propagation remains a central challenge in language model\narchitectures, particularly in tasks requiring the retention of long-range\ndependencies. Conventional attention mechanisms, while effective in many\napplications, exhibit limitations in maintaining coherent contextual\nrepresentations over extended sequences due to their reliance on discrete token\ninteractions. A novel approach is introduced through the formulation of\nIntrinsic Tensor Field Propagation (ITFP), which models contextual\nrelationships as continuous tensor fields distributed across token embeddings.\nThe propagation dynamics are governed through differential equations that\nenable a structured flow of contextual information, augmenting the standard\nattention mechanism to enhance coherence and recall. A series of experiments\nconducted on an open-source transformer-based model demonstrate that ITFP\nprovides measurable improvements in contextual retention, dependency\nresolution, and inference stability across various linguistic structures.\nComparisons with baseline models reveal a reduction in syntactic\ninconsistencies and factual errors, while ablation studies indicate that the\nchoice of propagation depth and integration strength significantly impacts\nmodel performance. Additional evaluations assessing domain generalization\nsuggest that ITFP effectively adapts across different text genres, reinforcing\nits applicability beyond conventional language modeling tasks. Although\ncomputational trade-offs are introduced through the inclusion of tensor field\ncomputations, empirical findings suggest that the benefits in accuracy and\ncoherence outweigh the increased processing demands.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context propagation remains a central challenge in language model\narchitectures, particularly in tasks requiring the retention of long-range\ndependencies. Conventional attention mechanisms, while effective in many\napplications, exhibit limitations in maintaining coherent contextual\nrepresentations over extended sequences due to their reliance on discrete token\ninteractions. A novel approach is introduced through the formulation of\nIntrinsic Tensor Field Propagation (ITFP), which models contextual\nrelationships as continuous tensor fields distributed across token embeddings.\nThe propagation dynamics are governed through differential equations that\nenable a structured flow of contextual information, augmenting the standard\nattention mechanism to enhance coherence and recall. A series of experiments\nconducted on an open-source transformer-based model demonstrate that ITFP\nprovides measurable improvements in contextual retention, dependency\nresolution, and inference stability across various linguistic structures.\nComparisons with baseline models reveal a reduction in syntactic\ninconsistencies and factual errors, while ablation studies indicate that the\nchoice of propagation depth and integration strength significantly impacts\nmodel performance. Additional evaluations assessing domain generalization\nsuggest that ITFP effectively adapts across different text genres, reinforcing\nits applicability beyond conventional language modeling tasks. Although\ncomputational trade-offs are introduced through the inclusion of tensor field\ncomputations, empirical findings suggest that the benefits in accuracy and\ncoherence outweigh the increased processing demands."
                },
                "authors": [
                    {
                        "name": "Alfred Bexley"
                    },
                    {
                        "name": "Lukas Radcliffe"
                    },
                    {
                        "name": "Giles Weatherstone"
                    },
                    {
                        "name": "Joseph Sakau"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Sakau"
                },
                "author": "Joseph Sakau",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18957v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18957v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13999v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13999v2",
                "updated": "2025-03-25T12:59:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    59,
                    14,
                    1,
                    84,
                    0
                ],
                "published": "2025-01-23T11:34:04Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    34,
                    4,
                    3,
                    23,
                    0
                ],
                "title": "Framework for Progressive Knowledge Fusion in Large Language Models\n  Through Structured Conceptual Redundancy Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Framework for Progressive Knowledge Fusion in Large Language Models\n  Through Structured Conceptual Redundancy Analysis"
                },
                "summary": "The organization of latent knowledge within large-scale models poses unique\nchallenges when addressing overlapping representations and optimizing\ncontextual accuracy. Conceptual redundancies embedded across layers often\nresult in inefficiencies that affect both computational demands and\ntask-specific outcomes. A framework was proposed to restructure these\nredundancies through advanced clustering techniques and dynamic thresholding,\nensuring that critical semantic relationships are preserved while removing\nunnecessary overlaps. Evaluations revealed improved memory efficiency and\nfaster inference times, alongside better alignment in latent knowledge clusters\nthat enhanced interpretability. Improvements in error rates and adversarial\nrobustness suggest that restructuring redundancies has broader implications for\nincreasing model reliability across diverse applications. Comparative analyses\nhighlighted reductions in resource consumption and notable gains in\nperformance, particularly in translation and summarization tasks. Energy\nmetrics demonstrated significant savings during training phases, further\nvalidating the practicality of the approach for real-world deployments.\nRepresentational fidelity was also enhanced, with latent space evaluations\nindicating better cluster alignment and higher semantic consistency. The\nmethodology bridges a key gap in model optimization through directly addressing\nredundancies at the structural level. Its application opens avenues for\nscalable, efficient, and contextually aware systems that can adapt to complex,\ndomain-specific tasks without compromising on performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The organization of latent knowledge within large-scale models poses unique\nchallenges when addressing overlapping representations and optimizing\ncontextual accuracy. Conceptual redundancies embedded across layers often\nresult in inefficiencies that affect both computational demands and\ntask-specific outcomes. A framework was proposed to restructure these\nredundancies through advanced clustering techniques and dynamic thresholding,\nensuring that critical semantic relationships are preserved while removing\nunnecessary overlaps. Evaluations revealed improved memory efficiency and\nfaster inference times, alongside better alignment in latent knowledge clusters\nthat enhanced interpretability. Improvements in error rates and adversarial\nrobustness suggest that restructuring redundancies has broader implications for\nincreasing model reliability across diverse applications. Comparative analyses\nhighlighted reductions in resource consumption and notable gains in\nperformance, particularly in translation and summarization tasks. Energy\nmetrics demonstrated significant savings during training phases, further\nvalidating the practicality of the approach for real-world deployments.\nRepresentational fidelity was also enhanced, with latent space evaluations\nindicating better cluster alignment and higher semantic consistency. The\nmethodology bridges a key gap in model optimization through directly addressing\nredundancies at the structural level. Its application opens avenues for\nscalable, efficient, and contextually aware systems that can adapt to complex,\ndomain-specific tasks without compromising on performance."
                },
                "authors": [
                    {
                        "name": "Joseph Sakau"
                    },
                    {
                        "name": "Evander Kozlowski"
                    },
                    {
                        "name": "Roderick Thistledown"
                    },
                    {
                        "name": "Basil Steinberger"
                    }
                ],
                "author_detail": {
                    "name": "Basil Steinberger"
                },
                "author": "Basil Steinberger",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13999v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13999v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08026v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08026v2",
                "updated": "2025-03-25T12:58:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    58,
                    31,
                    1,
                    84,
                    0
                ],
                "published": "2025-02-12T00:00:37Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    0,
                    0,
                    37,
                    2,
                    43,
                    0
                ],
                "title": "Contextual Subspace Manifold Projection for Structural Refinement of\n  Large Language Model Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual Subspace Manifold Projection for Structural Refinement of\n  Large Language Model Representations"
                },
                "summary": "Internal representations within deep neural architectures encode\nhigh-dimensional abstractions of linguistic structures, yet they often exhibit\ninefficiencies in feature distribution, limiting expressiveness and\nadaptability. Contextual Subspace Manifold Projection introduces a structured\nrefinement technique that selectively reconfigures token embeddings through\ncontrolled subspace constraints, ensuring more stable and geometrically\nwell-defined feature distributions. Empirical evaluations demonstrated that the\nstructured intervention reduced anisotropy, leading to improved representation\ncompactness while preserving semantic fidelity across transformer layers.\nClustering analyses indicated that token embeddings exhibited greater feature\nseparability, reinforcing the hypothesis that structured projection techniques\nenhance internal representation organization without sacrificing linguistic\ncoherence. Gradient magnitude distributions suggested that the method\nintroduced a smoother optimization trajectory, potentially contributing to more\nstable parameter updates throughout training. Computational overhead associated\nwith the projection operations remained minimal, ensuring that the refinements\ndid not introduce significant trade-offs in model efficiency or inference\nspeed. Comparisons with standard embedding refinement techniques highlighted\nthat structured manifold constraints provided a direct mechanism for improving\nrepresentation quality without requiring additional gradient-based\noptimization. Perplexity evaluations confirmed that the adjustments did not\nnegatively impact sequence coherence, further validating the effectiveness of\nthe proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internal representations within deep neural architectures encode\nhigh-dimensional abstractions of linguistic structures, yet they often exhibit\ninefficiencies in feature distribution, limiting expressiveness and\nadaptability. Contextual Subspace Manifold Projection introduces a structured\nrefinement technique that selectively reconfigures token embeddings through\ncontrolled subspace constraints, ensuring more stable and geometrically\nwell-defined feature distributions. Empirical evaluations demonstrated that the\nstructured intervention reduced anisotropy, leading to improved representation\ncompactness while preserving semantic fidelity across transformer layers.\nClustering analyses indicated that token embeddings exhibited greater feature\nseparability, reinforcing the hypothesis that structured projection techniques\nenhance internal representation organization without sacrificing linguistic\ncoherence. Gradient magnitude distributions suggested that the method\nintroduced a smoother optimization trajectory, potentially contributing to more\nstable parameter updates throughout training. Computational overhead associated\nwith the projection operations remained minimal, ensuring that the refinements\ndid not introduce significant trade-offs in model efficiency or inference\nspeed. Comparisons with standard embedding refinement techniques highlighted\nthat structured manifold constraints provided a direct mechanism for improving\nrepresentation quality without requiring additional gradient-based\noptimization. Perplexity evaluations confirmed that the adjustments did not\nnegatively impact sequence coherence, further validating the effectiveness of\nthe proposed approach."
                },
                "authors": [
                    {
                        "name": "Alistair Wren"
                    },
                    {
                        "name": "Beatrice Loxley"
                    },
                    {
                        "name": "Hamish Cadwallader"
                    },
                    {
                        "name": "Simon Beckwith"
                    },
                    {
                        "name": "Fabian Pargeter"
                    },
                    {
                        "name": "James Blades"
                    }
                ],
                "author_detail": {
                    "name": "James Blades"
                },
                "author": "James Blades",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08026v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08026v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18205v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18205v2",
                "updated": "2025-03-25T12:58:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    58,
                    16,
                    1,
                    84,
                    0
                ],
                "published": "2025-01-30T08:51:48Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    8,
                    51,
                    48,
                    3,
                    30,
                    0
                ],
                "title": "Contextually Structured Token Dependency Encoding for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextually Structured Token Dependency Encoding for Large Language\n  Models"
                },
                "summary": "Token representation strategies within large-scale neural architectures often\nrely on contextually refined embeddings, yet conventional approaches seldom\nencode structured relationships explicitly within token interactions.\nSelf-attention mechanisms effectively capture dynamic contextual dependencies,\nbut their reliance on learned weight distributions limits the preservation of\nlong-range hierarchical structures in generated sequences. Dependency-aware\ntoken encoding introduces a structured approach to embedding initialization,\nensuring that relational constraints are embedded within token representations\nrather than inferred solely through attention dynamics. The proposed encoding\nmechanism refines token interactions through dependency-weighted attention\ncomputations, ensuring that syntactic and semantic dependencies are retained\nacross multiple processing layers. Empirical evaluations indicate reductions in\nperplexity across diverse linguistic benchmarks, suggesting improvements in\ncontextual coherence and predictive consistency in autoregressive text\ngeneration. Computational efficiency assessments reveal a moderate increase in\nmemory consumption and training time, attributed to additional matrix\ncomputations within the encoding module, yet scalability remains feasible\nwithin conventional transformer architectures. Structured encoding enhances\nlexical variation and dependency retention, reinforcing linguistic coherence\nwithout requiring external syntactic annotations or auxiliary training\nobjectives. Statistical comparisons highlight improvements in dependency\nalignment, particularly in longer sequences where conventional self-attention\nmodels exhibit degradation in hierarchical consistency. Sentence length\ndistributions indicate a reduction in abrupt phrase transitions, further\nsupporting the hypothesis that explicit dependency encoding facilitates more\nstructured phrase generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token representation strategies within large-scale neural architectures often\nrely on contextually refined embeddings, yet conventional approaches seldom\nencode structured relationships explicitly within token interactions.\nSelf-attention mechanisms effectively capture dynamic contextual dependencies,\nbut their reliance on learned weight distributions limits the preservation of\nlong-range hierarchical structures in generated sequences. Dependency-aware\ntoken encoding introduces a structured approach to embedding initialization,\nensuring that relational constraints are embedded within token representations\nrather than inferred solely through attention dynamics. The proposed encoding\nmechanism refines token interactions through dependency-weighted attention\ncomputations, ensuring that syntactic and semantic dependencies are retained\nacross multiple processing layers. Empirical evaluations indicate reductions in\nperplexity across diverse linguistic benchmarks, suggesting improvements in\ncontextual coherence and predictive consistency in autoregressive text\ngeneration. Computational efficiency assessments reveal a moderate increase in\nmemory consumption and training time, attributed to additional matrix\ncomputations within the encoding module, yet scalability remains feasible\nwithin conventional transformer architectures. Structured encoding enhances\nlexical variation and dependency retention, reinforcing linguistic coherence\nwithout requiring external syntactic annotations or auxiliary training\nobjectives. Statistical comparisons highlight improvements in dependency\nalignment, particularly in longer sequences where conventional self-attention\nmodels exhibit degradation in hierarchical consistency. Sentence length\ndistributions indicate a reduction in abrupt phrase transitions, further\nsupporting the hypothesis that explicit dependency encoding facilitates more\nstructured phrase generation."
                },
                "authors": [
                    {
                        "name": "James Blades"
                    },
                    {
                        "name": "Frederick Somerfield"
                    },
                    {
                        "name": "William Langley"
                    },
                    {
                        "name": "Susan Everingham"
                    },
                    {
                        "name": "Maurice Witherington"
                    }
                ],
                "author_detail": {
                    "name": "Maurice Witherington"
                },
                "author": "Maurice Witherington",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18205v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18205v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15405v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15405v2",
                "updated": "2025-03-25T12:55:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    55,
                    17,
                    1,
                    84,
                    0
                ],
                "published": "2025-01-26T05:17:04Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    5,
                    17,
                    4,
                    6,
                    26,
                    0
                ],
                "title": "Semantic Layered Embedding Diffusion in Large Language Models for\n  Multi-Contextual Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Layered Embedding Diffusion in Large Language Models for\n  Multi-Contextual Consistency"
                },
                "summary": "The Semantic Layered Embedding Diffusion (SLED) mechanism redefines the\nrepresentation of hierarchical semantics within transformer-based\narchitectures, enabling enhanced contextual consistency across a wide array of\nlinguistic tasks. By introducing a multi-layered diffusion process grounded in\nspectral analysis, it achieves a complex balance between global and local\nsemantic coherence. Experimental results demonstrate significant improvements\nin perplexity and BLEU scores, emphasizing the mechanism's ability to adapt\neffectively across diverse domains, including multilingual and cross-domain\ntext generation. A rigorous mathematical framework underpins the embedding\ndiffusion process, incorporating weighted adjacency matrices, kernel-based\nrefinements, and dynamic layer-wise normalization. Error distribution analysis\nreveals that SLED addresses challenges in semantic alignment and coherence,\noutperforming baseline approaches across varied benchmarks. Scalability studies\nillustrate that its performance gains are maintained consistently across\ndifferent model sizes, reflecting a practical balance between computational\nefficiency and linguistic precision. The implementation also achieves energy\nefficiency, reducing resource consumption during training and inference phases\nwithout compromising accuracy. Qualitative case studies further validate its\nadaptability to extended narratives and context-intensive scenarios,\nhighlighting the mechanism's potential for real-world applications. SLED offers\na different perspective on embedding design and its implications for advancing\nlanguage modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Semantic Layered Embedding Diffusion (SLED) mechanism redefines the\nrepresentation of hierarchical semantics within transformer-based\narchitectures, enabling enhanced contextual consistency across a wide array of\nlinguistic tasks. By introducing a multi-layered diffusion process grounded in\nspectral analysis, it achieves a complex balance between global and local\nsemantic coherence. Experimental results demonstrate significant improvements\nin perplexity and BLEU scores, emphasizing the mechanism's ability to adapt\neffectively across diverse domains, including multilingual and cross-domain\ntext generation. A rigorous mathematical framework underpins the embedding\ndiffusion process, incorporating weighted adjacency matrices, kernel-based\nrefinements, and dynamic layer-wise normalization. Error distribution analysis\nreveals that SLED addresses challenges in semantic alignment and coherence,\noutperforming baseline approaches across varied benchmarks. Scalability studies\nillustrate that its performance gains are maintained consistently across\ndifferent model sizes, reflecting a practical balance between computational\nefficiency and linguistic precision. The implementation also achieves energy\nefficiency, reducing resource consumption during training and inference phases\nwithout compromising accuracy. Qualitative case studies further validate its\nadaptability to extended narratives and context-intensive scenarios,\nhighlighting the mechanism's potential for real-world applications. SLED offers\na different perspective on embedding design and its implications for advancing\nlanguage modeling."
                },
                "authors": [
                    {
                        "name": "Irin Kabakum"
                    },
                    {
                        "name": "Thomas Montgomery"
                    },
                    {
                        "name": "Daniel Ravenwood"
                    },
                    {
                        "name": "Genevieve Harrington"
                    }
                ],
                "author_detail": {
                    "name": "Genevieve Harrington"
                },
                "author": "Genevieve Harrington",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15405v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15405v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17598v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17598v2",
                "updated": "2025-03-25T12:53:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    53,
                    2,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-22T00:59:22Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    0,
                    59,
                    22,
                    5,
                    81,
                    0
                ],
                "title": "Coarse-Grained Games: A Framework for Bounded Perception in Game Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-Grained Games: A Framework for Bounded Perception in Game Theory"
                },
                "summary": "In everyday life, we frequently make coarse-grained judgments. When we say\nthat Olivia and Noah excel in mathematics, we disregard the specific\ndifferences in their mathematical abilities. Similarly, when we claim that a\nparticular automobile manufacturer produces high-quality cars, we overlook the\nminor variations among individual vehicles. These coarse-grained assessments\nare distinct from erroneous or deceptive judgments, such as those resulting\nfrom student cheating or false advertising by corporations. Despite the\nprevalence of such judgments, little attention has been given to their\nunderlying mathematical structure. In this paper, we introduce the concept of\ncoarse-graining into game theory, analyzing games where players may perceive\ndifferent payoffs as identical while preserving the underlying order structure.\nWe call it a Coarse-Grained Game (CGG). This framework allows us to examine the\nrational inference processes that arise when players equate distinct\nmicro-level payoffs at a macro level, and to explore how Nash equilibria are\npreserved or altered as a result. Our key findings suggest that CGGs possess\nseveral desirable properties that make them suitable for modeling phenomena in\nthe social sciences. This paper demonstrates two such applications: first, in\ncases of overly minor product updates, consumers may encounter an equilibrium\nselection problem, resulting in market behavior that is not driven by objective\nquality differences; second, the lemon market can be analyzed not only through\nobjective information asymmetry but also through asymmetries in perceptual\nresolution or recognition ability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In everyday life, we frequently make coarse-grained judgments. When we say\nthat Olivia and Noah excel in mathematics, we disregard the specific\ndifferences in their mathematical abilities. Similarly, when we claim that a\nparticular automobile manufacturer produces high-quality cars, we overlook the\nminor variations among individual vehicles. These coarse-grained assessments\nare distinct from erroneous or deceptive judgments, such as those resulting\nfrom student cheating or false advertising by corporations. Despite the\nprevalence of such judgments, little attention has been given to their\nunderlying mathematical structure. In this paper, we introduce the concept of\ncoarse-graining into game theory, analyzing games where players may perceive\ndifferent payoffs as identical while preserving the underlying order structure.\nWe call it a Coarse-Grained Game (CGG). This framework allows us to examine the\nrational inference processes that arise when players equate distinct\nmicro-level payoffs at a macro level, and to explore how Nash equilibria are\npreserved or altered as a result. Our key findings suggest that CGGs possess\nseveral desirable properties that make them suitable for modeling phenomena in\nthe social sciences. This paper demonstrates two such applications: first, in\ncases of overly minor product updates, consumers may encounter an equilibrium\nselection problem, resulting in market behavior that is not driven by objective\nquality differences; second, the lemon market can be analyzed not only through\nobjective information asymmetry but also through asymmetries in perceptual\nresolution or recognition ability."
                },
                "authors": [
                    {
                        "name": "Takashi Izumo"
                    }
                ],
                "author_detail": {
                    "name": "Takashi Izumo"
                },
                "author": "Takashi Izumo",
                "arxiv_comment": "49 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17598v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17598v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.TH",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19612v1",
                "updated": "2025-03-25T12:52:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    52,
                    38,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T12:52:38Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    52,
                    38,
                    1,
                    84,
                    0
                ],
                "title": "RL-finetuning LLMs from on- and off-policy data with a single algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RL-finetuning LLMs from on- and off-policy data with a single algorithm"
                },
                "summary": "We introduce a novel reinforcement learning algorithm (AGRO, for\nAny-Generation Reward Optimization) for fine-tuning large-language models. AGRO\nleverages the concept of generation consistency, which states that the optimal\npolicy satisfies the notion of consistency across any possible generation of\nthe model. We derive algorithms that find optimal solutions via the\nsample-based policy gradient and provide theoretical guarantees on their\nconvergence. Our experiments demonstrate the effectiveness of AGRO in both\non-policy and off-policy settings, showing improved performance on the\nmathematical reasoning dataset over baseline algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel reinforcement learning algorithm (AGRO, for\nAny-Generation Reward Optimization) for fine-tuning large-language models. AGRO\nleverages the concept of generation consistency, which states that the optimal\npolicy satisfies the notion of consistency across any possible generation of\nthe model. We derive algorithms that find optimal solutions via the\nsample-based policy gradient and provide theoretical guarantees on their\nconvergence. Our experiments demonstrate the effectiveness of AGRO in both\non-policy and off-policy settings, showing improved performance on the\nmathematical reasoning dataset over baseline algorithms."
                },
                "authors": [
                    {
                        "name": "Yunhao Tang"
                    },
                    {
                        "name": "Taco Cohen"
                    },
                    {
                        "name": "David W. Zhang"
                    },
                    {
                        "name": "Michal Valko"
                    },
                    {
                        "name": "Rémi Munos"
                    }
                ],
                "author_detail": {
                    "name": "Rémi Munos"
                },
                "author": "Rémi Munos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03235v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03235v2",
                "updated": "2025-03-25T12:49:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    49,
                    43,
                    1,
                    84,
                    0
                ],
                "published": "2024-12-04T11:36:37Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    36,
                    37,
                    2,
                    339,
                    0
                ],
                "title": "Does Safety Training of LLMs Generalize to Semantically Related Natural\n  Prompts?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Safety Training of LLMs Generalize to Semantically Related Natural\n  Prompts?"
                },
                "summary": "Large Language Models (LLMs) are known to be susceptible to crafted\nadversarial attacks or jailbreaks that lead to the generation of objectionable\ncontent despite being aligned to human preferences using safety fine-tuning\nmethods. While the large dimensionality of input token space makes it\ninevitable to find adversarial prompts that can jailbreak these models, we aim\nto evaluate whether safety fine-tuned LLMs are safe against natural prompts\nwhich are semantically related to toxic seed prompts that elicit safe responses\nafter alignment. We surprisingly find that popular aligned LLMs such as GPT-4\ncan be compromised using naive prompts that are NOT even crafted with an\nobjective of jailbreaking the model. Furthermore, we empirically show that\ngiven a seed prompt that elicits a toxic response from an unaligned model, one\ncan systematically generate several semantically related natural prompts that\ncan jailbreak aligned LLMs. Towards this, we propose a method of Response\nGuided Question Augmentation (ReG-QA) to evaluate the generalization of safety\naligned LLMs to natural prompts, that first generates several toxic answers\ngiven a seed question using an unaligned LLM (Q to A), and further leverages an\nLLM to generate questions that are likely to produce these answers (A to Q). We\ninterestingly find that safety fine-tuned LLMs such as GPT-4o are vulnerable to\nproducing natural jailbreak questions from unsafe content (without denial) and\ncan thus be used for the latter (A to Q) step. We obtain attack success rates\nthat are comparable to/ better than leading adversarial attack methods on the\nJailbreakBench leaderboard, while being significantly more stable against\ndefenses such as Smooth-LLM and Synonym Substitution, which are effective\nagainst existing all attacks on the leaderboard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are known to be susceptible to crafted\nadversarial attacks or jailbreaks that lead to the generation of objectionable\ncontent despite being aligned to human preferences using safety fine-tuning\nmethods. While the large dimensionality of input token space makes it\ninevitable to find adversarial prompts that can jailbreak these models, we aim\nto evaluate whether safety fine-tuned LLMs are safe against natural prompts\nwhich are semantically related to toxic seed prompts that elicit safe responses\nafter alignment. We surprisingly find that popular aligned LLMs such as GPT-4\ncan be compromised using naive prompts that are NOT even crafted with an\nobjective of jailbreaking the model. Furthermore, we empirically show that\ngiven a seed prompt that elicits a toxic response from an unaligned model, one\ncan systematically generate several semantically related natural prompts that\ncan jailbreak aligned LLMs. Towards this, we propose a method of Response\nGuided Question Augmentation (ReG-QA) to evaluate the generalization of safety\naligned LLMs to natural prompts, that first generates several toxic answers\ngiven a seed question using an unaligned LLM (Q to A), and further leverages an\nLLM to generate questions that are likely to produce these answers (A to Q). We\ninterestingly find that safety fine-tuned LLMs such as GPT-4o are vulnerable to\nproducing natural jailbreak questions from unsafe content (without denial) and\ncan thus be used for the latter (A to Q) step. We obtain attack success rates\nthat are comparable to/ better than leading adversarial attack methods on the\nJailbreakBench leaderboard, while being significantly more stable against\ndefenses such as Smooth-LLM and Synonym Substitution, which are effective\nagainst existing all attacks on the leaderboard."
                },
                "authors": [
                    {
                        "name": "Sravanti Addepalli"
                    },
                    {
                        "name": "Yerram Varun"
                    },
                    {
                        "name": "Arun Suggala"
                    },
                    {
                        "name": "Karthikeyan Shanmugam"
                    },
                    {
                        "name": "Prateek Jain"
                    }
                ],
                "author_detail": {
                    "name": "Prateek Jain"
                },
                "author": "Prateek Jain",
                "arxiv_comment": "Accepted in ICLR 2025",
                "arxiv_journal_ref": "Addepalli, S., Varun, Y., Suggala, A., Shanmugam, K., & Jain, P.\n  (2025). Does safety training of LLMs generalize to semantically related\n  natural prompts? In The Thirteenth International Conference on Learning\n  Representations 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03235v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03235v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19602v1",
                "updated": "2025-03-25T12:37:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    37,
                    22,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T12:37:22Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    37,
                    22,
                    1,
                    84,
                    0
                ],
                "title": "Innate Reasoning is Not Enough: In-Context Learning Enhances Reasoning\n  Large Language Models with Less Overthinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Innate Reasoning is Not Enough: In-Context Learning Enhances Reasoning\n  Large Language Models with Less Overthinking"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have introduced Reasoning\nLarge Language Models (RLLMs), which employ extended thinking processes with\nreflection and self-correction capabilities, demonstrating the effectiveness of\ntest-time scaling. RLLMs exhibit innate Chain-of-Thought (CoT) reasoning\ncapability obtained from training, leading to a natural question: \"Is CoT\nprompting, a popular In-Context Learning (ICL) method for chat LLMs, necessary\nto enhance the reasoning capability of RLLMs?\" In this work, we present the\nfirst comprehensive analysis of the impacts of Zero-shot CoT and Few-shot CoT\non RLLMs across mathematical reasoning tasks. We examine models ranging from\n1.5B to 32B parameters, finding that contrary to concerns, CoT prompting\nsignificantly enhances RLLMs' performance in most scenarios. Our results reveal\ndistinct patterns: large-capacity models show minimal improvement on simple\ntasks but substantial gains on complex problems, while smaller models exhibit\nthe opposite behavior. Further analysis demonstrates that CoT prompting\neffectively controls the distribution of the numbers of thinking tokens and\nreasoning steps, reducing excessive reflections by approximately 90% in some\ncases. Moreover, attention logits analysis reveals the RLLMs' overfitting to\nreflection-related words, which is mitigated by external CoT guidance. Notably,\nour experiments indicate that for RLLMs, one-shot CoT consistently yields\nsuperior performance compared to Few-shot CoT approaches. Our findings provide\nimportant insights for optimizing RLLMs' performance through appropriate\nprompting strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have introduced Reasoning\nLarge Language Models (RLLMs), which employ extended thinking processes with\nreflection and self-correction capabilities, demonstrating the effectiveness of\ntest-time scaling. RLLMs exhibit innate Chain-of-Thought (CoT) reasoning\ncapability obtained from training, leading to a natural question: \"Is CoT\nprompting, a popular In-Context Learning (ICL) method for chat LLMs, necessary\nto enhance the reasoning capability of RLLMs?\" In this work, we present the\nfirst comprehensive analysis of the impacts of Zero-shot CoT and Few-shot CoT\non RLLMs across mathematical reasoning tasks. We examine models ranging from\n1.5B to 32B parameters, finding that contrary to concerns, CoT prompting\nsignificantly enhances RLLMs' performance in most scenarios. Our results reveal\ndistinct patterns: large-capacity models show minimal improvement on simple\ntasks but substantial gains on complex problems, while smaller models exhibit\nthe opposite behavior. Further analysis demonstrates that CoT prompting\neffectively controls the distribution of the numbers of thinking tokens and\nreasoning steps, reducing excessive reflections by approximately 90% in some\ncases. Moreover, attention logits analysis reveals the RLLMs' overfitting to\nreflection-related words, which is mitigated by external CoT guidance. Notably,\nour experiments indicate that for RLLMs, one-shot CoT consistently yields\nsuperior performance compared to Few-shot CoT approaches. Our findings provide\nimportant insights for optimizing RLLMs' performance through appropriate\nprompting strategies."
                },
                "authors": [
                    {
                        "name": "Yuyao Ge"
                    },
                    {
                        "name": "Shenghua Liu"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Lingrui Mei"
                    },
                    {
                        "name": "Lizhe Chen"
                    },
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12216v2",
                "updated": "2025-03-25T12:33:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    33,
                    41,
                    1,
                    84,
                    0
                ],
                "published": "2025-01-21T15:36:08Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    15,
                    36,
                    8,
                    1,
                    21,
                    0
                ],
                "title": "RL-RC-DoT: A Block-level RL agent for Task-Aware Video Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RL-RC-DoT: A Block-level RL agent for Task-Aware Video Compression"
                },
                "summary": "Video encoders optimize compression for human perception by minimizing\nreconstruction error under bit-rate constraints. In many modern applications\nsuch as autonomous driving, an overwhelming majority of videos serve as input\nfor AI systems performing tasks like object recognition or segmentation, rather\nthan being watched by humans. It is therefore useful to optimize the encoder\nfor a downstream task instead of for perceptual image quality. However, a major\nchallenge is how to combine such downstream optimization with existing standard\nvideo encoders, which are highly efficient and popular. Here, we address this\nchallenge by controlling the Quantization Parameters (QPs) at the macro-block\nlevel to optimize the downstream task. This granular control allows us to\nprioritize encoding for task-relevant regions within each frame. We formulate\nthis optimization problem as a Reinforcement Learning (RL) task, where the\nagent learns to balance long-term implications of choosing QPs on both task\nperformance and bit-rate constraints. Notably, our policy does not require the\ndownstream task as an input during inference, making it suitable for streaming\napplications and edge devices such as vehicles. We demonstrate significant\nimprovements in two tasks, car detection, and ROI (saliency) encoding. Our\napproach improves task performance for a given bit rate compared to traditional\ntask agnostic encoding methods, paving the way for more efficient task-aware\nvideo compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video encoders optimize compression for human perception by minimizing\nreconstruction error under bit-rate constraints. In many modern applications\nsuch as autonomous driving, an overwhelming majority of videos serve as input\nfor AI systems performing tasks like object recognition or segmentation, rather\nthan being watched by humans. It is therefore useful to optimize the encoder\nfor a downstream task instead of for perceptual image quality. However, a major\nchallenge is how to combine such downstream optimization with existing standard\nvideo encoders, which are highly efficient and popular. Here, we address this\nchallenge by controlling the Quantization Parameters (QPs) at the macro-block\nlevel to optimize the downstream task. This granular control allows us to\nprioritize encoding for task-relevant regions within each frame. We formulate\nthis optimization problem as a Reinforcement Learning (RL) task, where the\nagent learns to balance long-term implications of choosing QPs on both task\nperformance and bit-rate constraints. Notably, our policy does not require the\ndownstream task as an input during inference, making it suitable for streaming\napplications and edge devices such as vehicles. We demonstrate significant\nimprovements in two tasks, car detection, and ROI (saliency) encoding. Our\napproach improves task performance for a given bit rate compared to traditional\ntask agnostic encoding methods, paving the way for more efficient task-aware\nvideo compression."
                },
                "authors": [
                    {
                        "name": "Uri Gadot"
                    },
                    {
                        "name": "Assaf Shocher"
                    },
                    {
                        "name": "Shie Mannor"
                    },
                    {
                        "name": "Gal Chechik"
                    },
                    {
                        "name": "Assaf Hallak"
                    }
                ],
                "author_detail": {
                    "name": "Assaf Hallak"
                },
                "author": "Assaf Hallak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19599v1",
                "updated": "2025-03-25T12:30:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    30,
                    30,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T12:30:30Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    30,
                    30,
                    1,
                    84,
                    0
                ],
                "title": "HoarePrompt: Structural Reasoning About Program Correctness in Natural\n  Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HoarePrompt: Structural Reasoning About Program Correctness in Natural\n  Language"
                },
                "summary": "While software requirements are often expressed in natural language,\nverifying the correctness of a program against natural language requirements is\na hard and underexplored problem. Large language models (LLMs) are promising\ncandidates for addressing this challenge, however our experience shows that\nthey are ineffective in this task, often failing to detect even straightforward\nbugs. To address this gap, we introduce HoarePrompt, a novel approach that\nadapts fundamental ideas from program analysis and verification to natural\nlanguage artifacts. Drawing inspiration from the strongest postcondition\ncalculus, HoarePrompt employs a systematic, step-by-step process in which an\nLLM generates natural language descriptions of reachable program states at\nvarious points in the code. To manage loops, we propose few-shot-driven\nk-induction, an adaptation of the k-induction method widely used in model\nchecking. Once program states are described, HoarePrompt leverages the LLM to\nassess whether the program, annotated with these state descriptions, conforms\nto the natural language requirements. For evaluating the quality of classifiers\nof program correctness with respect to natural language requirements, we\nconstructed CoCoClaNeL, a challenging dataset of solutions to programming\ncompetition problems. Our experiments show that HoarePrompt improves the MCC by\n62% compared to directly using Zero-shot-CoT prompts for correctness\nclassification. Furthermore, HoarePrompt outperforms a classifier that assesses\ncorrectness via LLM-based test generation by increasing the MCC by 93%. The\ninductive reasoning mechanism contributes a 28% boost to MCC, underscoring its\neffectiveness in managing loops.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While software requirements are often expressed in natural language,\nverifying the correctness of a program against natural language requirements is\na hard and underexplored problem. Large language models (LLMs) are promising\ncandidates for addressing this challenge, however our experience shows that\nthey are ineffective in this task, often failing to detect even straightforward\nbugs. To address this gap, we introduce HoarePrompt, a novel approach that\nadapts fundamental ideas from program analysis and verification to natural\nlanguage artifacts. Drawing inspiration from the strongest postcondition\ncalculus, HoarePrompt employs a systematic, step-by-step process in which an\nLLM generates natural language descriptions of reachable program states at\nvarious points in the code. To manage loops, we propose few-shot-driven\nk-induction, an adaptation of the k-induction method widely used in model\nchecking. Once program states are described, HoarePrompt leverages the LLM to\nassess whether the program, annotated with these state descriptions, conforms\nto the natural language requirements. For evaluating the quality of classifiers\nof program correctness with respect to natural language requirements, we\nconstructed CoCoClaNeL, a challenging dataset of solutions to programming\ncompetition problems. Our experiments show that HoarePrompt improves the MCC by\n62% compared to directly using Zero-shot-CoT prompts for correctness\nclassification. Furthermore, HoarePrompt outperforms a classifier that assesses\ncorrectness via LLM-based test generation by increasing the MCC by 93%. The\ninductive reasoning mechanism contributes a 28% boost to MCC, underscoring its\neffectiveness in managing loops."
                },
                "authors": [
                    {
                        "name": "Dimitrios Stamatios Bouras"
                    },
                    {
                        "name": "Yihan Dai"
                    },
                    {
                        "name": "Tairan Wang"
                    },
                    {
                        "name": "Yingfei Xiong"
                    },
                    {
                        "name": "Sergey Mechtaev"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Mechtaev"
                },
                "author": "Sergey Mechtaev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19598v1",
                "updated": "2025-03-25T12:29:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    29,
                    53,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T12:29:53Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    29,
                    53,
                    1,
                    84,
                    0
                ],
                "title": "The Greatest Good Benchmark: Measuring LLMs' Alignment with Utilitarian\n  Moral Dilemmas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Greatest Good Benchmark: Measuring LLMs' Alignment with Utilitarian\n  Moral Dilemmas"
                },
                "summary": "The question of how to make decisions that maximise the well-being of all\npersons is very relevant to design language models that are beneficial to\nhumanity and free from harm. We introduce the Greatest Good Benchmark to\nevaluate the moral judgments of LLMs using utilitarian dilemmas. Our analysis\nacross 15 diverse LLMs reveals consistently encoded moral preferences that\ndiverge from established moral theories and lay population moral standards.\nMost LLMs have a marked preference for impartial beneficence and rejection of\ninstrumental harm. These findings showcase the 'artificial moral compass' of\nLLMs, offering insights into their moral alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The question of how to make decisions that maximise the well-being of all\npersons is very relevant to design language models that are beneficial to\nhumanity and free from harm. We introduce the Greatest Good Benchmark to\nevaluate the moral judgments of LLMs using utilitarian dilemmas. Our analysis\nacross 15 diverse LLMs reveals consistently encoded moral preferences that\ndiverge from established moral theories and lay population moral standards.\nMost LLMs have a marked preference for impartial beneficence and rejection of\ninstrumental harm. These findings showcase the 'artificial moral compass' of\nLLMs, offering insights into their moral alignment."
                },
                "authors": [
                    {
                        "name": "Giovanni Franco Gabriel Marraffini"
                    },
                    {
                        "name": "Andrés Cotton"
                    },
                    {
                        "name": "Noe Fabian Hsueh"
                    },
                    {
                        "name": "Axel Fridman"
                    },
                    {
                        "name": "Juan Wisznia"
                    },
                    {
                        "name": "Luciano Del Corro"
                    }
                ],
                "author_detail": {
                    "name": "Luciano Del Corro"
                },
                "author": "Luciano Del Corro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19595v1",
                "updated": "2025-03-25T12:21:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    21,
                    26,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T12:21:26Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    21,
                    26,
                    1,
                    84,
                    0
                ],
                "title": "Optimizing Language Models for Inference Time Objectives using\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Language Models for Inference Time Objectives using\n  Reinforcement Learning"
                },
                "summary": "In this work, we investigate the merits of explicitly optimizing for\ninference time algorithmic performance during model training. We show how\noptimizing for inference time performance can improve overall model efficacy.\nWe consider generic inference time objectives with $k$ samples, with a focus on\npass@$k$ and majority voting as two main applications. With language model\ntraining on reasoning datasets, we showcase the performance trade-off enabled\nby training with such objectives. When training on code generation tasks, we\nshow that the approach significantly improves pass@$k$ objectives compared to\nthe baseline method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we investigate the merits of explicitly optimizing for\ninference time algorithmic performance during model training. We show how\noptimizing for inference time performance can improve overall model efficacy.\nWe consider generic inference time objectives with $k$ samples, with a focus on\npass@$k$ and majority voting as two main applications. With language model\ntraining on reasoning datasets, we showcase the performance trade-off enabled\nby training with such objectives. When training on code generation tasks, we\nshow that the approach significantly improves pass@$k$ objectives compared to\nthe baseline method."
                },
                "authors": [
                    {
                        "name": "Yunhao Tang"
                    },
                    {
                        "name": "Kunhao Zheng"
                    },
                    {
                        "name": "Gabriel Synnaeve"
                    },
                    {
                        "name": "Rémi Munos"
                    }
                ],
                "author_detail": {
                    "name": "Rémi Munos"
                },
                "author": "Rémi Munos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05374v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05374v3",
                "updated": "2025-03-25T12:18:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    18,
                    42,
                    1,
                    84,
                    0
                ],
                "published": "2025-02-07T23:03:55Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    23,
                    3,
                    55,
                    4,
                    38,
                    0
                ],
                "title": "Towards LLM Unlearning Resilient to Relearning Attacks: A\n  Sharpness-Aware Minimization Perspective and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards LLM Unlearning Resilient to Relearning Attacks: A\n  Sharpness-Aware Minimization Perspective and Beyond"
                },
                "summary": "The LLM unlearning technique has recently been introduced to comply with data\nregulations and address the safety and ethical concerns of LLMs by removing the\nundesired data-model influence. However, state-of-the-art unlearning methods\nface a critical vulnerability: they are susceptible to ``relearning'' the\nremoved information from a small number of forget data points, known as\nrelearning attacks. In this paper, we systematically investigate how to make\nunlearned models robust against such attacks. For the first time, we establish\na connection between robust unlearning and sharpness-aware minimization (SAM)\nthrough a unified robust optimization framework, in an analogy to adversarial\ntraining designed to defend against adversarial attacks. Our analysis for SAM\nreveals that smoothness optimization plays a pivotal role in mitigating\nrelearning attacks. Thus, we further explore diverse smoothing strategies to\nenhance unlearning robustness. Extensive experiments on benchmark datasets,\nincluding WMDP and MUSE, demonstrate that SAM and other smoothness optimization\napproaches consistently improve the resistance of LLM unlearning to relearning\nattacks. Notably, smoothness-enhanced unlearning also helps defend against\n(input-level) jailbreaking attacks, broadening our proposal's impact in\nrobustifying LLM unlearning. Codes are available at\nhttps://github.com/OPTML-Group/Unlearn-Smooth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LLM unlearning technique has recently been introduced to comply with data\nregulations and address the safety and ethical concerns of LLMs by removing the\nundesired data-model influence. However, state-of-the-art unlearning methods\nface a critical vulnerability: they are susceptible to ``relearning'' the\nremoved information from a small number of forget data points, known as\nrelearning attacks. In this paper, we systematically investigate how to make\nunlearned models robust against such attacks. For the first time, we establish\na connection between robust unlearning and sharpness-aware minimization (SAM)\nthrough a unified robust optimization framework, in an analogy to adversarial\ntraining designed to defend against adversarial attacks. Our analysis for SAM\nreveals that smoothness optimization plays a pivotal role in mitigating\nrelearning attacks. Thus, we further explore diverse smoothing strategies to\nenhance unlearning robustness. Extensive experiments on benchmark datasets,\nincluding WMDP and MUSE, demonstrate that SAM and other smoothness optimization\napproaches consistently improve the resistance of LLM unlearning to relearning\nattacks. Notably, smoothness-enhanced unlearning also helps defend against\n(input-level) jailbreaking attacks, broadening our proposal's impact in\nrobustifying LLM unlearning. Codes are available at\nhttps://github.com/OPTML-Group/Unlearn-Smooth."
                },
                "authors": [
                    {
                        "name": "Chongyu Fan"
                    },
                    {
                        "name": "Jinghan Jia"
                    },
                    {
                        "name": "Yihua Zhang"
                    },
                    {
                        "name": "Anil Ramakrishna"
                    },
                    {
                        "name": "Mingyi Hong"
                    },
                    {
                        "name": "Sijia Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sijia Liu"
                },
                "author": "Sijia Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05374v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05374v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19592v1",
                "updated": "2025-03-25T12:14:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    14,
                    21,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T12:14:21Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    14,
                    21,
                    1,
                    84,
                    0
                ],
                "title": "SACB-Net: Spatial-awareness Convolutions for Medical Image Registration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SACB-Net: Spatial-awareness Convolutions for Medical Image Registration"
                },
                "summary": "Deep learning-based image registration methods have shown state-of-the-art\nperformance and rapid inference speeds. Despite these advances, many existing\napproaches fall short in capturing spatially varying information in non-local\nregions of feature maps due to the reliance on spatially-shared convolution\nkernels. This limitation leads to suboptimal estimation of deformation fields.\nIn this paper, we propose a 3D Spatial-Awareness Convolution Block (SACB) to\nenhance the spatial information within feature representations. Our SACB\nestimates the spatial clusters within feature maps by leveraging feature\nsimilarity and subsequently parameterizes the adaptive convolution kernels\nacross diverse regions. This adaptive mechanism generates the convolution\nkernels (weights and biases) tailored to spatial variations, thereby enabling\nthe network to effectively capture spatially varying information. Building on\nSACB, we introduce a pyramid flow estimator (named SACB-Net) that integrates\nSACBs to facilitate multi-scale flow composition, particularly addressing large\ndeformations. Experimental results on the brain IXI and LPBA datasets as well\nas Abdomen CT datasets demonstrate the effectiveness of SACB and the\nsuperiority of SACB-Net over the state-of-the-art learning-based registration\nmethods. The code is available at https://github.com/x-xc/SACB_Net .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning-based image registration methods have shown state-of-the-art\nperformance and rapid inference speeds. Despite these advances, many existing\napproaches fall short in capturing spatially varying information in non-local\nregions of feature maps due to the reliance on spatially-shared convolution\nkernels. This limitation leads to suboptimal estimation of deformation fields.\nIn this paper, we propose a 3D Spatial-Awareness Convolution Block (SACB) to\nenhance the spatial information within feature representations. Our SACB\nestimates the spatial clusters within feature maps by leveraging feature\nsimilarity and subsequently parameterizes the adaptive convolution kernels\nacross diverse regions. This adaptive mechanism generates the convolution\nkernels (weights and biases) tailored to spatial variations, thereby enabling\nthe network to effectively capture spatially varying information. Building on\nSACB, we introduce a pyramid flow estimator (named SACB-Net) that integrates\nSACBs to facilitate multi-scale flow composition, particularly addressing large\ndeformations. Experimental results on the brain IXI and LPBA datasets as well\nas Abdomen CT datasets demonstrate the effectiveness of SACB and the\nsuperiority of SACB-Net over the state-of-the-art learning-based registration\nmethods. The code is available at https://github.com/x-xc/SACB_Net ."
                },
                "authors": [
                    {
                        "name": "Xinxing Cheng"
                    },
                    {
                        "name": "Tianyang Zhang"
                    },
                    {
                        "name": "Wenqi Lu"
                    },
                    {
                        "name": "Qingjie Meng"
                    },
                    {
                        "name": "Alejandro F. Frangi"
                    },
                    {
                        "name": "Jinming Duan"
                    }
                ],
                "author_detail": {
                    "name": "Jinming Duan"
                },
                "author": "Jinming Duan",
                "arxiv_comment": "CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18002v2",
                "updated": "2025-03-25T12:05:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    5,
                    26,
                    1,
                    84,
                    0
                ],
                "published": "2025-02-12T02:40:44Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    2,
                    40,
                    44,
                    2,
                    43,
                    0
                ],
                "title": "Neuromorphic Principles for Efficient Large Language Models on Intel\n  Loihi 2",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuromorphic Principles for Efficient Large Language Models on Intel\n  Loihi 2"
                },
                "summary": "Large language models (LLMs) deliver impressive performance but require large\namounts of energy. In this work, we present a MatMul-free LLM architecture\nadapted for Intel's neuromorphic processor, Loihi 2. Our approach leverages\nLoihi 2's support for low-precision, event-driven computation and stateful\nprocessing. Our hardware-aware quantized model on GPU demonstrates that a 370M\nparameter MatMul-free model can be quantized with no accuracy loss. Based on\npreliminary results, we report up to 3x higher throughput with 2x less energy,\ncompared to transformer-based LLMs on an edge GPU, with significantly better\nscaling. Further hardware optimizations will increase throughput and decrease\nenergy consumption. These results show the potential of neuromorphic hardware\nfor efficient inference and pave the way for efficient reasoning models capable\nof generating complex, long-form text rapidly and cost-effectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) deliver impressive performance but require large\namounts of energy. In this work, we present a MatMul-free LLM architecture\nadapted for Intel's neuromorphic processor, Loihi 2. Our approach leverages\nLoihi 2's support for low-precision, event-driven computation and stateful\nprocessing. Our hardware-aware quantized model on GPU demonstrates that a 370M\nparameter MatMul-free model can be quantized with no accuracy loss. Based on\npreliminary results, we report up to 3x higher throughput with 2x less energy,\ncompared to transformer-based LLMs on an edge GPU, with significantly better\nscaling. Further hardware optimizations will increase throughput and decrease\nenergy consumption. These results show the potential of neuromorphic hardware\nfor efficient inference and pave the way for efficient reasoning models capable\nof generating complex, long-form text rapidly and cost-effectively."
                },
                "authors": [
                    {
                        "name": "Steven Abreu"
                    },
                    {
                        "name": "Sumit Bam Shrestha"
                    },
                    {
                        "name": "Rui-Jie Zhu"
                    },
                    {
                        "name": "Jason Eshraghian"
                    }
                ],
                "author_detail": {
                    "name": "Jason Eshraghian"
                },
                "author": "Jason Eshraghian",
                "arxiv_comment": "Accepted to International Conference on Learning Representations\n  (ICLR) Workshop on Scalable Optimization for Efficient and Adaptive\n  Foundation Models (SCOPE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19574v1",
                "updated": "2025-03-25T11:48:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    11,
                    48,
                    22,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T11:48:22Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    11,
                    48,
                    22,
                    1,
                    84,
                    0
                ],
                "title": "Context-Efficient Retrieval with Factual Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Efficient Retrieval with Factual Decomposition"
                },
                "summary": "There has recently been considerable interest in incorporating information\nretrieval into large language models (LLMs). Retrieval from a dynamically\nexpanding external corpus of text allows a model to incorporate current events\nand can be viewed as a form of episodic memory. Here we demonstrate that\npre-processing the external corpus into semi-structured ''atomic facts'' makes\nretrieval more efficient. More specifically, we demonstrate that our particular\nform of atomic facts improves performance on various question answering tasks\nwhen the amount of retrieved text is limited. Limiting the amount of retrieval\nreduces the size of the context and improves inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has recently been considerable interest in incorporating information\nretrieval into large language models (LLMs). Retrieval from a dynamically\nexpanding external corpus of text allows a model to incorporate current events\nand can be viewed as a form of episodic memory. Here we demonstrate that\npre-processing the external corpus into semi-structured ''atomic facts'' makes\nretrieval more efficient. More specifically, we demonstrate that our particular\nform of atomic facts improves performance on various question answering tasks\nwhen the amount of retrieved text is limited. Limiting the amount of retrieval\nreduces the size of the context and improves inference efficiency."
                },
                "authors": [
                    {
                        "name": "Yanhong Li"
                    },
                    {
                        "name": "David Yunis"
                    },
                    {
                        "name": "David McAllester"
                    },
                    {
                        "name": "Jiawei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Zhou"
                },
                "author": "Jiawei Zhou",
                "arxiv_comment": "NAACL 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19573v1",
                "updated": "2025-03-25T11:48:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    11,
                    48,
                    5,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T11:48:05Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    11,
                    48,
                    5,
                    1,
                    84,
                    0
                ],
                "title": "Motif Counting in Complex Networks: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motif Counting in Complex Networks: A Comprehensive Survey"
                },
                "summary": "Motif counting plays a crucial role in understanding the structural\nproperties of networks. By computing motif frequencies, researchers can draw\nkey insights into the structural properties of the underlying network. As\nnetworks become increasingly complex, different graph models have been\nproposed, giving rise to diverse motif patterns. These variations introduce\nunique computational challenges that require specialized algorithms tailored to\nspecific motifs within different graph structures. This survey provides a\ncomprehensive and structured overview of motif counting techniques across\ngeneral graphs, heterogeneous graphs, and hypergraphs. We categorize existing\nalgorithms according to their underlying computational strategies, emphasizing\nkey similarities and distinctions. In addition to reviewing current\nmethodologies, we examine their strengths, limitations, and computational\ntrade-offs. Furthermore, we explore future directions in motif counting,\nincluding scalable implementations to improve efficiency in large-scale\nnetworks, algorithmic adaptations for dynamic, temporal, and attributed graphs,\nand deeper integration with large language models (LLMs) and graph-based\nretrieval-augmented generation (GraphRAG). By offering a detailed analysis of\nthese approaches, this survey aims to support researchers and practitioners in\nadvancing motif counting for increasingly complex network data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motif counting plays a crucial role in understanding the structural\nproperties of networks. By computing motif frequencies, researchers can draw\nkey insights into the structural properties of the underlying network. As\nnetworks become increasingly complex, different graph models have been\nproposed, giving rise to diverse motif patterns. These variations introduce\nunique computational challenges that require specialized algorithms tailored to\nspecific motifs within different graph structures. This survey provides a\ncomprehensive and structured overview of motif counting techniques across\ngeneral graphs, heterogeneous graphs, and hypergraphs. We categorize existing\nalgorithms according to their underlying computational strategies, emphasizing\nkey similarities and distinctions. In addition to reviewing current\nmethodologies, we examine their strengths, limitations, and computational\ntrade-offs. Furthermore, we explore future directions in motif counting,\nincluding scalable implementations to improve efficiency in large-scale\nnetworks, algorithmic adaptations for dynamic, temporal, and attributed graphs,\nand deeper integration with large language models (LLMs) and graph-based\nretrieval-augmented generation (GraphRAG). By offering a detailed analysis of\nthese approaches, this survey aims to support researchers and practitioners in\nadvancing motif counting for increasingly complex network data."
                },
                "authors": [
                    {
                        "name": "Haozhe Yin"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Wenjie Zhang"
                    },
                    {
                        "name": "Yizhang He"
                    },
                    {
                        "name": "Ying Zhang"
                    },
                    {
                        "name": "Xuemin Lin"
                    }
                ],
                "author_detail": {
                    "name": "Xuemin Lin"
                },
                "author": "Xuemin Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01475v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01475v3",
                "updated": "2025-03-25T11:40:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    11,
                    40,
                    10,
                    1,
                    84,
                    0
                ],
                "published": "2024-06-03T16:03:04Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    16,
                    3,
                    4,
                    0,
                    155,
                    0
                ],
                "title": "New limits on warm inflation from pulsar timing arrays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New limits on warm inflation from pulsar timing arrays"
                },
                "summary": "In this paper, we investigate scalar-induced gravitational waves (GWs)\ngenerated in the post-inflationary universe to infer new limits on warm\ninflation. We specifically examine the evolution of primordial GWs by scalar\nperturbations produced during the radiation-dominated epoch. For this purpose,\nwe assume a weak regime of warm inflation under the slow-roll approximation,\nwith a dissipation coefficient linearly dependent on the temperature of the\nradiation bath. We then derive analytical expressions for the curvature power\nspectrum and the scalar index, in the cases of chaotic and exponential\npotentials of the inflationary field. Subsequently, we compare the theoretical\npredictions regarding the relic energy density of GWs with the stochastic GW\nbackground signal recently detected by the NANOGrav collaboration through the\nuse of pulsar timing array measurements. In so doing, we obtain numerical\nconstraints on the free parameters of the inflationary models under study.\nFinally, we conduct a model selection analysis through the Bayesian inference\nmethod to measure the statistical performance of the different theoretical\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate scalar-induced gravitational waves (GWs)\ngenerated in the post-inflationary universe to infer new limits on warm\ninflation. We specifically examine the evolution of primordial GWs by scalar\nperturbations produced during the radiation-dominated epoch. For this purpose,\nwe assume a weak regime of warm inflation under the slow-roll approximation,\nwith a dissipation coefficient linearly dependent on the temperature of the\nradiation bath. We then derive analytical expressions for the curvature power\nspectrum and the scalar index, in the cases of chaotic and exponential\npotentials of the inflationary field. Subsequently, we compare the theoretical\npredictions regarding the relic energy density of GWs with the stochastic GW\nbackground signal recently detected by the NANOGrav collaboration through the\nuse of pulsar timing array measurements. In so doing, we obtain numerical\nconstraints on the free parameters of the inflationary models under study.\nFinally, we conduct a model selection analysis through the Bayesian inference\nmethod to measure the statistical performance of the different theoretical\nscenarios."
                },
                "authors": [
                    {
                        "name": "Rocco D'Agostino"
                    },
                    {
                        "name": "Matteo Califano"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Califano"
                },
                "author": "Matteo Califano",
                "arxiv_comment": "14 pages, 5 figures. To be published in Physics of the Dark Universe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01475v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01475v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19554v1",
                "updated": "2025-03-25T11:14:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    11,
                    14,
                    37,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T11:14:37Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    11,
                    14,
                    37,
                    1,
                    84,
                    0
                ],
                "title": "Causal Bayesian Optimization with Unknown Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Bayesian Optimization with Unknown Graphs"
                },
                "summary": "Causal Bayesian Optimization (CBO) is a methodology designed to optimize an\noutcome variable by leveraging known causal relationships through targeted\ninterventions. Traditional CBO methods require a fully and accurately specified\ncausal graph, which is a limitation in many real-world scenarios where such\ngraphs are unknown. To address this, we propose a new method for the CBO\nframework that operates without prior knowledge of the causal graph. Consistent\nwith causal bandit theory, we demonstrate through theoretical analysis and that\nfocusing on the direct causal parents of the target variable is sufficient for\noptimization, and provide empirical validation in the context of CBO.\nFurthermore we introduce a new method that learns a Bayesian posterior over the\ndirect parents of the target variable. This allows us to optimize the outcome\nvariable while simultaneously learning the causal structure. Our contributions\ninclude a derivation of the closed-form posterior distribution for the linear\ncase. In the nonlinear case where the posterior is not tractable, we present a\nGaussian Process (GP) approximation that still enables CBO by inferring the\nparents of the outcome variable. The proposed method performs competitively\nwith existing benchmarks and scales well to larger graphs, making it a\npractical tool for real-world applications where causal information is\nincomplete.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Bayesian Optimization (CBO) is a methodology designed to optimize an\noutcome variable by leveraging known causal relationships through targeted\ninterventions. Traditional CBO methods require a fully and accurately specified\ncausal graph, which is a limitation in many real-world scenarios where such\ngraphs are unknown. To address this, we propose a new method for the CBO\nframework that operates without prior knowledge of the causal graph. Consistent\nwith causal bandit theory, we demonstrate through theoretical analysis and that\nfocusing on the direct causal parents of the target variable is sufficient for\noptimization, and provide empirical validation in the context of CBO.\nFurthermore we introduce a new method that learns a Bayesian posterior over the\ndirect parents of the target variable. This allows us to optimize the outcome\nvariable while simultaneously learning the causal structure. Our contributions\ninclude a derivation of the closed-form posterior distribution for the linear\ncase. In the nonlinear case where the posterior is not tractable, we present a\nGaussian Process (GP) approximation that still enables CBO by inferring the\nparents of the outcome variable. The proposed method performs competitively\nwith existing benchmarks and scales well to larger graphs, making it a\npractical tool for real-world applications where causal information is\nincomplete."
                },
                "authors": [
                    {
                        "name": "Jean Durand"
                    },
                    {
                        "name": "Yashas Annadani"
                    },
                    {
                        "name": "Stefan Bauer"
                    },
                    {
                        "name": "Sonali Parbhoo"
                    }
                ],
                "author_detail": {
                    "name": "Sonali Parbhoo"
                },
                "author": "Sonali Parbhoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18710v2",
                "updated": "2025-03-25T11:11:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    11,
                    11,
                    3,
                    1,
                    84,
                    0
                ],
                "published": "2024-05-29T02:42:23Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    2,
                    42,
                    23,
                    2,
                    150,
                    0
                ],
                "title": "To FP8 and Back Again: Quantifying Reduced Precision Effects on LLM\n  Training Stability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To FP8 and Back Again: Quantifying Reduced Precision Effects on LLM\n  Training Stability"
                },
                "summary": "The massive computational costs associated with large language model (LLM)\npretraining have spurred great interest in reduced-precision floating-point\nrepresentations to accelerate the process. As a result, the BrainFloat16 (BF16)\nprecision has become the de facto standard for LLM training, with hardware\nsupport included in recent generations of accelerators. This trend has gone\neven further in the latest processors, where FP8 has recently been introduced.\nHowever, prior experience with FP16, which was found to be less stable than\nBF16, raises concerns as to whether FP8, with even fewer bits than FP16, can be\na cost-effective option for LLM training. We argue that reduced-precision\ntraining schemes must have similar training stability and hyperparameter\nsensitivities to their higher-precision counterparts in order to be\ncost-effective. However, we find that currently available methods for FP8\ntraining are not robust enough to allow their use as economical replacements.\nThis prompts us to investigate the stability of reduced-precision LLM training\nin terms of robustness across random seeds, learning rates, and datasets. To\nthis end, we propose new evaluation techniques and a new metric for quantifying\nloss landscape sharpness in autoregressive language models. By simulating\nincremental bit reductions in floating-point representations, we analyze the\nrelationship between representational power and training stability with the\nintent of aiding future research into the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The massive computational costs associated with large language model (LLM)\npretraining have spurred great interest in reduced-precision floating-point\nrepresentations to accelerate the process. As a result, the BrainFloat16 (BF16)\nprecision has become the de facto standard for LLM training, with hardware\nsupport included in recent generations of accelerators. This trend has gone\neven further in the latest processors, where FP8 has recently been introduced.\nHowever, prior experience with FP16, which was found to be less stable than\nBF16, raises concerns as to whether FP8, with even fewer bits than FP16, can be\na cost-effective option for LLM training. We argue that reduced-precision\ntraining schemes must have similar training stability and hyperparameter\nsensitivities to their higher-precision counterparts in order to be\ncost-effective. However, we find that currently available methods for FP8\ntraining are not robust enough to allow their use as economical replacements.\nThis prompts us to investigate the stability of reduced-precision LLM training\nin terms of robustness across random seeds, learning rates, and datasets. To\nthis end, we propose new evaluation techniques and a new metric for quantifying\nloss landscape sharpness in autoregressive language models. By simulating\nincremental bit reductions in floating-point representations, we analyze the\nrelationship between representational power and training stability with the\nintent of aiding future research into the field."
                },
                "authors": [
                    {
                        "name": "Joonhyung Lee"
                    },
                    {
                        "name": "Jeongin Bae"
                    },
                    {
                        "name": "Byeongwook Kim"
                    },
                    {
                        "name": "Se Jung Kwon"
                    },
                    {
                        "name": "Dongsoo Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongsoo Lee"
                },
                "author": "Dongsoo Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19551v1",
                "updated": "2025-03-25T11:07:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    11,
                    7,
                    12,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T11:07:12Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    11,
                    7,
                    12,
                    1,
                    84,
                    0
                ],
                "title": "Scaling Laws of Synthetic Data for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Laws of Synthetic Data for Language Models"
                },
                "summary": "Large language models (LLMs) achieve strong performance across diverse tasks,\nlargely driven by high-quality web data used in pre-training. However, recent\nstudies indicate this data source is rapidly depleting. Synthetic data emerges\nas a promising alternative, but it remains unclear whether synthetic datasets\nexhibit predictable scalability comparable to raw pre-training data. In this\nwork, we systematically investigate the scaling laws of synthetic data by\nintroducing SynthLLM, a scalable framework that transforms pre-training corpora\ninto diverse, high-quality synthetic datasets. Our approach achieves this by\nautomatically extracting and recombining high-level concepts across multiple\ndocuments using a graph algorithm. Key findings from our extensive mathematical\nexperiments on SynthLLM include: (1) SynthLLM generates synthetic data that\nreliably adheres to the \\emph{rectified scaling law} across various model\nsizes; (2) Performance improvements plateau near 300B tokens; and (3) Larger\nmodels approach optimal performance with fewer training tokens. For instance,\nan 8B model peaks at 1T tokens, while a 3B model requires 4T. Moreover,\ncomparisons with existing synthetic data generation and augmentation methods\ndemonstrate that SynthLLM achieves superior performance and scalability. Our\nfindings highlight synthetic data as a scalable and reliable alternative to\norganic pre-training corpora, offering a viable path toward continued\nimprovement in model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve strong performance across diverse tasks,\nlargely driven by high-quality web data used in pre-training. However, recent\nstudies indicate this data source is rapidly depleting. Synthetic data emerges\nas a promising alternative, but it remains unclear whether synthetic datasets\nexhibit predictable scalability comparable to raw pre-training data. In this\nwork, we systematically investigate the scaling laws of synthetic data by\nintroducing SynthLLM, a scalable framework that transforms pre-training corpora\ninto diverse, high-quality synthetic datasets. Our approach achieves this by\nautomatically extracting and recombining high-level concepts across multiple\ndocuments using a graph algorithm. Key findings from our extensive mathematical\nexperiments on SynthLLM include: (1) SynthLLM generates synthetic data that\nreliably adheres to the \\emph{rectified scaling law} across various model\nsizes; (2) Performance improvements plateau near 300B tokens; and (3) Larger\nmodels approach optimal performance with fewer training tokens. For instance,\nan 8B model peaks at 1T tokens, while a 3B model requires 4T. Moreover,\ncomparisons with existing synthetic data generation and augmentation methods\ndemonstrate that SynthLLM achieves superior performance and scalability. Our\nfindings highlight synthetic data as a scalable and reliable alternative to\norganic pre-training corpora, offering a viable path toward continued\nimprovement in model performance."
                },
                "authors": [
                    {
                        "name": "Zeyu Qin"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Xingxing Zhang"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Xiaolong Huang"
                    },
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Mahmoud Khademi"
                    },
                    {
                        "name": "Dongdong Zhang"
                    },
                    {
                        "name": "Hany Hassan Awadalla"
                    },
                    {
                        "name": "Yi R. Fung"
                    },
                    {
                        "name": "Weizhu Chen"
                    },
                    {
                        "name": "Minhao Cheng"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18596v2",
                "updated": "2025-03-25T11:04:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    11,
                    4,
                    18,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-24T11:53:06Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    53,
                    6,
                    0,
                    83,
                    0
                ],
                "title": "LinkAlign: Scalable Schema Linking for Real-World Large-Scale\n  Multi-Database Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LinkAlign: Scalable Schema Linking for Real-World Large-Scale\n  Multi-Database Text-to-SQL"
                },
                "summary": "Schema linking is a critical bottleneck in achieving human-level performance\nin Text-to-SQL tasks, particularly in real-world large-scale multi-database\nscenarios. Addressing schema linking faces two major challenges: (1) Database\nRetrieval: selecting the correct database from a large schema pool in\nmulti-database settings, while filtering out irrelevant ones. (2) Schema Item\nGrounding: accurately identifying the relevant tables and columns from within a\nlarge and redundant schema for SQL generation. To address this, we introduce\nLinkAlign, a novel framework that can effectively adapt existing baselines to\nreal-world environments by systematically addressing schema linking. Our\nframework comprises three key steps: multi-round semantic enhanced retrieval\nand irrelevant information isolation for Challenge 1, and schema extraction\nenhancement for Challenge 2. We evaluate our method performance of schema\nlinking on the SPIDER and BIRD benchmarks, and the ability to adapt existing\nText-to-SQL models to real-world environments on the SPIDER 2.0-lite benchmark.\nExperiments show that LinkAlign outperforms existing baselines in\nmulti-database settings, demonstrating its effectiveness and robustness. On the\nother hand, our method ranks highest among models excluding those using long\nchain-of-thought reasoning LLMs. This work bridges the gap between current\nresearch and real-world scenarios, providing a practical solution for robust\nand scalable schema linking. The codes are available at\nhttps://github.com/Satissss/LinkAlign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Schema linking is a critical bottleneck in achieving human-level performance\nin Text-to-SQL tasks, particularly in real-world large-scale multi-database\nscenarios. Addressing schema linking faces two major challenges: (1) Database\nRetrieval: selecting the correct database from a large schema pool in\nmulti-database settings, while filtering out irrelevant ones. (2) Schema Item\nGrounding: accurately identifying the relevant tables and columns from within a\nlarge and redundant schema for SQL generation. To address this, we introduce\nLinkAlign, a novel framework that can effectively adapt existing baselines to\nreal-world environments by systematically addressing schema linking. Our\nframework comprises three key steps: multi-round semantic enhanced retrieval\nand irrelevant information isolation for Challenge 1, and schema extraction\nenhancement for Challenge 2. We evaluate our method performance of schema\nlinking on the SPIDER and BIRD benchmarks, and the ability to adapt existing\nText-to-SQL models to real-world environments on the SPIDER 2.0-lite benchmark.\nExperiments show that LinkAlign outperforms existing baselines in\nmulti-database settings, demonstrating its effectiveness and robustness. On the\nother hand, our method ranks highest among models excluding those using long\nchain-of-thought reasoning LLMs. This work bridges the gap between current\nresearch and real-world scenarios, providing a practical solution for robust\nand scalable schema linking. The codes are available at\nhttps://github.com/Satissss/LinkAlign."
                },
                "authors": [
                    {
                        "name": "Yihan Wang"
                    },
                    {
                        "name": "Peiyu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Peiyu Liu"
                },
                "author": "Peiyu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19545v1",
                "updated": "2025-03-25T11:00:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    11,
                    0,
                    37,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T11:00:37Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    11,
                    0,
                    37,
                    1,
                    84,
                    0
                ],
                "title": "Tiling artifacts and trade-offs of feature normalization in the\n  segmentation of large biological images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tiling artifacts and trade-offs of feature normalization in the\n  segmentation of large biological images"
                },
                "summary": "Segmentation of very large images is a common problem in microscopy, medical\nimaging or remote sensing. The problem is usually addressed by sliding window\ninference, which can theoretically lead to seamlessly stitched predictions.\nHowever, in practice many of the popular pipelines still suffer from tiling\nartifacts. We investigate the root cause of these issues and show that they\nstem from the normalization layers within the neural networks. We propose\nindicators to detect normalization issues and further explore the trade-offs\nbetween artifact-free and high-quality predictions, using three diverse\nmicroscopy datasets as examples. Finally, we propose to use BatchRenorm as the\nmost suitable normalization strategy, which effectively removes tiling\nartifacts and enhances transfer performance, thereby improving the reusability\nof trained networks for new datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Segmentation of very large images is a common problem in microscopy, medical\nimaging or remote sensing. The problem is usually addressed by sliding window\ninference, which can theoretically lead to seamlessly stitched predictions.\nHowever, in practice many of the popular pipelines still suffer from tiling\nartifacts. We investigate the root cause of these issues and show that they\nstem from the normalization layers within the neural networks. We propose\nindicators to detect normalization issues and further explore the trade-offs\nbetween artifact-free and high-quality predictions, using three diverse\nmicroscopy datasets as examples. Finally, we propose to use BatchRenorm as the\nmost suitable normalization strategy, which effectively removes tiling\nartifacts and enhances transfer performance, thereby improving the reusability\nof trained networks for new datasets."
                },
                "authors": [
                    {
                        "name": "Elena Buglakova"
                    },
                    {
                        "name": "Anwai Archit"
                    },
                    {
                        "name": "Edoardo D'Imprima"
                    },
                    {
                        "name": "Julia Mahamid"
                    },
                    {
                        "name": "Constantin Pape"
                    },
                    {
                        "name": "Anna Kreshuk"
                    }
                ],
                "author_detail": {
                    "name": "Anna Kreshuk"
                },
                "author": "Anna Kreshuk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18608v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18608v2",
                "updated": "2025-03-25T10:55:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    10,
                    55,
                    49,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-24T12:05:45Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    12,
                    5,
                    45,
                    0,
                    83,
                    0
                ],
                "title": "AutoBayes: A Compositional Framework for Generalized Variational\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoBayes: A Compositional Framework for Generalized Variational\n  Inference"
                },
                "summary": "We introduce a new compositional framework for generalized variational\ninference, clarifying the different parts of a model, how they interact, and\nhow they compose. We explain that both exact Bayesian inference and the loss\nfunctions typical of variational inference (such as variational free energy and\nits generalizations) satisfy chain rules akin to that of reverse-mode automatic\ndifferentiation, and we advocate for exploiting this to build and optimize\nmodels accordingly. To this end, we construct a series of compositional tools:\nfor building models; for constructing their inversions; for attaching local\nloss functions; and for exposing parameters. Finally, we explain how the\nresulting parameterized statistical games may be optimized locally, too. We\nillustrate our framework with a number of classic examples, pointing to new\nareas of extensibility that are revealed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new compositional framework for generalized variational\ninference, clarifying the different parts of a model, how they interact, and\nhow they compose. We explain that both exact Bayesian inference and the loss\nfunctions typical of variational inference (such as variational free energy and\nits generalizations) satisfy chain rules akin to that of reverse-mode automatic\ndifferentiation, and we advocate for exploiting this to build and optimize\nmodels accordingly. To this end, we construct a series of compositional tools:\nfor building models; for constructing their inversions; for attaching local\nloss functions; and for exposing parameters. Finally, we explain how the\nresulting parameterized statistical games may be optimized locally, too. We\nillustrate our framework with a number of classic examples, pointing to new\nareas of extensibility that are revealed."
                },
                "authors": [
                    {
                        "name": "Toby St Clere Smithe"
                    },
                    {
                        "name": "Marco Perin"
                    }
                ],
                "author_detail": {
                    "name": "Marco Perin"
                },
                "author": "Marco Perin",
                "arxiv_comment": "15 pages. v2: fixed a typo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18608v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18608v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18075v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18075v2",
                "updated": "2025-03-25T10:55:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    10,
                    55,
                    38,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-23T13:52:35Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    13,
                    52,
                    35,
                    6,
                    82,
                    0
                ],
                "title": "Variational inference for hierarchical models with conditional scale and\n  skewness corrections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational inference for hierarchical models with conditional scale and\n  skewness corrections"
                },
                "summary": "Gaussian variational approximations are widely used for summarizing posterior\ndistributions in Bayesian models, especially in high-dimensional settings.\nHowever, a drawback of such approximations is the inability to capture skewness\nor more complex features of the posterior. Recent work suggests applying\nskewness corrections to existing Gaussian or other symmetric approximations to\naddress this limitation. We propose to incorporate the skewness correction into\nthe definition of an approximating variational family. We consider\napproximating the posterior for hierarchical models, in which there are\n``global'' and ``local'' parameters. A baseline variational approximation is\ndefined as the product of a Gaussian marginal posterior for global parameters\nand a Gaussian conditional posterior for local parameters given the global\nones. Skewness corrections are then considered. The adjustment of the\nconditional posterior term for local variables is adaptive to the global\nparameter value. Optimization of baseline variational parameters is performed\njointly with the skewness correction. Our approach allows the location, scale\nand skewness to be captured separately, without using additional parameters for\nskewness adjustments. The proposed method substantially improves accuracy for\nonly a modest increase in computational cost compared to state-of-the-art\nGaussian approximations. Good performance is demonstrated in generalized linear\nmixed models and multinomial logit discrete choice models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian variational approximations are widely used for summarizing posterior\ndistributions in Bayesian models, especially in high-dimensional settings.\nHowever, a drawback of such approximations is the inability to capture skewness\nor more complex features of the posterior. Recent work suggests applying\nskewness corrections to existing Gaussian or other symmetric approximations to\naddress this limitation. We propose to incorporate the skewness correction into\nthe definition of an approximating variational family. We consider\napproximating the posterior for hierarchical models, in which there are\n``global'' and ``local'' parameters. A baseline variational approximation is\ndefined as the product of a Gaussian marginal posterior for global parameters\nand a Gaussian conditional posterior for local parameters given the global\nones. Skewness corrections are then considered. The adjustment of the\nconditional posterior term for local variables is adaptive to the global\nparameter value. Optimization of baseline variational parameters is performed\njointly with the skewness correction. Our approach allows the location, scale\nand skewness to be captured separately, without using additional parameters for\nskewness adjustments. The proposed method substantially improves accuracy for\nonly a modest increase in computational cost compared to state-of-the-art\nGaussian approximations. Good performance is demonstrated in generalized linear\nmixed models and multinomial logit discrete choice models."
                },
                "authors": [
                    {
                        "name": "Lucas Kock"
                    },
                    {
                        "name": "Linda S. L. Tan"
                    },
                    {
                        "name": "Prateek Bansal"
                    },
                    {
                        "name": "David J. Nott"
                    }
                ],
                "author_detail": {
                    "name": "David J. Nott"
                },
                "author": "David J. Nott",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18075v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18075v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05763v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05763v2",
                "updated": "2025-03-25T10:53:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    10,
                    53,
                    10,
                    1,
                    84,
                    0
                ],
                "published": "2025-01-10T07:41:47Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    7,
                    41,
                    47,
                    4,
                    10,
                    0
                ],
                "title": "StarGen: A Spatiotemporal Autoregression Framework with Video Diffusion\n  Model for Scalable and Controllable Scene Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StarGen: A Spatiotemporal Autoregression Framework with Video Diffusion\n  Model for Scalable and Controllable Scene Generation"
                },
                "summary": "Recent advances in large reconstruction and generative models have\nsignificantly improved scene reconstruction and novel view generation. However,\ndue to compute limitations, each inference with these large models is confined\nto a small area, making long-range consistent scene generation challenging. To\naddress this, we propose StarGen, a novel framework that employs a pre-trained\nvideo diffusion model in an autoregressive manner for long-range scene\ngeneration. The generation of each video clip is conditioned on the 3D warping\nof spatially adjacent images and the temporally overlapping image from\npreviously generated clips, improving spatiotemporal consistency in long-range\nscene generation with precise pose control. The spatiotemporal condition is\ncompatible with various input conditions, facilitating diverse tasks, including\nsparse view interpolation, perpetual view generation, and layout-conditioned\ncity generation. Quantitative and qualitative evaluations demonstrate StarGen's\nsuperior scalability, fidelity, and pose accuracy compared to state-of-the-art\nmethods. Project page: https://zju3dv.github.io/StarGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large reconstruction and generative models have\nsignificantly improved scene reconstruction and novel view generation. However,\ndue to compute limitations, each inference with these large models is confined\nto a small area, making long-range consistent scene generation challenging. To\naddress this, we propose StarGen, a novel framework that employs a pre-trained\nvideo diffusion model in an autoregressive manner for long-range scene\ngeneration. The generation of each video clip is conditioned on the 3D warping\nof spatially adjacent images and the temporally overlapping image from\npreviously generated clips, improving spatiotemporal consistency in long-range\nscene generation with precise pose control. The spatiotemporal condition is\ncompatible with various input conditions, facilitating diverse tasks, including\nsparse view interpolation, perpetual view generation, and layout-conditioned\ncity generation. Quantitative and qualitative evaluations demonstrate StarGen's\nsuperior scalability, fidelity, and pose accuracy compared to state-of-the-art\nmethods. Project page: https://zju3dv.github.io/StarGen."
                },
                "authors": [
                    {
                        "name": "Shangjin Zhai"
                    },
                    {
                        "name": "Zhichao Ye"
                    },
                    {
                        "name": "Jialin Liu"
                    },
                    {
                        "name": "Weijian Xie"
                    },
                    {
                        "name": "Jiaqi Hu"
                    },
                    {
                        "name": "Zhen Peng"
                    },
                    {
                        "name": "Hua Xue"
                    },
                    {
                        "name": "Danpeng Chen"
                    },
                    {
                        "name": "Xiaomeng Wang"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Haomin Liu"
                    },
                    {
                        "name": "Guofeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Guofeng Zhang"
                },
                "author": "Guofeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05763v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05763v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18840v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18840v2",
                "updated": "2025-03-25T10:52:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    10,
                    52,
                    26,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-24T16:13:04Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    13,
                    4,
                    0,
                    83,
                    0
                ],
                "title": "Learning to segment anatomy and lesions from disparately labeled sources\n  in brain MRI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to segment anatomy and lesions from disparately labeled sources\n  in brain MRI"
                },
                "summary": "Segmenting healthy tissue structures alongside lesions in brain Magnetic\nResonance Images (MRI) remains a challenge for today's algorithms due to\nlesion-caused disruption of the anatomy and lack of jointly labeled training\ndatasets, where both healthy tissues and lesions are labeled on the same\nimages. In this paper, we propose a method that is robust to lesion-caused\ndisruptions and can be trained from disparately labeled training sets, i.e.,\nwithout requiring jointly labeled samples, to automatically segment both. In\ncontrast to prior work, we decouple healthy tissue and lesion segmentation in\ntwo paths to leverage multi-sequence acquisitions and merge information with an\nattention mechanism. During inference, an image-specific adaptation reduces\nadverse influences of lesion regions on healthy tissue predictions. During\ntraining, the adaptation is taken into account through meta-learning and\nco-training is used to learn from disparately labeled training images. Our\nmodel shows an improved performance on several anatomical structures and\nlesions on a publicly available brain glioblastoma dataset compared to the\nstate-of-the-art segmentation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Segmenting healthy tissue structures alongside lesions in brain Magnetic\nResonance Images (MRI) remains a challenge for today's algorithms due to\nlesion-caused disruption of the anatomy and lack of jointly labeled training\ndatasets, where both healthy tissues and lesions are labeled on the same\nimages. In this paper, we propose a method that is robust to lesion-caused\ndisruptions and can be trained from disparately labeled training sets, i.e.,\nwithout requiring jointly labeled samples, to automatically segment both. In\ncontrast to prior work, we decouple healthy tissue and lesion segmentation in\ntwo paths to leverage multi-sequence acquisitions and merge information with an\nattention mechanism. During inference, an image-specific adaptation reduces\nadverse influences of lesion regions on healthy tissue predictions. During\ntraining, the adaptation is taken into account through meta-learning and\nco-training is used to learn from disparately labeled training images. Our\nmodel shows an improved performance on several anatomical structures and\nlesions on a publicly available brain glioblastoma dataset compared to the\nstate-of-the-art segmentation methods."
                },
                "authors": [
                    {
                        "name": "Meva Himmetoglu"
                    },
                    {
                        "name": "Ilja Ciernik"
                    },
                    {
                        "name": "Ender Konukoglu"
                    }
                ],
                "author_detail": {
                    "name": "Ender Konukoglu"
                },
                "arxiv_affiliation": "for the Alzheimer's Disease Neuroimaging Initiative",
                "author": "Ender Konukoglu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18840v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18840v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18584v2",
                "updated": "2025-03-25T10:50:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    10,
                    50,
                    57,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-24T11:41:47Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    41,
                    47,
                    0,
                    83,
                    0
                ],
                "title": "A Universal Model Combining Differential Equations and Neural Networks\n  for Ball Trajectory Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Universal Model Combining Differential Equations and Neural Networks\n  for Ball Trajectory Prediction"
                },
                "summary": "This paper presents a data driven universal ball trajectory prediction method\nintegrated with physics equations. Existing methods are designed for specific\nball types and struggle to generalize. This challenge arises from three key\nfactors. First, learning-based models require large datasets but suffer from\naccuracy drops in unseen scenarios. Second, physics-based models rely on\ncomplex formulas and detailed inputs, yet accurately obtaining ball states,\nsuch as spin, is often impractical. Third, integrating physical principles with\nneural networks to achieve high accuracy, fast inference, and strong\ngeneralization remains difficult. To address these issues, we propose an\ninnovative approach that incorporates physics-based equations and neural\nnetworks. We first derive three generalized physical formulas. Then, using a\nneural network and observed trajectory points, we infer certain parameters\nwhile fitting the remaining ones. These formulas enable precise trajectory\nprediction with minimal training data: only a few dozen samples. Extensive\nexperiments demonstrate our method superiority in generalization, real-time\nperformance, and accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a data driven universal ball trajectory prediction method\nintegrated with physics equations. Existing methods are designed for specific\nball types and struggle to generalize. This challenge arises from three key\nfactors. First, learning-based models require large datasets but suffer from\naccuracy drops in unseen scenarios. Second, physics-based models rely on\ncomplex formulas and detailed inputs, yet accurately obtaining ball states,\nsuch as spin, is often impractical. Third, integrating physical principles with\nneural networks to achieve high accuracy, fast inference, and strong\ngeneralization remains difficult. To address these issues, we propose an\ninnovative approach that incorporates physics-based equations and neural\nnetworks. We first derive three generalized physical formulas. Then, using a\nneural network and observed trajectory points, we infer certain parameters\nwhile fitting the remaining ones. These formulas enable precise trajectory\nprediction with minimal training data: only a few dozen samples. Extensive\nexperiments demonstrate our method superiority in generalization, real-time\nperformance, and accuracy."
                },
                "authors": [
                    {
                        "name": "Zhiwei Shi"
                    },
                    {
                        "name": "Chengxi Zhu"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Jun Yan"
                    },
                    {
                        "name": "Zheyun Qin"
                    },
                    {
                        "name": "Songquan Shi"
                    },
                    {
                        "name": "Zhumin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhumin Chen"
                },
                "author": "Zhumin Chen",
                "arxiv_comment": "This submission was made without my advisor's consent, and I\n  mistakenly uploaded an incorrect version of the paper. Additionally, some\n  content in the paper should not be made publicly available at this time, as\n  per my advisor's wishes. I apologize for any inconvenience this may have\n  caused",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17056v2",
                "updated": "2025-03-25T10:50:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    10,
                    50,
                    21,
                    1,
                    84,
                    0
                ],
                "published": "2024-12-22T15:08:24Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    15,
                    8,
                    24,
                    6,
                    357,
                    0
                ],
                "title": "The HalluRAG Dataset: Detecting Closed-Domain Hallucinations in RAG\n  Applications Using an LLM's Internal States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The HalluRAG Dataset: Detecting Closed-Domain Hallucinations in RAG\n  Applications Using an LLM's Internal States"
                },
                "summary": "Detecting hallucinations in large language models (LLMs) is critical for\nenhancing their reliability and trustworthiness. Most research focuses on\nhallucinations as deviations from information seen during training. However,\nthe opaque nature of an LLM's parametric knowledge complicates the\nunderstanding of why generated texts appear ungrounded: The LLM might not have\npicked up the necessary knowledge from large and often inaccessible datasets,\nor the information might have been changed or contradicted during further\ntraining. Our focus is on hallucinations involving information not used in\ntraining, which we determine by using recency to ensure the information emerged\nafter a cut-off date. This study investigates these hallucinations by detecting\nthem at sentence level using different internal states of various LLMs. We\npresent HalluRAG, a dataset designed to train classifiers on these\nhallucinations. Depending on the model and quantization, MLPs trained on\nHalluRAG detect hallucinations with test accuracies ranging up to 75 %, with\nMistral-7B-Instruct-v0.1 achieving the highest test accuracies. Our results\nshow that IAVs detect hallucinations as effectively as CEVs and reveal that\nanswerable and unanswerable prompts are encoded differently as separate\nclassifiers for these categories improved accuracy. However, HalluRAG showed\nsome limited generalizability, advocating for more diversity in datasets on\nhallucinations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting hallucinations in large language models (LLMs) is critical for\nenhancing their reliability and trustworthiness. Most research focuses on\nhallucinations as deviations from information seen during training. However,\nthe opaque nature of an LLM's parametric knowledge complicates the\nunderstanding of why generated texts appear ungrounded: The LLM might not have\npicked up the necessary knowledge from large and often inaccessible datasets,\nor the information might have been changed or contradicted during further\ntraining. Our focus is on hallucinations involving information not used in\ntraining, which we determine by using recency to ensure the information emerged\nafter a cut-off date. This study investigates these hallucinations by detecting\nthem at sentence level using different internal states of various LLMs. We\npresent HalluRAG, a dataset designed to train classifiers on these\nhallucinations. Depending on the model and quantization, MLPs trained on\nHalluRAG detect hallucinations with test accuracies ranging up to 75 %, with\nMistral-7B-Instruct-v0.1 achieving the highest test accuracies. Our results\nshow that IAVs detect hallucinations as effectively as CEVs and reveal that\nanswerable and unanswerable prompts are encoded differently as separate\nclassifiers for these categories improved accuracy. However, HalluRAG showed\nsome limited generalizability, advocating for more diversity in datasets on\nhallucinations."
                },
                "authors": [
                    {
                        "name": "Fabian Ridder"
                    },
                    {
                        "name": "Malte Schilling"
                    }
                ],
                "author_detail": {
                    "name": "Malte Schilling"
                },
                "author": "Malte Schilling",
                "arxiv_comment": "19 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19540v1",
                "updated": "2025-03-25T10:48:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    10,
                    48,
                    33,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T10:48:33Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    10,
                    48,
                    33,
                    1,
                    84,
                    0
                ],
                "title": "FLEX: A Benchmark for Evaluating Robustness of Fairness in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLEX: A Benchmark for Evaluating Robustness of Fairness in Large\n  Language Models"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced interactions between users and models. These advancements concurrently\nunderscore the need for rigorous safety evaluations due to the manifestation of\nsocial biases, which can lead to harmful societal impacts. Despite these\nconcerns, existing benchmarks may overlook the intrinsic weaknesses of LLMs,\nwhich can generate biased responses even with simple adversarial instructions.\nTo address this critical gap, we introduce a new benchmark, Fairness Benchmark\nin LLM under Extreme Scenarios (FLEX), designed to test whether LLMs can\nsustain fairness even when exposed to prompts constructed to induce bias. To\nthoroughly evaluate the robustness of LLMs, we integrate prompts that amplify\npotential biases into the fairness assessment. Comparative experiments between\nFLEX and existing benchmarks demonstrate that traditional evaluations may\nunderestimate the inherent risks in models. This highlights the need for more\nstringent LLM evaluation benchmarks to guarantee safety and fairness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced interactions between users and models. These advancements concurrently\nunderscore the need for rigorous safety evaluations due to the manifestation of\nsocial biases, which can lead to harmful societal impacts. Despite these\nconcerns, existing benchmarks may overlook the intrinsic weaknesses of LLMs,\nwhich can generate biased responses even with simple adversarial instructions.\nTo address this critical gap, we introduce a new benchmark, Fairness Benchmark\nin LLM under Extreme Scenarios (FLEX), designed to test whether LLMs can\nsustain fairness even when exposed to prompts constructed to induce bias. To\nthoroughly evaluate the robustness of LLMs, we integrate prompts that amplify\npotential biases into the fairness assessment. Comparative experiments between\nFLEX and existing benchmarks demonstrate that traditional evaluations may\nunderestimate the inherent risks in models. This highlights the need for more\nstringent LLM evaluation benchmarks to guarantee safety and fairness."
                },
                "authors": [
                    {
                        "name": "Dahyun Jung"
                    },
                    {
                        "name": "Seungyoon Lee"
                    },
                    {
                        "name": "Hyeonseok Moon"
                    },
                    {
                        "name": "Chanjun Park"
                    },
                    {
                        "name": "Heuiseok Lim"
                    }
                ],
                "author_detail": {
                    "name": "Heuiseok Lim"
                },
                "author": "Heuiseok Lim",
                "arxiv_comment": "Accepted to NAACL 2025 findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19537v1",
                "updated": "2025-03-25T10:46:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    10,
                    46,
                    8,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T10:46:08Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    10,
                    46,
                    8,
                    1,
                    84,
                    0
                ],
                "title": "Agent-Initiated Interaction in Phone UI Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-Initiated Interaction in Phone UI Automation"
                },
                "summary": "Phone automation agents aim to autonomously perform a given natural-language\nuser request, such as scheduling appointments or booking a hotel. While much\nresearch effort has been devoted to screen understanding and action planning,\ncomplex tasks often necessitate user interaction for successful completion.\nAligning the agent with the user's expectations is crucial for building trust\nand enabling personalized experiences. This requires the agent to proactively\nengage the user when necessary, avoiding actions that violate their preferences\nwhile refraining from unnecessary questions where a default action is expected.\nWe argue that such subtle agent-initiated interaction with the user deserves\nfocused research attention.\n  To promote such research, this paper introduces a task formulation for\ndetecting the need for user interaction and generating appropriate messages. We\nthoroughly define the task, including aspects like interaction timing and the\nscope of the agent's autonomy. Using this definition, we derived annotation\nguidelines and created AndroidInteraction, a diverse dataset for the task,\nleveraging an existing UI automation dataset. We tested several text-based and\nmultimodal baseline models for the task, finding that it is very challenging\nfor current LLMs. We suggest that our task formulation, dataset, baseline\nmodels and analysis will be valuable for future UI automation research,\nspecifically in addressing this crucial yet often overlooked aspect of\nagent-initiated interaction. This work provides a needed foundation to allow\npersonalized agents to properly engage the user when needed, within the context\nof phone UI automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phone automation agents aim to autonomously perform a given natural-language\nuser request, such as scheduling appointments or booking a hotel. While much\nresearch effort has been devoted to screen understanding and action planning,\ncomplex tasks often necessitate user interaction for successful completion.\nAligning the agent with the user's expectations is crucial for building trust\nand enabling personalized experiences. This requires the agent to proactively\nengage the user when necessary, avoiding actions that violate their preferences\nwhile refraining from unnecessary questions where a default action is expected.\nWe argue that such subtle agent-initiated interaction with the user deserves\nfocused research attention.\n  To promote such research, this paper introduces a task formulation for\ndetecting the need for user interaction and generating appropriate messages. We\nthoroughly define the task, including aspects like interaction timing and the\nscope of the agent's autonomy. Using this definition, we derived annotation\nguidelines and created AndroidInteraction, a diverse dataset for the task,\nleveraging an existing UI automation dataset. We tested several text-based and\nmultimodal baseline models for the task, finding that it is very challenging\nfor current LLMs. We suggest that our task formulation, dataset, baseline\nmodels and analysis will be valuable for future UI automation research,\nspecifically in addressing this crucial yet often overlooked aspect of\nagent-initiated interaction. This work provides a needed foundation to allow\npersonalized agents to properly engage the user when needed, within the context\nof phone UI automation."
                },
                "authors": [
                    {
                        "name": "Noam Kahlon"
                    },
                    {
                        "name": "Guy Rom"
                    },
                    {
                        "name": "Anatoly Efros"
                    },
                    {
                        "name": "Filippo Galgani"
                    },
                    {
                        "name": "Omri Berkovitch"
                    },
                    {
                        "name": "Sapir Caduri"
                    },
                    {
                        "name": "William E. Bishop"
                    },
                    {
                        "name": "Oriana Riva"
                    },
                    {
                        "name": "Ido Dagan"
                    }
                ],
                "author_detail": {
                    "name": "Ido Dagan"
                },
                "author": "Ido Dagan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12355v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12355v2",
                "updated": "2025-03-25T10:31:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    10,
                    31,
                    35,
                    1,
                    84,
                    0
                ],
                "published": "2024-11-19T09:16:54Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    9,
                    16,
                    54,
                    1,
                    324,
                    0
                ],
                "title": "DynFocus: Dynamic Cooperative Network Empowers LLMs with Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynFocus: Dynamic Cooperative Network Empowers LLMs with Video\n  Understanding"
                },
                "summary": "The challenge in LLM-based video understanding lies in preserving visual and\nsemantic information in long videos while maintaining a memory-affordable token\ncount. However, redundancy and correspondence in videos have hindered the\nperformance potential of existing methods. Through statistical learning on\ncurrent datasets, we observe that redundancy occurs in both repeated and\nanswer-irrelevant frames, and the corresponding frames vary with different\nquestions. This suggests the possibility of adopting dynamic encoding to\nbalance detailed video information preservation with token budget reduction. To\nthis end, we propose a dynamic cooperative network, DynFocus, for\nmemory-efficient video encoding in this paper. Specifically, i) a Dynamic Event\nPrototype Estimation (DPE) module to dynamically select meaningful frames for\nquestion answering; (ii) a Compact Cooperative Encoding (CCE) module that\nencodes meaningful frames with detailed visual appearance and the remaining\nframes with sketchy perception separately. We evaluate our method on five\npublicly available benchmarks, and experimental results consistently\ndemonstrate that our method achieves competitive performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The challenge in LLM-based video understanding lies in preserving visual and\nsemantic information in long videos while maintaining a memory-affordable token\ncount. However, redundancy and correspondence in videos have hindered the\nperformance potential of existing methods. Through statistical learning on\ncurrent datasets, we observe that redundancy occurs in both repeated and\nanswer-irrelevant frames, and the corresponding frames vary with different\nquestions. This suggests the possibility of adopting dynamic encoding to\nbalance detailed video information preservation with token budget reduction. To\nthis end, we propose a dynamic cooperative network, DynFocus, for\nmemory-efficient video encoding in this paper. Specifically, i) a Dynamic Event\nPrototype Estimation (DPE) module to dynamically select meaningful frames for\nquestion answering; (ii) a Compact Cooperative Encoding (CCE) module that\nencodes meaningful frames with detailed visual appearance and the remaining\nframes with sketchy perception separately. We evaluate our method on five\npublicly available benchmarks, and experimental results consistently\ndemonstrate that our method achieves competitive performance."
                },
                "authors": [
                    {
                        "name": "Yudong Han"
                    },
                    {
                        "name": "Qingpei Guo"
                    },
                    {
                        "name": "Liyuan Pan"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Yu Guan"
                    },
                    {
                        "name": "Ming Yang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Yang"
                },
                "author": "Ming Yang",
                "arxiv_comment": "Accepted by CVPR 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12355v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12355v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15052v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15052v3",
                "updated": "2025-03-26T03:58:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    3,
                    58,
                    42,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-19T09:42:32Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    9,
                    42,
                    32,
                    2,
                    78,
                    0
                ],
                "title": "Discrete treatment of inverse Compton scattering: implications on\n  parameter estimation in gamma-ray astronomy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete treatment of inverse Compton scattering: implications on\n  parameter estimation in gamma-ray astronomy"
                },
                "summary": "In gamma-ray astronomy and cosmic-ray physics, the continuous approximation\nof inverse Compton scattering (ICS) is widely adopted to model the evolution of\nelectron energy. However, when the initial electron energy approaches $\\sim100$\nTeV, the discrete nature of ICS becomes prominent, and the energy of evolved\nelectrons should be considered as a broad distribution rather than a\ndeterministic value. By simulating the evolution paths of individual electrons\nunder ICS, we capture this discrete nature and demonstrate that when the\nelectron injection spectrum exhibits a high-energy cutoff, the correct discrete\ntreatment yields a higher cutoff energy in the evolved spectrum compared to the\ncontinuous approximation. Applying the discrete ICS treatment to interpret the\ngamma-ray spectrum of the Geminga pulsar halo measured by HAWC, we find that\nthe inferred cutoff energy of the injection spectrum is correspondingly lower\nthan that derived using the continuous approximation at a $95\\%$ confidence\nlevel. This suggests that the systematic bias introduced by the approximation\nhas exceeded the measurement precision. We also expect the application of the\ndiscrete ICS correction in the PeV regime using the ultra-high-energy gamma-ray\nsource 1LHAASO J1954+2836u as a case study, pointing out that adopting the\ncontinuous approximation may considerably overestimate the electron\nacceleration capability of the source.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In gamma-ray astronomy and cosmic-ray physics, the continuous approximation\nof inverse Compton scattering (ICS) is widely adopted to model the evolution of\nelectron energy. However, when the initial electron energy approaches $\\sim100$\nTeV, the discrete nature of ICS becomes prominent, and the energy of evolved\nelectrons should be considered as a broad distribution rather than a\ndeterministic value. By simulating the evolution paths of individual electrons\nunder ICS, we capture this discrete nature and demonstrate that when the\nelectron injection spectrum exhibits a high-energy cutoff, the correct discrete\ntreatment yields a higher cutoff energy in the evolved spectrum compared to the\ncontinuous approximation. Applying the discrete ICS treatment to interpret the\ngamma-ray spectrum of the Geminga pulsar halo measured by HAWC, we find that\nthe inferred cutoff energy of the injection spectrum is correspondingly lower\nthan that derived using the continuous approximation at a $95\\%$ confidence\nlevel. This suggests that the systematic bias introduced by the approximation\nhas exceeded the measurement precision. We also expect the application of the\ndiscrete ICS correction in the PeV regime using the ultra-high-energy gamma-ray\nsource 1LHAASO J1954+2836u as a case study, pointing out that adopting the\ncontinuous approximation may considerably overestimate the electron\nacceleration capability of the source."
                },
                "authors": [
                    {
                        "name": "Junji Xia"
                    },
                    {
                        "name": "Xingjian Lv"
                    },
                    {
                        "name": "Kun Fang"
                    },
                    {
                        "name": "Siming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Siming Liu"
                },
                "author": "Siming Liu",
                "arxiv_comment": "8 pages, 5 figures, submitted to PRD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15052v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15052v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19527v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19527v2",
                "updated": "2025-03-26T10:28:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    10,
                    28,
                    12,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-25T10:28:51Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    10,
                    28,
                    51,
                    1,
                    84,
                    0
                ],
                "title": "A multi-scale investigation into the diagnostic potential of the\n  HCN/HCO$^+$ ratio for AGN and starburst activity in nearby galaxies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A multi-scale investigation into the diagnostic potential of the\n  HCN/HCO$^+$ ratio for AGN and starburst activity in nearby galaxies"
                },
                "summary": "(Abridged) The identification of AGN and SB regions in galaxies is crucial\nfor understanding the role of various physical processes in galaxy evolution.\nMolecular line ratios, such as the HCN/HCO+ ratio, have been proposed as\npotential tracers of these distinct environments. This paper aims to assess the\nreliability of the HCN/HCO+ ratio, from J = 1-0 to J = 4-3 transitions, as a\ndiagnostic tool for differentiating AGN and SB activity across a diverse sample\nof nearby galaxies. We focus on evaluating the effect of spatial resolution on\nthe robustness of these ratios and investigate the underlying physical\nconditions that drive observed variations. We compile observations of HCN and\nHCO+ lines across multiple J transitions from various sources, covering\ndifferent galaxy types, including Seyferts, starbursts, and (ultra-)luminous\ninfrared galaxies (U/LIRGs). The observations span spatial scales from\ncloud-sized regions to kiloparsec scales. We analyse the behaviour of these\nratios at varying resolutions and employ non-LTE radiative transfer models to\ninfer the physical conditions that drive the observed ratios. We find that the\nHCN/HCO+ ratio from higher J transitions can differentiate between AGN and SB\nactivity when observed at high spatial resolution. This distinction occurs\naround unity. However, at lower resolutions, contamination from multiple\nemission sources and beam averaging effects destroy these distinctions.\nModelling suggests that elevated HCN/HCO+ ratios in AGN-dominated regions are\nlargely driven by an enhancement in HCN abundance relative to HCO+, likely due\nto high-temperature chemistry or increased excitation. Our study confirms that\nthe HCN/HCO+ ratio, particularly of higher J transitions, can be a reliable\ntracer of AGN versus SB activity if observations are conducted at sufficiently\nhigh spatial resolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "(Abridged) The identification of AGN and SB regions in galaxies is crucial\nfor understanding the role of various physical processes in galaxy evolution.\nMolecular line ratios, such as the HCN/HCO+ ratio, have been proposed as\npotential tracers of these distinct environments. This paper aims to assess the\nreliability of the HCN/HCO+ ratio, from J = 1-0 to J = 4-3 transitions, as a\ndiagnostic tool for differentiating AGN and SB activity across a diverse sample\nof nearby galaxies. We focus on evaluating the effect of spatial resolution on\nthe robustness of these ratios and investigate the underlying physical\nconditions that drive observed variations. We compile observations of HCN and\nHCO+ lines across multiple J transitions from various sources, covering\ndifferent galaxy types, including Seyferts, starbursts, and (ultra-)luminous\ninfrared galaxies (U/LIRGs). The observations span spatial scales from\ncloud-sized regions to kiloparsec scales. We analyse the behaviour of these\nratios at varying resolutions and employ non-LTE radiative transfer models to\ninfer the physical conditions that drive the observed ratios. We find that the\nHCN/HCO+ ratio from higher J transitions can differentiate between AGN and SB\nactivity when observed at high spatial resolution. This distinction occurs\naround unity. However, at lower resolutions, contamination from multiple\nemission sources and beam averaging effects destroy these distinctions.\nModelling suggests that elevated HCN/HCO+ ratios in AGN-dominated regions are\nlargely driven by an enhancement in HCN abundance relative to HCO+, likely due\nto high-temperature chemistry or increased excitation. Our study confirms that\nthe HCN/HCO+ ratio, particularly of higher J transitions, can be a reliable\ntracer of AGN versus SB activity if observations are conducted at sufficiently\nhigh spatial resolution."
                },
                "authors": [
                    {
                        "name": "J. Butterworth"
                    },
                    {
                        "name": "S. Viti"
                    },
                    {
                        "name": "Y. Wang"
                    }
                ],
                "author_detail": {
                    "name": "Y. Wang"
                },
                "author": "Y. Wang",
                "arxiv_comment": "11 pages (including appendices), 9 figures (including those within\n  appendices), Accepted for publication to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19527v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19527v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16728v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16728v2",
                "updated": "2025-03-25T10:03:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    10,
                    3,
                    25,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-20T22:12:08Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    22,
                    12,
                    8,
                    3,
                    79,
                    0
                ],
                "title": "Natural Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Generation"
                },
                "summary": "This article provides a brief overview of the field of Natural Language\nGeneration. The term Natural Language Generation (NLG), in its broadest\ndefinition, refers to the study of systems that verbalize some form of\ninformation through natural language. That information could be stored in a\nlarge database or knowledge graph (in data-to-text applications), but NLG\nresearchers may also study summarisation (text-to-text) or image captioning\n(image-to-text), for example. As a subfield of Natural Language Processing, NLG\nis closely related to other sub-disciplines such as Machine Translation (MT)\nand Dialog Systems. Some NLG researchers exclude MT from their definition of\nthe field, since there is no content selection involved where the system has to\ndetermine what to say. Conversely, dialog systems do not typically fall under\nthe header of Natural Language Generation since NLG is just one component of\ndialog systems (the others being Natural Language Understanding and Dialog\nManagement). However, with the rise of Large Language Models (LLMs), different\nsubfields of Natural Language Processing have converged on similar\nmethodologies for the production of natural language and the evaluation of\nautomatically generated text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article provides a brief overview of the field of Natural Language\nGeneration. The term Natural Language Generation (NLG), in its broadest\ndefinition, refers to the study of systems that verbalize some form of\ninformation through natural language. That information could be stored in a\nlarge database or knowledge graph (in data-to-text applications), but NLG\nresearchers may also study summarisation (text-to-text) or image captioning\n(image-to-text), for example. As a subfield of Natural Language Processing, NLG\nis closely related to other sub-disciplines such as Machine Translation (MT)\nand Dialog Systems. Some NLG researchers exclude MT from their definition of\nthe field, since there is no content selection involved where the system has to\ndetermine what to say. Conversely, dialog systems do not typically fall under\nthe header of Natural Language Generation since NLG is just one component of\ndialog systems (the others being Natural Language Understanding and Dialog\nManagement). However, with the rise of Large Language Models (LLMs), different\nsubfields of Natural Language Processing have converged on similar\nmethodologies for the production of natural language and the evaluation of\nautomatically generated text."
                },
                "authors": [
                    {
                        "name": "Emiel van Miltenburg"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "arxiv_comment": "3 pages + references. Submitted for publication in the Encyclopedia\n  of Language & Linguistics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16728v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16728v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19505v1",
                "updated": "2025-03-25T09:56:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    9,
                    56,
                    21,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T09:56:21Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    9,
                    56,
                    21,
                    1,
                    84,
                    0
                ],
                "title": "Single-Step Latent Consistency Model for Remote Sensing Image\n  Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-Step Latent Consistency Model for Remote Sensing Image\n  Super-Resolution"
                },
                "summary": "Recent advancements in diffusion models (DMs) have greatly advanced remote\nsensing image super-resolution (RSISR). However, their iterative sampling\nprocesses often result in slow inference speeds, limiting their application in\nreal-time tasks. To address this challenge, we propose the latent consistency\nmodel for super-resolution (LCMSR), a novel single-step diffusion approach\ndesigned to enhance both efficiency and visual quality in RSISR tasks. Our\nproposal is structured into two distinct stages. In the first stage, we\npretrain a residual autoencoder to encode the differential information between\nhigh-resolution (HR) and low-resolution (LR) images, transitioning the\ndiffusion process into a latent space to reduce computational costs. The second\nstage focuses on consistency diffusion learning, which aims to learn the\ndistribution of residual encodings in the latent space, conditioned on LR\nimages. The consistency constraint enforces that predictions at any two\ntimesteps along the reverse diffusion trajectory remain consistent, enabling\ndirect mapping from noise to data. As a result, the proposed LCMSR reduces the\niterative steps of traditional diffusion models from 50-1000 or more to just a\nsingle step, significantly improving efficiency. Experimental results\ndemonstrate that LCMSR effectively balances efficiency and performance,\nachieving inference times comparable to non-diffusion models while maintaining\nhigh-quality output.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in diffusion models (DMs) have greatly advanced remote\nsensing image super-resolution (RSISR). However, their iterative sampling\nprocesses often result in slow inference speeds, limiting their application in\nreal-time tasks. To address this challenge, we propose the latent consistency\nmodel for super-resolution (LCMSR), a novel single-step diffusion approach\ndesigned to enhance both efficiency and visual quality in RSISR tasks. Our\nproposal is structured into two distinct stages. In the first stage, we\npretrain a residual autoencoder to encode the differential information between\nhigh-resolution (HR) and low-resolution (LR) images, transitioning the\ndiffusion process into a latent space to reduce computational costs. The second\nstage focuses on consistency diffusion learning, which aims to learn the\ndistribution of residual encodings in the latent space, conditioned on LR\nimages. The consistency constraint enforces that predictions at any two\ntimesteps along the reverse diffusion trajectory remain consistent, enabling\ndirect mapping from noise to data. As a result, the proposed LCMSR reduces the\niterative steps of traditional diffusion models from 50-1000 or more to just a\nsingle step, significantly improving efficiency. Experimental results\ndemonstrate that LCMSR effectively balances efficiency and performance,\nachieving inference times comparable to non-diffusion models while maintaining\nhigh-quality output."
                },
                "authors": [
                    {
                        "name": "Xiaohui Sun"
                    },
                    {
                        "name": "Jiangwei Mo"
                    },
                    {
                        "name": "Hanlin Wu"
                    },
                    {
                        "name": "Jie Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jie Ma"
                },
                "author": "Jie Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05529v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05529v2",
                "updated": "2025-03-25T09:52:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    9,
                    52,
                    28,
                    1,
                    84,
                    0
                ],
                "published": "2024-09-09T11:37:32Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    11,
                    37,
                    32,
                    0,
                    253,
                    0
                ],
                "title": "Bootstrapping Estimators based on the Block Maxima Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bootstrapping Estimators based on the Block Maxima Method"
                },
                "summary": "The block maxima method is a standard approach for analyzing the extremal\nbehavior of a potentially multivariate time series. It has recently been found\nthat the classical approach based on disjoint block maxima may be universally\nimproved by considering sliding block maxima instead. However, the asymptotic\nvariance formula for estimators based on sliding block maxima involves an\nintegral over the covariance of a certain family of multivariate extreme value\ndistributions, which makes its estimation, and inference in general, an\nintricate problem. As an alternative, one may rely on bootstrap approximations:\nwe show that naive block-bootstrap approaches from time series analysis are\ninconsistent even in i.i.d.\\ situations, and provide a consistent alternative\nbased on resampling circular block maxima. As a by-product, we show consistency\nof the classical resampling bootstrap for disjoint block maxima, and that\nestimators based on circular block maxima have the same asymptotic variance as\ntheir sliding block maxima counterparts. The finite sample properties are\nillustrated by Monte Carlo experiments, and the methods are demonstrated by a\ncase study of precipitation extremes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The block maxima method is a standard approach for analyzing the extremal\nbehavior of a potentially multivariate time series. It has recently been found\nthat the classical approach based on disjoint block maxima may be universally\nimproved by considering sliding block maxima instead. However, the asymptotic\nvariance formula for estimators based on sliding block maxima involves an\nintegral over the covariance of a certain family of multivariate extreme value\ndistributions, which makes its estimation, and inference in general, an\nintricate problem. As an alternative, one may rely on bootstrap approximations:\nwe show that naive block-bootstrap approaches from time series analysis are\ninconsistent even in i.i.d.\\ situations, and provide a consistent alternative\nbased on resampling circular block maxima. As a by-product, we show consistency\nof the classical resampling bootstrap for disjoint block maxima, and that\nestimators based on circular block maxima have the same asymptotic variance as\ntheir sliding block maxima counterparts. The finite sample properties are\nillustrated by Monte Carlo experiments, and the methods are demonstrated by a\ncase study of precipitation extremes."
                },
                "authors": [
                    {
                        "name": "Axel Bücher"
                    },
                    {
                        "name": "Torben Staud"
                    }
                ],
                "author_detail": {
                    "name": "Torben Staud"
                },
                "author": "Torben Staud",
                "arxiv_comment": "Main article: 27 pages, supplement: 30 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05529v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05529v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "Primary 62F40, 62G32, Secondary 62E20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07752v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07752v3",
                "updated": "2025-03-25T09:46:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    9,
                    46,
                    2,
                    1,
                    84,
                    0
                ],
                "published": "2024-10-10T09:28:36Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    9,
                    28,
                    36,
                    3,
                    284,
                    0
                ],
                "title": "Lost in Time: A New Temporal Benchmark for VideoLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Time: A New Temporal Benchmark for VideoLLMs"
                },
                "summary": "Large language models have demonstrated impressive performance when\nintegrated with vision models even enabling video understanding. However,\nevaluating video models presents its own unique challenges, for which several\nbenchmarks have been proposed. In this paper, we show that the currently most\nused video-language benchmarks can be solved without requiring much temporal\nreasoning. We identified three main issues in existing datasets: (i) static\ninformation from single frames is often sufficient to solve the tasks (ii) the\ntext of the questions and candidate answers is overly informative, allowing\nmodels to answer correctly without relying on any visual input (iii) world\nknowledge alone can answer many of the questions, making the benchmarks a test\nof knowledge replication rather than video reasoning. In addition, we found\nthat open-ended question-answering benchmarks for video understanding suffer\nfrom similar issues while the automatic evaluation process with LLMs is\nunreliable, making it an unsuitable alternative. As a solution, we propose\nTVBench, a novel open-source video multiple-choice question-answering\nbenchmark, and demonstrate through extensive evaluations that it requires a\nhigh level of temporal understanding. Surprisingly, we find that most recent\nstate-of-the-art video-language models perform similarly to random performance\non TVBench, with only a few models such as Qwen2-VL, and Tarsier clearly\nsurpassing this baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated impressive performance when\nintegrated with vision models even enabling video understanding. However,\nevaluating video models presents its own unique challenges, for which several\nbenchmarks have been proposed. In this paper, we show that the currently most\nused video-language benchmarks can be solved without requiring much temporal\nreasoning. We identified three main issues in existing datasets: (i) static\ninformation from single frames is often sufficient to solve the tasks (ii) the\ntext of the questions and candidate answers is overly informative, allowing\nmodels to answer correctly without relying on any visual input (iii) world\nknowledge alone can answer many of the questions, making the benchmarks a test\nof knowledge replication rather than video reasoning. In addition, we found\nthat open-ended question-answering benchmarks for video understanding suffer\nfrom similar issues while the automatic evaluation process with LLMs is\nunreliable, making it an unsuitable alternative. As a solution, we propose\nTVBench, a novel open-source video multiple-choice question-answering\nbenchmark, and demonstrate through extensive evaluations that it requires a\nhigh level of temporal understanding. Surprisingly, we find that most recent\nstate-of-the-art video-language models perform similarly to random performance\non TVBench, with only a few models such as Qwen2-VL, and Tarsier clearly\nsurpassing this baseline."
                },
                "authors": [
                    {
                        "name": "Daniel Cores"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "Manuel Mucientes"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07752v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07752v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00171v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00171v3",
                "updated": "2025-03-25T09:43:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    9,
                    43,
                    25,
                    1,
                    84,
                    0
                ],
                "published": "2024-11-29T17:36:03Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    36,
                    3,
                    4,
                    334,
                    0
                ],
                "title": "RoboMatrix: A Skill-centric Hierarchical Framework for Scalable Robot\n  Task Planning and Execution in Open-World",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboMatrix: A Skill-centric Hierarchical Framework for Scalable Robot\n  Task Planning and Execution in Open-World"
                },
                "summary": "Existing robot policies predominantly adopt the task-centric approach,\nrequiring end-to-end task data collection. This results in limited\ngeneralization to new tasks and difficulties in pinpointing errors within\nlong-horizon, multi-stage tasks. To address this, we propose RoboMatrix, a\nskill-centric hierarchical framework designed for scalable robot task planning\nand execution in open-world environments. RoboMatrix extracts general\nmeta-skills from diverse complex tasks, enabling the completion of unseen tasks\nthrough skill composition. Its architecture consists of a high-level scheduling\nlayer that utilizes large language models (LLMs) for task decomposition, an\nintermediate skill layer housing meta-skill models, and a low-level hardware\nlayer for robot control. A key innovation of our work is the introduction of\nthe first unified vision-language-action (VLA) model capable of seamlessly\nintegrating both movement and manipulation within one model. This is achieved\nby combining vision and language prompts to generate discrete actions.\nExperimental results demonstrate that RoboMatrix achieves a 50% higher success\nrate than task-centric baselines when applied to unseen objects, scenes, and\ntasks. To advance open-world robotics research, we will open-source code,\nhardware designs, model weights, and datasets at\nhttps://github.com/WayneMao/RoboMatrix.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing robot policies predominantly adopt the task-centric approach,\nrequiring end-to-end task data collection. This results in limited\ngeneralization to new tasks and difficulties in pinpointing errors within\nlong-horizon, multi-stage tasks. To address this, we propose RoboMatrix, a\nskill-centric hierarchical framework designed for scalable robot task planning\nand execution in open-world environments. RoboMatrix extracts general\nmeta-skills from diverse complex tasks, enabling the completion of unseen tasks\nthrough skill composition. Its architecture consists of a high-level scheduling\nlayer that utilizes large language models (LLMs) for task decomposition, an\nintermediate skill layer housing meta-skill models, and a low-level hardware\nlayer for robot control. A key innovation of our work is the introduction of\nthe first unified vision-language-action (VLA) model capable of seamlessly\nintegrating both movement and manipulation within one model. This is achieved\nby combining vision and language prompts to generate discrete actions.\nExperimental results demonstrate that RoboMatrix achieves a 50% higher success\nrate than task-centric baselines when applied to unseen objects, scenes, and\ntasks. To advance open-world robotics research, we will open-source code,\nhardware designs, model weights, and datasets at\nhttps://github.com/WayneMao/RoboMatrix."
                },
                "authors": [
                    {
                        "name": "Weixin Mao"
                    },
                    {
                        "name": "Weiheng Zhong"
                    },
                    {
                        "name": "Zhou Jiang"
                    },
                    {
                        "name": "Dong Fang"
                    },
                    {
                        "name": "Zhongyue Zhang"
                    },
                    {
                        "name": "Zihan Lan"
                    },
                    {
                        "name": "Haosheng Li"
                    },
                    {
                        "name": "Fan Jia"
                    },
                    {
                        "name": "Tiancai Wang"
                    },
                    {
                        "name": "Haoqiang Fan"
                    },
                    {
                        "name": "Osamu Yoshie"
                    }
                ],
                "author_detail": {
                    "name": "Osamu Yoshie"
                },
                "author": "Osamu Yoshie",
                "arxiv_comment": "17 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00171v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00171v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00088v2",
                "updated": "2025-03-25T09:27:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    9,
                    27,
                    16,
                    1,
                    84,
                    0
                ],
                "published": "2024-06-25T08:38:38Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    8,
                    38,
                    38,
                    1,
                    177,
                    0
                ],
                "title": "T-MAC: CPU Renaissance via Table Lookup for Low-Bit LLM Deployment on\n  Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T-MAC: CPU Renaissance via Table Lookup for Low-Bit LLM Deployment on\n  Edge"
                },
                "summary": "The deployment of Large Language Models (LLMs) on edge devices is\nincreasingly important to enhance on-device intelligence. Weight quantization\nis crucial for reducing the memory footprint of LLMs on devices. However,\nlow-bit LLMs necessitate mixed precision matrix multiplication (mpGEMM) of low\nprecision weights and high precision activations during inference. Existing\nsystems, lacking native support for mpGEMM, resort to dequantize weights for\nhigh precision computation. Such an indirect way can lead to a significant\ninference overhead.\n  In this paper, we introduce T-MAC, an innovative lookup table(LUT)-based\nmethod designed for efficient low-bit LLM (i.e., weight-quantized LLM)\ninference on CPUs. T-MAC directly supports mpGEMM without dequantization, while\nsimultaneously eliminating multiplications and reducing additions required.\nSpecifically, T-MAC transforms the traditional data-type-centric multiplication\nto bit-wise table lookup, and enables a unified and scalable mpGEMM solution.\n  Our LUT-based kernels scale linearly to the weight bit-width. Evaluated on\nlow-bit Llama and BitNet models, T-MAC demonstrates up to 4x increase in\nthroughput and 70% reduction in energy consumption compared to llama.cpp. For\nBitNet-b1.58-3B, T-MAC delivers a token generation throughput of 30 tokens/s\nwith a single core and 71 tokens/s with eight cores on M2-Ultra, and 11\ntokens/s on lower-end devices like Raspberry Pi 5, which significantly exceeds\nthe adult average reading speed. T-MAC with LUT-based computing paradigm, paves\nthe way for the practical deployment of low-bit LLMs on resource-constrained\nedge devices without compromising computational efficiency. The system is\nopen-sourced at https://github.com/microsoft/T-MAC .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Large Language Models (LLMs) on edge devices is\nincreasingly important to enhance on-device intelligence. Weight quantization\nis crucial for reducing the memory footprint of LLMs on devices. However,\nlow-bit LLMs necessitate mixed precision matrix multiplication (mpGEMM) of low\nprecision weights and high precision activations during inference. Existing\nsystems, lacking native support for mpGEMM, resort to dequantize weights for\nhigh precision computation. Such an indirect way can lead to a significant\ninference overhead.\n  In this paper, we introduce T-MAC, an innovative lookup table(LUT)-based\nmethod designed for efficient low-bit LLM (i.e., weight-quantized LLM)\ninference on CPUs. T-MAC directly supports mpGEMM without dequantization, while\nsimultaneously eliminating multiplications and reducing additions required.\nSpecifically, T-MAC transforms the traditional data-type-centric multiplication\nto bit-wise table lookup, and enables a unified and scalable mpGEMM solution.\n  Our LUT-based kernels scale linearly to the weight bit-width. Evaluated on\nlow-bit Llama and BitNet models, T-MAC demonstrates up to 4x increase in\nthroughput and 70% reduction in energy consumption compared to llama.cpp. For\nBitNet-b1.58-3B, T-MAC delivers a token generation throughput of 30 tokens/s\nwith a single core and 71 tokens/s with eight cores on M2-Ultra, and 11\ntokens/s on lower-end devices like Raspberry Pi 5, which significantly exceeds\nthe adult average reading speed. T-MAC with LUT-based computing paradigm, paves\nthe way for the practical deployment of low-bit LLMs on resource-constrained\nedge devices without compromising computational efficiency. The system is\nopen-sourced at https://github.com/microsoft/T-MAC ."
                },
                "authors": [
                    {
                        "name": "Jianyu Wei"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Lingxiao Ma"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Yanyong Zhang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "arxiv_doi": "10.1145/3689031.3696099",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689031.3696099",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.00088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "EuroSys 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13516v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13516v4",
                "updated": "2025-03-25T09:23:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    9,
                    23,
                    55,
                    1,
                    84,
                    0
                ],
                "published": "2024-12-18T05:33:16Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    33,
                    16,
                    2,
                    353,
                    0
                ],
                "title": "Learning Causal Transition Matrix for Instance-dependent Label Noise",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Causal Transition Matrix for Instance-dependent Label Noise"
                },
                "summary": "Noisy labels are both inevitable and problematic in machine learning methods,\nas they negatively impact models' generalization ability by causing\noverfitting. In the context of learning with noise, the transition matrix plays\na crucial role in the design of statistically consistent algorithms. However,\nthe transition matrix is often considered unidentifiable. One strand of methods\ntypically addresses this problem by assuming that the transition matrix is\ninstance-independent; that is, the probability of mislabeling a particular\ninstance is not influenced by its characteristics or attributes. This\nassumption is clearly invalid in complex real-world scenarios. To better\nunderstand the transition relationship and relax this assumption, we propose to\nstudy the data generation process of noisy labels from a causal perspective. We\ndiscover that an unobservable latent variable can affect either the instance\nitself, the label annotation procedure, or both, which complicates the\nidentification of the transition matrix. To address various scenarios, we have\nunified these observations within a new causal graph. In this graph, the input\ninstance is divided into a noise-resistant component and a noise-sensitive\ncomponent based on whether they are affected by the latent variable. These two\ncomponents contribute to identifying the ``causal transition matrix'', which\napproximates the true transition matrix with theoretical guarantee. In line\nwith this, we have designed a novel training framework that explicitly models\nthis causal relationship and, as a result, achieves a more accurate model for\ninferring the clean label.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noisy labels are both inevitable and problematic in machine learning methods,\nas they negatively impact models' generalization ability by causing\noverfitting. In the context of learning with noise, the transition matrix plays\na crucial role in the design of statistically consistent algorithms. However,\nthe transition matrix is often considered unidentifiable. One strand of methods\ntypically addresses this problem by assuming that the transition matrix is\ninstance-independent; that is, the probability of mislabeling a particular\ninstance is not influenced by its characteristics or attributes. This\nassumption is clearly invalid in complex real-world scenarios. To better\nunderstand the transition relationship and relax this assumption, we propose to\nstudy the data generation process of noisy labels from a causal perspective. We\ndiscover that an unobservable latent variable can affect either the instance\nitself, the label annotation procedure, or both, which complicates the\nidentification of the transition matrix. To address various scenarios, we have\nunified these observations within a new causal graph. In this graph, the input\ninstance is divided into a noise-resistant component and a noise-sensitive\ncomponent based on whether they are affected by the latent variable. These two\ncomponents contribute to identifying the ``causal transition matrix'', which\napproximates the true transition matrix with theoretical guarantee. In line\nwith this, we have designed a novel training framework that explicitly models\nthis causal relationship and, as a result, achieves a more accurate model for\ninferring the clean label."
                },
                "authors": [
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Tai-Wei Chang"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Ximing Li"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Jun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhou"
                },
                "author": "Jun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13516v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13516v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19482v1",
                "updated": "2025-03-25T09:18:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    9,
                    18,
                    27,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T09:18:27Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    9,
                    18,
                    27,
                    1,
                    84,
                    0
                ],
                "title": "KSHSeek: Data-Driven Approaches to Mitigating and Detecting\n  Knowledge-Shortcut Hallucinations in Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KSHSeek: Data-Driven Approaches to Mitigating and Detecting\n  Knowledge-Shortcut Hallucinations in Generative Models"
                },
                "summary": "The emergence of large language models (LLMs) has significantly advanced the\ndevelopment of natural language processing (NLP), especially in text generation\ntasks like question answering. However, model hallucinations remain a major\nchallenge in natural language generation (NLG) tasks due to their complex\ncauses. We systematically expand on the causes of factual hallucinations from\nthe perspective of knowledge shortcuts, analyzing hallucinations arising from\ncorrect and defect-free data and demonstrating that knowledge-shortcut\nhallucinations are prevalent in generative models. To mitigate this issue, we\npropose a high similarity pruning algorithm at the data preprocessing level to\nreduce spurious correlations in the data. Additionally, we design a specific\ndetection method for knowledge-shortcut hallucinations to evaluate the\neffectiveness of our mitigation strategy. Experimental results show that our\napproach effectively reduces knowledge-shortcut hallucinations, particularly in\nfine-tuning tasks, without negatively impacting model performance in question\nanswering. This work introduces a new paradigm for mitigating specific\nhallucination issues in generative models, enhancing their robustness and\nreliability in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) has significantly advanced the\ndevelopment of natural language processing (NLP), especially in text generation\ntasks like question answering. However, model hallucinations remain a major\nchallenge in natural language generation (NLG) tasks due to their complex\ncauses. We systematically expand on the causes of factual hallucinations from\nthe perspective of knowledge shortcuts, analyzing hallucinations arising from\ncorrect and defect-free data and demonstrating that knowledge-shortcut\nhallucinations are prevalent in generative models. To mitigate this issue, we\npropose a high similarity pruning algorithm at the data preprocessing level to\nreduce spurious correlations in the data. Additionally, we design a specific\ndetection method for knowledge-shortcut hallucinations to evaluate the\neffectiveness of our mitigation strategy. Experimental results show that our\napproach effectively reduces knowledge-shortcut hallucinations, particularly in\nfine-tuning tasks, without negatively impacting model performance in question\nanswering. This work introduces a new paradigm for mitigating specific\nhallucination issues in generative models, enhancing their robustness and\nreliability in real-world applications."
                },
                "authors": [
                    {
                        "name": "Zhiwei Wang"
                    },
                    {
                        "name": "Zhongxin Liu"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Meng Xu"
                    },
                    {
                        "name": "Yuqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yuqing Zhang"
                },
                "author": "Yuqing Zhang",
                "arxiv_comment": "16 pages, 34 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16821v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16821v4",
                "updated": "2025-03-25T09:09:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    9,
                    9,
                    7,
                    1,
                    84,
                    0
                ],
                "published": "2024-11-25T17:15:41Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    17,
                    15,
                    41,
                    0,
                    330,
                    0
                ],
                "title": "KL-geodesics flow matching with a novel sampling scheme",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KL-geodesics flow matching with a novel sampling scheme"
                },
                "summary": "Non-autoregressive language models generate all tokens simultaneously,\noffering potential speed advantages over traditional autoregressive models, but\nthey face challenges in modeling the complex dependencies inherent in text\ndata. In this work, we investigate a conditional flow matching approach for\ntext generation. We represent tokens as one-hot vectors in a \\(V\\)-dimensional\nsimplex and utilize geodesics under the Kullback-Leibler (KL) divergence, which\ncorrespond to linear interpolation in logit space. We provide a theoretical\njustification that maximizing the conditional likelihood \\(P_{\\theta}(x_1 \\mid\nx_t, t)\\) yields the exact flow matching velocity under logit interpolation. To\naddress the suboptimal performance of basic inference, we propose a novel\nempirical sampling scheme that iteratively samples from the conditional\ndistribution and introduces additional noise, significantly improving results\ndespite lacking full theoretical underpinnings. Furthermore, we propose a\nhybrid inference method that combines the basic approach with the sampling\nscheme. This method demonstrates superior performance on both conditional and\nunconditional text generation experiments compared to previous SOTA method for\ndiscrete flow matching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-autoregressive language models generate all tokens simultaneously,\noffering potential speed advantages over traditional autoregressive models, but\nthey face challenges in modeling the complex dependencies inherent in text\ndata. In this work, we investigate a conditional flow matching approach for\ntext generation. We represent tokens as one-hot vectors in a \\(V\\)-dimensional\nsimplex and utilize geodesics under the Kullback-Leibler (KL) divergence, which\ncorrespond to linear interpolation in logit space. We provide a theoretical\njustification that maximizing the conditional likelihood \\(P_{\\theta}(x_1 \\mid\nx_t, t)\\) yields the exact flow matching velocity under logit interpolation. To\naddress the suboptimal performance of basic inference, we propose a novel\nempirical sampling scheme that iteratively samples from the conditional\ndistribution and introduces additional noise, significantly improving results\ndespite lacking full theoretical underpinnings. Furthermore, we propose a\nhybrid inference method that combines the basic approach with the sampling\nscheme. This method demonstrates superior performance on both conditional and\nunconditional text generation experiments compared to previous SOTA method for\ndiscrete flow matching."
                },
                "authors": [
                    {
                        "name": "Egor Sevriugov"
                    },
                    {
                        "name": "Ivan Oseledets"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Oseledets"
                },
                "author": "Ivan Oseledets",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16821v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16821v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13652v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13652v2",
                "updated": "2025-03-25T09:06:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    9,
                    6,
                    8,
                    1,
                    84,
                    0
                ],
                "published": "2024-12-18T09:31:06Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    31,
                    6,
                    2,
                    353,
                    0
                ],
                "title": "RelationField: Relate Anything in Radiance Fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RelationField: Relate Anything in Radiance Fields"
                },
                "summary": "Neural radiance fields are an emerging 3D scene representation and recently\neven been extended to learn features for scene understanding by distilling\nopen-vocabulary features from vision-language models. However, current method\nprimarily focus on object-centric representations, supporting object\nsegmentation or detection, while understanding semantic relationships between\nobjects remains largely unexplored. To address this gap, we propose\nRelationField, the first method to extract inter-object relationships directly\nfrom neural radiance fields. RelationField represents relationships between\nobjects as pairs of rays within a neural radiance field, effectively extending\nits formulation to include implicit relationship queries. To teach\nRelationField complex, open-vocabulary relationships, relationship knowledge is\ndistilled from multi-modal LLMs. To evaluate RelationField, we solve\nopen-vocabulary 3D scene graph generation tasks and relationship-guided\ninstance segmentation, achieving state-of-the-art performance in both tasks.\nSee the project website at https://relationfield.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural radiance fields are an emerging 3D scene representation and recently\neven been extended to learn features for scene understanding by distilling\nopen-vocabulary features from vision-language models. However, current method\nprimarily focus on object-centric representations, supporting object\nsegmentation or detection, while understanding semantic relationships between\nobjects remains largely unexplored. To address this gap, we propose\nRelationField, the first method to extract inter-object relationships directly\nfrom neural radiance fields. RelationField represents relationships between\nobjects as pairs of rays within a neural radiance field, effectively extending\nits formulation to include implicit relationship queries. To teach\nRelationField complex, open-vocabulary relationships, relationship knowledge is\ndistilled from multi-modal LLMs. To evaluate RelationField, we solve\nopen-vocabulary 3D scene graph generation tasks and relationship-guided\ninstance segmentation, achieving state-of-the-art performance in both tasks.\nSee the project website at https://relationfield.github.io."
                },
                "authors": [
                    {
                        "name": "Sebastian Koch"
                    },
                    {
                        "name": "Johanna Wald"
                    },
                    {
                        "name": "Mirco Colosi"
                    },
                    {
                        "name": "Narunas Vaskevicius"
                    },
                    {
                        "name": "Pedro Hermosilla"
                    },
                    {
                        "name": "Federico Tombari"
                    },
                    {
                        "name": "Timo Ropinski"
                    }
                ],
                "author_detail": {
                    "name": "Timo Ropinski"
                },
                "author": "Timo Ropinski",
                "arxiv_comment": "CVPR 2025. Project page: https://relationfield.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13652v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13652v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19470v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19470v1",
                "updated": "2025-03-25T09:00:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    9,
                    0,
                    58,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T09:00:58Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    9,
                    0,
                    58,
                    1,
                    84,
                    0
                ],
                "title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement\n  Learning"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities in reasoning,\nexemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating\nreasoning with external search processes remains challenging, especially for\ncomplex multi-hop questions requiring multiple retrieval steps. We propose\nReSearch, a novel framework that trains LLMs to Reason with Search via\nreinforcement learning without using any supervised data on reasoning steps.\nOur approach treats search operations as integral components of the reasoning\nchain, where when and how to perform searches is guided by text-based thinking,\nand search results subsequently influence further reasoning. We train ReSearch\non Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct\nextensive experiments. Despite being trained on only one dataset, our models\ndemonstrate strong generalizability across various benchmarks. Analysis reveals\nthat ReSearch naturally elicits advanced reasoning capabilities such as\nreflection and self-correction during the reinforcement learning process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities in reasoning,\nexemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating\nreasoning with external search processes remains challenging, especially for\ncomplex multi-hop questions requiring multiple retrieval steps. We propose\nReSearch, a novel framework that trains LLMs to Reason with Search via\nreinforcement learning without using any supervised data on reasoning steps.\nOur approach treats search operations as integral components of the reasoning\nchain, where when and how to perform searches is guided by text-based thinking,\nand search results subsequently influence further reasoning. We train ReSearch\non Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct\nextensive experiments. Despite being trained on only one dataset, our models\ndemonstrate strong generalizability across various benchmarks. Analysis reveals\nthat ReSearch naturally elicits advanced reasoning capabilities such as\nreflection and self-correction during the reinforcement learning process."
                },
                "authors": [
                    {
                        "name": "Mingyang Chen"
                    },
                    {
                        "name": "Tianpeng Li"
                    },
                    {
                        "name": "Haoze Sun"
                    },
                    {
                        "name": "Yijie Zhou"
                    },
                    {
                        "name": "Chenzheng Zhu"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Zenan Zhou"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Haofen Wang"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    },
                    {
                        "name": "Wen Zhang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19470v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19470v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19468v1",
                "updated": "2025-03-25T08:59:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    8,
                    59,
                    11,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T08:59:11Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    8,
                    59,
                    11,
                    1,
                    84,
                    0
                ],
                "title": "Noisier2Inverse: Self-Supervised Learning for Image Reconstruction with\n  Correlated Noise",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noisier2Inverse: Self-Supervised Learning for Image Reconstruction with\n  Correlated Noise"
                },
                "summary": "We propose Noisier2Inverse, a correction-free self-supervised deep learning\napproach for general inverse prob- lems. The proposed method learns a\nreconstruction function without the need for ground truth samples and is ap-\nplicable in cases where measurement noise is statistically correlated. This\nincludes computed tomography, where detector imperfections or photon scattering\ncreate correlated noise patterns, as well as microscopy and seismic imaging,\nwhere physical interactions during measurement introduce dependencies in the\nnoise structure. Similar to Noisier2Noise, a key step in our approach is the\ngeneration of noisier data from which the reconstruction net- work learns.\nHowever, unlike Noisier2Noise, the proposed loss function operates in\nmeasurement space and is trained to recover an extrapolated image instead of\nthe original noisy one. This eliminates the need for an extrap- olation step\nduring inference, which would otherwise suffer from ill-posedness. We\nnumerically demonstrate that our method clearly outperforms previous\nself-supervised approaches that account for correlated noise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Noisier2Inverse, a correction-free self-supervised deep learning\napproach for general inverse prob- lems. The proposed method learns a\nreconstruction function without the need for ground truth samples and is ap-\nplicable in cases where measurement noise is statistically correlated. This\nincludes computed tomography, where detector imperfections or photon scattering\ncreate correlated noise patterns, as well as microscopy and seismic imaging,\nwhere physical interactions during measurement introduce dependencies in the\nnoise structure. Similar to Noisier2Noise, a key step in our approach is the\ngeneration of noisier data from which the reconstruction net- work learns.\nHowever, unlike Noisier2Noise, the proposed loss function operates in\nmeasurement space and is trained to recover an extrapolated image instead of\nthe original noisy one. This eliminates the need for an extrap- olation step\nduring inference, which would otherwise suffer from ill-posedness. We\nnumerically demonstrate that our method clearly outperforms previous\nself-supervised approaches that account for correlated noise."
                },
                "authors": [
                    {
                        "name": "Nadja Gruber"
                    },
                    {
                        "name": "Johannes Schwab"
                    },
                    {
                        "name": "Markus Haltmeier"
                    },
                    {
                        "name": "Ander Biguri"
                    },
                    {
                        "name": "Clemens Dlaska"
                    },
                    {
                        "name": "Gyeongha Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Gyeongha Hwang"
                },
                "author": "Gyeongha Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "94A08, 92C55",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.4.5; I.4.4; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19467v1",
                "updated": "2025-03-25T08:58:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    8,
                    58,
                    39,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T08:58:39Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    8,
                    58,
                    39,
                    1,
                    84,
                    0
                ],
                "title": "EFIT-mini: An Embedded, Multi-task Neural Network-driven Equilibrium\n  Inversion Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EFIT-mini: An Embedded, Multi-task Neural Network-driven Equilibrium\n  Inversion Algorithm"
                },
                "summary": "Equilibrium reconstruction, which infers internal magnetic fields, plasmas\ncurrent, and pressure distributions in tokamaks using diagnostic and coil\ncurrent data, is crucial for controlled magnetic confinement nuclear fusion\nresearch. However, traditional numerical methods often fall short of real-time\ncontrol needs due to time-consuming computations or iteration convergence\nissues. This paper introduces EFIT-mini, a novel algorithm blending machine\nlearning with numerical simulation. It employs a multi-task neural network to\nreplace complex steps in numerical equilibrium inversion, such as magnetic\nsurface boundary identification, combining the strengths of both approaches\nwhile mitigating their individual drawbacks. The neural network processes coil\ncurrents and magnetic measurements to directly output plasmas parameters,\nincluding polynomial coefficients for $p'$ and $ff'$, providing high-precision\ninitial values for subsequent Picard iterations. Compared to existing AI-driven\nmethods, EFIT-mini incorporates more physical priors (e.g., least squares\nconstraints) to enhance inversion accuracy. Validated on EXL-50U tokamak\ndischarge data, EFIT-mini achieves over 98% overlap in the last closed flux\nsurface area with traditional methods. Besides, EFIT-mini's neural network and\nfull algorithm compute single time slices in just 0.11ms and 0.36ms at\n129$\\times$129 resolution, respectively, representing a\nthree-order-of-magnitude speedup. This innovative approach leverages machine\nlearning's speed and numerical algorithms' explainability, offering a robust\nsolution for real-time plasmas shape control and potential extension to kinetic\nequilibrium reconstruction. Its efficiency and versatility position EFIT-mini\nas a promising tool for tokamak real-time monitoring and control, as well as\nfor providing key inputs to other real-time inversion algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Equilibrium reconstruction, which infers internal magnetic fields, plasmas\ncurrent, and pressure distributions in tokamaks using diagnostic and coil\ncurrent data, is crucial for controlled magnetic confinement nuclear fusion\nresearch. However, traditional numerical methods often fall short of real-time\ncontrol needs due to time-consuming computations or iteration convergence\nissues. This paper introduces EFIT-mini, a novel algorithm blending machine\nlearning with numerical simulation. It employs a multi-task neural network to\nreplace complex steps in numerical equilibrium inversion, such as magnetic\nsurface boundary identification, combining the strengths of both approaches\nwhile mitigating their individual drawbacks. The neural network processes coil\ncurrents and magnetic measurements to directly output plasmas parameters,\nincluding polynomial coefficients for $p'$ and $ff'$, providing high-precision\ninitial values for subsequent Picard iterations. Compared to existing AI-driven\nmethods, EFIT-mini incorporates more physical priors (e.g., least squares\nconstraints) to enhance inversion accuracy. Validated on EXL-50U tokamak\ndischarge data, EFIT-mini achieves over 98% overlap in the last closed flux\nsurface area with traditional methods. Besides, EFIT-mini's neural network and\nfull algorithm compute single time slices in just 0.11ms and 0.36ms at\n129$\\times$129 resolution, respectively, representing a\nthree-order-of-magnitude speedup. This innovative approach leverages machine\nlearning's speed and numerical algorithms' explainability, offering a robust\nsolution for real-time plasmas shape control and potential extension to kinetic\nequilibrium reconstruction. Its efficiency and versatility position EFIT-mini\nas a promising tool for tokamak real-time monitoring and control, as well as\nfor providing key inputs to other real-time inversion algorithms."
                },
                "authors": [
                    {
                        "name": "Guohui Zheng"
                    },
                    {
                        "name": "Songfen Liu"
                    },
                    {
                        "name": "Huasheng Xie"
                    },
                    {
                        "name": "Hanyue Zhao"
                    },
                    {
                        "name": "Yapeng Zhang"
                    },
                    {
                        "name": "Xiang Gu"
                    },
                    {
                        "name": "Zhengyuan Chen"
                    },
                    {
                        "name": "Tiantian Sun"
                    },
                    {
                        "name": "Yanan Xu"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Dong Guo"
                    },
                    {
                        "name": "Renyi Tao"
                    },
                    {
                        "name": "Youjun Hu"
                    },
                    {
                        "name": "Zongyu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zongyu Yang"
                },
                "author": "Zongyu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.19910v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19910v1",
                "updated": "2025-03-25T17:59:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    59,
                    50,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T17:59:50Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    59,
                    50,
                    1,
                    84,
                    0
                ],
                "title": "CoLLM: A Large Language Model for Composed Image Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoLLM: A Large Language Model for Composed Image Retrieval"
                },
                "summary": "Composed Image Retrieval (CIR) is a complex task that aims to retrieve images\nbased on a multimodal query. Typical training data consists of triplets\ncontaining a reference image, a textual description of desired modifications,\nand the target image, which are expensive and time-consuming to acquire. The\nscarcity of CIR datasets has led to zero-shot approaches utilizing synthetic\ntriplets or leveraging vision-language models (VLMs) with ubiquitous\nweb-crawled image-caption pairs. However, these methods have significant\nlimitations: synthetic triplets suffer from limited scale, lack of diversity,\nand unnatural modification text, while image-caption pairs hinder joint\nembedding learning of the multimodal query due to the absence of triplet data.\nMoreover, existing approaches struggle with complex and nuanced modification\ntexts that demand sophisticated fusion and understanding of vision and language\nmodalities. We present CoLLM, a one-stop framework that effectively addresses\nthese limitations. Our approach generates triplets on-the-fly from\nimage-caption pairs, enabling supervised training without manual annotation. We\nleverage Large Language Models (LLMs) to generate joint embeddings of reference\nimages and modification texts, facilitating deeper multimodal fusion.\nAdditionally, we introduce Multi-Text CIR (MTCIR), a large-scale dataset\ncomprising 3.4M samples, and refine existing CIR benchmarks (CIRR and\nFashion-IQ) to enhance evaluation reliability. Experimental results demonstrate\nthat CoLLM achieves state-of-the-art performance across multiple CIR benchmarks\nand settings. MTCIR yields competitive results, with up to 15% performance\nimprovement. Our refined benchmarks provide more reliable evaluation metrics\nfor CIR models, contributing to the advancement of this important field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Composed Image Retrieval (CIR) is a complex task that aims to retrieve images\nbased on a multimodal query. Typical training data consists of triplets\ncontaining a reference image, a textual description of desired modifications,\nand the target image, which are expensive and time-consuming to acquire. The\nscarcity of CIR datasets has led to zero-shot approaches utilizing synthetic\ntriplets or leveraging vision-language models (VLMs) with ubiquitous\nweb-crawled image-caption pairs. However, these methods have significant\nlimitations: synthetic triplets suffer from limited scale, lack of diversity,\nand unnatural modification text, while image-caption pairs hinder joint\nembedding learning of the multimodal query due to the absence of triplet data.\nMoreover, existing approaches struggle with complex and nuanced modification\ntexts that demand sophisticated fusion and understanding of vision and language\nmodalities. We present CoLLM, a one-stop framework that effectively addresses\nthese limitations. Our approach generates triplets on-the-fly from\nimage-caption pairs, enabling supervised training without manual annotation. We\nleverage Large Language Models (LLMs) to generate joint embeddings of reference\nimages and modification texts, facilitating deeper multimodal fusion.\nAdditionally, we introduce Multi-Text CIR (MTCIR), a large-scale dataset\ncomprising 3.4M samples, and refine existing CIR benchmarks (CIRR and\nFashion-IQ) to enhance evaluation reliability. Experimental results demonstrate\nthat CoLLM achieves state-of-the-art performance across multiple CIR benchmarks\nand settings. MTCIR yields competitive results, with up to 15% performance\nimprovement. Our refined benchmarks provide more reliable evaluation metrics\nfor CIR models, contributing to the advancement of this important field."
                },
                "authors": [
                    {
                        "name": "Chuong Huynh"
                    },
                    {
                        "name": "Jinyu Yang"
                    },
                    {
                        "name": "Ashish Tawari"
                    },
                    {
                        "name": "Mubarak Shah"
                    },
                    {
                        "name": "Son Tran"
                    },
                    {
                        "name": "Raffay Hamid"
                    },
                    {
                        "name": "Trishul Chilimbi"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Shrivastava"
                },
                "author": "Abhinav Shrivastava",
                "arxiv_comment": "CVPR 2025. Project page: https://collm-cvpr25.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19910v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19910v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19903v1",
                "updated": "2025-03-25T17:58:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    58,
                    37,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T17:58:37Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    58,
                    37,
                    1,
                    84,
                    0
                ],
                "title": "Scaling Vision Pre-Training to 4K Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Vision Pre-Training to 4K Resolution"
                },
                "summary": "High-resolution perception of visual details is crucial for daily tasks.\nCurrent vision pre-training, however, is still limited to low resolutions\n(e.g., 378 x 378 pixels) due to the quadratic cost of processing larger images.\nWe introduce PS3 that scales CLIP-style vision pre-training to 4K resolution\nwith a near-constant cost. Instead of contrastive learning on global image\nrepresentation, PS3 is pre-trained by selectively processing local regions and\ncontrasting them with local detailed captions, enabling high-resolution\nrepresentation learning with greatly reduced computational overhead. The\npre-trained PS3 is able to both encode the global image at low resolution and\nselectively process local high-resolution regions based on their saliency or\nrelevance to a text prompt. When applying PS3 to multi-modal LLM (MLLM), the\nresulting model, named VILA-HD, significantly improves high-resolution visual\nperception compared to baselines without high-resolution vision pre-training\nsuch as AnyRes and S^2 while using up to 4.3x fewer tokens. PS3 also unlocks\nappealing scaling properties of VILA-HD, including scaling up resolution for\nfree and scaling up test-time compute for better performance. Compared to state\nof the arts, VILA-HD outperforms previous MLLMs such as NVILA and Qwen2-VL\nacross multiple benchmarks and achieves better efficiency than latest token\npruning approaches. Finally, we find current benchmarks do not require\n4K-resolution perception, which motivates us to propose 4KPro, a new benchmark\nof image QA at 4K resolution, on which VILA-HD outperforms all previous MLLMs,\nincluding a 14.5% improvement over GPT-4o, and a 3.2% improvement and 2.96x\nspeedup over Qwen2-VL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-resolution perception of visual details is crucial for daily tasks.\nCurrent vision pre-training, however, is still limited to low resolutions\n(e.g., 378 x 378 pixels) due to the quadratic cost of processing larger images.\nWe introduce PS3 that scales CLIP-style vision pre-training to 4K resolution\nwith a near-constant cost. Instead of contrastive learning on global image\nrepresentation, PS3 is pre-trained by selectively processing local regions and\ncontrasting them with local detailed captions, enabling high-resolution\nrepresentation learning with greatly reduced computational overhead. The\npre-trained PS3 is able to both encode the global image at low resolution and\nselectively process local high-resolution regions based on their saliency or\nrelevance to a text prompt. When applying PS3 to multi-modal LLM (MLLM), the\nresulting model, named VILA-HD, significantly improves high-resolution visual\nperception compared to baselines without high-resolution vision pre-training\nsuch as AnyRes and S^2 while using up to 4.3x fewer tokens. PS3 also unlocks\nappealing scaling properties of VILA-HD, including scaling up resolution for\nfree and scaling up test-time compute for better performance. Compared to state\nof the arts, VILA-HD outperforms previous MLLMs such as NVILA and Qwen2-VL\nacross multiple benchmarks and achieves better efficiency than latest token\npruning approaches. Finally, we find current benchmarks do not require\n4K-resolution perception, which motivates us to propose 4KPro, a new benchmark\nof image QA at 4K resolution, on which VILA-HD outperforms all previous MLLMs,\nincluding a 14.5% improvement over GPT-4o, and a 3.2% improvement and 2.96x\nspeedup over Qwen2-VL."
                },
                "authors": [
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Boyi Li"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Marco Pavone"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Trevor Darrell"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Hongxu Yin"
                    }
                ],
                "author_detail": {
                    "name": "Hongxu Yin"
                },
                "author": "Hongxu Yin",
                "arxiv_comment": "CVPR 2025. Project Page: https://nvlabs.github.io/PS3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19887v1",
                "updated": "2025-03-25T17:51:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    51,
                    50,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T17:51:50Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    51,
                    50,
                    1,
                    84,
                    0
                ],
                "title": "A proposal for an incident regime that tracks and counters threats to\n  national security posed by AI systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A proposal for an incident regime that tracks and counters threats to\n  national security posed by AI systems"
                },
                "summary": "Recent progress in AI capabilities has heightened concerns that AI systems\ncould pose a threat to national security, for example, by making it easier for\nmalicious actors to perform cyberattacks on critical national infrastructure,\nor through loss of control of autonomous AI systems. In parallel, federal\nlegislators in the US have proposed nascent 'AI incident regimes' to identify\nand counter similar threats. In this paper, we consolidate these two trends and\npresent a proposal for a legally mandated post-deployment AI incident regie\nthat aims to counter potential national security threats from AI systems. We\nstart the paper by introducing the concept of 'security-critical' to describe\ndoctors that pose extreme risks to national security, before arguing that\n'security-critical' describes civilian nuclear power, aviation, life science\ndual-use research of concern, and frontier AI development. We then present in\ndetail our AI incident regime proposal,, justifying each component of the\nproposal by demonstrating its similarity to US domestic incident regimes in\nother 'security-critical' sectors. Finally, we sketch a hypothetical scenario\nwhere our proposed AI incident regime deals with an AI cyber incident. Our\nproposed AI incident regime is split into three phases. The first phase\nrevolves around a novel operationalization of what counts as an 'AI incident'\nand we suggest that AI providers must create a 'national security case' before\ndeploying a frontier AI system. The second and third phases spell out that AI\nproviders should notify a government agency about incidents, and that the\ngovernment agency should be involved in amending AI providers' security and\nsafety procedures, in order to counter future threats to national security. Our\nproposal is timely, given ongoing policy interest in the potential national\nsecurity threats posed by AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in AI capabilities has heightened concerns that AI systems\ncould pose a threat to national security, for example, by making it easier for\nmalicious actors to perform cyberattacks on critical national infrastructure,\nor through loss of control of autonomous AI systems. In parallel, federal\nlegislators in the US have proposed nascent 'AI incident regimes' to identify\nand counter similar threats. In this paper, we consolidate these two trends and\npresent a proposal for a legally mandated post-deployment AI incident regie\nthat aims to counter potential national security threats from AI systems. We\nstart the paper by introducing the concept of 'security-critical' to describe\ndoctors that pose extreme risks to national security, before arguing that\n'security-critical' describes civilian nuclear power, aviation, life science\ndual-use research of concern, and frontier AI development. We then present in\ndetail our AI incident regime proposal,, justifying each component of the\nproposal by demonstrating its similarity to US domestic incident regimes in\nother 'security-critical' sectors. Finally, we sketch a hypothetical scenario\nwhere our proposed AI incident regime deals with an AI cyber incident. Our\nproposed AI incident regime is split into three phases. The first phase\nrevolves around a novel operationalization of what counts as an 'AI incident'\nand we suggest that AI providers must create a 'national security case' before\ndeploying a frontier AI system. The second and third phases spell out that AI\nproviders should notify a government agency about incidents, and that the\ngovernment agency should be involved in amending AI providers' security and\nsafety procedures, in order to counter future threats to national security. Our\nproposal is timely, given ongoing policy interest in the potential national\nsecurity threats posed by AI systems."
                },
                "authors": [
                    {
                        "name": "Alejandro Ortega"
                    }
                ],
                "author_detail": {
                    "name": "Alejandro Ortega"
                },
                "author": "Alejandro Ortega",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.01390v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.01390v3",
                "updated": "2025-03-25T17:44:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    44,
                    19,
                    1,
                    84,
                    0
                ],
                "published": "2024-03-03T04:22:13Z",
                "published_parsed": [
                    2024,
                    3,
                    3,
                    4,
                    22,
                    13,
                    6,
                    63,
                    0
                ],
                "title": "Right for Right Reasons: Large Language Models for Verifiable\n  Commonsense Knowledge Graph Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Right for Right Reasons: Large Language Models for Verifiable\n  Commonsense Knowledge Graph Question Answering"
                },
                "summary": "Knowledge Graph Question Answering (KGQA) methods seek to answer Natural\nLanguage questions using the relational information stored in Knowledge Graphs\n(KGs). With the recent advancements of Large Language Models (LLMs) and their\nremarkable reasoning abilities, there is a growing trend to leverage them for\nKGQA. However, existing methodologies have only focused on answering factual\nquestions, e.g., \"In which city was Silvio Berlusconi's first wife born?\",\nleaving questions involving commonsense reasoning that real-world users may\npose more often, e.g., \"Do I need separate visas to see the Venus of Willendorf\nand attend the Olympics this summer?\" unaddressed. In this work, we first\nobserve that existing LLM-based methods for KGQA struggle with hallucination on\nsuch questions, especially on queries targeting long-tail entities (e.g.,\nnon-mainstream and recent entities), thus hindering their applicability in\nreal-world applications especially since their reasoning processes are not\neasily verifiable. In response, we propose Right for Right Reasons (R3), a\ncommonsense KGQA methodology that allows for a verifiable reasoning procedure\nby axiomatically surfacing intrinsic commonsense knowledge of LLMs and\ngrounding every factual reasoning step on KG triples. Through experimental\nevaluations across three different tasks--question answering, claim\nverification, and preference matching--our findings showcase R3 as a superior\napproach, outperforming existing methodologies and notably reducing instances\nof hallucination and reasoning errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph Question Answering (KGQA) methods seek to answer Natural\nLanguage questions using the relational information stored in Knowledge Graphs\n(KGs). With the recent advancements of Large Language Models (LLMs) and their\nremarkable reasoning abilities, there is a growing trend to leverage them for\nKGQA. However, existing methodologies have only focused on answering factual\nquestions, e.g., \"In which city was Silvio Berlusconi's first wife born?\",\nleaving questions involving commonsense reasoning that real-world users may\npose more often, e.g., \"Do I need separate visas to see the Venus of Willendorf\nand attend the Olympics this summer?\" unaddressed. In this work, we first\nobserve that existing LLM-based methods for KGQA struggle with hallucination on\nsuch questions, especially on queries targeting long-tail entities (e.g.,\nnon-mainstream and recent entities), thus hindering their applicability in\nreal-world applications especially since their reasoning processes are not\neasily verifiable. In response, we propose Right for Right Reasons (R3), a\ncommonsense KGQA methodology that allows for a verifiable reasoning procedure\nby axiomatically surfacing intrinsic commonsense knowledge of LLMs and\ngrounding every factual reasoning step on KG triples. Through experimental\nevaluations across three different tasks--question answering, claim\nverification, and preference matching--our findings showcase R3 as a superior\napproach, outperforming existing methodologies and notably reducing instances\nof hallucination and reasoning errors."
                },
                "authors": [
                    {
                        "name": "Armin Toroghi"
                    },
                    {
                        "name": "Willis Guo"
                    },
                    {
                        "name": "Mohammad Mahdi Abdollah Pour"
                    },
                    {
                        "name": "Scott Sanner"
                    }
                ],
                "author_detail": {
                    "name": "Scott Sanner"
                },
                "author": "Scott Sanner",
                "arxiv_comment": "33 pages, EMNLP24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.01390v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.01390v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19878v1",
                "updated": "2025-03-25T17:43:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    43,
                    8,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T17:43:08Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    43,
                    8,
                    1,
                    84,
                    0
                ],
                "title": "CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation"
                },
                "summary": "Large language models (LLMs) have revolutionized natural language processing\n(NLP), particularly through Retrieval-Augmented Generation (RAG), which\nenhances LLM capabilities by integrating external knowledge. However,\ntraditional RAG systems face critical limitations, including disrupted\ncontextual integrity due to text chunking, and over-reliance on semantic\nsimilarity for retrieval. To address these issues, we propose CausalRAG, a\nnovel framework that incorporates causal graphs into the retrieval process. By\nconstructing and tracing causal relationships, CausalRAG preserves contextual\ncontinuity and improves retrieval precision, leading to more accurate and\ninterpretable responses. We evaluate CausalRAG against regular RAG and\ngraph-based RAG approaches, demonstrating its superiority across several\nmetrics. Our findings suggest that grounding retrieval in causal reasoning\nprovides a promising approach to knowledge-intensive tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized natural language processing\n(NLP), particularly through Retrieval-Augmented Generation (RAG), which\nenhances LLM capabilities by integrating external knowledge. However,\ntraditional RAG systems face critical limitations, including disrupted\ncontextual integrity due to text chunking, and over-reliance on semantic\nsimilarity for retrieval. To address these issues, we propose CausalRAG, a\nnovel framework that incorporates causal graphs into the retrieval process. By\nconstructing and tracing causal relationships, CausalRAG preserves contextual\ncontinuity and improves retrieval precision, leading to more accurate and\ninterpretable responses. We evaluate CausalRAG against regular RAG and\ngraph-based RAG approaches, demonstrating its superiority across several\nmetrics. Our findings suggest that grounding retrieval in causal reasoning\nprovides a promising approach to knowledge-intensive tasks."
                },
                "authors": [
                    {
                        "name": "Nengbo Wang"
                    },
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Jagdip Singh"
                    },
                    {
                        "name": "Jing Ma"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    }
                ],
                "author_detail": {
                    "name": "Vipin Chaudhary"
                },
                "author": "Vipin Chaudhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19855v1",
                "updated": "2025-03-25T17:19:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    19,
                    38,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T17:19:38Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    19,
                    38,
                    1,
                    84,
                    0
                ],
                "title": "Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time\n  Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time\n  Thinking"
                },
                "summary": "Recent advances in large language models (LLMs), such as OpenAI-o1 and\nDeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where\nextended reasoning processes substantially enhance model performance. Despite\nthis, current models are constrained by limitations in handling long texts and\nreinforcement learning (RL) training efficiency. To address these issues, we\npropose a simple yet effective test-time scaling approach Multi-round Thinking.\nThis method iteratively refines model reasoning by leveraging previous answers\nas prompts for subsequent rounds. Extensive experiments across multiple models,\nincluding QwQ-32B and DeepSeek-R1, consistently show performance improvements\non various benchmarks such as AIME 2024, MATH-500, GPQA-diamond, and\nLiveCodeBench. For instance, the accuracy of QwQ-32B improved from 80.3% (Round\n1) to 82.1% (Round 2) on the AIME 2024 dataset, while DeepSeek-R1 showed a\nsimilar increase from 79.7% to 82.0%. These results confirm that Multi-round\nThinking is a broadly applicable, straightforward approach to achieving stable\nenhancements in model performance, underscoring its potential for future\ndevelopments in test-time scaling techniques. The key prompt: {Original\nquestion prompt} The assistant's previous answer is: <answer> {last round\nanswer} </answer>, and please re-answer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs), such as OpenAI-o1 and\nDeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where\nextended reasoning processes substantially enhance model performance. Despite\nthis, current models are constrained by limitations in handling long texts and\nreinforcement learning (RL) training efficiency. To address these issues, we\npropose a simple yet effective test-time scaling approach Multi-round Thinking.\nThis method iteratively refines model reasoning by leveraging previous answers\nas prompts for subsequent rounds. Extensive experiments across multiple models,\nincluding QwQ-32B and DeepSeek-R1, consistently show performance improvements\non various benchmarks such as AIME 2024, MATH-500, GPQA-diamond, and\nLiveCodeBench. For instance, the accuracy of QwQ-32B improved from 80.3% (Round\n1) to 82.1% (Round 2) on the AIME 2024 dataset, while DeepSeek-R1 showed a\nsimilar increase from 79.7% to 82.0%. These results confirm that Multi-round\nThinking is a broadly applicable, straightforward approach to achieving stable\nenhancements in model performance, underscoring its potential for future\ndevelopments in test-time scaling techniques. The key prompt: {Original\nquestion prompt} The assistant's previous answer is: <answer> {last round\nanswer} </answer>, and please re-answer."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Tian"
                    },
                    {
                        "name": "Sitong Zhao"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Shuaiting Chen"
                    },
                    {
                        "name": "Yunjie Ji"
                    },
                    {
                        "name": "Yiping Peng"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Xiangang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangang Li"
                },
                "author": "Xiangang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19850v1",
                "updated": "2025-03-25T17:17:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    17,
                    19,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T17:17:19Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    17,
                    19,
                    1,
                    84,
                    0
                ],
                "title": "FALCONEye: Finding Answers and Localizing Content in ONE-hour-long\n  videos with multi-modal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FALCONEye: Finding Answers and Localizing Content in ONE-hour-long\n  videos with multi-modal LLMs"
                },
                "summary": "Information retrieval in hour-long videos presents a significant challenge,\neven for state-of-the-art Vision-Language Models (VLMs), particularly when the\ndesired information is localized within a small subset of frames. Long video\ndata presents challenges for VLMs due to context window limitations and the\ndifficulty of pinpointing frames containing the answer. Our novel video agent,\nFALCONEye, combines a VLM and a Large Language Model (LLM) to search relevant\ninformation along the video, and locate the frames with the answer. FALCONEye\nnovelty relies on 1) the proposed meta-architecture, which is better suited to\ntackle hour-long videos compared to short video approaches in the\nstate-of-the-art; 2) a new efficient exploration algorithm to locate the\ninformation using short clips, captions and answer confidence; and 3) our\nstate-of-the-art VLMs calibration analysis for the answer confidence. Our agent\nis built over a small-size VLM and a medium-size LLM being accessible to run on\nstandard computational resources. We also release FALCON-Bench, a benchmark to\nevaluate long (average > 1 hour) Video Answer Search challenges, highlighting\nthe need for open-ended question evaluation. Our experiments show FALCONEye's\nsuperior performance than the state-of-the-art in FALCON-Bench, and similar or\nbetter performance in related benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information retrieval in hour-long videos presents a significant challenge,\neven for state-of-the-art Vision-Language Models (VLMs), particularly when the\ndesired information is localized within a small subset of frames. Long video\ndata presents challenges for VLMs due to context window limitations and the\ndifficulty of pinpointing frames containing the answer. Our novel video agent,\nFALCONEye, combines a VLM and a Large Language Model (LLM) to search relevant\ninformation along the video, and locate the frames with the answer. FALCONEye\nnovelty relies on 1) the proposed meta-architecture, which is better suited to\ntackle hour-long videos compared to short video approaches in the\nstate-of-the-art; 2) a new efficient exploration algorithm to locate the\ninformation using short clips, captions and answer confidence; and 3) our\nstate-of-the-art VLMs calibration analysis for the answer confidence. Our agent\nis built over a small-size VLM and a medium-size LLM being accessible to run on\nstandard computational resources. We also release FALCON-Bench, a benchmark to\nevaluate long (average > 1 hour) Video Answer Search challenges, highlighting\nthe need for open-ended question evaluation. Our experiments show FALCONEye's\nsuperior performance than the state-of-the-art in FALCON-Bench, and similar or\nbetter performance in related benchmarks."
                },
                "authors": [
                    {
                        "name": "Carlos Plou"
                    },
                    {
                        "name": "Cesar Borja"
                    },
                    {
                        "name": "Ruben Martinez-Cantin"
                    },
                    {
                        "name": "Ana C. Murillo"
                    }
                ],
                "author_detail": {
                    "name": "Ana C. Murillo"
                },
                "author": "Ana C. Murillo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19844v1",
                "updated": "2025-03-25T17:07:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    7,
                    21,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T17:07:21Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    7,
                    21,
                    1,
                    84,
                    0
                ],
                "title": "A Comparative Analysis of Word Segmentation, Part-of-Speech Tagging, and\n  Named Entity Recognition for Historical Chinese Sources, 1900-1950",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative Analysis of Word Segmentation, Part-of-Speech Tagging, and\n  Named Entity Recognition for Historical Chinese Sources, 1900-1950"
                },
                "summary": "This paper compares large language models (LLMs) and traditional natural\nlanguage processing (NLP) tools for performing word segmentation,\npart-of-speech (POS) tagging, and named entity recognition (NER) on Chinese\ntexts from 1900 to 1950. Historical Chinese documents pose challenges for text\nanalysis due to their logographic script, the absence of natural word\nboundaries, and significant linguistic changes. Using a sample dataset from the\nShanghai Library Republican Journal corpus, traditional tools such as Jieba and\nspaCy are compared to LLMs, including GPT-4o, Claude 3.5, and the GLM series.\nThe results show that LLMs outperform traditional methods in all metrics,\nalbeit at considerably higher computational costs, highlighting a trade-off\nbetween accuracy and efficiency. Additionally, LLMs better handle\ngenre-specific challenges such as poetry and temporal variations (i.e.,\npre-1920 versus post-1920 texts), demonstrating that their contextual learning\ncapabilities can advance NLP approaches to historical texts by reducing the\nneed for domain-specific training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper compares large language models (LLMs) and traditional natural\nlanguage processing (NLP) tools for performing word segmentation,\npart-of-speech (POS) tagging, and named entity recognition (NER) on Chinese\ntexts from 1900 to 1950. Historical Chinese documents pose challenges for text\nanalysis due to their logographic script, the absence of natural word\nboundaries, and significant linguistic changes. Using a sample dataset from the\nShanghai Library Republican Journal corpus, traditional tools such as Jieba and\nspaCy are compared to LLMs, including GPT-4o, Claude 3.5, and the GLM series.\nThe results show that LLMs outperform traditional methods in all metrics,\nalbeit at considerably higher computational costs, highlighting a trade-off\nbetween accuracy and efficiency. Additionally, LLMs better handle\ngenre-specific challenges such as poetry and temporal variations (i.e.,\npre-1920 versus post-1920 texts), demonstrating that their contextual learning\ncapabilities can advance NLP approaches to historical texts by reducing the\nneed for domain-specific training data."
                },
                "authors": [
                    {
                        "name": "Zhao Fang"
                    },
                    {
                        "name": "Liang-Chun Wu"
                    },
                    {
                        "name": "Xuening Kong"
                    },
                    {
                        "name": "Spencer Dean Stewart"
                    }
                ],
                "author_detail": {
                    "name": "Spencer Dean Stewart"
                },
                "author": "Spencer Dean Stewart",
                "arxiv_comment": "Accepted to NLP4DH 2025 at NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19839v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19839v1",
                "updated": "2025-03-25T16:59:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    59,
                    42,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T16:59:42Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    59,
                    42,
                    1,
                    84,
                    0
                ],
                "title": "FireEdit: Fine-grained Instruction-based Image Editing via Region-aware\n  Vision Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FireEdit: Fine-grained Instruction-based Image Editing via Region-aware\n  Vision Language Model"
                },
                "summary": "Currently, instruction-based image editing methods have made significant\nprogress by leveraging the powerful cross-modal understanding capabilities of\nvision language models (VLMs). However, they still face challenges in three key\nareas: 1) complex scenarios; 2) semantic consistency; and 3) fine-grained\nediting. To address these issues, we propose FireEdit, an innovative\nFine-grained Instruction-based image editing framework that exploits a\nREgion-aware VLM. FireEdit is designed to accurately comprehend user\ninstructions and ensure effective control over the editing process.\nSpecifically, we enhance the fine-grained visual perception capabilities of the\nVLM by introducing additional region tokens. Relying solely on the output of\nthe LLM to guide the diffusion model may lead to suboptimal editing results.\nTherefore, we propose a Time-Aware Target Injection module and a Hybrid Visual\nCross Attention module. The former dynamically adjusts the guidance strength at\nvarious denoising stages by integrating timestep embeddings with the text\nembeddings. The latter enhances visual details for image editing, thereby\npreserving semantic consistency between the edited result and the source image.\nBy combining the VLM enhanced with fine-grained region tokens and the\ntime-dependent diffusion model, FireEdit demonstrates significant advantages in\ncomprehending editing instructions and maintaining high semantic consistency.\nExtensive experiments indicate that our approach surpasses the state-of-the-art\ninstruction-based image editing methods. Our project is available at\nhttps://zjgans.github.io/fireedit.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Currently, instruction-based image editing methods have made significant\nprogress by leveraging the powerful cross-modal understanding capabilities of\nvision language models (VLMs). However, they still face challenges in three key\nareas: 1) complex scenarios; 2) semantic consistency; and 3) fine-grained\nediting. To address these issues, we propose FireEdit, an innovative\nFine-grained Instruction-based image editing framework that exploits a\nREgion-aware VLM. FireEdit is designed to accurately comprehend user\ninstructions and ensure effective control over the editing process.\nSpecifically, we enhance the fine-grained visual perception capabilities of the\nVLM by introducing additional region tokens. Relying solely on the output of\nthe LLM to guide the diffusion model may lead to suboptimal editing results.\nTherefore, we propose a Time-Aware Target Injection module and a Hybrid Visual\nCross Attention module. The former dynamically adjusts the guidance strength at\nvarious denoising stages by integrating timestep embeddings with the text\nembeddings. The latter enhances visual details for image editing, thereby\npreserving semantic consistency between the edited result and the source image.\nBy combining the VLM enhanced with fine-grained region tokens and the\ntime-dependent diffusion model, FireEdit demonstrates significant advantages in\ncomprehending editing instructions and maintaining high semantic consistency.\nExtensive experiments indicate that our approach surpasses the state-of-the-art\ninstruction-based image editing methods. Our project is available at\nhttps://zjgans.github.io/fireedit.github.io."
                },
                "authors": [
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Jiahao Li"
                    },
                    {
                        "name": "Zunnan Xu"
                    },
                    {
                        "name": "Hanhui Li"
                    },
                    {
                        "name": "Yiji Cheng"
                    },
                    {
                        "name": "Fa-Ting Hong"
                    },
                    {
                        "name": "Qin Lin"
                    },
                    {
                        "name": "Qinglin Lu"
                    },
                    {
                        "name": "Xiaodan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Liang"
                },
                "author": "Xiaodan Liang",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19839v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19839v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19838v1",
                "updated": "2025-03-25T16:59:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    59,
                    38,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T16:59:38Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    59,
                    38,
                    1,
                    84,
                    0
                ],
                "title": "Compact and stable source of polarization-entangled photon-pairs based\n  on a folded linear displacement interferometer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compact and stable source of polarization-entangled photon-pairs based\n  on a folded linear displacement interferometer"
                },
                "summary": "The realization of quantum networks requires the development of robust low\nsize, weight and power (SWaP) systems suitable for operation under harsh\nenvironments in remote and mobile nodes such as satellites. We present a source\nof polarization-entangled photon-pairs in a folded linear displacement\ninterferometer based on spontaneous parametric down conversion using a Type-0\nperiodically poled potassium titanyl phosphate crystal. Featuring a compact and\nstable double-pass geometry using a corner-cube retroreflector, the source has\na detected pair rate of 2.5 M pairs/s/mW with a Bell state fidelity of 94.1%\n+/- 2.1%. The quality and demonstrated performance of the source are suitable\nfor deployment in entanglement-based quantum networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The realization of quantum networks requires the development of robust low\nsize, weight and power (SWaP) systems suitable for operation under harsh\nenvironments in remote and mobile nodes such as satellites. We present a source\nof polarization-entangled photon-pairs in a folded linear displacement\ninterferometer based on spontaneous parametric down conversion using a Type-0\nperiodically poled potassium titanyl phosphate crystal. Featuring a compact and\nstable double-pass geometry using a corner-cube retroreflector, the source has\na detected pair rate of 2.5 M pairs/s/mW with a Bell state fidelity of 94.1%\n+/- 2.1%. The quality and demonstrated performance of the source are suitable\nfor deployment in entanglement-based quantum networks."
                },
                "authors": [
                    {
                        "name": "Sarah. E. McCarthy"
                    },
                    {
                        "name": "Ali Anwar"
                    },
                    {
                        "name": "Daniel K. L. Oi"
                    },
                    {
                        "name": "Loyd J. McKnight"
                    }
                ],
                "author_detail": {
                    "name": "Loyd J. McKnight"
                },
                "author": "Loyd J. McKnight",
                "arxiv_comment": "11 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19827v1",
                "updated": "2025-03-25T16:42:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    42,
                    4,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T16:42:04Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    42,
                    4,
                    1,
                    84,
                    0
                ],
                "title": "Nordic perspective on System Integrity Protection Schemes in relation to\n  capacity allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nordic perspective on System Integrity Protection Schemes in relation to\n  capacity allocation"
                },
                "summary": "The urgent need to address climate change prompts societies worldwide to\nadopt carbon neutral energy and electrification. To facilitate this, a range of\ntechnologies and policies will be needed. Alternatives to traditional power\ngrid reinforcement, such as grid-enhancing technologies and system automation,\nare particularly attractive due to their potentially low cost and fast\ndeployment time. One alternative is System Integrity Protection Schemes (SIPS)\n- automatic and curative remedial actions (RAs) which can boost grid transfer\ncapacities without compromising with reliability since they can act faster than\nmanual control. The use of SIPS however is scattered, with limited coordination\nbetween countries, and the full potential of using SIPS for capacity\nenhancement is not yet realized. The aim of this paper is to provide a case\nstudy and comparison of SIPS in the Nordic countries, particularly in relation\nto capacity allocation. It also seeks to harmonize terminology relating to\nancillary services, RAs, and SIPS. Finally, it examines and compares the\ninclusion of RAs and SIPS in different Capacity Calculation Methodologies\n(CCMs). In both main EU CCMs - Net Transfer Capacity (NTC) and Flow-Based (FB)\n- RAs play a pronounced role. The paper is based on a survey and interviews\nwith Nordic stakeholders, along with a literature review and analysis of public\ndata. The results indicate a large variation in SIPS use across the Nordics.\nRegarding terminology, we suggest that SIPS is a subcategory of RAs which\noverlaps with ancillary services. Concerning CCMs, NTC is unable to fully\nrepresent capacity constraints in meshed AC systems, which in turn hinders\nsystematic capacity enhancement using RAs. FB on the other hand explicitly\nincludes RAs in the capacity domain. A lower bound for the economic value of\nRAs can be calculated, amounting to 11.5 million EUR in the Nordics in Nov and\nDec 2024.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The urgent need to address climate change prompts societies worldwide to\nadopt carbon neutral energy and electrification. To facilitate this, a range of\ntechnologies and policies will be needed. Alternatives to traditional power\ngrid reinforcement, such as grid-enhancing technologies and system automation,\nare particularly attractive due to their potentially low cost and fast\ndeployment time. One alternative is System Integrity Protection Schemes (SIPS)\n- automatic and curative remedial actions (RAs) which can boost grid transfer\ncapacities without compromising with reliability since they can act faster than\nmanual control. The use of SIPS however is scattered, with limited coordination\nbetween countries, and the full potential of using SIPS for capacity\nenhancement is not yet realized. The aim of this paper is to provide a case\nstudy and comparison of SIPS in the Nordic countries, particularly in relation\nto capacity allocation. It also seeks to harmonize terminology relating to\nancillary services, RAs, and SIPS. Finally, it examines and compares the\ninclusion of RAs and SIPS in different Capacity Calculation Methodologies\n(CCMs). In both main EU CCMs - Net Transfer Capacity (NTC) and Flow-Based (FB)\n- RAs play a pronounced role. The paper is based on a survey and interviews\nwith Nordic stakeholders, along with a literature review and analysis of public\ndata. The results indicate a large variation in SIPS use across the Nordics.\nRegarding terminology, we suggest that SIPS is a subcategory of RAs which\noverlaps with ancillary services. Concerning CCMs, NTC is unable to fully\nrepresent capacity constraints in meshed AC systems, which in turn hinders\nsystematic capacity enhancement using RAs. FB on the other hand explicitly\nincludes RAs in the capacity domain. A lower bound for the economic value of\nRAs can be calculated, amounting to 11.5 million EUR in the Nordics in Nov and\nDec 2024."
                },
                "authors": [
                    {
                        "name": "Gabriel Malmer"
                    },
                    {
                        "name": "Arvid Rolander"
                    },
                    {
                        "name": "Emil Hillberg"
                    },
                    {
                        "name": "Olof Samuelsson"
                    },
                    {
                        "name": "Susanne Ackeby"
                    },
                    {
                        "name": "Lars Nordström"
                    }
                ],
                "author_detail": {
                    "name": "Lars Nordström"
                },
                "author": "Lars Nordström",
                "arxiv_comment": "Accepted for CIGRE Symposium 2025 in Trondheim, 10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.04535v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.04535v2",
                "updated": "2025-03-25T16:32:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    32,
                    46,
                    1,
                    84,
                    0
                ],
                "published": "2023-10-06T19:02:04Z",
                "published_parsed": [
                    2023,
                    10,
                    6,
                    19,
                    2,
                    4,
                    4,
                    279,
                    0
                ],
                "title": "LLM4DV: Using Large Language Models for Hardware Test Stimuli Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4DV: Using Large Language Models for Hardware Test Stimuli Generation"
                },
                "summary": "Hardware design verification (DV) is a process that checks the functional\nequivalence of a hardware design against its specifications, improving hardware\nreliability and robustness. A key task in the DV process is the test stimuli\ngeneration, which creates a set of conditions or inputs for testing. These test\nconditions are often complex and specific to the given hardware design,\nrequiring substantial human engineering effort to optimize. We seek a solution\nof automated and efficient testing for arbitrary hardware designs that takes\nadvantage of large language models (LLMs). LLMs have already shown promising\nresults for improving hardware design automation, but remain under-explored for\nhardware DV. In this paper, we propose an open-source benchmarking framework\nnamed LLM4DV that efficiently orchestrates LLMs for automated hardware test\nstimuli generation. Our analysis evaluates six different LLMs involving six\nprompting improvements over eight hardware designs and provides insight for\nfuture work on LLMs development for efficient automated DV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware design verification (DV) is a process that checks the functional\nequivalence of a hardware design against its specifications, improving hardware\nreliability and robustness. A key task in the DV process is the test stimuli\ngeneration, which creates a set of conditions or inputs for testing. These test\nconditions are often complex and specific to the given hardware design,\nrequiring substantial human engineering effort to optimize. We seek a solution\nof automated and efficient testing for arbitrary hardware designs that takes\nadvantage of large language models (LLMs). LLMs have already shown promising\nresults for improving hardware design automation, but remain under-explored for\nhardware DV. In this paper, we propose an open-source benchmarking framework\nnamed LLM4DV that efficiently orchestrates LLMs for automated hardware test\nstimuli generation. Our analysis evaluates six different LLMs involving six\nprompting improvements over eight hardware designs and provides insight for\nfuture work on LLMs development for efficient automated DV."
                },
                "authors": [
                    {
                        "name": "Zixi Zhang"
                    },
                    {
                        "name": "Balint Szekely"
                    },
                    {
                        "name": "Pedro Gimenes"
                    },
                    {
                        "name": "Greg Chadwick"
                    },
                    {
                        "name": "Hugo McNally"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Robert Mullins"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.04535v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.04535v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19817v1",
                "updated": "2025-03-25T16:29:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    29,
                    17,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T16:29:17Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    29,
                    17,
                    1,
                    84,
                    0
                ],
                "title": "Bitstream Collisions in Neural Image Compression via Adversarial\n  Perturbations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bitstream Collisions in Neural Image Compression via Adversarial\n  Perturbations"
                },
                "summary": "Neural image compression (NIC) has emerged as a promising alternative to\nclassical compression techniques, offering improved compression ratios. Despite\nits progress towards standardization and practical deployment, there has been\nminimal exploration into it's robustness and security. This study reveals an\nunexpected vulnerability in NIC - bitstream collisions - where semantically\ndifferent images produce identical compressed bitstreams. Utilizing a novel\nwhitebox adversarial attack algorithm, this paper demonstrates that adding\ncarefully crafted perturbations to semantically different images can cause\ntheir compressed bitstreams to collide exactly. The collision vulnerability\nposes a threat to the practical usability of NIC, particularly in\nsecurity-critical applications. The cause of the collision is analyzed, and a\nsimple yet effective mitigation method is presented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural image compression (NIC) has emerged as a promising alternative to\nclassical compression techniques, offering improved compression ratios. Despite\nits progress towards standardization and practical deployment, there has been\nminimal exploration into it's robustness and security. This study reveals an\nunexpected vulnerability in NIC - bitstream collisions - where semantically\ndifferent images produce identical compressed bitstreams. Utilizing a novel\nwhitebox adversarial attack algorithm, this paper demonstrates that adding\ncarefully crafted perturbations to semantically different images can cause\ntheir compressed bitstreams to collide exactly. The collision vulnerability\nposes a threat to the practical usability of NIC, particularly in\nsecurity-critical applications. The cause of the collision is analyzed, and a\nsimple yet effective mitigation method is presented."
                },
                "authors": [
                    {
                        "name": "Jordan Madden"
                    },
                    {
                        "name": "Lhamo Dorje"
                    },
                    {
                        "name": "Xiaohua Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohua Li"
                },
                "author": "Xiaohua Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16815v2",
                "updated": "2025-03-25T16:19:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    19,
                    52,
                    1,
                    84,
                    0
                ],
                "published": "2024-11-25T15:35:01Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    35,
                    1,
                    0,
                    330,
                    0
                ],
                "title": "FREE-Merging: Fourier Transform for Efficient Model Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FREE-Merging: Fourier Transform for Efficient Model Merging"
                },
                "summary": "With the rapid growth of deep learning, there is an increasing availability\nof open-source models for various tasks. However, single fine-tuned models\noften fall short of meeting the diverse needs of users. Model merging has thus\nemerged as an efficient method to integrate the capabilities of existing models\ninto a unified model. Nevertheless, existing model merging methods face\nchallenging trade-offs between performance and deployment costs, primarily due\nto task interference. For the first time, we reveal that task interference is\nevident in the frequency domain of model parameters, yet current efforts only\nfocus on spatial domain solutions, which are largely ineffective in addressing\nfrequency domain interference. To mitigate the impact of frequency domain\ninterference, we propose FR-Merging, an innovative method that effectively\nfilters harmful frequency domain interference on the backbone with minimal\ncomputational overhead. Since performance loss is inevitable with cost-free\nmethods, we propose a lightweight task-specific expert module that dynamically\ncompensates for information loss during merging. This proposed framework,\nFREE-Merging (FR-Merging with experts), strikes a balanced trade-off between\ntraining cost, inference latency, storage requirements, and performance. We\ndemonstrate the effectiveness of both FR-Merging and FREE-Merging on multiple\ntasks across CV, NLP, and Multi-Modal domains and show that they can be\nflexibly adapted to specific needs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of deep learning, there is an increasing availability\nof open-source models for various tasks. However, single fine-tuned models\noften fall short of meeting the diverse needs of users. Model merging has thus\nemerged as an efficient method to integrate the capabilities of existing models\ninto a unified model. Nevertheless, existing model merging methods face\nchallenging trade-offs between performance and deployment costs, primarily due\nto task interference. For the first time, we reveal that task interference is\nevident in the frequency domain of model parameters, yet current efforts only\nfocus on spatial domain solutions, which are largely ineffective in addressing\nfrequency domain interference. To mitigate the impact of frequency domain\ninterference, we propose FR-Merging, an innovative method that effectively\nfilters harmful frequency domain interference on the backbone with minimal\ncomputational overhead. Since performance loss is inevitable with cost-free\nmethods, we propose a lightweight task-specific expert module that dynamically\ncompensates for information loss during merging. This proposed framework,\nFREE-Merging (FR-Merging with experts), strikes a balanced trade-off between\ntraining cost, inference latency, storage requirements, and performance. We\ndemonstrate the effectiveness of both FR-Merging and FREE-Merging on multiple\ntasks across CV, NLP, and Multi-Modal domains and show that they can be\nflexibly adapted to specific needs."
                },
                "authors": [
                    {
                        "name": "Shenghe Zheng"
                    },
                    {
                        "name": "Hongzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Wang"
                },
                "author": "Hongzhi Wang",
                "arxiv_comment": "20 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19794v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19794v1",
                "updated": "2025-03-25T16:02:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    2,
                    37,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T16:02:37Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    2,
                    37,
                    1,
                    84,
                    0
                ],
                "title": "PAVE: Patching and Adapting Video Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAVE: Patching and Adapting Video Large Language Models"
                },
                "summary": "Pre-trained video large language models (Video LLMs) exhibit remarkable\nreasoning capabilities, yet adapting these models to new tasks involving\nadditional modalities or data types (e.g., audio or 3D information) remains\nchallenging. In this paper, we present PAVE, a flexible framework for adapting\npre-trained Video LLMs to downstream tasks with side-channel signals, such as\naudio, 3D cues, or multi-view videos. PAVE introduces lightweight adapters,\nreferred to as \"patches,\" which add a small number of parameters and operations\nto a base model without modifying its architecture or pre-trained weights. In\ndoing so, PAVE can effectively adapt the pre-trained base model to support\ndiverse downstream tasks, including audio-visual question answering, 3D\nreasoning, multi-view video recognition, and high frame rate video\nunderstanding. Across these tasks, PAVE significantly enhances the performance\nof the base model, surpassing state-of-the-art task-specific models while\nincurring a minor cost of ~0.1% additional FLOPs and parameters. Further, PAVE\nsupports multi-task learning and generalizes well across different Video LLMs.\nOur code is available at https://github.com/dragonlzm/PAVE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained video large language models (Video LLMs) exhibit remarkable\nreasoning capabilities, yet adapting these models to new tasks involving\nadditional modalities or data types (e.g., audio or 3D information) remains\nchallenging. In this paper, we present PAVE, a flexible framework for adapting\npre-trained Video LLMs to downstream tasks with side-channel signals, such as\naudio, 3D cues, or multi-view videos. PAVE introduces lightweight adapters,\nreferred to as \"patches,\" which add a small number of parameters and operations\nto a base model without modifying its architecture or pre-trained weights. In\ndoing so, PAVE can effectively adapt the pre-trained base model to support\ndiverse downstream tasks, including audio-visual question answering, 3D\nreasoning, multi-view video recognition, and high frame rate video\nunderstanding. Across these tasks, PAVE significantly enhances the performance\nof the base model, surpassing state-of-the-art task-specific models while\nincurring a minor cost of ~0.1% additional FLOPs and parameters. Further, PAVE\nsupports multi-task learning and generalizes well across different Video LLMs.\nOur code is available at https://github.com/dragonlzm/PAVE."
                },
                "authors": [
                    {
                        "name": "Zhuoming Liu"
                    },
                    {
                        "name": "Yiquan Li"
                    },
                    {
                        "name": "Khoi Duc Nguyen"
                    },
                    {
                        "name": "Yiwu Zhong"
                    },
                    {
                        "name": "Yin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yin Li"
                },
                "author": "Yin Li",
                "arxiv_comment": "CVPR2025 Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19794v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19794v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17534v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17534v2",
                "updated": "2025-03-25T16:00:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    0,
                    7,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-21T20:31:47Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    20,
                    31,
                    47,
                    4,
                    80,
                    0
                ],
                "title": "MetaSel: A Test Selection Approach for Fine-tuned DNN Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaSel: A Test Selection Approach for Fine-tuned DNN Models"
                },
                "summary": "Deep Neural Networks (DNNs) face challenges during deployment due to data\ndistribution shifts. Fine-tuning adapts pre-trained models to new contexts\nrequiring smaller labeled sets. However, testing fine-tuned models under\nconstrained labeling budgets remains a critical challenge. This paper\nintroduces MetaSel, a new approach, tailored for fine-tuned DNN models, to\nselect tests from unlabeled inputs. MetaSel assumes that fine-tuned and\npre-trained models share related data distributions and exhibit similar\nbehaviors for many inputs. However, their behaviors diverge within the input\nsubspace where fine-tuning alters decision boundaries, making those inputs more\nprone to misclassification. Unlike general approaches that rely solely on the\nDNN model and its input set, MetaSel leverages information from both the\nfine-tuned and pre-trained models and their behavioral differences to estimate\nmisclassification probability for unlabeled test inputs, enabling more\neffective test selection. Our extensive empirical evaluation, comparing MetaSel\nagainst 10 state-of-the-art approaches and involving 68 fine-tuned models\nacross weak, medium, and strong distribution shifts, demonstrates that MetaSel\nconsistently delivers significant improvements in Test Relative Coverage (TRC)\nover existing baselines, particularly under highly constrained labeling\nbudgets. MetaSel shows average TRC improvements of 28.46% to 56.18% over the\nmost frequent second-best baselines while maintaining a high TRC median and low\nvariability. Our results confirm MetaSel's practicality, robustness, and\ncost-effectiveness for test selection in the context of fine-tuned models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Networks (DNNs) face challenges during deployment due to data\ndistribution shifts. Fine-tuning adapts pre-trained models to new contexts\nrequiring smaller labeled sets. However, testing fine-tuned models under\nconstrained labeling budgets remains a critical challenge. This paper\nintroduces MetaSel, a new approach, tailored for fine-tuned DNN models, to\nselect tests from unlabeled inputs. MetaSel assumes that fine-tuned and\npre-trained models share related data distributions and exhibit similar\nbehaviors for many inputs. However, their behaviors diverge within the input\nsubspace where fine-tuning alters decision boundaries, making those inputs more\nprone to misclassification. Unlike general approaches that rely solely on the\nDNN model and its input set, MetaSel leverages information from both the\nfine-tuned and pre-trained models and their behavioral differences to estimate\nmisclassification probability for unlabeled test inputs, enabling more\neffective test selection. Our extensive empirical evaluation, comparing MetaSel\nagainst 10 state-of-the-art approaches and involving 68 fine-tuned models\nacross weak, medium, and strong distribution shifts, demonstrates that MetaSel\nconsistently delivers significant improvements in Test Relative Coverage (TRC)\nover existing baselines, particularly under highly constrained labeling\nbudgets. MetaSel shows average TRC improvements of 28.46% to 56.18% over the\nmost frequent second-best baselines while maintaining a high TRC median and low\nvariability. Our results confirm MetaSel's practicality, robustness, and\ncost-effectiveness for test selection in the context of fine-tuned models."
                },
                "authors": [
                    {
                        "name": "Amin Abbasishahkoo"
                    },
                    {
                        "name": "Mahboubeh Dadkhah"
                    },
                    {
                        "name": "Lionel Briand"
                    },
                    {
                        "name": "Dayi Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dayi Lin"
                },
                "author": "Dayi Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17534v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17534v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18155v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18155v2",
                "updated": "2025-03-25T15:58:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    58,
                    36,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-23T17:48:44Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    17,
                    48,
                    44,
                    6,
                    82,
                    0
                ],
                "title": "Decorum: A Language-Based Approach For Style-Conditioned Synthesis of\n  Indoor 3D Scenes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decorum: A Language-Based Approach For Style-Conditioned Synthesis of\n  Indoor 3D Scenes"
                },
                "summary": "3D indoor scene generation is an important problem for the design of digital\nand real-world environments. To automate this process, a scene generation model\nshould be able to not only generate plausible scene layouts, but also take into\nconsideration visual features and style preferences. Existing methods for this\ntask exhibit very limited control over these attributes, only allowing text\ninputs in the form of simple object-level descriptions or pairwise spatial\nrelationships. Our proposed method Decorum enables users to control the scene\ngeneration process with natural language by adopting language-based\nrepresentations at each stage. This enables us to harness recent advancements\nin Large Language Models (LLMs) to model language-to-language mappings. In\naddition, we show that using a text-based representation allows us to select\nfurniture for our scenes using a novel object retrieval method based on\nmultimodal LLMs. Evaluations on the benchmark 3D-FRONT dataset show that our\nmethods achieve improvements over existing work in text-conditioned scene\nsynthesis and object retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D indoor scene generation is an important problem for the design of digital\nand real-world environments. To automate this process, a scene generation model\nshould be able to not only generate plausible scene layouts, but also take into\nconsideration visual features and style preferences. Existing methods for this\ntask exhibit very limited control over these attributes, only allowing text\ninputs in the form of simple object-level descriptions or pairwise spatial\nrelationships. Our proposed method Decorum enables users to control the scene\ngeneration process with natural language by adopting language-based\nrepresentations at each stage. This enables us to harness recent advancements\nin Large Language Models (LLMs) to model language-to-language mappings. In\naddition, we show that using a text-based representation allows us to select\nfurniture for our scenes using a novel object retrieval method based on\nmultimodal LLMs. Evaluations on the benchmark 3D-FRONT dataset show that our\nmethods achieve improvements over existing work in text-conditioned scene\nsynthesis and object retrieval."
                },
                "authors": [
                    {
                        "name": "Kelly O. Marshall"
                    },
                    {
                        "name": "Omid Poursaeed"
                    },
                    {
                        "name": "Sergiu Oprea"
                    },
                    {
                        "name": "Amit Kumar"
                    },
                    {
                        "name": "Anushrut Jignasu"
                    },
                    {
                        "name": "Chinmay Hegde"
                    },
                    {
                        "name": "Yilei Li"
                    },
                    {
                        "name": "Rakesh Ranjan"
                    }
                ],
                "author_detail": {
                    "name": "Rakesh Ranjan"
                },
                "author": "Rakesh Ranjan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18155v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18155v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02341v2",
                "updated": "2025-03-25T15:55:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    55,
                    33,
                    1,
                    84,
                    0
                ],
                "published": "2025-01-04T17:32:12Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    17,
                    32,
                    12,
                    5,
                    4,
                    0
                ],
                "title": "UAVs Meet LLMs: Overviews and Perspectives Toward Agentic Low-Altitude\n  Mobility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UAVs Meet LLMs: Overviews and Perspectives Toward Agentic Low-Altitude\n  Mobility"
                },
                "summary": "Low-altitude mobility, exemplified by unmanned aerial vehicles (UAVs), has\nintroduced transformative advancements across various domains, like\ntransportation, logistics, and agriculture. Leveraging flexible perspectives\nand rapid maneuverability, UAVs extend traditional systems' perception and\naction capabilities, garnering widespread attention from academia and industry.\nHowever, current UAV operations primarily depend on human control, with only\nlimited autonomy in simple scenarios, and lack the intelligence and\nadaptability needed for more complex environments and tasks. The emergence of\nlarge language models (LLMs) demonstrates remarkable problem-solving and\ngeneralization capabilities, offering a promising pathway for advancing UAV\nintelligence. This paper explores the integration of LLMs and UAVs, beginning\nwith an overview of UAV systems' fundamental components and functionalities,\nfollowed by an overview of the state-of-the-art in LLM technology.\nSubsequently, it systematically highlights the multimodal data resources\navailable for UAVs, which provide critical support for training and evaluation.\nFurthermore, it categorizes and analyzes key tasks and application scenarios\nwhere UAVs and LLMs converge. Finally, a reference roadmap towards agentic UAVs\nis proposed, aiming to enable UAVs to achieve agentic intelligence through\nautonomous perception, memory, reasoning, and tool utilization. Related\nresources are available at https://github.com/Hub-Tian/UAVs_Meet_LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-altitude mobility, exemplified by unmanned aerial vehicles (UAVs), has\nintroduced transformative advancements across various domains, like\ntransportation, logistics, and agriculture. Leveraging flexible perspectives\nand rapid maneuverability, UAVs extend traditional systems' perception and\naction capabilities, garnering widespread attention from academia and industry.\nHowever, current UAV operations primarily depend on human control, with only\nlimited autonomy in simple scenarios, and lack the intelligence and\nadaptability needed for more complex environments and tasks. The emergence of\nlarge language models (LLMs) demonstrates remarkable problem-solving and\ngeneralization capabilities, offering a promising pathway for advancing UAV\nintelligence. This paper explores the integration of LLMs and UAVs, beginning\nwith an overview of UAV systems' fundamental components and functionalities,\nfollowed by an overview of the state-of-the-art in LLM technology.\nSubsequently, it systematically highlights the multimodal data resources\navailable for UAVs, which provide critical support for training and evaluation.\nFurthermore, it categorizes and analyzes key tasks and application scenarios\nwhere UAVs and LLMs converge. Finally, a reference roadmap towards agentic UAVs\nis proposed, aiming to enable UAVs to achieve agentic intelligence through\nautonomous perception, memory, reasoning, and tool utilization. Related\nresources are available at https://github.com/Hub-Tian/UAVs_Meet_LLMs."
                },
                "authors": [
                    {
                        "name": "Yonglin Tian"
                    },
                    {
                        "name": "Fei Lin"
                    },
                    {
                        "name": "Yiduo Li"
                    },
                    {
                        "name": "Tengchao Zhang"
                    },
                    {
                        "name": "Qiyao Zhang"
                    },
                    {
                        "name": "Xuan Fu"
                    },
                    {
                        "name": "Jun Huang"
                    },
                    {
                        "name": "Xingyuan Dai"
                    },
                    {
                        "name": "Yutong Wang"
                    },
                    {
                        "name": "Chunwei Tian"
                    },
                    {
                        "name": "Bai Li"
                    },
                    {
                        "name": "Yisheng Lv"
                    },
                    {
                        "name": "Levente Kovács"
                    },
                    {
                        "name": "Fei-Yue Wang"
                    }
                ],
                "author_detail": {
                    "name": "Fei-Yue Wang"
                },
                "author": "Fei-Yue Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19779v1",
                "updated": "2025-03-25T15:47:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    47,
                    54,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T15:47:54Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    47,
                    54,
                    1,
                    84,
                    0
                ],
                "title": "PyGraph: Robust Compiler Support for CUDA Graphs in PyTorch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PyGraph: Robust Compiler Support for CUDA Graphs in PyTorch"
                },
                "summary": "CUDA Graphs -- a recent hardware feature introduced for NVIDIA GPUs -- aim to\nreduce CPU launch overhead by capturing and launching a series of GPU tasks\n(kernels) as a DAG. However, deploying CUDA Graphs faces several challenges\ntoday due to the static structure of a graph. It also incurs performance\noverhead due to data copy. In fact, we show a counter-intuitive result --\ndeploying CUDA Graphs hurts performance in many cases.\n  We introduce PyGraph, a novel approach to automatically harness the power of\nCUDA Graphs within PyTorch2. Driven by three key observations, PyGraph embodies\nthree novel optimizations: it enables wider deployment of CUDA Graphs, reduces\nGPU kernel parameter copy overheads, and selectively deploys CUDA Graphs based\non a cost-benefit analysis. PyGraph seamlessly integrates with PyTorch2's\ncompilation toolchain, enabling efficient use of CUDA Graphs without manual\nmodifications to the code. We evaluate PyGraph across various machine learning\nbenchmarks, demonstrating substantial performance improvements over PyTorch2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CUDA Graphs -- a recent hardware feature introduced for NVIDIA GPUs -- aim to\nreduce CPU launch overhead by capturing and launching a series of GPU tasks\n(kernels) as a DAG. However, deploying CUDA Graphs faces several challenges\ntoday due to the static structure of a graph. It also incurs performance\noverhead due to data copy. In fact, we show a counter-intuitive result --\ndeploying CUDA Graphs hurts performance in many cases.\n  We introduce PyGraph, a novel approach to automatically harness the power of\nCUDA Graphs within PyTorch2. Driven by three key observations, PyGraph embodies\nthree novel optimizations: it enables wider deployment of CUDA Graphs, reduces\nGPU kernel parameter copy overheads, and selectively deploys CUDA Graphs based\non a cost-benefit analysis. PyGraph seamlessly integrates with PyTorch2's\ncompilation toolchain, enabling efficient use of CUDA Graphs without manual\nmodifications to the code. We evaluate PyGraph across various machine learning\nbenchmarks, demonstrating substantial performance improvements over PyTorch2."
                },
                "authors": [
                    {
                        "name": "Abhishek Ghosh"
                    },
                    {
                        "name": "Ajay Nayak"
                    },
                    {
                        "name": "Ashish Panwar"
                    },
                    {
                        "name": "Arkaprava Basu"
                    }
                ],
                "author_detail": {
                    "name": "Arkaprava Basu"
                },
                "author": "Arkaprava Basu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11102v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11102v3",
                "updated": "2025-03-25T15:35:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    35,
                    29,
                    1,
                    84,
                    0
                ],
                "published": "2024-12-15T07:49:31Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    7,
                    49,
                    31,
                    6,
                    350,
                    0
                ],
                "title": "Empowering LLMs to Understand and Generate Complex Vector Graphics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering LLMs to Understand and Generate Complex Vector Graphics"
                },
                "summary": "The unprecedented advancements in Large Language Models (LLMs) have\nprofoundly impacted natural language processing but have yet to fully embrace\nthe realm of scalable vector graphics (SVG) generation. While LLMs encode\npartial knowledge of SVG data from web pages during training, recent findings\nsuggest that semantically ambiguous and tokenized representations within LLMs\nmay result in hallucinations in vector primitive predictions. Additionally, LLM\ntraining typically lacks modeling and understanding of the rendering sequence\nof vector paths, which can lead to occlusion between output vector primitives.\nIn this paper, we present LLM4SVG, an initial yet substantial step toward\nbridging this gap by enabling LLMs to better understand and generate vector\ngraphics. LLM4SVG facilitates a deeper understanding of SVG components through\nlearnable semantic tokens, which precisely encode these tokens and their\ncorresponding properties to generate semantically aligned SVG outputs. Using a\nseries of learnable semantic tokens, a structured dataset for instruction\nfollowing is developed to support comprehension and generation across two\nprimary tasks. Our method introduces a modular architecture to existing large\nlanguage models, integrating semantic tags, vector instruction encoders,\nfine-tuned commands, and powerful LLMs to tightly combine geometric,\nappearance, and language information. To overcome the scarcity of SVG-text\ninstruction data, we developed an automated data generation pipeline that\ncollected our SVGX-SFT Dataset, consisting of high-quality human-designed SVGs\nand 580k SVG instruction following data specifically crafted for LLM training,\nwhich facilitated the adoption of the supervised fine-tuning strategy popular\nin LLM development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The unprecedented advancements in Large Language Models (LLMs) have\nprofoundly impacted natural language processing but have yet to fully embrace\nthe realm of scalable vector graphics (SVG) generation. While LLMs encode\npartial knowledge of SVG data from web pages during training, recent findings\nsuggest that semantically ambiguous and tokenized representations within LLMs\nmay result in hallucinations in vector primitive predictions. Additionally, LLM\ntraining typically lacks modeling and understanding of the rendering sequence\nof vector paths, which can lead to occlusion between output vector primitives.\nIn this paper, we present LLM4SVG, an initial yet substantial step toward\nbridging this gap by enabling LLMs to better understand and generate vector\ngraphics. LLM4SVG facilitates a deeper understanding of SVG components through\nlearnable semantic tokens, which precisely encode these tokens and their\ncorresponding properties to generate semantically aligned SVG outputs. Using a\nseries of learnable semantic tokens, a structured dataset for instruction\nfollowing is developed to support comprehension and generation across two\nprimary tasks. Our method introduces a modular architecture to existing large\nlanguage models, integrating semantic tags, vector instruction encoders,\nfine-tuned commands, and powerful LLMs to tightly combine geometric,\nappearance, and language information. To overcome the scarcity of SVG-text\ninstruction data, we developed an automated data generation pipeline that\ncollected our SVGX-SFT Dataset, consisting of high-quality human-designed SVGs\nand 580k SVG instruction following data specifically crafted for LLM training,\nwhich facilitated the adoption of the supervised fine-tuning strategy popular\nin LLM development."
                },
                "authors": [
                    {
                        "name": "Ximing Xing"
                    },
                    {
                        "name": "Juncheng Hu"
                    },
                    {
                        "name": "Guotao Liang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Dong Xu"
                    },
                    {
                        "name": "Qian Yu"
                    }
                ],
                "author_detail": {
                    "name": "Qian Yu"
                },
                "author": "Qian Yu",
                "arxiv_comment": "Accepted by CVPR 2025. Project Page:\n  https://ximinng.github.io/LLM4SVGProject/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11102v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11102v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19755v1",
                "updated": "2025-03-25T15:18:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    18,
                    43,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T15:18:43Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    18,
                    43,
                    1,
                    84,
                    0
                ],
                "title": "ORION: A Holistic End-to-End Autonomous Driving Framework by\n  Vision-Language Instructed Action Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORION: A Holistic End-to-End Autonomous Driving Framework by\n  Vision-Language Instructed Action Generation"
                },
                "summary": "End-to-end (E2E) autonomous driving methods still struggle to make correct\ndecisions in interactive closed-loop evaluation due to limited causal reasoning\ncapability. Current methods attempt to leverage the powerful understanding and\nreasoning abilities of Vision-Language Models (VLMs) to resolve this dilemma.\nHowever, the problem is still open that few VLMs for E2E methods perform well\nin the closed-loop evaluation due to the gap between the semantic reasoning\nspace and the purely numerical trajectory output in the action space. To tackle\nthis issue, we propose ORION, a holistic E2E autonomous driving framework by\nvision-language instructed action generation. ORION uniquely combines a\nQT-Former to aggregate long-term history context, a Large Language Model (LLM)\nfor driving scenario reasoning, and a generative planner for precision\ntrajectory prediction. ORION further aligns the reasoning space and the action\nspace to implement a unified E2E optimization for both visual\nquestion-answering (VQA) and planning tasks. Our method achieves an impressive\nclosed-loop performance of 77.74 Driving Score (DS) and 54.62% Success Rate\n(SR) on the challenge Bench2Drive datasets, which outperforms state-of-the-art\n(SOTA) methods by a large margin of 14.28 DS and 19.61% SR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end (E2E) autonomous driving methods still struggle to make correct\ndecisions in interactive closed-loop evaluation due to limited causal reasoning\ncapability. Current methods attempt to leverage the powerful understanding and\nreasoning abilities of Vision-Language Models (VLMs) to resolve this dilemma.\nHowever, the problem is still open that few VLMs for E2E methods perform well\nin the closed-loop evaluation due to the gap between the semantic reasoning\nspace and the purely numerical trajectory output in the action space. To tackle\nthis issue, we propose ORION, a holistic E2E autonomous driving framework by\nvision-language instructed action generation. ORION uniquely combines a\nQT-Former to aggregate long-term history context, a Large Language Model (LLM)\nfor driving scenario reasoning, and a generative planner for precision\ntrajectory prediction. ORION further aligns the reasoning space and the action\nspace to implement a unified E2E optimization for both visual\nquestion-answering (VQA) and planning tasks. Our method achieves an impressive\nclosed-loop performance of 77.74 Driving Score (DS) and 54.62% Success Rate\n(SR) on the challenge Bench2Drive datasets, which outperforms state-of-the-art\n(SOTA) methods by a large margin of 14.28 DS and 19.61% SR."
                },
                "authors": [
                    {
                        "name": "Haoyu Fu"
                    },
                    {
                        "name": "Diankun Zhang"
                    },
                    {
                        "name": "Zongchuang Zhao"
                    },
                    {
                        "name": "Jianfeng Cui"
                    },
                    {
                        "name": "Dingkang Liang"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Dingyuan Zhang"
                    },
                    {
                        "name": "Hongwei Xie"
                    },
                    {
                        "name": "Bing Wang"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19752v1",
                "updated": "2025-03-25T15:16:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    16,
                    35,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T15:16:35Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    16,
                    35,
                    1,
                    84,
                    0
                ],
                "title": "Inducing Personality in LLM-Based Honeypot Agents: Measuring the Effect\n  on Human-Like Agenda Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inducing Personality in LLM-Based Honeypot Agents: Measuring the Effect\n  on Human-Like Agenda Generation"
                },
                "summary": "This paper presents SANDMAN, an architecture for cyber deception that\nleverages Language Agents to emulate convincing human simulacra. Our 'Deceptive\nAgents' serve as advanced cyber decoys, designed for high-fidelity engagement\nwith attackers by extending the observation period of attack behaviours.\nThrough experimentation, measurement, and analysis, we demonstrate how a prompt\nschema based on the five-factor model of personality systematically induces\ndistinct 'personalities' in Large Language Models. Our results highlight the\nfeasibility of persona-driven Language Agents for generating diverse, realistic\nbehaviours, ultimately improving cyber deception strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents SANDMAN, an architecture for cyber deception that\nleverages Language Agents to emulate convincing human simulacra. Our 'Deceptive\nAgents' serve as advanced cyber decoys, designed for high-fidelity engagement\nwith attackers by extending the observation period of attack behaviours.\nThrough experimentation, measurement, and analysis, we demonstrate how a prompt\nschema based on the five-factor model of personality systematically induces\ndistinct 'personalities' in Large Language Models. Our results highlight the\nfeasibility of persona-driven Language Agents for generating diverse, realistic\nbehaviours, ultimately improving cyber deception strategies."
                },
                "authors": [
                    {
                        "name": "Lewis Newsham"
                    },
                    {
                        "name": "Ryan Hyland"
                    },
                    {
                        "name": "Daniel Prince"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Prince"
                },
                "author": "Daniel Prince",
                "arxiv_comment": "11 pages, 1 figure, 6 tables. Accepted to NLPAICS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09360v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09360v2",
                "updated": "2025-03-25T15:10:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    10,
                    24,
                    1,
                    84,
                    0
                ],
                "published": "2024-12-12T15:27:47Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    27,
                    47,
                    3,
                    347,
                    0
                ],
                "title": "Doc2OracLL: Investigating the Impact of Documentation on LLM-based Test\n  Oracle Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doc2OracLL: Investigating the Impact of Documentation on LLM-based Test\n  Oracle Generation"
                },
                "summary": "Code documentation is a critical aspect of software development, serving as a\nbridge between human understanding and machine-readable code. Beyond assisting\ndevelopers in understanding and maintaining code, documentation also plays a\ncritical role in automating various software engineering tasks, such as test\noracle generation (TOG). In Java, Javadoc comments provide structured, natural\nlanguage documentation embedded directly in the source code, typically\ndetailing functionality, usage, parameters, return values, and exceptions.\nWhile prior research has utilized Javadoc comments in test oracle generation\n(TOG), there has not been a thorough investigation into their impact when\ncombined with other contextual information, nor into identifying the most\nrelevant components for generating correct and strong test oracles, or\nunderstanding their role in detecting real bugs. In this study, we dive deep\ninto investigating the impact of Javadoc comments on TOG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code documentation is a critical aspect of software development, serving as a\nbridge between human understanding and machine-readable code. Beyond assisting\ndevelopers in understanding and maintaining code, documentation also plays a\ncritical role in automating various software engineering tasks, such as test\noracle generation (TOG). In Java, Javadoc comments provide structured, natural\nlanguage documentation embedded directly in the source code, typically\ndetailing functionality, usage, parameters, return values, and exceptions.\nWhile prior research has utilized Javadoc comments in test oracle generation\n(TOG), there has not been a thorough investigation into their impact when\ncombined with other contextual information, nor into identifying the most\nrelevant components for generating correct and strong test oracles, or\nunderstanding their role in detecting real bugs. In this study, we dive deep\ninto investigating the impact of Javadoc comments on TOG."
                },
                "authors": [
                    {
                        "name": "Soneya Binta Hossain"
                    },
                    {
                        "name": "Raygan Taylor"
                    },
                    {
                        "name": "Matthew Dwyer"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Dwyer"
                },
                "author": "Matthew Dwyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09360v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09360v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19742v1",
                "updated": "2025-03-25T15:05:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    5,
                    25,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T15:05:25Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    5,
                    25,
                    1,
                    84,
                    0
                ],
                "title": "Optimizing Photonic Structures with Large Language Model Driven\n  Algorithm Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Photonic Structures with Large Language Model Driven\n  Algorithm Discovery"
                },
                "summary": "We study how large language models can be used in combination with\nevolutionary computation techniques to automatically discover optimization\nalgorithms for the design of photonic structures. Building on the Large\nLanguage Model Evolutionary Algorithm (LLaMEA) framework, we introduce\nstructured prompt engineering tailored to multilayer photonic problems such as\nBragg mirror, ellipsometry inverse analysis, and solar cell antireflection\ncoatings. We systematically explore multiple evolutionary strategies, including\n(1+1), (1+5), (2+10), and others, to balance exploration and exploitation. Our\nexperiments show that LLM-generated algorithms, generated using small-scale\nproblem instances, can match or surpass established methods like\nquasi-oppositional differential evolution on large-scale realistic real-world\nproblem instances. Notably, LLaMEA's self-debugging mutation loop, augmented by\nautomatically extracted problem-specific insights, achieves strong anytime\nperformance and reliable convergence across diverse problem scales. This work\ndemonstrates the feasibility of domain-focused LLM prompts and evolutionary\napproaches in solving optical design tasks, paving the way for rapid, automated\nphotonic inverse design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study how large language models can be used in combination with\nevolutionary computation techniques to automatically discover optimization\nalgorithms for the design of photonic structures. Building on the Large\nLanguage Model Evolutionary Algorithm (LLaMEA) framework, we introduce\nstructured prompt engineering tailored to multilayer photonic problems such as\nBragg mirror, ellipsometry inverse analysis, and solar cell antireflection\ncoatings. We systematically explore multiple evolutionary strategies, including\n(1+1), (1+5), (2+10), and others, to balance exploration and exploitation. Our\nexperiments show that LLM-generated algorithms, generated using small-scale\nproblem instances, can match or surpass established methods like\nquasi-oppositional differential evolution on large-scale realistic real-world\nproblem instances. Notably, LLaMEA's self-debugging mutation loop, augmented by\nautomatically extracted problem-specific insights, achieves strong anytime\nperformance and reliable convergence across diverse problem scales. This work\ndemonstrates the feasibility of domain-focused LLM prompts and evolutionary\napproaches in solving optical design tasks, paving the way for rapid, automated\nphotonic inverse design."
                },
                "authors": [
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Anna V. Kononova"
                    },
                    {
                        "name": "Thomas Bäck"
                    },
                    {
                        "name": "Niki van Stein"
                    }
                ],
                "author_detail": {
                    "name": "Niki van Stein"
                },
                "author": "Niki van Stein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19739v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19739v2",
                "updated": "2025-03-26T06:54:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    6,
                    54,
                    19,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-25T15:04:53Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    4,
                    53,
                    1,
                    84,
                    0
                ],
                "title": "FUSE: Label-Free Image-Event Joint Monocular Depth Estimation via\n  Frequency-Decoupled Alignment and Degradation-Robust Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FUSE: Label-Free Image-Event Joint Monocular Depth Estimation via\n  Frequency-Decoupled Alignment and Degradation-Robust Fusion"
                },
                "summary": "Image-event joint depth estimation methods leverage complementary modalities\nfor robust perception, yet face challenges in generalizability stemming from\ntwo factors: 1) limited annotated image-event-depth datasets causing\ninsufficient cross-modal supervision, and 2) inherent frequency mismatches\nbetween static images and dynamic event streams with distinct spatiotemporal\npatterns, leading to ineffective feature fusion. To address this dual\nchallenge, we propose Frequency-decoupled Unified Self-supervised Encoder\n(FUSE) with two synergistic components: The Parameter-efficient Self-supervised\nTransfer (PST) establishes cross-modal knowledge transfer through latent space\nalignment with image foundation models, effectively mitigating data scarcity by\nenabling joint encoding without depth ground truth. Complementing this, we\npropose the Frequency-Decoupled Fusion module (FreDFuse) to explicitly decouple\nhigh-frequency edge features from low-frequency structural components,\nresolving modality-specific frequency mismatches through physics-aware fusion.\nThis combined approach enables FUSE to construct a universal image-event\nencoder that only requires lightweight decoder adaptation for target datasets.\nExtensive experiments demonstrate state-of-the-art performance with 14% and\n24.9% improvements in Abs.Rel on MVSEC and DENSE datasets. The framework\nexhibits remarkable zero-shot adaptability to challenging scenarios including\nextreme lighting and motion blur, significantly advancing real-world deployment\ncapabilities. The source code for our method is publicly available at:\nhttps://github.com/sunpihai-up/FUSE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image-event joint depth estimation methods leverage complementary modalities\nfor robust perception, yet face challenges in generalizability stemming from\ntwo factors: 1) limited annotated image-event-depth datasets causing\ninsufficient cross-modal supervision, and 2) inherent frequency mismatches\nbetween static images and dynamic event streams with distinct spatiotemporal\npatterns, leading to ineffective feature fusion. To address this dual\nchallenge, we propose Frequency-decoupled Unified Self-supervised Encoder\n(FUSE) with two synergistic components: The Parameter-efficient Self-supervised\nTransfer (PST) establishes cross-modal knowledge transfer through latent space\nalignment with image foundation models, effectively mitigating data scarcity by\nenabling joint encoding without depth ground truth. Complementing this, we\npropose the Frequency-Decoupled Fusion module (FreDFuse) to explicitly decouple\nhigh-frequency edge features from low-frequency structural components,\nresolving modality-specific frequency mismatches through physics-aware fusion.\nThis combined approach enables FUSE to construct a universal image-event\nencoder that only requires lightweight decoder adaptation for target datasets.\nExtensive experiments demonstrate state-of-the-art performance with 14% and\n24.9% improvements in Abs.Rel on MVSEC and DENSE datasets. The framework\nexhibits remarkable zero-shot adaptability to challenging scenarios including\nextreme lighting and motion blur, significantly advancing real-world deployment\ncapabilities. The source code for our method is publicly available at:\nhttps://github.com/sunpihai-up/FUSE"
                },
                "authors": [
                    {
                        "name": "Pihai Sun"
                    },
                    {
                        "name": "Junjun Jiang"
                    },
                    {
                        "name": "Yuanqi Yao"
                    },
                    {
                        "name": "Youyu Chen"
                    },
                    {
                        "name": "Wenbo Zhao"
                    },
                    {
                        "name": "Kui Jiang"
                    },
                    {
                        "name": "Xianming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xianming Liu"
                },
                "author": "Xianming Liu",
                "arxiv_comment": "8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19739v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19739v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12893v3",
                "updated": "2025-03-25T15:02:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    2,
                    17,
                    1,
                    84,
                    0
                ],
                "published": "2024-10-16T12:24:42Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    24,
                    42,
                    2,
                    290,
                    0
                ],
                "title": "MIRROR: A Novel Approach for the Automated Evaluation of Open-Ended\n  Question Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIRROR: A Novel Approach for the Automated Evaluation of Open-Ended\n  Question Generation"
                },
                "summary": "Automatic question generation is a critical task that involves evaluating\nquestion quality by considering factors such as engagement, pedagogical value,\nand the ability to stimulate critical thinking. These aspects require\nhuman-like understanding and judgment, which automated systems currently lack.\nHowever, human evaluations are costly and impractical for large-scale samples\nof generated questions. Therefore, we propose a novel system, MIRROR (Multi-LLM\nIterative Review and Response for Optimized Rating), which leverages large\nlanguage models (LLMs) to automate the evaluation process for questions\ngenerated by automated question generation systems. We experimented with\nseveral state-of-the-art LLMs, such as GPT-4, Gemini, and Llama2-70b. We\nobserved that the scores of human evaluation metrics, namely relevance,\nappropriateness, novelty, complexity, and grammaticality, improved when using\nthe feedback-based approach called MIRROR, tending to be closer to the human\nbaseline scores. Furthermore, we observed that Pearson's correlation\ncoefficient between GPT-4 and human experts improved when using our proposed\nfeedback-based approach, MIRROR, compared to direct prompting for evaluation.\nError analysis shows that our proposed approach, MIRROR, significantly helps to\nimprove relevance and appropriateness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic question generation is a critical task that involves evaluating\nquestion quality by considering factors such as engagement, pedagogical value,\nand the ability to stimulate critical thinking. These aspects require\nhuman-like understanding and judgment, which automated systems currently lack.\nHowever, human evaluations are costly and impractical for large-scale samples\nof generated questions. Therefore, we propose a novel system, MIRROR (Multi-LLM\nIterative Review and Response for Optimized Rating), which leverages large\nlanguage models (LLMs) to automate the evaluation process for questions\ngenerated by automated question generation systems. We experimented with\nseveral state-of-the-art LLMs, such as GPT-4, Gemini, and Llama2-70b. We\nobserved that the scores of human evaluation metrics, namely relevance,\nappropriateness, novelty, complexity, and grammaticality, improved when using\nthe feedback-based approach called MIRROR, tending to be closer to the human\nbaseline scores. Furthermore, we observed that Pearson's correlation\ncoefficient between GPT-4 and human experts improved when using our proposed\nfeedback-based approach, MIRROR, compared to direct prompting for evaluation.\nError analysis shows that our proposed approach, MIRROR, significantly helps to\nimprove relevance and appropriateness."
                },
                "authors": [
                    {
                        "name": "Aniket Deroy"
                    },
                    {
                        "name": "Subhankar Maity"
                    },
                    {
                        "name": "Sudeshna Sarkar"
                    }
                ],
                "author_detail": {
                    "name": "Sudeshna Sarkar"
                },
                "author": "Sudeshna Sarkar",
                "arxiv_comment": "Updated Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05215v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05215v3",
                "updated": "2025-03-25T14:48:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    14,
                    48,
                    1,
                    1,
                    84,
                    0
                ],
                "published": "2023-12-08T18:07:05Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    18,
                    7,
                    5,
                    4,
                    342,
                    0
                ],
                "title": "DeltaZip: Efficient Serving of Multiple Full-Model-Tuned LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeltaZip: Efficient Serving of Multiple Full-Model-Tuned LLMs"
                },
                "summary": "Fine-tuning large language models (LLMs) greatly improves model quality for\ndownstream tasks. However, serving many fine-tuned LLMs concurrently is\nchallenging due to the sporadic, bursty, and varying request patterns of\ndifferent LLMs. To bridge this gap, we present DeltaZip, an LLM serving system\nthat efficiently serves multiple full-parameter fine-tuned models concurrently\nby aggressively compressing model deltas by up to 10x while maintaining high\nmodel quality. The key insight behind this design is that fine-tuning results\nin small-magnitude changes to the pre-trained model. By co-designing the\nserving system with the compression algorithm, DeltaZip achieves 2x to 12x\nimprovement in throughput compared to the state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) greatly improves model quality for\ndownstream tasks. However, serving many fine-tuned LLMs concurrently is\nchallenging due to the sporadic, bursty, and varying request patterns of\ndifferent LLMs. To bridge this gap, we present DeltaZip, an LLM serving system\nthat efficiently serves multiple full-parameter fine-tuned models concurrently\nby aggressively compressing model deltas by up to 10x while maintaining high\nmodel quality. The key insight behind this design is that fine-tuning results\nin small-magnitude changes to the pre-trained model. By co-designing the\nserving system with the compression algorithm, DeltaZip achieves 2x to 12x\nimprovement in throughput compared to the state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Xiaozhe Yao"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Ana Klimovic"
                    }
                ],
                "author_detail": {
                    "name": "Ana Klimovic"
                },
                "author": "Ana Klimovic",
                "arxiv_comment": "EuroSys 2025'",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05215v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05215v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17662v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17662v2",
                "updated": "2025-03-25T14:43:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    14,
                    43,
                    35,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-22T06:12:34Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    6,
                    12,
                    34,
                    5,
                    81,
                    0
                ],
                "title": "Enhancing Persona Consistency for LLMs' Role-Playing using Persona-Aware\n  Contrastive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Persona Consistency for LLMs' Role-Playing using Persona-Aware\n  Contrastive Learning"
                },
                "summary": "In recent years, large language models (LLMs) have achieved breakthrough\nprogress in many dialogue generation tasks. However, their lack of emotion and\nfine-grained role awareness limits the model's ability to provide personalized\nand diverse interactions further. Current methods face high costs in collecting\nhigh-quality annotated data for scenarios such as role-playing, and traditional\nhuman alignment methods are difficult to deploy due to the inherent diversity\nof model behavior in role-playing scenarios. Inspired by the alignment of\nmodels for safety behaviors through RLHF (Reinforcement Learning from Human\nFeedback), in this paper, we revisit model role-playing behavior from the\nperspective of persona alignment and propose a novel annotation-free framework\nnamed \\textbf{\\underline{P}}ersona-Aware \\textbf{\\underline{C}}ontrastive\n\\textbf{\\underline{L}}earning (PCL) to align LLMs' behavior during\nrole-playing, enhancing the model's role consistency. Specifically, we first\ndesign a role chain method to encourage the model to self-question based on the\nrole characteristics and dialogue context to adjust personality consistency.\nThen, we further enhance the model's role-playing strategy through iterative\ncontrastive learning between the use of role characteristics and not.\nExperiments on both black-box and white-box LLMs show that LLMs equipped with\nPCL significantly outperform vanilla LLMs under automatic evaluation methods\n(CharEval \\& GPT-4) and human expert evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have achieved breakthrough\nprogress in many dialogue generation tasks. However, their lack of emotion and\nfine-grained role awareness limits the model's ability to provide personalized\nand diverse interactions further. Current methods face high costs in collecting\nhigh-quality annotated data for scenarios such as role-playing, and traditional\nhuman alignment methods are difficult to deploy due to the inherent diversity\nof model behavior in role-playing scenarios. Inspired by the alignment of\nmodels for safety behaviors through RLHF (Reinforcement Learning from Human\nFeedback), in this paper, we revisit model role-playing behavior from the\nperspective of persona alignment and propose a novel annotation-free framework\nnamed \\textbf{\\underline{P}}ersona-Aware \\textbf{\\underline{C}}ontrastive\n\\textbf{\\underline{L}}earning (PCL) to align LLMs' behavior during\nrole-playing, enhancing the model's role consistency. Specifically, we first\ndesign a role chain method to encourage the model to self-question based on the\nrole characteristics and dialogue context to adjust personality consistency.\nThen, we further enhance the model's role-playing strategy through iterative\ncontrastive learning between the use of role characteristics and not.\nExperiments on both black-box and white-box LLMs show that LLMs equipped with\nPCL significantly outperform vanilla LLMs under automatic evaluation methods\n(CharEval \\& GPT-4) and human expert evaluation."
                },
                "authors": [
                    {
                        "name": "Ke Ji"
                    },
                    {
                        "name": "Yixin Lian"
                    },
                    {
                        "name": "Linxu Li"
                    },
                    {
                        "name": "Jingsheng Gao"
                    },
                    {
                        "name": "Weiyuan Li"
                    },
                    {
                        "name": "Bin Dai"
                    }
                ],
                "author_detail": {
                    "name": "Bin Dai"
                },
                "author": "Bin Dai",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17662v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17662v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19711v1",
                "updated": "2025-03-25T14:38:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    14,
                    38,
                    36,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T14:38:36Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    14,
                    38,
                    36,
                    1,
                    84,
                    0
                ],
                "title": "Writing as a testbed for open ended agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing as a testbed for open ended agents"
                },
                "summary": "Open-ended tasks are particularly challenging for LLMs due to the vast\nsolution space, demanding both expansive exploration and adaptable strategies,\nespecially when success lacks a clear, objective definition. Writing, with its\nvast solution space and subjective evaluation criteria, provides a compelling\ntestbed for studying such problems. In this paper, we investigate the potential\nof LLMs to act as collaborative co-writers, capable of suggesting and\nimplementing text improvements autonomously. We analyse three prominent LLMs -\nGemini 1.5 Pro, Claude 3.5 Sonnet, and GPT-4o - focusing on how their action\ndiversity, human alignment, and iterative improvement capabilities impact\noverall performance. This work establishes a framework for benchmarking\nautonomous writing agents and, more broadly, highlights fundamental challenges\nand potential solutions for building systems capable of excelling in diverse\nopen-ended domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-ended tasks are particularly challenging for LLMs due to the vast\nsolution space, demanding both expansive exploration and adaptable strategies,\nespecially when success lacks a clear, objective definition. Writing, with its\nvast solution space and subjective evaluation criteria, provides a compelling\ntestbed for studying such problems. In this paper, we investigate the potential\nof LLMs to act as collaborative co-writers, capable of suggesting and\nimplementing text improvements autonomously. We analyse three prominent LLMs -\nGemini 1.5 Pro, Claude 3.5 Sonnet, and GPT-4o - focusing on how their action\ndiversity, human alignment, and iterative improvement capabilities impact\noverall performance. This work establishes a framework for benchmarking\nautonomous writing agents and, more broadly, highlights fundamental challenges\nand potential solutions for building systems capable of excelling in diverse\nopen-ended domains."
                },
                "authors": [
                    {
                        "name": "Sian Gooding"
                    },
                    {
                        "name": "Lucia Lopez-Rivilla"
                    },
                    {
                        "name": "Edward Grefenstette"
                    }
                ],
                "author_detail": {
                    "name": "Edward Grefenstette"
                },
                "author": "Edward Grefenstette",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18865v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18865v2",
                "updated": "2025-03-25T14:21:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    14,
                    21,
                    15,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-24T16:41:17Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    41,
                    17,
                    0,
                    83,
                    0
                ],
                "title": "Structuring Scientific Innovation: A Framework for Modeling and\n  Discovering Impactful Knowledge Combinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structuring Scientific Innovation: A Framework for Modeling and\n  Discovering Impactful Knowledge Combinations"
                },
                "summary": "The emergence of large language models offers new possibilities for\nstructured exploration of scientific knowledge. Rather than viewing scientific\ndiscovery as isolated ideas or content, we propose a structured approach that\nemphasizes the role of method combinations in shaping disruptive insights.\nSpecifically, we investigate how knowledge unit--especially those tied to\nmethodological design--can be modeled and recombined to yield research\nbreakthroughs. Our proposed framework addresses two key challenges. First, we\nintroduce a contrastive learning-based mechanism to identify distinguishing\nfeatures of historically disruptive method combinations within problem-driven\ncontexts. Second, we propose a reasoning-guided Monte Carlo search algorithm\nthat leverages the chain-of-thought capability of LLMs to identify promising\nknowledge recombinations for new problem statements.Empirical studies across\nmultiple domains show that the framework is capable of modeling the structural\ndynamics of innovation and successfully highlights combinations with high\ndisruptive potential. This research provides a new path for computationally\nguided scientific ideation grounded in structured reasoning and historical data\nmodeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models offers new possibilities for\nstructured exploration of scientific knowledge. Rather than viewing scientific\ndiscovery as isolated ideas or content, we propose a structured approach that\nemphasizes the role of method combinations in shaping disruptive insights.\nSpecifically, we investigate how knowledge unit--especially those tied to\nmethodological design--can be modeled and recombined to yield research\nbreakthroughs. Our proposed framework addresses two key challenges. First, we\nintroduce a contrastive learning-based mechanism to identify distinguishing\nfeatures of historically disruptive method combinations within problem-driven\ncontexts. Second, we propose a reasoning-guided Monte Carlo search algorithm\nthat leverages the chain-of-thought capability of LLMs to identify promising\nknowledge recombinations for new problem statements.Empirical studies across\nmultiple domains show that the framework is capable of modeling the structural\ndynamics of innovation and successfully highlights combinations with high\ndisruptive potential. This research provides a new path for computationally\nguided scientific ideation grounded in structured reasoning and historical data\nmodeling."
                },
                "authors": [
                    {
                        "name": "Junlan Chen"
                    },
                    {
                        "name": "Kexin Zhang"
                    },
                    {
                        "name": "Daifeng Li"
                    },
                    {
                        "name": "Yangyang Feng"
                    },
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Bowen Deng"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Deng"
                },
                "author": "Bowen Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18865v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18865v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19693v1",
                "updated": "2025-03-25T14:18:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    14,
                    18,
                    21,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T14:18:21Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    14,
                    18,
                    21,
                    1,
                    84,
                    0
                ],
                "title": "AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through\n  Lightweight Vocabulary Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through\n  Lightweight Vocabulary Adaptation"
                },
                "summary": "Large Language Models (LLMs) have shown impressive versatility as general\npurpose models. However, their broad applicability comes at a high-cost\ncomputational overhead, particularly in auto-regressive decoding where each\nstep requires a forward pass. In domain-specific settings, general-purpose\ncapabilities are unnecessary and can be exchanged for efficiency. In this work,\nwe take a novel perspective on domain adaptation, reducing latency and\ncomputational costs by adapting the vocabulary to focused domains of interest.\nWe introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation,\ndesigned to enhance LLM efficiency in low-resource domains. AdaptiVocab can be\napplied to any tokenizer and architecture, modifying the vocabulary by\nreplacing tokens with domain-specific n-gram-based tokens, thereby reducing the\nnumber of tokens required for both input processing and output generation.\nAdaptiVocab initializes new n-token embeddings using an exponentially weighted\ncombination of existing embeddings and employs a lightweight fine-tuning phase\nthat can be efficiently performed on a single GPU. We evaluate two 7B LLMs\nacross three niche domains, assessing efficiency, generation quality, and\nend-task performance. Our results show that AdaptiVocab reduces token usage by\nover 25% without compromising performance",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive versatility as general\npurpose models. However, their broad applicability comes at a high-cost\ncomputational overhead, particularly in auto-regressive decoding where each\nstep requires a forward pass. In domain-specific settings, general-purpose\ncapabilities are unnecessary and can be exchanged for efficiency. In this work,\nwe take a novel perspective on domain adaptation, reducing latency and\ncomputational costs by adapting the vocabulary to focused domains of interest.\nWe introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation,\ndesigned to enhance LLM efficiency in low-resource domains. AdaptiVocab can be\napplied to any tokenizer and architecture, modifying the vocabulary by\nreplacing tokens with domain-specific n-gram-based tokens, thereby reducing the\nnumber of tokens required for both input processing and output generation.\nAdaptiVocab initializes new n-token embeddings using an exponentially weighted\ncombination of existing embeddings and employs a lightweight fine-tuning phase\nthat can be efficiently performed on a single GPU. We evaluate two 7B LLMs\nacross three niche domains, assessing efficiency, generation quality, and\nend-task performance. Our results show that AdaptiVocab reduces token usage by\nover 25% without compromising performance"
                },
                "authors": [
                    {
                        "name": "Itay Nakash"
                    },
                    {
                        "name": "Nitay Calderon"
                    },
                    {
                        "name": "Eyal Ben David"
                    },
                    {
                        "name": "Elad Hoffer"
                    },
                    {
                        "name": "Roi Reichart"
                    }
                ],
                "author_detail": {
                    "name": "Roi Reichart"
                },
                "author": "Roi Reichart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19677v1",
                "updated": "2025-03-25T14:02:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    14,
                    2,
                    10,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T14:02:10Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    14,
                    2,
                    10,
                    1,
                    84,
                    0
                ],
                "title": "Deep Learning for Speech Emotion Recognition: A CNN Approach Utilizing\n  Mel Spectrograms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning for Speech Emotion Recognition: A CNN Approach Utilizing\n  Mel Spectrograms"
                },
                "summary": "This paper explores the application of Convolutional Neural Networks CNNs for\nclassifying emotions in speech through Mel Spectrogram representations of audio\nfiles. Traditional methods such as Gaussian Mixture Models and Hidden Markov\nModels have proven insufficient for practical deployment, prompting a shift\ntowards deep learning techniques. By transforming audio data into a visual\nformat, the CNN model autonomously learns to identify intricate patterns,\nenhancing classification accuracy. The developed model is integrated into a\nuser-friendly graphical interface, facilitating realtime predictions and\npotential applications in educational environments. The study aims to advance\nthe understanding of deep learning in speech emotion recognition, assess the\nmodels feasibility, and contribute to the integration of technology in learning\ncontexts",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the application of Convolutional Neural Networks CNNs for\nclassifying emotions in speech through Mel Spectrogram representations of audio\nfiles. Traditional methods such as Gaussian Mixture Models and Hidden Markov\nModels have proven insufficient for practical deployment, prompting a shift\ntowards deep learning techniques. By transforming audio data into a visual\nformat, the CNN model autonomously learns to identify intricate patterns,\nenhancing classification accuracy. The developed model is integrated into a\nuser-friendly graphical interface, facilitating realtime predictions and\npotential applications in educational environments. The study aims to advance\nthe understanding of deep learning in speech emotion recognition, assess the\nmodels feasibility, and contribute to the integration of technology in learning\ncontexts"
                },
                "authors": [
                    {
                        "name": "Niketa Penumajji"
                    }
                ],
                "author_detail": {
                    "name": "Niketa Penumajji"
                },
                "author": "Niketa Penumajji",
                "arxiv_comment": "5 pages 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19650v1",
                "updated": "2025-03-25T13:40:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    40,
                    22,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T13:40:22Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    40,
                    22,
                    1,
                    84,
                    0
                ],
                "title": "HausaNLP at SemEval-2025 Task 3: Towards a Fine-Grained Model-Aware\n  Hallucination Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HausaNLP at SemEval-2025 Task 3: Towards a Fine-Grained Model-Aware\n  Hallucination Detection"
                },
                "summary": "This paper presents our findings of the Multilingual Shared Task on\nHallucinations and Related Observable Overgeneration Mistakes, MU-SHROOM, which\nfocuses on identifying hallucinations and related overgeneration errors in\nlarge language models (LLMs). The shared task involves detecting specific text\nspans that constitute hallucinations in the outputs generated by LLMs in 14\nlanguages. To address this task, we aim to provide a nuanced, model-aware\nunderstanding of hallucination occurrences and severity in English. We used\nnatural language inference and fine-tuned a ModernBERT model using a synthetic\ndataset of 400 samples, achieving an Intersection over Union (IoU) score of\n0.032 and a correlation score of 0.422. These results indicate a moderately\npositive correlation between the model's confidence scores and the actual\npresence of hallucinations. The IoU score indicates that our model has a\nrelatively low overlap between the predicted hallucination span and the truth\nannotation. The performance is unsurprising, given the intricate nature of\nhallucination detection. Hallucinations often manifest subtly, relying on\ncontext, making pinpointing their exact boundaries formidable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents our findings of the Multilingual Shared Task on\nHallucinations and Related Observable Overgeneration Mistakes, MU-SHROOM, which\nfocuses on identifying hallucinations and related overgeneration errors in\nlarge language models (LLMs). The shared task involves detecting specific text\nspans that constitute hallucinations in the outputs generated by LLMs in 14\nlanguages. To address this task, we aim to provide a nuanced, model-aware\nunderstanding of hallucination occurrences and severity in English. We used\nnatural language inference and fine-tuned a ModernBERT model using a synthetic\ndataset of 400 samples, achieving an Intersection over Union (IoU) score of\n0.032 and a correlation score of 0.422. These results indicate a moderately\npositive correlation between the model's confidence scores and the actual\npresence of hallucinations. The IoU score indicates that our model has a\nrelatively low overlap between the predicted hallucination span and the truth\nannotation. The performance is unsurprising, given the intricate nature of\nhallucination detection. Hallucinations often manifest subtly, relying on\ncontext, making pinpointing their exact boundaries formidable."
                },
                "authors": [
                    {
                        "name": "Maryam Bala"
                    },
                    {
                        "name": "Amina Imam Abubakar"
                    },
                    {
                        "name": "Abdulhamid Abubakar"
                    },
                    {
                        "name": "Abdulkadir Shehu Bichi"
                    },
                    {
                        "name": "Hafsa Kabir Ahmad"
                    },
                    {
                        "name": "Sani Abdullahi Sani"
                    },
                    {
                        "name": "Idris Abdulmumin"
                    },
                    {
                        "name": "Shamsuddeen Hassan Muhamad"
                    },
                    {
                        "name": "Ibrahim Said Ahmad"
                    }
                ],
                "author_detail": {
                    "name": "Ibrahim Said Ahmad"
                },
                "author": "Ibrahim Said Ahmad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18227v2",
                "updated": "2025-03-25T13:25:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    25,
                    6,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-23T22:06:07Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    22,
                    6,
                    7,
                    6,
                    82,
                    0
                ],
                "title": "PG-SAM: Prior-Guided SAM with Medical for Multi-organ Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PG-SAM: Prior-Guided SAM with Medical for Multi-organ Segmentation"
                },
                "summary": "Segment Anything Model (SAM) demonstrates powerful zero-shot capabilities;\nhowever, its accuracy and robustness significantly decrease when applied to\nmedical image segmentation. Existing methods address this issue through\nmodality fusion, integrating textual and image information to provide more\ndetailed priors. In this study, we argue that the granularity of text and the\ndomain gap affect the accuracy of the priors. Furthermore, the discrepancy\nbetween high-level abstract semantics and pixel-level boundary details in\nimages can introduce noise into the fusion process. To address this, we propose\nPrior-Guided SAM (PG-SAM), which employs a fine-grained modality prior aligner\nto leverage specialized medical knowledge for better modality alignment. The\ncore of our method lies in efficiently addressing the domain gap with\nfine-grained text from a medical LLM. Meanwhile, it also enhances the priors'\nquality after modality alignment, ensuring more accurate segmentation. In\naddition, our decoder enhances the model's expressive capabilities through\nmulti-level feature fusion and iterative mask optimizer operations, supporting\nunprompted learning. We also propose a unified pipeline that effectively\nsupplies high-quality semantic information to SAM. Extensive experiments on the\nSynapse dataset demonstrate that the proposed PG-SAM achieves state-of-the-art\nperformance. Our anonymous code is released at\nhttps://github.com/logan-0623/PG-SAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Segment Anything Model (SAM) demonstrates powerful zero-shot capabilities;\nhowever, its accuracy and robustness significantly decrease when applied to\nmedical image segmentation. Existing methods address this issue through\nmodality fusion, integrating textual and image information to provide more\ndetailed priors. In this study, we argue that the granularity of text and the\ndomain gap affect the accuracy of the priors. Furthermore, the discrepancy\nbetween high-level abstract semantics and pixel-level boundary details in\nimages can introduce noise into the fusion process. To address this, we propose\nPrior-Guided SAM (PG-SAM), which employs a fine-grained modality prior aligner\nto leverage specialized medical knowledge for better modality alignment. The\ncore of our method lies in efficiently addressing the domain gap with\nfine-grained text from a medical LLM. Meanwhile, it also enhances the priors'\nquality after modality alignment, ensuring more accurate segmentation. In\naddition, our decoder enhances the model's expressive capabilities through\nmulti-level feature fusion and iterative mask optimizer operations, supporting\nunprompted learning. We also propose a unified pipeline that effectively\nsupplies high-quality semantic information to SAM. Extensive experiments on the\nSynapse dataset demonstrate that the proposed PG-SAM achieves state-of-the-art\nperformance. Our anonymous code is released at\nhttps://github.com/logan-0623/PG-SAM."
                },
                "authors": [
                    {
                        "name": "Yiheng Zhong"
                    },
                    {
                        "name": "Zihong Luo"
                    },
                    {
                        "name": "Chengzhi Liu"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Zelin Peng"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Yingzhen Hu"
                    },
                    {
                        "name": "Jionglong Su"
                    },
                    {
                        "name": "Zongyuan Geand"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19633v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19633v1",
                "updated": "2025-03-25T13:19:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    19,
                    46,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T13:19:46Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    19,
                    46,
                    1,
                    84,
                    0
                ],
                "title": "1.4 Million Open-Source Distilled Reasoning Dataset to Empower Large\n  Language Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "1.4 Million Open-Source Distilled Reasoning Dataset to Empower Large\n  Language Model Training"
                },
                "summary": "The AM-DeepSeek-R1-Distilled is a large-scale dataset with thinking traces\nfor general reasoning tasks, composed of high-quality and challenging reasoning\nproblems. These problems are collected from a multitude of open-source\ndatasets, subjected to semantic deduplication and meticulous cleaning to\neliminate test set contamination. All responses within the dataset are\ndistilled from reasoning models (predominantly DeepSeek-R1) and have undergone\nrigorous verification procedures. Mathematical problems are validated by\nchecking against reference answers, code problems are verified using test\ncases, and other tasks are evaluated with the aid of a reward model. The\nAM-Distill-Qwen-32B model, which was trained through only simple Supervised\nFine-Tuning (SFT) using this batch of data, outperformed the\nDeepSeek-R1-Distill-Qwen-32B model on four benchmarks: AIME2024, MATH-500,\nGPQA-Diamond, and LiveCodeBench. Additionally, the AM-Distill-Qwen-72B model\nsurpassed the DeepSeek-R1-Distill-Llama-70B model on all benchmarks as well. We\nare releasing these 1.4 million problems and their corresponding responses to\nthe research community with the objective of fostering the development of\npowerful reasoning-oriented Large Language Models (LLMs). The dataset was\npublished in\n\\href{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M}{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The AM-DeepSeek-R1-Distilled is a large-scale dataset with thinking traces\nfor general reasoning tasks, composed of high-quality and challenging reasoning\nproblems. These problems are collected from a multitude of open-source\ndatasets, subjected to semantic deduplication and meticulous cleaning to\neliminate test set contamination. All responses within the dataset are\ndistilled from reasoning models (predominantly DeepSeek-R1) and have undergone\nrigorous verification procedures. Mathematical problems are validated by\nchecking against reference answers, code problems are verified using test\ncases, and other tasks are evaluated with the aid of a reward model. The\nAM-Distill-Qwen-32B model, which was trained through only simple Supervised\nFine-Tuning (SFT) using this batch of data, outperformed the\nDeepSeek-R1-Distill-Qwen-32B model on four benchmarks: AIME2024, MATH-500,\nGPQA-Diamond, and LiveCodeBench. Additionally, the AM-Distill-Qwen-72B model\nsurpassed the DeepSeek-R1-Distill-Llama-70B model on all benchmarks as well. We\nare releasing these 1.4 million problems and their corresponding responses to\nthe research community with the objective of fostering the development of\npowerful reasoning-oriented Large Language Models (LLMs). The dataset was\npublished in\n\\href{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M}{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M}."
                },
                "authors": [
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yiping Peng"
                    },
                    {
                        "name": "Sitong Zhao"
                    },
                    {
                        "name": "Xiaoyu Tian"
                    },
                    {
                        "name": "Shuaiting Chen"
                    },
                    {
                        "name": "Yunjie Ji"
                    },
                    {
                        "name": "Xiangang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangang Li"
                },
                "author": "Xiangang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19633v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19633v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03882v2",
                "updated": "2025-03-25T13:14:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    14,
                    37,
                    1,
                    84,
                    0
                ],
                "published": "2025-02-06T08:55:11Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    8,
                    55,
                    11,
                    3,
                    37,
                    0
                ],
                "title": "Hierarchical Entropic Diffusion for Ransomware Detection: A\n  Probabilistic Approach to Behavioral Anomaly Isolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Entropic Diffusion for Ransomware Detection: A\n  Probabilistic Approach to Behavioral Anomaly Isolation"
                },
                "summary": "The increasing complexity of cryptographic extortion techniques has\nnecessitated the development of adaptive detection frameworks capable of\nidentifying adversarial encryption behaviors without reliance on predefined\nsignatures. Hierarchical Entropic Diffusion (HED) introduces a structured\nentropy-based anomaly classification mechanism that systematically tracks\nfluctuations in entropy evolution to differentiate between benign cryptographic\nprocesses and unauthorized encryption attempts. The integration of hierarchical\nclustering, entropy profiling, and probabilistic diffusion modeling refines\ndetection granularity, ensuring that encryption anomalies are identified\ndespite obfuscation strategies or incremental execution methodologies.\nExperimental evaluations demonstrated that HED maintained high classification\naccuracy across diverse ransomware families, outperforming traditional\nheuristic-based and signature-driven approaches while reducing false positive\noccurrences. Comparative analysis highlighted that entropy-driven anomaly\nsegmentation improved detection efficiency under variable system workload\nconditions, ensuring real-time classification feasibility. The computational\noverhead associated with entropy anomaly detection remained within operational\nconstraints, reinforcing the suitability of entropy-driven classification for\nlarge-scale deployment. The ability to identify adversarial entropy\nmanipulations before encryption completion contributes to broader cybersecurity\ndefenses, offering a structured methodology for isolating unauthorized\ncryptographic activities within heterogeneous computing environments. The\nresults further emphasized that entropy evolution modeling facilitates\npredictive anomaly detection, enhancing resilience against encryption evasion\ntechniques designed to circumvent traditional detection mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of cryptographic extortion techniques has\nnecessitated the development of adaptive detection frameworks capable of\nidentifying adversarial encryption behaviors without reliance on predefined\nsignatures. Hierarchical Entropic Diffusion (HED) introduces a structured\nentropy-based anomaly classification mechanism that systematically tracks\nfluctuations in entropy evolution to differentiate between benign cryptographic\nprocesses and unauthorized encryption attempts. The integration of hierarchical\nclustering, entropy profiling, and probabilistic diffusion modeling refines\ndetection granularity, ensuring that encryption anomalies are identified\ndespite obfuscation strategies or incremental execution methodologies.\nExperimental evaluations demonstrated that HED maintained high classification\naccuracy across diverse ransomware families, outperforming traditional\nheuristic-based and signature-driven approaches while reducing false positive\noccurrences. Comparative analysis highlighted that entropy-driven anomaly\nsegmentation improved detection efficiency under variable system workload\nconditions, ensuring real-time classification feasibility. The computational\noverhead associated with entropy anomaly detection remained within operational\nconstraints, reinforcing the suitability of entropy-driven classification for\nlarge-scale deployment. The ability to identify adversarial entropy\nmanipulations before encryption completion contributes to broader cybersecurity\ndefenses, offering a structured methodology for isolating unauthorized\ncryptographic activities within heterogeneous computing environments. The\nresults further emphasized that entropy evolution modeling facilitates\npredictive anomaly detection, enhancing resilience against encryption evasion\ntechniques designed to circumvent traditional detection mechanisms."
                },
                "authors": [
                    {
                        "name": "Vasili Iskorohodov"
                    },
                    {
                        "name": "Maximilian Ravensdale"
                    },
                    {
                        "name": "Matthias von Holstein"
                    },
                    {
                        "name": "Hugo Petrovic"
                    },
                    {
                        "name": "Adrian Yardley"
                    }
                ],
                "author_detail": {
                    "name": "Adrian Yardley"
                },
                "author": "Adrian Yardley",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03480v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03480v3",
                "updated": "2025-03-25T13:10:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    10,
                    28,
                    1,
                    84,
                    0
                ],
                "published": "2024-10-04T14:52:18Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    14,
                    52,
                    18,
                    4,
                    278,
                    0
                ],
                "title": "SeBS-Flow: Benchmarking Serverless Cloud Function Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeBS-Flow: Benchmarking Serverless Cloud Function Workflows"
                },
                "summary": "Serverless computing has emerged as a prominent paradigm, with a significant\nadoption rate among cloud customers. While this model offers advantages such as\nabstraction from the deployment and resource scheduling, it also poses\nlimitations in handling complex use cases due to the restricted nature of\nindividual functions. Serverless workflows address this limitation by\norchestrating multiple functions into a cohesive application. However, existing\nserverless workflow platforms exhibit significant differences in their\nprogramming models and infrastructure, making fair and consistent performance\nevaluations difficult in practice. To address this gap, we propose the first\nserverless workflow benchmarking suite SeBS-Flow, providing a platform-agnostic\nworkflow model that enables consistent benchmarking across various platforms.\nSeBS-Flow includes six real-world application benchmarks and four\nmicrobenchmarks representing different computational patterns. We conduct\ncomprehensive evaluations on three major cloud platforms, assessing\nperformance, cost, scalability, and runtime deviations. We make our benchmark\nsuite open-source, enabling rigorous and comparable evaluations of serverless\nworkflows over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing has emerged as a prominent paradigm, with a significant\nadoption rate among cloud customers. While this model offers advantages such as\nabstraction from the deployment and resource scheduling, it also poses\nlimitations in handling complex use cases due to the restricted nature of\nindividual functions. Serverless workflows address this limitation by\norchestrating multiple functions into a cohesive application. However, existing\nserverless workflow platforms exhibit significant differences in their\nprogramming models and infrastructure, making fair and consistent performance\nevaluations difficult in practice. To address this gap, we propose the first\nserverless workflow benchmarking suite SeBS-Flow, providing a platform-agnostic\nworkflow model that enables consistent benchmarking across various platforms.\nSeBS-Flow includes six real-world application benchmarks and four\nmicrobenchmarks representing different computational patterns. We conduct\ncomprehensive evaluations on three major cloud platforms, assessing\nperformance, cost, scalability, and runtime deviations. We make our benchmark\nsuite open-source, enabling rigorous and comparable evaluations of serverless\nworkflows over time."
                },
                "authors": [
                    {
                        "name": "Larissa Schmid"
                    },
                    {
                        "name": "Marcin Copik"
                    },
                    {
                        "name": "Alexandru Calotoiu"
                    },
                    {
                        "name": "Laurin Brandner"
                    },
                    {
                        "name": "Anne Koziolek"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "arxiv_doi": "10.1145/3689031.3717465",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689031.3717465",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.03480v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03480v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19620v1",
                "updated": "2025-03-25T13:08:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    8,
                    46,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T13:08:46Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    8,
                    46,
                    1,
                    84,
                    0
                ],
                "title": "Optimization through In-Context Learning and Iterative LLM Prompting for\n  Nuclear Engineering Design Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization through In-Context Learning and Iterative LLM Prompting for\n  Nuclear Engineering Design Problems"
                },
                "summary": "The optimization of nuclear engineering designs, such as nuclear fuel\nassembly configurations, involves managing competing objectives like reactivity\ncontrol and power distribution. This study explores the use of Optimization by\nPrompting, an iterative approach utilizing large language models (LLMs), to\naddress these challenges. The method is straightforward to implement, requiring\nno hyperparameter tuning or complex mathematical formulations. Optimization\nproblems can be described in plain English, with only an evaluator and a\nparsing script needed for execution. The in-context learning capabilities of\nLLMs enable them to understand problem nuances, therefore, they have the\npotential to surpass traditional metaheuristic optimization methods. This study\ndemonstrates the application of LLMs as optimizers to Boiling Water Reactor\n(BWR) fuel lattice design, showing the capability of commercial LLMs to achieve\nsuperior optimization results compared to traditional methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The optimization of nuclear engineering designs, such as nuclear fuel\nassembly configurations, involves managing competing objectives like reactivity\ncontrol and power distribution. This study explores the use of Optimization by\nPrompting, an iterative approach utilizing large language models (LLMs), to\naddress these challenges. The method is straightforward to implement, requiring\nno hyperparameter tuning or complex mathematical formulations. Optimization\nproblems can be described in plain English, with only an evaluator and a\nparsing script needed for execution. The in-context learning capabilities of\nLLMs enable them to understand problem nuances, therefore, they have the\npotential to surpass traditional metaheuristic optimization methods. This study\ndemonstrates the application of LLMs as optimizers to Boiling Water Reactor\n(BWR) fuel lattice design, showing the capability of commercial LLMs to achieve\nsuperior optimization results compared to traditional methods."
                },
                "authors": [
                    {
                        "name": "M. Rizki Oktavian"
                    },
                    {
                        "name": "Anirudh Tunga"
                    },
                    {
                        "name": "Amandeep Bakshi"
                    },
                    {
                        "name": "Michael J. Mueterthies"
                    },
                    {
                        "name": "J. Thomas Gruenwald"
                    },
                    {
                        "name": "Jonathan Nistor"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Nistor"
                },
                "author": "Jonathan Nistor",
                "arxiv_comment": "Codes and data are available upon request",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19619v1",
                "updated": "2025-03-25T13:08:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    8,
                    26,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T13:08:26Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    8,
                    26,
                    1,
                    84,
                    0
                ],
                "title": "Exploring Next Token Prediction For Optimizing Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Next Token Prediction For Optimizing Databases"
                },
                "summary": "The Next Token Prediction paradigm (NTP, for short) lies at the forefront of\nmodern large foundational models that are pre-trained on diverse and large\ndatasets. These models generalize effectively and have proven to be very\nsuccessful in Natural Language Processing (NLP). Inspired by the generalization\ncapabilities of Large Language Models (LLMs), we investigate whether the same\nNTP paradigm can also be applied to DBMS design and optimization tasks.\nAdopting NTP directly for database optimization is non-trivial due to the\nfundamental differences between the domains. In this paper, we present a\nframework termed Probe and Learn (PoLe) for applying NTP to optimize database\nsystems. PoLe leverages Decision Transformers and hardware-generated tokens to\neffectively incorporate NTP into database systems. Preliminary results from the\nmain-memory index scheduling task demonstrate that adopting NTP can improve\nboth performance and generalizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Next Token Prediction paradigm (NTP, for short) lies at the forefront of\nmodern large foundational models that are pre-trained on diverse and large\ndatasets. These models generalize effectively and have proven to be very\nsuccessful in Natural Language Processing (NLP). Inspired by the generalization\ncapabilities of Large Language Models (LLMs), we investigate whether the same\nNTP paradigm can also be applied to DBMS design and optimization tasks.\nAdopting NTP directly for database optimization is non-trivial due to the\nfundamental differences between the domains. In this paper, we present a\nframework termed Probe and Learn (PoLe) for applying NTP to optimize database\nsystems. PoLe leverages Decision Transformers and hardware-generated tokens to\neffectively incorporate NTP into database systems. Preliminary results from the\nmain-memory index scheduling task demonstrate that adopting NTP can improve\nboth performance and generalizability."
                },
                "authors": [
                    {
                        "name": "Yeasir Rayhan"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17486v2",
                "updated": "2025-03-25T13:03:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    3,
                    48,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-21T18:55:14Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    18,
                    55,
                    14,
                    4,
                    80,
                    0
                ],
                "title": "ProtoGS: Efficient and High-Quality Rendering with 3D Gaussian\n  Prototypes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProtoGS: Efficient and High-Quality Rendering with 3D Gaussian\n  Prototypes"
                },
                "summary": "3D Gaussian Splatting (3DGS) has made significant strides in novel view\nsynthesis but is limited by the substantial number of Gaussian primitives\nrequired, posing challenges for deployment on lightweight devices. Recent\nmethods address this issue by compressing the storage size of densified\nGaussians, yet fail to preserve rendering quality and efficiency. To overcome\nthese limitations, we propose ProtoGS to learn Gaussian prototypes to represent\nGaussian primitives, significantly reducing the total Gaussian amount without\nsacrificing visual quality. Our method directly uses Gaussian prototypes to\nenable efficient rendering and leverage the resulting reconstruction loss to\nguide prototype learning. To further optimize memory efficiency during\ntraining, we incorporate structure-from-motion (SfM) points as anchor points to\ngroup Gaussian primitives. Gaussian prototypes are derived within each group by\nclustering of K-means, and both the anchor points and the prototypes are\noptimized jointly. Our experiments on real-world and synthetic datasets prove\nthat we outperform existing methods, achieving a substantial reduction in the\nnumber of Gaussians, and enabling high rendering speed while maintaining or\neven enhancing rendering fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting (3DGS) has made significant strides in novel view\nsynthesis but is limited by the substantial number of Gaussian primitives\nrequired, posing challenges for deployment on lightweight devices. Recent\nmethods address this issue by compressing the storage size of densified\nGaussians, yet fail to preserve rendering quality and efficiency. To overcome\nthese limitations, we propose ProtoGS to learn Gaussian prototypes to represent\nGaussian primitives, significantly reducing the total Gaussian amount without\nsacrificing visual quality. Our method directly uses Gaussian prototypes to\nenable efficient rendering and leverage the resulting reconstruction loss to\nguide prototype learning. To further optimize memory efficiency during\ntraining, we incorporate structure-from-motion (SfM) points as anchor points to\ngroup Gaussian primitives. Gaussian prototypes are derived within each group by\nclustering of K-means, and both the anchor points and the prototypes are\noptimized jointly. Our experiments on real-world and synthetic datasets prove\nthat we outperform existing methods, achieving a substantial reduction in the\nnumber of Gaussians, and enabling high rendering speed while maintaining or\neven enhancing rendering fidelity."
                },
                "authors": [
                    {
                        "name": "Zhengqing Gao"
                    },
                    {
                        "name": "Dongting Hu"
                    },
                    {
                        "name": "Jia-Wang Bian"
                    },
                    {
                        "name": "Huan Fu"
                    },
                    {
                        "name": "Yan Li"
                    },
                    {
                        "name": "Tongliang Liu"
                    },
                    {
                        "name": "Mingming Gong"
                    },
                    {
                        "name": "Kun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Zhang"
                },
                "author": "Kun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13999v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13999v2",
                "updated": "2025-03-25T12:59:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    59,
                    14,
                    1,
                    84,
                    0
                ],
                "published": "2025-01-23T11:34:04Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    34,
                    4,
                    3,
                    23,
                    0
                ],
                "title": "Framework for Progressive Knowledge Fusion in Large Language Models\n  Through Structured Conceptual Redundancy Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Framework for Progressive Knowledge Fusion in Large Language Models\n  Through Structured Conceptual Redundancy Analysis"
                },
                "summary": "The organization of latent knowledge within large-scale models poses unique\nchallenges when addressing overlapping representations and optimizing\ncontextual accuracy. Conceptual redundancies embedded across layers often\nresult in inefficiencies that affect both computational demands and\ntask-specific outcomes. A framework was proposed to restructure these\nredundancies through advanced clustering techniques and dynamic thresholding,\nensuring that critical semantic relationships are preserved while removing\nunnecessary overlaps. Evaluations revealed improved memory efficiency and\nfaster inference times, alongside better alignment in latent knowledge clusters\nthat enhanced interpretability. Improvements in error rates and adversarial\nrobustness suggest that restructuring redundancies has broader implications for\nincreasing model reliability across diverse applications. Comparative analyses\nhighlighted reductions in resource consumption and notable gains in\nperformance, particularly in translation and summarization tasks. Energy\nmetrics demonstrated significant savings during training phases, further\nvalidating the practicality of the approach for real-world deployments.\nRepresentational fidelity was also enhanced, with latent space evaluations\nindicating better cluster alignment and higher semantic consistency. The\nmethodology bridges a key gap in model optimization through directly addressing\nredundancies at the structural level. Its application opens avenues for\nscalable, efficient, and contextually aware systems that can adapt to complex,\ndomain-specific tasks without compromising on performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The organization of latent knowledge within large-scale models poses unique\nchallenges when addressing overlapping representations and optimizing\ncontextual accuracy. Conceptual redundancies embedded across layers often\nresult in inefficiencies that affect both computational demands and\ntask-specific outcomes. A framework was proposed to restructure these\nredundancies through advanced clustering techniques and dynamic thresholding,\nensuring that critical semantic relationships are preserved while removing\nunnecessary overlaps. Evaluations revealed improved memory efficiency and\nfaster inference times, alongside better alignment in latent knowledge clusters\nthat enhanced interpretability. Improvements in error rates and adversarial\nrobustness suggest that restructuring redundancies has broader implications for\nincreasing model reliability across diverse applications. Comparative analyses\nhighlighted reductions in resource consumption and notable gains in\nperformance, particularly in translation and summarization tasks. Energy\nmetrics demonstrated significant savings during training phases, further\nvalidating the practicality of the approach for real-world deployments.\nRepresentational fidelity was also enhanced, with latent space evaluations\nindicating better cluster alignment and higher semantic consistency. The\nmethodology bridges a key gap in model optimization through directly addressing\nredundancies at the structural level. Its application opens avenues for\nscalable, efficient, and contextually aware systems that can adapt to complex,\ndomain-specific tasks without compromising on performance."
                },
                "authors": [
                    {
                        "name": "Joseph Sakau"
                    },
                    {
                        "name": "Evander Kozlowski"
                    },
                    {
                        "name": "Roderick Thistledown"
                    },
                    {
                        "name": "Basil Steinberger"
                    }
                ],
                "author_detail": {
                    "name": "Basil Steinberger"
                },
                "author": "Basil Steinberger",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13999v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13999v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19612v1",
                "updated": "2025-03-25T12:52:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    52,
                    38,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T12:52:38Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    52,
                    38,
                    1,
                    84,
                    0
                ],
                "title": "RL-finetuning LLMs from on- and off-policy data with a single algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RL-finetuning LLMs from on- and off-policy data with a single algorithm"
                },
                "summary": "We introduce a novel reinforcement learning algorithm (AGRO, for\nAny-Generation Reward Optimization) for fine-tuning large-language models. AGRO\nleverages the concept of generation consistency, which states that the optimal\npolicy satisfies the notion of consistency across any possible generation of\nthe model. We derive algorithms that find optimal solutions via the\nsample-based policy gradient and provide theoretical guarantees on their\nconvergence. Our experiments demonstrate the effectiveness of AGRO in both\non-policy and off-policy settings, showing improved performance on the\nmathematical reasoning dataset over baseline algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel reinforcement learning algorithm (AGRO, for\nAny-Generation Reward Optimization) for fine-tuning large-language models. AGRO\nleverages the concept of generation consistency, which states that the optimal\npolicy satisfies the notion of consistency across any possible generation of\nthe model. We derive algorithms that find optimal solutions via the\nsample-based policy gradient and provide theoretical guarantees on their\nconvergence. Our experiments demonstrate the effectiveness of AGRO in both\non-policy and off-policy settings, showing improved performance on the\nmathematical reasoning dataset over baseline algorithms."
                },
                "authors": [
                    {
                        "name": "Yunhao Tang"
                    },
                    {
                        "name": "Taco Cohen"
                    },
                    {
                        "name": "David W. Zhang"
                    },
                    {
                        "name": "Michal Valko"
                    },
                    {
                        "name": "Rémi Munos"
                    }
                ],
                "author_detail": {
                    "name": "Rémi Munos"
                },
                "author": "Rémi Munos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03235v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03235v2",
                "updated": "2025-03-25T12:49:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    49,
                    43,
                    1,
                    84,
                    0
                ],
                "published": "2024-12-04T11:36:37Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    36,
                    37,
                    2,
                    339,
                    0
                ],
                "title": "Does Safety Training of LLMs Generalize to Semantically Related Natural\n  Prompts?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Safety Training of LLMs Generalize to Semantically Related Natural\n  Prompts?"
                },
                "summary": "Large Language Models (LLMs) are known to be susceptible to crafted\nadversarial attacks or jailbreaks that lead to the generation of objectionable\ncontent despite being aligned to human preferences using safety fine-tuning\nmethods. While the large dimensionality of input token space makes it\ninevitable to find adversarial prompts that can jailbreak these models, we aim\nto evaluate whether safety fine-tuned LLMs are safe against natural prompts\nwhich are semantically related to toxic seed prompts that elicit safe responses\nafter alignment. We surprisingly find that popular aligned LLMs such as GPT-4\ncan be compromised using naive prompts that are NOT even crafted with an\nobjective of jailbreaking the model. Furthermore, we empirically show that\ngiven a seed prompt that elicits a toxic response from an unaligned model, one\ncan systematically generate several semantically related natural prompts that\ncan jailbreak aligned LLMs. Towards this, we propose a method of Response\nGuided Question Augmentation (ReG-QA) to evaluate the generalization of safety\naligned LLMs to natural prompts, that first generates several toxic answers\ngiven a seed question using an unaligned LLM (Q to A), and further leverages an\nLLM to generate questions that are likely to produce these answers (A to Q). We\ninterestingly find that safety fine-tuned LLMs such as GPT-4o are vulnerable to\nproducing natural jailbreak questions from unsafe content (without denial) and\ncan thus be used for the latter (A to Q) step. We obtain attack success rates\nthat are comparable to/ better than leading adversarial attack methods on the\nJailbreakBench leaderboard, while being significantly more stable against\ndefenses such as Smooth-LLM and Synonym Substitution, which are effective\nagainst existing all attacks on the leaderboard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are known to be susceptible to crafted\nadversarial attacks or jailbreaks that lead to the generation of objectionable\ncontent despite being aligned to human preferences using safety fine-tuning\nmethods. While the large dimensionality of input token space makes it\ninevitable to find adversarial prompts that can jailbreak these models, we aim\nto evaluate whether safety fine-tuned LLMs are safe against natural prompts\nwhich are semantically related to toxic seed prompts that elicit safe responses\nafter alignment. We surprisingly find that popular aligned LLMs such as GPT-4\ncan be compromised using naive prompts that are NOT even crafted with an\nobjective of jailbreaking the model. Furthermore, we empirically show that\ngiven a seed prompt that elicits a toxic response from an unaligned model, one\ncan systematically generate several semantically related natural prompts that\ncan jailbreak aligned LLMs. Towards this, we propose a method of Response\nGuided Question Augmentation (ReG-QA) to evaluate the generalization of safety\naligned LLMs to natural prompts, that first generates several toxic answers\ngiven a seed question using an unaligned LLM (Q to A), and further leverages an\nLLM to generate questions that are likely to produce these answers (A to Q). We\ninterestingly find that safety fine-tuned LLMs such as GPT-4o are vulnerable to\nproducing natural jailbreak questions from unsafe content (without denial) and\ncan thus be used for the latter (A to Q) step. We obtain attack success rates\nthat are comparable to/ better than leading adversarial attack methods on the\nJailbreakBench leaderboard, while being significantly more stable against\ndefenses such as Smooth-LLM and Synonym Substitution, which are effective\nagainst existing all attacks on the leaderboard."
                },
                "authors": [
                    {
                        "name": "Sravanti Addepalli"
                    },
                    {
                        "name": "Yerram Varun"
                    },
                    {
                        "name": "Arun Suggala"
                    },
                    {
                        "name": "Karthikeyan Shanmugam"
                    },
                    {
                        "name": "Prateek Jain"
                    }
                ],
                "author_detail": {
                    "name": "Prateek Jain"
                },
                "author": "Prateek Jain",
                "arxiv_comment": "Accepted in ICLR 2025",
                "arxiv_journal_ref": "Addepalli, S., Varun, Y., Suggala, A., Shanmugam, K., & Jain, P.\n  (2025). Does safety training of LLMs generalize to semantically related\n  natural prompts? In The Thirteenth International Conference on Learning\n  Representations 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03235v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03235v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19607v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19607v1",
                "updated": "2025-03-25T12:43:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    43,
                    18,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T12:43:18Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    43,
                    18,
                    1,
                    84,
                    0
                ],
                "title": "Enabling Rapid Shared Human-AI Mental Model Alignment via the\n  After-Action Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Rapid Shared Human-AI Mental Model Alignment via the\n  After-Action Review"
                },
                "summary": "In this work, we present two novel contributions toward improving research in\nhuman-machine teaming (HMT): 1) a Minecraft testbed to accelerate testing and\ndeployment of collaborative AI agents and 2) a tool to allow users to revisit\nand analyze behaviors within an HMT episode to facilitate shared mental model\ndevelopment. Our browser-based Minecraft testbed allows for rapid testing of\ncollaborative agents in a continuous-space, real-time, partially-observable\nenvironment with real humans without cumbersome setup typical to human-AI\ninteraction user studies. As Minecraft has an extensive player base and a rich\necosystem of pre-built AI agents, we hope this contribution can help to\nfacilitate research quickly in the design of new collaborative agents and in\nunderstanding different human factors within HMT. Our mental model alignment\ntool facilitates user-led post-mission analysis by including video displays of\nfirst-person perspectives of the team members (i.e., the human and AI) that can\nbe replayed, and a chat interface that leverages GPT-4 to provide answers to\nvarious queries regarding the AI's experiences and model details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present two novel contributions toward improving research in\nhuman-machine teaming (HMT): 1) a Minecraft testbed to accelerate testing and\ndeployment of collaborative AI agents and 2) a tool to allow users to revisit\nand analyze behaviors within an HMT episode to facilitate shared mental model\ndevelopment. Our browser-based Minecraft testbed allows for rapid testing of\ncollaborative agents in a continuous-space, real-time, partially-observable\nenvironment with real humans without cumbersome setup typical to human-AI\ninteraction user studies. As Minecraft has an extensive player base and a rich\necosystem of pre-built AI agents, we hope this contribution can help to\nfacilitate research quickly in the design of new collaborative agents and in\nunderstanding different human factors within HMT. Our mental model alignment\ntool facilitates user-led post-mission analysis by including video displays of\nfirst-person perspectives of the team members (i.e., the human and AI) that can\nbe replayed, and a chat interface that leverages GPT-4 to provide answers to\nvarious queries regarding the AI's experiences and model details."
                },
                "authors": [
                    {
                        "name": "Edward Gu"
                    },
                    {
                        "name": "Ho Chit Siu"
                    },
                    {
                        "name": "Melanie Platt"
                    },
                    {
                        "name": "Isabelle Hurley"
                    },
                    {
                        "name": "Jaime Peña"
                    },
                    {
                        "name": "Rohan Paleja"
                    }
                ],
                "author_detail": {
                    "name": "Rohan Paleja"
                },
                "author": "Rohan Paleja",
                "arxiv_comment": "Accepted to the Cooperative Multi-Agent Systems Decision-making and\n  Learning:Human-Multi-Agent Cognitive Fusion Workshop at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19607v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19602v1",
                "updated": "2025-03-25T12:37:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    37,
                    22,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T12:37:22Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    37,
                    22,
                    1,
                    84,
                    0
                ],
                "title": "Innate Reasoning is Not Enough: In-Context Learning Enhances Reasoning\n  Large Language Models with Less Overthinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Innate Reasoning is Not Enough: In-Context Learning Enhances Reasoning\n  Large Language Models with Less Overthinking"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have introduced Reasoning\nLarge Language Models (RLLMs), which employ extended thinking processes with\nreflection and self-correction capabilities, demonstrating the effectiveness of\ntest-time scaling. RLLMs exhibit innate Chain-of-Thought (CoT) reasoning\ncapability obtained from training, leading to a natural question: \"Is CoT\nprompting, a popular In-Context Learning (ICL) method for chat LLMs, necessary\nto enhance the reasoning capability of RLLMs?\" In this work, we present the\nfirst comprehensive analysis of the impacts of Zero-shot CoT and Few-shot CoT\non RLLMs across mathematical reasoning tasks. We examine models ranging from\n1.5B to 32B parameters, finding that contrary to concerns, CoT prompting\nsignificantly enhances RLLMs' performance in most scenarios. Our results reveal\ndistinct patterns: large-capacity models show minimal improvement on simple\ntasks but substantial gains on complex problems, while smaller models exhibit\nthe opposite behavior. Further analysis demonstrates that CoT prompting\neffectively controls the distribution of the numbers of thinking tokens and\nreasoning steps, reducing excessive reflections by approximately 90% in some\ncases. Moreover, attention logits analysis reveals the RLLMs' overfitting to\nreflection-related words, which is mitigated by external CoT guidance. Notably,\nour experiments indicate that for RLLMs, one-shot CoT consistently yields\nsuperior performance compared to Few-shot CoT approaches. Our findings provide\nimportant insights for optimizing RLLMs' performance through appropriate\nprompting strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have introduced Reasoning\nLarge Language Models (RLLMs), which employ extended thinking processes with\nreflection and self-correction capabilities, demonstrating the effectiveness of\ntest-time scaling. RLLMs exhibit innate Chain-of-Thought (CoT) reasoning\ncapability obtained from training, leading to a natural question: \"Is CoT\nprompting, a popular In-Context Learning (ICL) method for chat LLMs, necessary\nto enhance the reasoning capability of RLLMs?\" In this work, we present the\nfirst comprehensive analysis of the impacts of Zero-shot CoT and Few-shot CoT\non RLLMs across mathematical reasoning tasks. We examine models ranging from\n1.5B to 32B parameters, finding that contrary to concerns, CoT prompting\nsignificantly enhances RLLMs' performance in most scenarios. Our results reveal\ndistinct patterns: large-capacity models show minimal improvement on simple\ntasks but substantial gains on complex problems, while smaller models exhibit\nthe opposite behavior. Further analysis demonstrates that CoT prompting\neffectively controls the distribution of the numbers of thinking tokens and\nreasoning steps, reducing excessive reflections by approximately 90% in some\ncases. Moreover, attention logits analysis reveals the RLLMs' overfitting to\nreflection-related words, which is mitigated by external CoT guidance. Notably,\nour experiments indicate that for RLLMs, one-shot CoT consistently yields\nsuperior performance compared to Few-shot CoT approaches. Our findings provide\nimportant insights for optimizing RLLMs' performance through appropriate\nprompting strategies."
                },
                "authors": [
                    {
                        "name": "Yuyao Ge"
                    },
                    {
                        "name": "Shenghua Liu"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Lingrui Mei"
                    },
                    {
                        "name": "Lizhe Chen"
                    },
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19599v1",
                "updated": "2025-03-25T12:30:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    30,
                    30,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T12:30:30Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    30,
                    30,
                    1,
                    84,
                    0
                ],
                "title": "HoarePrompt: Structural Reasoning About Program Correctness in Natural\n  Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HoarePrompt: Structural Reasoning About Program Correctness in Natural\n  Language"
                },
                "summary": "While software requirements are often expressed in natural language,\nverifying the correctness of a program against natural language requirements is\na hard and underexplored problem. Large language models (LLMs) are promising\ncandidates for addressing this challenge, however our experience shows that\nthey are ineffective in this task, often failing to detect even straightforward\nbugs. To address this gap, we introduce HoarePrompt, a novel approach that\nadapts fundamental ideas from program analysis and verification to natural\nlanguage artifacts. Drawing inspiration from the strongest postcondition\ncalculus, HoarePrompt employs a systematic, step-by-step process in which an\nLLM generates natural language descriptions of reachable program states at\nvarious points in the code. To manage loops, we propose few-shot-driven\nk-induction, an adaptation of the k-induction method widely used in model\nchecking. Once program states are described, HoarePrompt leverages the LLM to\nassess whether the program, annotated with these state descriptions, conforms\nto the natural language requirements. For evaluating the quality of classifiers\nof program correctness with respect to natural language requirements, we\nconstructed CoCoClaNeL, a challenging dataset of solutions to programming\ncompetition problems. Our experiments show that HoarePrompt improves the MCC by\n62% compared to directly using Zero-shot-CoT prompts for correctness\nclassification. Furthermore, HoarePrompt outperforms a classifier that assesses\ncorrectness via LLM-based test generation by increasing the MCC by 93%. The\ninductive reasoning mechanism contributes a 28% boost to MCC, underscoring its\neffectiveness in managing loops.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While software requirements are often expressed in natural language,\nverifying the correctness of a program against natural language requirements is\na hard and underexplored problem. Large language models (LLMs) are promising\ncandidates for addressing this challenge, however our experience shows that\nthey are ineffective in this task, often failing to detect even straightforward\nbugs. To address this gap, we introduce HoarePrompt, a novel approach that\nadapts fundamental ideas from program analysis and verification to natural\nlanguage artifacts. Drawing inspiration from the strongest postcondition\ncalculus, HoarePrompt employs a systematic, step-by-step process in which an\nLLM generates natural language descriptions of reachable program states at\nvarious points in the code. To manage loops, we propose few-shot-driven\nk-induction, an adaptation of the k-induction method widely used in model\nchecking. Once program states are described, HoarePrompt leverages the LLM to\nassess whether the program, annotated with these state descriptions, conforms\nto the natural language requirements. For evaluating the quality of classifiers\nof program correctness with respect to natural language requirements, we\nconstructed CoCoClaNeL, a challenging dataset of solutions to programming\ncompetition problems. Our experiments show that HoarePrompt improves the MCC by\n62% compared to directly using Zero-shot-CoT prompts for correctness\nclassification. Furthermore, HoarePrompt outperforms a classifier that assesses\ncorrectness via LLM-based test generation by increasing the MCC by 93%. The\ninductive reasoning mechanism contributes a 28% boost to MCC, underscoring its\neffectiveness in managing loops."
                },
                "authors": [
                    {
                        "name": "Dimitrios Stamatios Bouras"
                    },
                    {
                        "name": "Yihan Dai"
                    },
                    {
                        "name": "Tairan Wang"
                    },
                    {
                        "name": "Yingfei Xiong"
                    },
                    {
                        "name": "Sergey Mechtaev"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Mechtaev"
                },
                "author": "Sergey Mechtaev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19598v1",
                "updated": "2025-03-25T12:29:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    29,
                    53,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T12:29:53Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    29,
                    53,
                    1,
                    84,
                    0
                ],
                "title": "The Greatest Good Benchmark: Measuring LLMs' Alignment with Utilitarian\n  Moral Dilemmas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Greatest Good Benchmark: Measuring LLMs' Alignment with Utilitarian\n  Moral Dilemmas"
                },
                "summary": "The question of how to make decisions that maximise the well-being of all\npersons is very relevant to design language models that are beneficial to\nhumanity and free from harm. We introduce the Greatest Good Benchmark to\nevaluate the moral judgments of LLMs using utilitarian dilemmas. Our analysis\nacross 15 diverse LLMs reveals consistently encoded moral preferences that\ndiverge from established moral theories and lay population moral standards.\nMost LLMs have a marked preference for impartial beneficence and rejection of\ninstrumental harm. These findings showcase the 'artificial moral compass' of\nLLMs, offering insights into their moral alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The question of how to make decisions that maximise the well-being of all\npersons is very relevant to design language models that are beneficial to\nhumanity and free from harm. We introduce the Greatest Good Benchmark to\nevaluate the moral judgments of LLMs using utilitarian dilemmas. Our analysis\nacross 15 diverse LLMs reveals consistently encoded moral preferences that\ndiverge from established moral theories and lay population moral standards.\nMost LLMs have a marked preference for impartial beneficence and rejection of\ninstrumental harm. These findings showcase the 'artificial moral compass' of\nLLMs, offering insights into their moral alignment."
                },
                "authors": [
                    {
                        "name": "Giovanni Franco Gabriel Marraffini"
                    },
                    {
                        "name": "Andrés Cotton"
                    },
                    {
                        "name": "Noe Fabian Hsueh"
                    },
                    {
                        "name": "Axel Fridman"
                    },
                    {
                        "name": "Juan Wisznia"
                    },
                    {
                        "name": "Luciano Del Corro"
                    }
                ],
                "author_detail": {
                    "name": "Luciano Del Corro"
                },
                "author": "Luciano Del Corro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19594v1",
                "updated": "2025-03-25T12:21:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    21,
                    12,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T12:21:12Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    21,
                    12,
                    1,
                    84,
                    0
                ],
                "title": "Perception-Enhanced Multitask Multimodal Semantic Communication for\n  UAV-Assisted Integrated Sensing and Communication System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perception-Enhanced Multitask Multimodal Semantic Communication for\n  UAV-Assisted Integrated Sensing and Communication System"
                },
                "summary": "Recent advances in integrated sensing and communication (ISAC) unmanned\naerial vehicles (UAVs) have enabled their widespread deployment in critical\napplications such as emergency management. This paper investigates the\nchallenge of efficient multitask multimodal data communication in UAV-assisted\nISAC systems, in the considered system model, hyperspectral (HSI) and LiDAR\ndata are collected by UAV-mounted sensors for both target classification and\ndata reconstruction at the terrestrial BS. The limited channel capacity and\ncomplex environmental conditions pose significant challenges to effective\nair-to-ground communication. To tackle this issue, we propose a\nperception-enhanced multitask multimodal semantic communication (PE-MMSC)\nsystem that strategically leverages the onboard computational and sensing\ncapabilities of UAVs. In particular, we first propose a robust multimodal\nfeature fusion method that adaptively combines HSI and LiDAR semantics while\nconsidering channel noise and task requirements. Then the method introduces a\nperception-enhanced (PE) module incorporating attention mechanisms to perform\ncoarse classification on UAV side, thereby optimizing the attention-based\nmultimodal fusion and transmission. Experimental results demonstrate that the\nproposed PE-MMSC system achieves 5\\%--10\\% higher target classification\naccuracy compared to conventional systems without PE module, while maintaining\ncomparable data reconstruction quality with acceptable computational overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in integrated sensing and communication (ISAC) unmanned\naerial vehicles (UAVs) have enabled their widespread deployment in critical\napplications such as emergency management. This paper investigates the\nchallenge of efficient multitask multimodal data communication in UAV-assisted\nISAC systems, in the considered system model, hyperspectral (HSI) and LiDAR\ndata are collected by UAV-mounted sensors for both target classification and\ndata reconstruction at the terrestrial BS. The limited channel capacity and\ncomplex environmental conditions pose significant challenges to effective\nair-to-ground communication. To tackle this issue, we propose a\nperception-enhanced multitask multimodal semantic communication (PE-MMSC)\nsystem that strategically leverages the onboard computational and sensing\ncapabilities of UAVs. In particular, we first propose a robust multimodal\nfeature fusion method that adaptively combines HSI and LiDAR semantics while\nconsidering channel noise and task requirements. Then the method introduces a\nperception-enhanced (PE) module incorporating attention mechanisms to perform\ncoarse classification on UAV side, thereby optimizing the attention-based\nmultimodal fusion and transmission. Experimental results demonstrate that the\nproposed PE-MMSC system achieves 5\\%--10\\% higher target classification\naccuracy compared to conventional systems without PE module, while maintaining\ncomparable data reconstruction quality with acceptable computational overheads."
                },
                "authors": [
                    {
                        "name": "Ziji Guo"
                    },
                    {
                        "name": "Haonan Tong"
                    },
                    {
                        "name": "Zhilong Zhang"
                    },
                    {
                        "name": "Danpu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Danpu Liu"
                },
                "author": "Danpu Liu",
                "arxiv_journal_ref": "WS21 ICC 2025 Workshop - ISCLAN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05374v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05374v3",
                "updated": "2025-03-25T12:18:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    18,
                    42,
                    1,
                    84,
                    0
                ],
                "published": "2025-02-07T23:03:55Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    23,
                    3,
                    55,
                    4,
                    38,
                    0
                ],
                "title": "Towards LLM Unlearning Resilient to Relearning Attacks: A\n  Sharpness-Aware Minimization Perspective and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards LLM Unlearning Resilient to Relearning Attacks: A\n  Sharpness-Aware Minimization Perspective and Beyond"
                },
                "summary": "The LLM unlearning technique has recently been introduced to comply with data\nregulations and address the safety and ethical concerns of LLMs by removing the\nundesired data-model influence. However, state-of-the-art unlearning methods\nface a critical vulnerability: they are susceptible to ``relearning'' the\nremoved information from a small number of forget data points, known as\nrelearning attacks. In this paper, we systematically investigate how to make\nunlearned models robust against such attacks. For the first time, we establish\na connection between robust unlearning and sharpness-aware minimization (SAM)\nthrough a unified robust optimization framework, in an analogy to adversarial\ntraining designed to defend against adversarial attacks. Our analysis for SAM\nreveals that smoothness optimization plays a pivotal role in mitigating\nrelearning attacks. Thus, we further explore diverse smoothing strategies to\nenhance unlearning robustness. Extensive experiments on benchmark datasets,\nincluding WMDP and MUSE, demonstrate that SAM and other smoothness optimization\napproaches consistently improve the resistance of LLM unlearning to relearning\nattacks. Notably, smoothness-enhanced unlearning also helps defend against\n(input-level) jailbreaking attacks, broadening our proposal's impact in\nrobustifying LLM unlearning. Codes are available at\nhttps://github.com/OPTML-Group/Unlearn-Smooth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LLM unlearning technique has recently been introduced to comply with data\nregulations and address the safety and ethical concerns of LLMs by removing the\nundesired data-model influence. However, state-of-the-art unlearning methods\nface a critical vulnerability: they are susceptible to ``relearning'' the\nremoved information from a small number of forget data points, known as\nrelearning attacks. In this paper, we systematically investigate how to make\nunlearned models robust against such attacks. For the first time, we establish\na connection between robust unlearning and sharpness-aware minimization (SAM)\nthrough a unified robust optimization framework, in an analogy to adversarial\ntraining designed to defend against adversarial attacks. Our analysis for SAM\nreveals that smoothness optimization plays a pivotal role in mitigating\nrelearning attacks. Thus, we further explore diverse smoothing strategies to\nenhance unlearning robustness. Extensive experiments on benchmark datasets,\nincluding WMDP and MUSE, demonstrate that SAM and other smoothness optimization\napproaches consistently improve the resistance of LLM unlearning to relearning\nattacks. Notably, smoothness-enhanced unlearning also helps defend against\n(input-level) jailbreaking attacks, broadening our proposal's impact in\nrobustifying LLM unlearning. Codes are available at\nhttps://github.com/OPTML-Group/Unlearn-Smooth."
                },
                "authors": [
                    {
                        "name": "Chongyu Fan"
                    },
                    {
                        "name": "Jinghan Jia"
                    },
                    {
                        "name": "Yihua Zhang"
                    },
                    {
                        "name": "Anil Ramakrishna"
                    },
                    {
                        "name": "Mingyi Hong"
                    },
                    {
                        "name": "Sijia Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sijia Liu"
                },
                "author": "Sijia Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05374v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05374v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18002v2",
                "updated": "2025-03-25T12:05:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    5,
                    26,
                    1,
                    84,
                    0
                ],
                "published": "2025-02-12T02:40:44Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    2,
                    40,
                    44,
                    2,
                    43,
                    0
                ],
                "title": "Neuromorphic Principles for Efficient Large Language Models on Intel\n  Loihi 2",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuromorphic Principles for Efficient Large Language Models on Intel\n  Loihi 2"
                },
                "summary": "Large language models (LLMs) deliver impressive performance but require large\namounts of energy. In this work, we present a MatMul-free LLM architecture\nadapted for Intel's neuromorphic processor, Loihi 2. Our approach leverages\nLoihi 2's support for low-precision, event-driven computation and stateful\nprocessing. Our hardware-aware quantized model on GPU demonstrates that a 370M\nparameter MatMul-free model can be quantized with no accuracy loss. Based on\npreliminary results, we report up to 3x higher throughput with 2x less energy,\ncompared to transformer-based LLMs on an edge GPU, with significantly better\nscaling. Further hardware optimizations will increase throughput and decrease\nenergy consumption. These results show the potential of neuromorphic hardware\nfor efficient inference and pave the way for efficient reasoning models capable\nof generating complex, long-form text rapidly and cost-effectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) deliver impressive performance but require large\namounts of energy. In this work, we present a MatMul-free LLM architecture\nadapted for Intel's neuromorphic processor, Loihi 2. Our approach leverages\nLoihi 2's support for low-precision, event-driven computation and stateful\nprocessing. Our hardware-aware quantized model on GPU demonstrates that a 370M\nparameter MatMul-free model can be quantized with no accuracy loss. Based on\npreliminary results, we report up to 3x higher throughput with 2x less energy,\ncompared to transformer-based LLMs on an edge GPU, with significantly better\nscaling. Further hardware optimizations will increase throughput and decrease\nenergy consumption. These results show the potential of neuromorphic hardware\nfor efficient inference and pave the way for efficient reasoning models capable\nof generating complex, long-form text rapidly and cost-effectively."
                },
                "authors": [
                    {
                        "name": "Steven Abreu"
                    },
                    {
                        "name": "Sumit Bam Shrestha"
                    },
                    {
                        "name": "Rui-Jie Zhu"
                    },
                    {
                        "name": "Jason Eshraghian"
                    }
                ],
                "author_detail": {
                    "name": "Jason Eshraghian"
                },
                "author": "Jason Eshraghian",
                "arxiv_comment": "Accepted to International Conference on Learning Representations\n  (ICLR) Workshop on Scalable Optimization for Efficient and Adaptive\n  Foundation Models (SCOPE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19574v1",
                "updated": "2025-03-25T11:48:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    11,
                    48,
                    22,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T11:48:22Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    11,
                    48,
                    22,
                    1,
                    84,
                    0
                ],
                "title": "Context-Efficient Retrieval with Factual Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Efficient Retrieval with Factual Decomposition"
                },
                "summary": "There has recently been considerable interest in incorporating information\nretrieval into large language models (LLMs). Retrieval from a dynamically\nexpanding external corpus of text allows a model to incorporate current events\nand can be viewed as a form of episodic memory. Here we demonstrate that\npre-processing the external corpus into semi-structured ''atomic facts'' makes\nretrieval more efficient. More specifically, we demonstrate that our particular\nform of atomic facts improves performance on various question answering tasks\nwhen the amount of retrieved text is limited. Limiting the amount of retrieval\nreduces the size of the context and improves inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has recently been considerable interest in incorporating information\nretrieval into large language models (LLMs). Retrieval from a dynamically\nexpanding external corpus of text allows a model to incorporate current events\nand can be viewed as a form of episodic memory. Here we demonstrate that\npre-processing the external corpus into semi-structured ''atomic facts'' makes\nretrieval more efficient. More specifically, we demonstrate that our particular\nform of atomic facts improves performance on various question answering tasks\nwhen the amount of retrieved text is limited. Limiting the amount of retrieval\nreduces the size of the context and improves inference efficiency."
                },
                "authors": [
                    {
                        "name": "Yanhong Li"
                    },
                    {
                        "name": "David Yunis"
                    },
                    {
                        "name": "David McAllester"
                    },
                    {
                        "name": "Jiawei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Zhou"
                },
                "author": "Jiawei Zhou",
                "arxiv_comment": "NAACL 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19573v1",
                "updated": "2025-03-25T11:48:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    11,
                    48,
                    5,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T11:48:05Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    11,
                    48,
                    5,
                    1,
                    84,
                    0
                ],
                "title": "Motif Counting in Complex Networks: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motif Counting in Complex Networks: A Comprehensive Survey"
                },
                "summary": "Motif counting plays a crucial role in understanding the structural\nproperties of networks. By computing motif frequencies, researchers can draw\nkey insights into the structural properties of the underlying network. As\nnetworks become increasingly complex, different graph models have been\nproposed, giving rise to diverse motif patterns. These variations introduce\nunique computational challenges that require specialized algorithms tailored to\nspecific motifs within different graph structures. This survey provides a\ncomprehensive and structured overview of motif counting techniques across\ngeneral graphs, heterogeneous graphs, and hypergraphs. We categorize existing\nalgorithms according to their underlying computational strategies, emphasizing\nkey similarities and distinctions. In addition to reviewing current\nmethodologies, we examine their strengths, limitations, and computational\ntrade-offs. Furthermore, we explore future directions in motif counting,\nincluding scalable implementations to improve efficiency in large-scale\nnetworks, algorithmic adaptations for dynamic, temporal, and attributed graphs,\nand deeper integration with large language models (LLMs) and graph-based\nretrieval-augmented generation (GraphRAG). By offering a detailed analysis of\nthese approaches, this survey aims to support researchers and practitioners in\nadvancing motif counting for increasingly complex network data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motif counting plays a crucial role in understanding the structural\nproperties of networks. By computing motif frequencies, researchers can draw\nkey insights into the structural properties of the underlying network. As\nnetworks become increasingly complex, different graph models have been\nproposed, giving rise to diverse motif patterns. These variations introduce\nunique computational challenges that require specialized algorithms tailored to\nspecific motifs within different graph structures. This survey provides a\ncomprehensive and structured overview of motif counting techniques across\ngeneral graphs, heterogeneous graphs, and hypergraphs. We categorize existing\nalgorithms according to their underlying computational strategies, emphasizing\nkey similarities and distinctions. In addition to reviewing current\nmethodologies, we examine their strengths, limitations, and computational\ntrade-offs. Furthermore, we explore future directions in motif counting,\nincluding scalable implementations to improve efficiency in large-scale\nnetworks, algorithmic adaptations for dynamic, temporal, and attributed graphs,\nand deeper integration with large language models (LLMs) and graph-based\nretrieval-augmented generation (GraphRAG). By offering a detailed analysis of\nthese approaches, this survey aims to support researchers and practitioners in\nadvancing motif counting for increasingly complex network data."
                },
                "authors": [
                    {
                        "name": "Haozhe Yin"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Wenjie Zhang"
                    },
                    {
                        "name": "Yizhang He"
                    },
                    {
                        "name": "Ying Zhang"
                    },
                    {
                        "name": "Xuemin Lin"
                    }
                ],
                "author_detail": {
                    "name": "Xuemin Lin"
                },
                "author": "Xuemin Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18710v2",
                "updated": "2025-03-25T11:11:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    11,
                    11,
                    3,
                    1,
                    84,
                    0
                ],
                "published": "2024-05-29T02:42:23Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    2,
                    42,
                    23,
                    2,
                    150,
                    0
                ],
                "title": "To FP8 and Back Again: Quantifying Reduced Precision Effects on LLM\n  Training Stability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To FP8 and Back Again: Quantifying Reduced Precision Effects on LLM\n  Training Stability"
                },
                "summary": "The massive computational costs associated with large language model (LLM)\npretraining have spurred great interest in reduced-precision floating-point\nrepresentations to accelerate the process. As a result, the BrainFloat16 (BF16)\nprecision has become the de facto standard for LLM training, with hardware\nsupport included in recent generations of accelerators. This trend has gone\neven further in the latest processors, where FP8 has recently been introduced.\nHowever, prior experience with FP16, which was found to be less stable than\nBF16, raises concerns as to whether FP8, with even fewer bits than FP16, can be\na cost-effective option for LLM training. We argue that reduced-precision\ntraining schemes must have similar training stability and hyperparameter\nsensitivities to their higher-precision counterparts in order to be\ncost-effective. However, we find that currently available methods for FP8\ntraining are not robust enough to allow their use as economical replacements.\nThis prompts us to investigate the stability of reduced-precision LLM training\nin terms of robustness across random seeds, learning rates, and datasets. To\nthis end, we propose new evaluation techniques and a new metric for quantifying\nloss landscape sharpness in autoregressive language models. By simulating\nincremental bit reductions in floating-point representations, we analyze the\nrelationship between representational power and training stability with the\nintent of aiding future research into the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The massive computational costs associated with large language model (LLM)\npretraining have spurred great interest in reduced-precision floating-point\nrepresentations to accelerate the process. As a result, the BrainFloat16 (BF16)\nprecision has become the de facto standard for LLM training, with hardware\nsupport included in recent generations of accelerators. This trend has gone\neven further in the latest processors, where FP8 has recently been introduced.\nHowever, prior experience with FP16, which was found to be less stable than\nBF16, raises concerns as to whether FP8, with even fewer bits than FP16, can be\na cost-effective option for LLM training. We argue that reduced-precision\ntraining schemes must have similar training stability and hyperparameter\nsensitivities to their higher-precision counterparts in order to be\ncost-effective. However, we find that currently available methods for FP8\ntraining are not robust enough to allow their use as economical replacements.\nThis prompts us to investigate the stability of reduced-precision LLM training\nin terms of robustness across random seeds, learning rates, and datasets. To\nthis end, we propose new evaluation techniques and a new metric for quantifying\nloss landscape sharpness in autoregressive language models. By simulating\nincremental bit reductions in floating-point representations, we analyze the\nrelationship between representational power and training stability with the\nintent of aiding future research into the field."
                },
                "authors": [
                    {
                        "name": "Joonhyung Lee"
                    },
                    {
                        "name": "Jeongin Bae"
                    },
                    {
                        "name": "Byeongwook Kim"
                    },
                    {
                        "name": "Se Jung Kwon"
                    },
                    {
                        "name": "Dongsoo Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongsoo Lee"
                },
                "author": "Dongsoo Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19551v1",
                "updated": "2025-03-25T11:07:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    11,
                    7,
                    12,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T11:07:12Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    11,
                    7,
                    12,
                    1,
                    84,
                    0
                ],
                "title": "Scaling Laws of Synthetic Data for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Laws of Synthetic Data for Language Models"
                },
                "summary": "Large language models (LLMs) achieve strong performance across diverse tasks,\nlargely driven by high-quality web data used in pre-training. However, recent\nstudies indicate this data source is rapidly depleting. Synthetic data emerges\nas a promising alternative, but it remains unclear whether synthetic datasets\nexhibit predictable scalability comparable to raw pre-training data. In this\nwork, we systematically investigate the scaling laws of synthetic data by\nintroducing SynthLLM, a scalable framework that transforms pre-training corpora\ninto diverse, high-quality synthetic datasets. Our approach achieves this by\nautomatically extracting and recombining high-level concepts across multiple\ndocuments using a graph algorithm. Key findings from our extensive mathematical\nexperiments on SynthLLM include: (1) SynthLLM generates synthetic data that\nreliably adheres to the \\emph{rectified scaling law} across various model\nsizes; (2) Performance improvements plateau near 300B tokens; and (3) Larger\nmodels approach optimal performance with fewer training tokens. For instance,\nan 8B model peaks at 1T tokens, while a 3B model requires 4T. Moreover,\ncomparisons with existing synthetic data generation and augmentation methods\ndemonstrate that SynthLLM achieves superior performance and scalability. Our\nfindings highlight synthetic data as a scalable and reliable alternative to\norganic pre-training corpora, offering a viable path toward continued\nimprovement in model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve strong performance across diverse tasks,\nlargely driven by high-quality web data used in pre-training. However, recent\nstudies indicate this data source is rapidly depleting. Synthetic data emerges\nas a promising alternative, but it remains unclear whether synthetic datasets\nexhibit predictable scalability comparable to raw pre-training data. In this\nwork, we systematically investigate the scaling laws of synthetic data by\nintroducing SynthLLM, a scalable framework that transforms pre-training corpora\ninto diverse, high-quality synthetic datasets. Our approach achieves this by\nautomatically extracting and recombining high-level concepts across multiple\ndocuments using a graph algorithm. Key findings from our extensive mathematical\nexperiments on SynthLLM include: (1) SynthLLM generates synthetic data that\nreliably adheres to the \\emph{rectified scaling law} across various model\nsizes; (2) Performance improvements plateau near 300B tokens; and (3) Larger\nmodels approach optimal performance with fewer training tokens. For instance,\nan 8B model peaks at 1T tokens, while a 3B model requires 4T. Moreover,\ncomparisons with existing synthetic data generation and augmentation methods\ndemonstrate that SynthLLM achieves superior performance and scalability. Our\nfindings highlight synthetic data as a scalable and reliable alternative to\norganic pre-training corpora, offering a viable path toward continued\nimprovement in model performance."
                },
                "authors": [
                    {
                        "name": "Zeyu Qin"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Xingxing Zhang"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Xiaolong Huang"
                    },
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Mahmoud Khademi"
                    },
                    {
                        "name": "Dongdong Zhang"
                    },
                    {
                        "name": "Hany Hassan Awadalla"
                    },
                    {
                        "name": "Yi R. Fung"
                    },
                    {
                        "name": "Weizhu Chen"
                    },
                    {
                        "name": "Minhao Cheng"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18596v2",
                "updated": "2025-03-25T11:04:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    11,
                    4,
                    18,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-24T11:53:06Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    53,
                    6,
                    0,
                    83,
                    0
                ],
                "title": "LinkAlign: Scalable Schema Linking for Real-World Large-Scale\n  Multi-Database Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LinkAlign: Scalable Schema Linking for Real-World Large-Scale\n  Multi-Database Text-to-SQL"
                },
                "summary": "Schema linking is a critical bottleneck in achieving human-level performance\nin Text-to-SQL tasks, particularly in real-world large-scale multi-database\nscenarios. Addressing schema linking faces two major challenges: (1) Database\nRetrieval: selecting the correct database from a large schema pool in\nmulti-database settings, while filtering out irrelevant ones. (2) Schema Item\nGrounding: accurately identifying the relevant tables and columns from within a\nlarge and redundant schema for SQL generation. To address this, we introduce\nLinkAlign, a novel framework that can effectively adapt existing baselines to\nreal-world environments by systematically addressing schema linking. Our\nframework comprises three key steps: multi-round semantic enhanced retrieval\nand irrelevant information isolation for Challenge 1, and schema extraction\nenhancement for Challenge 2. We evaluate our method performance of schema\nlinking on the SPIDER and BIRD benchmarks, and the ability to adapt existing\nText-to-SQL models to real-world environments on the SPIDER 2.0-lite benchmark.\nExperiments show that LinkAlign outperforms existing baselines in\nmulti-database settings, demonstrating its effectiveness and robustness. On the\nother hand, our method ranks highest among models excluding those using long\nchain-of-thought reasoning LLMs. This work bridges the gap between current\nresearch and real-world scenarios, providing a practical solution for robust\nand scalable schema linking. The codes are available at\nhttps://github.com/Satissss/LinkAlign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Schema linking is a critical bottleneck in achieving human-level performance\nin Text-to-SQL tasks, particularly in real-world large-scale multi-database\nscenarios. Addressing schema linking faces two major challenges: (1) Database\nRetrieval: selecting the correct database from a large schema pool in\nmulti-database settings, while filtering out irrelevant ones. (2) Schema Item\nGrounding: accurately identifying the relevant tables and columns from within a\nlarge and redundant schema for SQL generation. To address this, we introduce\nLinkAlign, a novel framework that can effectively adapt existing baselines to\nreal-world environments by systematically addressing schema linking. Our\nframework comprises three key steps: multi-round semantic enhanced retrieval\nand irrelevant information isolation for Challenge 1, and schema extraction\nenhancement for Challenge 2. We evaluate our method performance of schema\nlinking on the SPIDER and BIRD benchmarks, and the ability to adapt existing\nText-to-SQL models to real-world environments on the SPIDER 2.0-lite benchmark.\nExperiments show that LinkAlign outperforms existing baselines in\nmulti-database settings, demonstrating its effectiveness and robustness. On the\nother hand, our method ranks highest among models excluding those using long\nchain-of-thought reasoning LLMs. This work bridges the gap between current\nresearch and real-world scenarios, providing a practical solution for robust\nand scalable schema linking. The codes are available at\nhttps://github.com/Satissss/LinkAlign."
                },
                "authors": [
                    {
                        "name": "Yihan Wang"
                    },
                    {
                        "name": "Peiyu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Peiyu Liu"
                },
                "author": "Peiyu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19548v1",
                "updated": "2025-03-25T11:02:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    11,
                    2,
                    8,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T11:02:08Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    11,
                    2,
                    8,
                    1,
                    84,
                    0
                ],
                "title": "On-Chain Analysis of Smart Contract Dependency Risks on Ethereum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-Chain Analysis of Smart Contract Dependency Risks on Ethereum"
                },
                "summary": "In this paper, we present the first large-scale empirical study of smart\ncontract dependencies, analyzing over 41 million contracts and 11 billion\ninteractions on Ethereum up to December 2024. Our results yield four key\ninsights: (1) 59% of contract transactions involve multiple contracts (median\nof 4 per transaction in 2024) indicating potential smart contract dependency\nrisks; (2) the ecosystem exhibits extreme centralization, with just 11 (0.001%)\ndeployers controlling 20.5 million (50%) of alive contracts, with major risks\nrelated to factory contracts and deployer privileges; (3) three most\ndepended-upon contracts are mutable, meaning large parts of the ecosystem rely\non contracts that can be altered at any time, which is a significant risk, (4)\nactual smart contract protocol dependencies are significantly more complex than\nofficially documented, undermining Ethereum's transparency ethos, and creating\nunnecessary attack surface. Our work provides the first large-scale empirical\nfoundation for understanding smart contract dependency risks, offering crucial\ninsights for developers, users, and security researchers in the blockchain\nspace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present the first large-scale empirical study of smart\ncontract dependencies, analyzing over 41 million contracts and 11 billion\ninteractions on Ethereum up to December 2024. Our results yield four key\ninsights: (1) 59% of contract transactions involve multiple contracts (median\nof 4 per transaction in 2024) indicating potential smart contract dependency\nrisks; (2) the ecosystem exhibits extreme centralization, with just 11 (0.001%)\ndeployers controlling 20.5 million (50%) of alive contracts, with major risks\nrelated to factory contracts and deployer privileges; (3) three most\ndepended-upon contracts are mutable, meaning large parts of the ecosystem rely\non contracts that can be altered at any time, which is a significant risk, (4)\nactual smart contract protocol dependencies are significantly more complex than\nofficially documented, undermining Ethereum's transparency ethos, and creating\nunnecessary attack surface. Our work provides the first large-scale empirical\nfoundation for understanding smart contract dependency risks, offering crucial\ninsights for developers, users, and security researchers in the blockchain\nspace."
                },
                "authors": [
                    {
                        "name": "Monica Jin"
                    },
                    {
                        "name": "Raphina Liu"
                    },
                    {
                        "name": "Martin Monperrus"
                    }
                ],
                "author_detail": {
                    "name": "Martin Monperrus"
                },
                "author": "Martin Monperrus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17056v2",
                "updated": "2025-03-25T10:50:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    10,
                    50,
                    21,
                    1,
                    84,
                    0
                ],
                "published": "2024-12-22T15:08:24Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    15,
                    8,
                    24,
                    6,
                    357,
                    0
                ],
                "title": "The HalluRAG Dataset: Detecting Closed-Domain Hallucinations in RAG\n  Applications Using an LLM's Internal States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The HalluRAG Dataset: Detecting Closed-Domain Hallucinations in RAG\n  Applications Using an LLM's Internal States"
                },
                "summary": "Detecting hallucinations in large language models (LLMs) is critical for\nenhancing their reliability and trustworthiness. Most research focuses on\nhallucinations as deviations from information seen during training. However,\nthe opaque nature of an LLM's parametric knowledge complicates the\nunderstanding of why generated texts appear ungrounded: The LLM might not have\npicked up the necessary knowledge from large and often inaccessible datasets,\nor the information might have been changed or contradicted during further\ntraining. Our focus is on hallucinations involving information not used in\ntraining, which we determine by using recency to ensure the information emerged\nafter a cut-off date. This study investigates these hallucinations by detecting\nthem at sentence level using different internal states of various LLMs. We\npresent HalluRAG, a dataset designed to train classifiers on these\nhallucinations. Depending on the model and quantization, MLPs trained on\nHalluRAG detect hallucinations with test accuracies ranging up to 75 %, with\nMistral-7B-Instruct-v0.1 achieving the highest test accuracies. Our results\nshow that IAVs detect hallucinations as effectively as CEVs and reveal that\nanswerable and unanswerable prompts are encoded differently as separate\nclassifiers for these categories improved accuracy. However, HalluRAG showed\nsome limited generalizability, advocating for more diversity in datasets on\nhallucinations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting hallucinations in large language models (LLMs) is critical for\nenhancing their reliability and trustworthiness. Most research focuses on\nhallucinations as deviations from information seen during training. However,\nthe opaque nature of an LLM's parametric knowledge complicates the\nunderstanding of why generated texts appear ungrounded: The LLM might not have\npicked up the necessary knowledge from large and often inaccessible datasets,\nor the information might have been changed or contradicted during further\ntraining. Our focus is on hallucinations involving information not used in\ntraining, which we determine by using recency to ensure the information emerged\nafter a cut-off date. This study investigates these hallucinations by detecting\nthem at sentence level using different internal states of various LLMs. We\npresent HalluRAG, a dataset designed to train classifiers on these\nhallucinations. Depending on the model and quantization, MLPs trained on\nHalluRAG detect hallucinations with test accuracies ranging up to 75 %, with\nMistral-7B-Instruct-v0.1 achieving the highest test accuracies. Our results\nshow that IAVs detect hallucinations as effectively as CEVs and reveal that\nanswerable and unanswerable prompts are encoded differently as separate\nclassifiers for these categories improved accuracy. However, HalluRAG showed\nsome limited generalizability, advocating for more diversity in datasets on\nhallucinations."
                },
                "authors": [
                    {
                        "name": "Fabian Ridder"
                    },
                    {
                        "name": "Malte Schilling"
                    }
                ],
                "author_detail": {
                    "name": "Malte Schilling"
                },
                "author": "Malte Schilling",
                "arxiv_comment": "19 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19540v1",
                "updated": "2025-03-25T10:48:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    10,
                    48,
                    33,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T10:48:33Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    10,
                    48,
                    33,
                    1,
                    84,
                    0
                ],
                "title": "FLEX: A Benchmark for Evaluating Robustness of Fairness in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLEX: A Benchmark for Evaluating Robustness of Fairness in Large\n  Language Models"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced interactions between users and models. These advancements concurrently\nunderscore the need for rigorous safety evaluations due to the manifestation of\nsocial biases, which can lead to harmful societal impacts. Despite these\nconcerns, existing benchmarks may overlook the intrinsic weaknesses of LLMs,\nwhich can generate biased responses even with simple adversarial instructions.\nTo address this critical gap, we introduce a new benchmark, Fairness Benchmark\nin LLM under Extreme Scenarios (FLEX), designed to test whether LLMs can\nsustain fairness even when exposed to prompts constructed to induce bias. To\nthoroughly evaluate the robustness of LLMs, we integrate prompts that amplify\npotential biases into the fairness assessment. Comparative experiments between\nFLEX and existing benchmarks demonstrate that traditional evaluations may\nunderestimate the inherent risks in models. This highlights the need for more\nstringent LLM evaluation benchmarks to guarantee safety and fairness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced interactions between users and models. These advancements concurrently\nunderscore the need for rigorous safety evaluations due to the manifestation of\nsocial biases, which can lead to harmful societal impacts. Despite these\nconcerns, existing benchmarks may overlook the intrinsic weaknesses of LLMs,\nwhich can generate biased responses even with simple adversarial instructions.\nTo address this critical gap, we introduce a new benchmark, Fairness Benchmark\nin LLM under Extreme Scenarios (FLEX), designed to test whether LLMs can\nsustain fairness even when exposed to prompts constructed to induce bias. To\nthoroughly evaluate the robustness of LLMs, we integrate prompts that amplify\npotential biases into the fairness assessment. Comparative experiments between\nFLEX and existing benchmarks demonstrate that traditional evaluations may\nunderestimate the inherent risks in models. This highlights the need for more\nstringent LLM evaluation benchmarks to guarantee safety and fairness."
                },
                "authors": [
                    {
                        "name": "Dahyun Jung"
                    },
                    {
                        "name": "Seungyoon Lee"
                    },
                    {
                        "name": "Hyeonseok Moon"
                    },
                    {
                        "name": "Chanjun Park"
                    },
                    {
                        "name": "Heuiseok Lim"
                    }
                ],
                "author_detail": {
                    "name": "Heuiseok Lim"
                },
                "author": "Heuiseok Lim",
                "arxiv_comment": "Accepted to NAACL 2025 findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19537v1",
                "updated": "2025-03-25T10:46:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    10,
                    46,
                    8,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T10:46:08Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    10,
                    46,
                    8,
                    1,
                    84,
                    0
                ],
                "title": "Agent-Initiated Interaction in Phone UI Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-Initiated Interaction in Phone UI Automation"
                },
                "summary": "Phone automation agents aim to autonomously perform a given natural-language\nuser request, such as scheduling appointments or booking a hotel. While much\nresearch effort has been devoted to screen understanding and action planning,\ncomplex tasks often necessitate user interaction for successful completion.\nAligning the agent with the user's expectations is crucial for building trust\nand enabling personalized experiences. This requires the agent to proactively\nengage the user when necessary, avoiding actions that violate their preferences\nwhile refraining from unnecessary questions where a default action is expected.\nWe argue that such subtle agent-initiated interaction with the user deserves\nfocused research attention.\n  To promote such research, this paper introduces a task formulation for\ndetecting the need for user interaction and generating appropriate messages. We\nthoroughly define the task, including aspects like interaction timing and the\nscope of the agent's autonomy. Using this definition, we derived annotation\nguidelines and created AndroidInteraction, a diverse dataset for the task,\nleveraging an existing UI automation dataset. We tested several text-based and\nmultimodal baseline models for the task, finding that it is very challenging\nfor current LLMs. We suggest that our task formulation, dataset, baseline\nmodels and analysis will be valuable for future UI automation research,\nspecifically in addressing this crucial yet often overlooked aspect of\nagent-initiated interaction. This work provides a needed foundation to allow\npersonalized agents to properly engage the user when needed, within the context\nof phone UI automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phone automation agents aim to autonomously perform a given natural-language\nuser request, such as scheduling appointments or booking a hotel. While much\nresearch effort has been devoted to screen understanding and action planning,\ncomplex tasks often necessitate user interaction for successful completion.\nAligning the agent with the user's expectations is crucial for building trust\nand enabling personalized experiences. This requires the agent to proactively\nengage the user when necessary, avoiding actions that violate their preferences\nwhile refraining from unnecessary questions where a default action is expected.\nWe argue that such subtle agent-initiated interaction with the user deserves\nfocused research attention.\n  To promote such research, this paper introduces a task formulation for\ndetecting the need for user interaction and generating appropriate messages. We\nthoroughly define the task, including aspects like interaction timing and the\nscope of the agent's autonomy. Using this definition, we derived annotation\nguidelines and created AndroidInteraction, a diverse dataset for the task,\nleveraging an existing UI automation dataset. We tested several text-based and\nmultimodal baseline models for the task, finding that it is very challenging\nfor current LLMs. We suggest that our task formulation, dataset, baseline\nmodels and analysis will be valuable for future UI automation research,\nspecifically in addressing this crucial yet often overlooked aspect of\nagent-initiated interaction. This work provides a needed foundation to allow\npersonalized agents to properly engage the user when needed, within the context\nof phone UI automation."
                },
                "authors": [
                    {
                        "name": "Noam Kahlon"
                    },
                    {
                        "name": "Guy Rom"
                    },
                    {
                        "name": "Anatoly Efros"
                    },
                    {
                        "name": "Filippo Galgani"
                    },
                    {
                        "name": "Omri Berkovitch"
                    },
                    {
                        "name": "Sapir Caduri"
                    },
                    {
                        "name": "William E. Bishop"
                    },
                    {
                        "name": "Oriana Riva"
                    },
                    {
                        "name": "Ido Dagan"
                    }
                ],
                "author_detail": {
                    "name": "Ido Dagan"
                },
                "author": "Ido Dagan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19535v1",
                "updated": "2025-03-25T10:42:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    10,
                    42,
                    19,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T10:42:19Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    10,
                    42,
                    19,
                    1,
                    84,
                    0
                ],
                "title": "Optimizing train dispatching for the Union Pacific Railroad",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing train dispatching for the Union Pacific Railroad"
                },
                "summary": "Union Pacific (UP) is one of the largest transportation companies in the\nworld, with over 50.000 kms of rail network covering 23 states in the United\nStates. In 2017 Union Pacific embarked on a project that within 5 years would\nlead it to become the only rail operator in the world equipped with a\ntechnology capable of fully automating the real-time management and\noptimization of train traffic. In 2021 the main milestone of such project has\nbeen reached with the first deployment of the automated dispatching system we\npresent here. To attack such large and complex problem, we decomposed it into\ndistinct but interrelated functional components, and developed optimization\nmodels and methods to handle such components. The models communicate with each\nother through variables and constraints, and by a careful timing of\ninvocations. In this paper we give an overview of the overall approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Union Pacific (UP) is one of the largest transportation companies in the\nworld, with over 50.000 kms of rail network covering 23 states in the United\nStates. In 2017 Union Pacific embarked on a project that within 5 years would\nlead it to become the only rail operator in the world equipped with a\ntechnology capable of fully automating the real-time management and\noptimization of train traffic. In 2021 the main milestone of such project has\nbeen reached with the first deployment of the automated dispatching system we\npresent here. To attack such large and complex problem, we decomposed it into\ndistinct but interrelated functional components, and developed optimization\nmodels and methods to handle such components. The models communicate with each\nother through variables and constraints, and by a careful timing of\ninvocations. In this paper we give an overview of the overall approach."
                },
                "authors": [
                    {
                        "name": "Maurizio Boccia"
                    },
                    {
                        "name": "Veronica Dal Sasso"
                    },
                    {
                        "name": "Leonardo Lamorgese"
                    },
                    {
                        "name": "Carlo Mannino"
                    },
                    {
                        "name": "Paolo Ventura"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Ventura"
                },
                "author": "Paolo Ventura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19530v1",
                "updated": "2025-03-25T10:36:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    10,
                    36,
                    27,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T10:36:27Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    10,
                    36,
                    27,
                    1,
                    84,
                    0
                ],
                "title": "VectorFit : Adaptive Singular & Bias Vector Fine-Tuning of Pre-trained\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VectorFit : Adaptive Singular & Bias Vector Fine-Tuning of Pre-trained\n  Foundation Models"
                },
                "summary": "Popular PEFT methods achieve parameter efficiency by assuming that\nincremental weight updates are inherently low-rank, which often leads to a\nperformance gap compared to full fine-tuning. While recent methods have\nattempted to address this limitation, they typically lack sufficient parameter\nand memory efficiency. We propose VectorFit, an effective and easily deployable\napproach that adaptively trains the singular vectors and biases of pre-trained\nweight matrices. We demonstrate that the utilization of structural and\ntransformational characteristics of pre-trained weights enables high-rank\nupdates comparable to those of full fine-tuning. As a result, VectorFit\nachieves superior performance with 9X less trainable parameters compared to\nstate-of-the-art PEFT methods. Through extensive experiments over 17 datasets\nspanning diverse language and vision tasks such as natural language\nunderstanding and generation, question answering, image classification, and\nimage generation, we exhibit that VectorFit consistently outperforms baselines,\neven in extremely low-budget scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Popular PEFT methods achieve parameter efficiency by assuming that\nincremental weight updates are inherently low-rank, which often leads to a\nperformance gap compared to full fine-tuning. While recent methods have\nattempted to address this limitation, they typically lack sufficient parameter\nand memory efficiency. We propose VectorFit, an effective and easily deployable\napproach that adaptively trains the singular vectors and biases of pre-trained\nweight matrices. We demonstrate that the utilization of structural and\ntransformational characteristics of pre-trained weights enables high-rank\nupdates comparable to those of full fine-tuning. As a result, VectorFit\nachieves superior performance with 9X less trainable parameters compared to\nstate-of-the-art PEFT methods. Through extensive experiments over 17 datasets\nspanning diverse language and vision tasks such as natural language\nunderstanding and generation, question answering, image classification, and\nimage generation, we exhibit that VectorFit consistently outperforms baselines,\neven in extremely low-budget scenarios."
                },
                "authors": [
                    {
                        "name": "Suhas G Hegde"
                    },
                    {
                        "name": "Shilpy Kaur"
                    },
                    {
                        "name": "Aruna Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Aruna Tiwari"
                },
                "author": "Aruna Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12355v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12355v2",
                "updated": "2025-03-25T10:31:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    10,
                    31,
                    35,
                    1,
                    84,
                    0
                ],
                "published": "2024-11-19T09:16:54Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    9,
                    16,
                    54,
                    1,
                    324,
                    0
                ],
                "title": "DynFocus: Dynamic Cooperative Network Empowers LLMs with Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynFocus: Dynamic Cooperative Network Empowers LLMs with Video\n  Understanding"
                },
                "summary": "The challenge in LLM-based video understanding lies in preserving visual and\nsemantic information in long videos while maintaining a memory-affordable token\ncount. However, redundancy and correspondence in videos have hindered the\nperformance potential of existing methods. Through statistical learning on\ncurrent datasets, we observe that redundancy occurs in both repeated and\nanswer-irrelevant frames, and the corresponding frames vary with different\nquestions. This suggests the possibility of adopting dynamic encoding to\nbalance detailed video information preservation with token budget reduction. To\nthis end, we propose a dynamic cooperative network, DynFocus, for\nmemory-efficient video encoding in this paper. Specifically, i) a Dynamic Event\nPrototype Estimation (DPE) module to dynamically select meaningful frames for\nquestion answering; (ii) a Compact Cooperative Encoding (CCE) module that\nencodes meaningful frames with detailed visual appearance and the remaining\nframes with sketchy perception separately. We evaluate our method on five\npublicly available benchmarks, and experimental results consistently\ndemonstrate that our method achieves competitive performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The challenge in LLM-based video understanding lies in preserving visual and\nsemantic information in long videos while maintaining a memory-affordable token\ncount. However, redundancy and correspondence in videos have hindered the\nperformance potential of existing methods. Through statistical learning on\ncurrent datasets, we observe that redundancy occurs in both repeated and\nanswer-irrelevant frames, and the corresponding frames vary with different\nquestions. This suggests the possibility of adopting dynamic encoding to\nbalance detailed video information preservation with token budget reduction. To\nthis end, we propose a dynamic cooperative network, DynFocus, for\nmemory-efficient video encoding in this paper. Specifically, i) a Dynamic Event\nPrototype Estimation (DPE) module to dynamically select meaningful frames for\nquestion answering; (ii) a Compact Cooperative Encoding (CCE) module that\nencodes meaningful frames with detailed visual appearance and the remaining\nframes with sketchy perception separately. We evaluate our method on five\npublicly available benchmarks, and experimental results consistently\ndemonstrate that our method achieves competitive performance."
                },
                "authors": [
                    {
                        "name": "Yudong Han"
                    },
                    {
                        "name": "Qingpei Guo"
                    },
                    {
                        "name": "Liyuan Pan"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Yu Guan"
                    },
                    {
                        "name": "Ming Yang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Yang"
                },
                "author": "Ming Yang",
                "arxiv_comment": "Accepted by CVPR 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12355v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12355v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16728v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16728v2",
                "updated": "2025-03-25T10:03:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    10,
                    3,
                    25,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-20T22:12:08Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    22,
                    12,
                    8,
                    3,
                    79,
                    0
                ],
                "title": "Natural Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Generation"
                },
                "summary": "This article provides a brief overview of the field of Natural Language\nGeneration. The term Natural Language Generation (NLG), in its broadest\ndefinition, refers to the study of systems that verbalize some form of\ninformation through natural language. That information could be stored in a\nlarge database or knowledge graph (in data-to-text applications), but NLG\nresearchers may also study summarisation (text-to-text) or image captioning\n(image-to-text), for example. As a subfield of Natural Language Processing, NLG\nis closely related to other sub-disciplines such as Machine Translation (MT)\nand Dialog Systems. Some NLG researchers exclude MT from their definition of\nthe field, since there is no content selection involved where the system has to\ndetermine what to say. Conversely, dialog systems do not typically fall under\nthe header of Natural Language Generation since NLG is just one component of\ndialog systems (the others being Natural Language Understanding and Dialog\nManagement). However, with the rise of Large Language Models (LLMs), different\nsubfields of Natural Language Processing have converged on similar\nmethodologies for the production of natural language and the evaluation of\nautomatically generated text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article provides a brief overview of the field of Natural Language\nGeneration. The term Natural Language Generation (NLG), in its broadest\ndefinition, refers to the study of systems that verbalize some form of\ninformation through natural language. That information could be stored in a\nlarge database or knowledge graph (in data-to-text applications), but NLG\nresearchers may also study summarisation (text-to-text) or image captioning\n(image-to-text), for example. As a subfield of Natural Language Processing, NLG\nis closely related to other sub-disciplines such as Machine Translation (MT)\nand Dialog Systems. Some NLG researchers exclude MT from their definition of\nthe field, since there is no content selection involved where the system has to\ndetermine what to say. Conversely, dialog systems do not typically fall under\nthe header of Natural Language Generation since NLG is just one component of\ndialog systems (the others being Natural Language Understanding and Dialog\nManagement). However, with the rise of Large Language Models (LLMs), different\nsubfields of Natural Language Processing have converged on similar\nmethodologies for the production of natural language and the evaluation of\nautomatically generated text."
                },
                "authors": [
                    {
                        "name": "Emiel van Miltenburg"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "arxiv_comment": "3 pages + references. Submitted for publication in the Encyclopedia\n  of Language & Linguistics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16728v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16728v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07752v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07752v3",
                "updated": "2025-03-25T09:46:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    9,
                    46,
                    2,
                    1,
                    84,
                    0
                ],
                "published": "2024-10-10T09:28:36Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    9,
                    28,
                    36,
                    3,
                    284,
                    0
                ],
                "title": "Lost in Time: A New Temporal Benchmark for VideoLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Time: A New Temporal Benchmark for VideoLLMs"
                },
                "summary": "Large language models have demonstrated impressive performance when\nintegrated with vision models even enabling video understanding. However,\nevaluating video models presents its own unique challenges, for which several\nbenchmarks have been proposed. In this paper, we show that the currently most\nused video-language benchmarks can be solved without requiring much temporal\nreasoning. We identified three main issues in existing datasets: (i) static\ninformation from single frames is often sufficient to solve the tasks (ii) the\ntext of the questions and candidate answers is overly informative, allowing\nmodels to answer correctly without relying on any visual input (iii) world\nknowledge alone can answer many of the questions, making the benchmarks a test\nof knowledge replication rather than video reasoning. In addition, we found\nthat open-ended question-answering benchmarks for video understanding suffer\nfrom similar issues while the automatic evaluation process with LLMs is\nunreliable, making it an unsuitable alternative. As a solution, we propose\nTVBench, a novel open-source video multiple-choice question-answering\nbenchmark, and demonstrate through extensive evaluations that it requires a\nhigh level of temporal understanding. Surprisingly, we find that most recent\nstate-of-the-art video-language models perform similarly to random performance\non TVBench, with only a few models such as Qwen2-VL, and Tarsier clearly\nsurpassing this baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated impressive performance when\nintegrated with vision models even enabling video understanding. However,\nevaluating video models presents its own unique challenges, for which several\nbenchmarks have been proposed. In this paper, we show that the currently most\nused video-language benchmarks can be solved without requiring much temporal\nreasoning. We identified three main issues in existing datasets: (i) static\ninformation from single frames is often sufficient to solve the tasks (ii) the\ntext of the questions and candidate answers is overly informative, allowing\nmodels to answer correctly without relying on any visual input (iii) world\nknowledge alone can answer many of the questions, making the benchmarks a test\nof knowledge replication rather than video reasoning. In addition, we found\nthat open-ended question-answering benchmarks for video understanding suffer\nfrom similar issues while the automatic evaluation process with LLMs is\nunreliable, making it an unsuitable alternative. As a solution, we propose\nTVBench, a novel open-source video multiple-choice question-answering\nbenchmark, and demonstrate through extensive evaluations that it requires a\nhigh level of temporal understanding. Surprisingly, we find that most recent\nstate-of-the-art video-language models perform similarly to random performance\non TVBench, with only a few models such as Qwen2-VL, and Tarsier clearly\nsurpassing this baseline."
                },
                "authors": [
                    {
                        "name": "Daniel Cores"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "Manuel Mucientes"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07752v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07752v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00171v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00171v3",
                "updated": "2025-03-25T09:43:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    9,
                    43,
                    25,
                    1,
                    84,
                    0
                ],
                "published": "2024-11-29T17:36:03Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    36,
                    3,
                    4,
                    334,
                    0
                ],
                "title": "RoboMatrix: A Skill-centric Hierarchical Framework for Scalable Robot\n  Task Planning and Execution in Open-World",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboMatrix: A Skill-centric Hierarchical Framework for Scalable Robot\n  Task Planning and Execution in Open-World"
                },
                "summary": "Existing robot policies predominantly adopt the task-centric approach,\nrequiring end-to-end task data collection. This results in limited\ngeneralization to new tasks and difficulties in pinpointing errors within\nlong-horizon, multi-stage tasks. To address this, we propose RoboMatrix, a\nskill-centric hierarchical framework designed for scalable robot task planning\nand execution in open-world environments. RoboMatrix extracts general\nmeta-skills from diverse complex tasks, enabling the completion of unseen tasks\nthrough skill composition. Its architecture consists of a high-level scheduling\nlayer that utilizes large language models (LLMs) for task decomposition, an\nintermediate skill layer housing meta-skill models, and a low-level hardware\nlayer for robot control. A key innovation of our work is the introduction of\nthe first unified vision-language-action (VLA) model capable of seamlessly\nintegrating both movement and manipulation within one model. This is achieved\nby combining vision and language prompts to generate discrete actions.\nExperimental results demonstrate that RoboMatrix achieves a 50% higher success\nrate than task-centric baselines when applied to unseen objects, scenes, and\ntasks. To advance open-world robotics research, we will open-source code,\nhardware designs, model weights, and datasets at\nhttps://github.com/WayneMao/RoboMatrix.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing robot policies predominantly adopt the task-centric approach,\nrequiring end-to-end task data collection. This results in limited\ngeneralization to new tasks and difficulties in pinpointing errors within\nlong-horizon, multi-stage tasks. To address this, we propose RoboMatrix, a\nskill-centric hierarchical framework designed for scalable robot task planning\nand execution in open-world environments. RoboMatrix extracts general\nmeta-skills from diverse complex tasks, enabling the completion of unseen tasks\nthrough skill composition. Its architecture consists of a high-level scheduling\nlayer that utilizes large language models (LLMs) for task decomposition, an\nintermediate skill layer housing meta-skill models, and a low-level hardware\nlayer for robot control. A key innovation of our work is the introduction of\nthe first unified vision-language-action (VLA) model capable of seamlessly\nintegrating both movement and manipulation within one model. This is achieved\nby combining vision and language prompts to generate discrete actions.\nExperimental results demonstrate that RoboMatrix achieves a 50% higher success\nrate than task-centric baselines when applied to unseen objects, scenes, and\ntasks. To advance open-world robotics research, we will open-source code,\nhardware designs, model weights, and datasets at\nhttps://github.com/WayneMao/RoboMatrix."
                },
                "authors": [
                    {
                        "name": "Weixin Mao"
                    },
                    {
                        "name": "Weiheng Zhong"
                    },
                    {
                        "name": "Zhou Jiang"
                    },
                    {
                        "name": "Dong Fang"
                    },
                    {
                        "name": "Zhongyue Zhang"
                    },
                    {
                        "name": "Zihan Lan"
                    },
                    {
                        "name": "Haosheng Li"
                    },
                    {
                        "name": "Fan Jia"
                    },
                    {
                        "name": "Tiancai Wang"
                    },
                    {
                        "name": "Haoqiang Fan"
                    },
                    {
                        "name": "Osamu Yoshie"
                    }
                ],
                "author_detail": {
                    "name": "Osamu Yoshie"
                },
                "author": "Osamu Yoshie",
                "arxiv_comment": "17 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00171v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00171v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00088v2",
                "updated": "2025-03-25T09:27:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    9,
                    27,
                    16,
                    1,
                    84,
                    0
                ],
                "published": "2024-06-25T08:38:38Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    8,
                    38,
                    38,
                    1,
                    177,
                    0
                ],
                "title": "T-MAC: CPU Renaissance via Table Lookup for Low-Bit LLM Deployment on\n  Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T-MAC: CPU Renaissance via Table Lookup for Low-Bit LLM Deployment on\n  Edge"
                },
                "summary": "The deployment of Large Language Models (LLMs) on edge devices is\nincreasingly important to enhance on-device intelligence. Weight quantization\nis crucial for reducing the memory footprint of LLMs on devices. However,\nlow-bit LLMs necessitate mixed precision matrix multiplication (mpGEMM) of low\nprecision weights and high precision activations during inference. Existing\nsystems, lacking native support for mpGEMM, resort to dequantize weights for\nhigh precision computation. Such an indirect way can lead to a significant\ninference overhead.\n  In this paper, we introduce T-MAC, an innovative lookup table(LUT)-based\nmethod designed for efficient low-bit LLM (i.e., weight-quantized LLM)\ninference on CPUs. T-MAC directly supports mpGEMM without dequantization, while\nsimultaneously eliminating multiplications and reducing additions required.\nSpecifically, T-MAC transforms the traditional data-type-centric multiplication\nto bit-wise table lookup, and enables a unified and scalable mpGEMM solution.\n  Our LUT-based kernels scale linearly to the weight bit-width. Evaluated on\nlow-bit Llama and BitNet models, T-MAC demonstrates up to 4x increase in\nthroughput and 70% reduction in energy consumption compared to llama.cpp. For\nBitNet-b1.58-3B, T-MAC delivers a token generation throughput of 30 tokens/s\nwith a single core and 71 tokens/s with eight cores on M2-Ultra, and 11\ntokens/s on lower-end devices like Raspberry Pi 5, which significantly exceeds\nthe adult average reading speed. T-MAC with LUT-based computing paradigm, paves\nthe way for the practical deployment of low-bit LLMs on resource-constrained\nedge devices without compromising computational efficiency. The system is\nopen-sourced at https://github.com/microsoft/T-MAC .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Large Language Models (LLMs) on edge devices is\nincreasingly important to enhance on-device intelligence. Weight quantization\nis crucial for reducing the memory footprint of LLMs on devices. However,\nlow-bit LLMs necessitate mixed precision matrix multiplication (mpGEMM) of low\nprecision weights and high precision activations during inference. Existing\nsystems, lacking native support for mpGEMM, resort to dequantize weights for\nhigh precision computation. Such an indirect way can lead to a significant\ninference overhead.\n  In this paper, we introduce T-MAC, an innovative lookup table(LUT)-based\nmethod designed for efficient low-bit LLM (i.e., weight-quantized LLM)\ninference on CPUs. T-MAC directly supports mpGEMM without dequantization, while\nsimultaneously eliminating multiplications and reducing additions required.\nSpecifically, T-MAC transforms the traditional data-type-centric multiplication\nto bit-wise table lookup, and enables a unified and scalable mpGEMM solution.\n  Our LUT-based kernels scale linearly to the weight bit-width. Evaluated on\nlow-bit Llama and BitNet models, T-MAC demonstrates up to 4x increase in\nthroughput and 70% reduction in energy consumption compared to llama.cpp. For\nBitNet-b1.58-3B, T-MAC delivers a token generation throughput of 30 tokens/s\nwith a single core and 71 tokens/s with eight cores on M2-Ultra, and 11\ntokens/s on lower-end devices like Raspberry Pi 5, which significantly exceeds\nthe adult average reading speed. T-MAC with LUT-based computing paradigm, paves\nthe way for the practical deployment of low-bit LLMs on resource-constrained\nedge devices without compromising computational efficiency. The system is\nopen-sourced at https://github.com/microsoft/T-MAC ."
                },
                "authors": [
                    {
                        "name": "Jianyu Wei"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Lingxiao Ma"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Yanyong Zhang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "arxiv_doi": "10.1145/3689031.3696099",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689031.3696099",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.00088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "EuroSys 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11342v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11342v2",
                "updated": "2025-03-25T09:27:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    9,
                    27,
                    3,
                    1,
                    84,
                    0
                ],
                "published": "2024-11-18T07:23:55Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    7,
                    23,
                    55,
                    0,
                    323,
                    0
                ],
                "title": "Multi-hop Differential Topology based Algorithms for Resilient Network\n  of UAV Swarm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-hop Differential Topology based Algorithms for Resilient Network\n  of UAV Swarm"
                },
                "summary": "Unmanned aerial vehicle (UAV) swarm networks face severe challenges of\ncommunication network split (CNS) issues caused by massive damage in hostile\nenvironments. In this paper, we propose a new paradigm to restore network\nconnectivity by repositioning remaining UAVs based on damage information within\nlocal topologies. Particularly, the locations of destroyed UAVs distributed in\ngaps between disconnected sub-nets are considered for recovery trajectory\nplanning. Specifically, we construct the multi-hop differential sub-graph\n(MDSG) to represent local damage-varying topologies. Based on this, we develop\ntwo distinct algorithms to address CNS issues. The first approach leverages an\nartificial potential field algorithm to calculate the recovery velocities via\nMDSG, enabling simple deployment on low-intelligence UAVs. In the second\napproach, we design an MDSG-based graph convolution framework to find the\nrecovery topology for high-intelligence swarms. As per the unique topology of\nMDSG, we propose a novel bipartite graph convolution operation, enhanced with a\nbatch-processing mechanism to improve graph convolution efficiency. Simulation\nresults show that the proposed algorithms expedite the recovery with\nsignificant margin while improving the spatial coverage and topology degree\nuniformity after recovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned aerial vehicle (UAV) swarm networks face severe challenges of\ncommunication network split (CNS) issues caused by massive damage in hostile\nenvironments. In this paper, we propose a new paradigm to restore network\nconnectivity by repositioning remaining UAVs based on damage information within\nlocal topologies. Particularly, the locations of destroyed UAVs distributed in\ngaps between disconnected sub-nets are considered for recovery trajectory\nplanning. Specifically, we construct the multi-hop differential sub-graph\n(MDSG) to represent local damage-varying topologies. Based on this, we develop\ntwo distinct algorithms to address CNS issues. The first approach leverages an\nartificial potential field algorithm to calculate the recovery velocities via\nMDSG, enabling simple deployment on low-intelligence UAVs. In the second\napproach, we design an MDSG-based graph convolution framework to find the\nrecovery topology for high-intelligence swarms. As per the unique topology of\nMDSG, we propose a novel bipartite graph convolution operation, enhanced with a\nbatch-processing mechanism to improve graph convolution efficiency. Simulation\nresults show that the proposed algorithms expedite the recovery with\nsignificant margin while improving the spatial coverage and topology degree\nuniformity after recovery."
                },
                "authors": [
                    {
                        "name": "Huan Lin"
                    },
                    {
                        "name": "Lianghui Ding"
                    }
                ],
                "author_detail": {
                    "name": "Lianghui Ding"
                },
                "author": "Lianghui Ding",
                "arxiv_comment": "16 pages, 12figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11342v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11342v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19482v1",
                "updated": "2025-03-25T09:18:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    9,
                    18,
                    27,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T09:18:27Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    9,
                    18,
                    27,
                    1,
                    84,
                    0
                ],
                "title": "KSHSeek: Data-Driven Approaches to Mitigating and Detecting\n  Knowledge-Shortcut Hallucinations in Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KSHSeek: Data-Driven Approaches to Mitigating and Detecting\n  Knowledge-Shortcut Hallucinations in Generative Models"
                },
                "summary": "The emergence of large language models (LLMs) has significantly advanced the\ndevelopment of natural language processing (NLP), especially in text generation\ntasks like question answering. However, model hallucinations remain a major\nchallenge in natural language generation (NLG) tasks due to their complex\ncauses. We systematically expand on the causes of factual hallucinations from\nthe perspective of knowledge shortcuts, analyzing hallucinations arising from\ncorrect and defect-free data and demonstrating that knowledge-shortcut\nhallucinations are prevalent in generative models. To mitigate this issue, we\npropose a high similarity pruning algorithm at the data preprocessing level to\nreduce spurious correlations in the data. Additionally, we design a specific\ndetection method for knowledge-shortcut hallucinations to evaluate the\neffectiveness of our mitigation strategy. Experimental results show that our\napproach effectively reduces knowledge-shortcut hallucinations, particularly in\nfine-tuning tasks, without negatively impacting model performance in question\nanswering. This work introduces a new paradigm for mitigating specific\nhallucination issues in generative models, enhancing their robustness and\nreliability in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) has significantly advanced the\ndevelopment of natural language processing (NLP), especially in text generation\ntasks like question answering. However, model hallucinations remain a major\nchallenge in natural language generation (NLG) tasks due to their complex\ncauses. We systematically expand on the causes of factual hallucinations from\nthe perspective of knowledge shortcuts, analyzing hallucinations arising from\ncorrect and defect-free data and demonstrating that knowledge-shortcut\nhallucinations are prevalent in generative models. To mitigate this issue, we\npropose a high similarity pruning algorithm at the data preprocessing level to\nreduce spurious correlations in the data. Additionally, we design a specific\ndetection method for knowledge-shortcut hallucinations to evaluate the\neffectiveness of our mitigation strategy. Experimental results show that our\napproach effectively reduces knowledge-shortcut hallucinations, particularly in\nfine-tuning tasks, without negatively impacting model performance in question\nanswering. This work introduces a new paradigm for mitigating specific\nhallucination issues in generative models, enhancing their robustness and\nreliability in real-world applications."
                },
                "authors": [
                    {
                        "name": "Zhiwei Wang"
                    },
                    {
                        "name": "Zhongxin Liu"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Meng Xu"
                    },
                    {
                        "name": "Yuqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yuqing Zhang"
                },
                "author": "Yuqing Zhang",
                "arxiv_comment": "16 pages, 34 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04832v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04832v5",
                "updated": "2025-03-25T09:08:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    9,
                    8,
                    9,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-05T10:59:32Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    10,
                    59,
                    32,
                    2,
                    64,
                    0
                ],
                "title": "Lightweight Embedded FPGA Deployment of Learned Image Compression with\n  Knowledge Distillation and Hybrid Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Embedded FPGA Deployment of Learned Image Compression with\n  Knowledge Distillation and Hybrid Quantization"
                },
                "summary": "Learnable Image Compression (LIC) has shown the potential to outperform\nstandardized video codecs in RD efficiency, prompting the research for\nhardware-friendly implementations. Most existing LIC hardware implementations\nprioritize latency to RD-efficiency and through an extensive exploration of the\nhardware design space. We present a novel design paradigm where the burden of\ntuning the design for a specific hardware platform is shifted towards model\ndimensioning and without compromising on RD-efficiency. First, we design a\nframework for distilling a leaner student LIC model from a reference teacher:\nby tuning a single model hyperparameters, we can meet the constraints of\ndifferent hardware platforms without a complex hardware design exploration.\nSecond, we propose a hardware-friendly implementation of the Generalized\nDivisive Normalization - GDN activation that preserves RD efficiency even post\nparameter quantization. Third, we design a pipelined FPGA configuration which\ntakes full advantage of available FPGA resources by leveraging parallel\nprocessing and optimizing resource allocation. Our experiments with a state of\nthe art LIC model show that we outperform all existing FPGA implementations\nwhile performing very close to the original model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learnable Image Compression (LIC) has shown the potential to outperform\nstandardized video codecs in RD efficiency, prompting the research for\nhardware-friendly implementations. Most existing LIC hardware implementations\nprioritize latency to RD-efficiency and through an extensive exploration of the\nhardware design space. We present a novel design paradigm where the burden of\ntuning the design for a specific hardware platform is shifted towards model\ndimensioning and without compromising on RD-efficiency. First, we design a\nframework for distilling a leaner student LIC model from a reference teacher:\nby tuning a single model hyperparameters, we can meet the constraints of\ndifferent hardware platforms without a complex hardware design exploration.\nSecond, we propose a hardware-friendly implementation of the Generalized\nDivisive Normalization - GDN activation that preserves RD efficiency even post\nparameter quantization. Third, we design a pipelined FPGA configuration which\ntakes full advantage of available FPGA resources by leveraging parallel\nprocessing and optimizing resource allocation. Our experiments with a state of\nthe art LIC model show that we outperform all existing FPGA implementations\nwhile performing very close to the original model."
                },
                "authors": [
                    {
                        "name": "Alaa Mazouz"
                    },
                    {
                        "name": "Sumanta Chaudhuri"
                    },
                    {
                        "name": "Marco Cagnanzzo"
                    },
                    {
                        "name": "Mihai Mitrea"
                    },
                    {
                        "name": "Enzo Tartaglione"
                    },
                    {
                        "name": "Attilio Fiandrotti"
                    }
                ],
                "author_detail": {
                    "name": "Attilio Fiandrotti"
                },
                "author": "Attilio Fiandrotti",
                "arxiv_comment": "1. Submitted to IEEE Transactions on Circuits and Systems for Video\n  Technology in March 2025. 2. Corrected numerous mistakes from previous\n  versions in results, citations and metrics numbers in figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04832v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04832v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13652v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13652v2",
                "updated": "2025-03-25T09:06:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    9,
                    6,
                    8,
                    1,
                    84,
                    0
                ],
                "published": "2024-12-18T09:31:06Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    31,
                    6,
                    2,
                    353,
                    0
                ],
                "title": "RelationField: Relate Anything in Radiance Fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RelationField: Relate Anything in Radiance Fields"
                },
                "summary": "Neural radiance fields are an emerging 3D scene representation and recently\neven been extended to learn features for scene understanding by distilling\nopen-vocabulary features from vision-language models. However, current method\nprimarily focus on object-centric representations, supporting object\nsegmentation or detection, while understanding semantic relationships between\nobjects remains largely unexplored. To address this gap, we propose\nRelationField, the first method to extract inter-object relationships directly\nfrom neural radiance fields. RelationField represents relationships between\nobjects as pairs of rays within a neural radiance field, effectively extending\nits formulation to include implicit relationship queries. To teach\nRelationField complex, open-vocabulary relationships, relationship knowledge is\ndistilled from multi-modal LLMs. To evaluate RelationField, we solve\nopen-vocabulary 3D scene graph generation tasks and relationship-guided\ninstance segmentation, achieving state-of-the-art performance in both tasks.\nSee the project website at https://relationfield.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural radiance fields are an emerging 3D scene representation and recently\neven been extended to learn features for scene understanding by distilling\nopen-vocabulary features from vision-language models. However, current method\nprimarily focus on object-centric representations, supporting object\nsegmentation or detection, while understanding semantic relationships between\nobjects remains largely unexplored. To address this gap, we propose\nRelationField, the first method to extract inter-object relationships directly\nfrom neural radiance fields. RelationField represents relationships between\nobjects as pairs of rays within a neural radiance field, effectively extending\nits formulation to include implicit relationship queries. To teach\nRelationField complex, open-vocabulary relationships, relationship knowledge is\ndistilled from multi-modal LLMs. To evaluate RelationField, we solve\nopen-vocabulary 3D scene graph generation tasks and relationship-guided\ninstance segmentation, achieving state-of-the-art performance in both tasks.\nSee the project website at https://relationfield.github.io."
                },
                "authors": [
                    {
                        "name": "Sebastian Koch"
                    },
                    {
                        "name": "Johanna Wald"
                    },
                    {
                        "name": "Mirco Colosi"
                    },
                    {
                        "name": "Narunas Vaskevicius"
                    },
                    {
                        "name": "Pedro Hermosilla"
                    },
                    {
                        "name": "Federico Tombari"
                    },
                    {
                        "name": "Timo Ropinski"
                    }
                ],
                "author_detail": {
                    "name": "Timo Ropinski"
                },
                "author": "Timo Ropinski",
                "arxiv_comment": "CVPR 2025. Project page: https://relationfield.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13652v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13652v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19470v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19470v1",
                "updated": "2025-03-25T09:00:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    9,
                    0,
                    58,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T09:00:58Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    9,
                    0,
                    58,
                    1,
                    84,
                    0
                ],
                "title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement\n  Learning"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities in reasoning,\nexemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating\nreasoning with external search processes remains challenging, especially for\ncomplex multi-hop questions requiring multiple retrieval steps. We propose\nReSearch, a novel framework that trains LLMs to Reason with Search via\nreinforcement learning without using any supervised data on reasoning steps.\nOur approach treats search operations as integral components of the reasoning\nchain, where when and how to perform searches is guided by text-based thinking,\nand search results subsequently influence further reasoning. We train ReSearch\non Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct\nextensive experiments. Despite being trained on only one dataset, our models\ndemonstrate strong generalizability across various benchmarks. Analysis reveals\nthat ReSearch naturally elicits advanced reasoning capabilities such as\nreflection and self-correction during the reinforcement learning process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities in reasoning,\nexemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating\nreasoning with external search processes remains challenging, especially for\ncomplex multi-hop questions requiring multiple retrieval steps. We propose\nReSearch, a novel framework that trains LLMs to Reason with Search via\nreinforcement learning without using any supervised data on reasoning steps.\nOur approach treats search operations as integral components of the reasoning\nchain, where when and how to perform searches is guided by text-based thinking,\nand search results subsequently influence further reasoning. We train ReSearch\non Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct\nextensive experiments. Despite being trained on only one dataset, our models\ndemonstrate strong generalizability across various benchmarks. Analysis reveals\nthat ReSearch naturally elicits advanced reasoning capabilities such as\nreflection and self-correction during the reinforcement learning process."
                },
                "authors": [
                    {
                        "name": "Mingyang Chen"
                    },
                    {
                        "name": "Tianpeng Li"
                    },
                    {
                        "name": "Haoze Sun"
                    },
                    {
                        "name": "Yijie Zhou"
                    },
                    {
                        "name": "Chenzheng Zhu"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Zenan Zhou"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Haofen Wang"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    },
                    {
                        "name": "Wen Zhang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19470v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19470v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19455v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19455v1",
                "updated": "2025-03-25T08:43:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    8,
                    43,
                    8,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T08:43:08Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    8,
                    43,
                    8,
                    1,
                    84,
                    0
                ],
                "title": "Data-centric Federated Graph Learning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-centric Federated Graph Learning with Large Language Models"
                },
                "summary": "In federated graph learning (FGL), a complete graph is divided into multiple\nsubgraphs stored in each client due to privacy concerns, and all clients\njointly train a global graph model by only transmitting model parameters. A\npain point of FGL is the heterogeneity problem, where nodes or structures\npresent non-IID properties among clients (e.g., different node label\ndistributions), dramatically undermining the convergence and performance of\nFGL. To address this, existing efforts focus on design strategies at the model\nlevel, i.e., they design models to extract common knowledge to mitigate\nheterogeneity. However, these model-level strategies fail to fundamentally\naddress the heterogeneity problem as the model needs to be designed from\nscratch when transferring to other tasks. Motivated by large language models\n(LLMs) having achieved remarkable success, we aim to utilize LLMs to fully\nunderstand and augment local text-attributed graphs, to address data\nheterogeneity at the data level. In this paper, we propose a general framework\nLLM4FGL that innovatively decomposes the task of LLM for FGL into two sub-tasks\ntheoretically. Specifically, for each client, it first utilizes the LLM to\ngenerate missing neighbors and then infers connections between generated nodes\nand raw nodes. To improve the quality of generated nodes, we design a novel\nfederated generation-and-reflection mechanism for LLMs, without the need to\nmodify the parameters of the LLM but relying solely on the collective feedback\nfrom all clients. After neighbor generation, all the clients utilize a\npre-trained edge predictor to infer the missing edges. Furthermore, our\nframework can seamlessly integrate as a plug-in with existing FGL methods.\nExperiments on three real-world datasets demonstrate the superiority of our\nmethod compared to advanced baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In federated graph learning (FGL), a complete graph is divided into multiple\nsubgraphs stored in each client due to privacy concerns, and all clients\njointly train a global graph model by only transmitting model parameters. A\npain point of FGL is the heterogeneity problem, where nodes or structures\npresent non-IID properties among clients (e.g., different node label\ndistributions), dramatically undermining the convergence and performance of\nFGL. To address this, existing efforts focus on design strategies at the model\nlevel, i.e., they design models to extract common knowledge to mitigate\nheterogeneity. However, these model-level strategies fail to fundamentally\naddress the heterogeneity problem as the model needs to be designed from\nscratch when transferring to other tasks. Motivated by large language models\n(LLMs) having achieved remarkable success, we aim to utilize LLMs to fully\nunderstand and augment local text-attributed graphs, to address data\nheterogeneity at the data level. In this paper, we propose a general framework\nLLM4FGL that innovatively decomposes the task of LLM for FGL into two sub-tasks\ntheoretically. Specifically, for each client, it first utilizes the LLM to\ngenerate missing neighbors and then infers connections between generated nodes\nand raw nodes. To improve the quality of generated nodes, we design a novel\nfederated generation-and-reflection mechanism for LLMs, without the need to\nmodify the parameters of the LLM but relying solely on the collective feedback\nfrom all clients. After neighbor generation, all the clients utilize a\npre-trained edge predictor to infer the missing edges. Furthermore, our\nframework can seamlessly integrate as a plug-in with existing FGL methods.\nExperiments on three real-world datasets demonstrate the superiority of our\nmethod compared to advanced baselines."
                },
                "authors": [
                    {
                        "name": "Bo Yan"
                    },
                    {
                        "name": "Zhongjian Zhang"
                    },
                    {
                        "name": "Huabin Sun"
                    },
                    {
                        "name": "Mengmei Zhang"
                    },
                    {
                        "name": "Yang Cao"
                    },
                    {
                        "name": "Chuan Shi"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Shi"
                },
                "author": "Chuan Shi",
                "arxiv_comment": "ongoing work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19455v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19455v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19449v1",
                "updated": "2025-03-25T08:39:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    8,
                    39,
                    35,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T08:39:35Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    8,
                    39,
                    35,
                    1,
                    84,
                    0
                ],
                "title": "VecTrans: LLM Transformation Framework for Better Auto-vectorization on\n  High-performance CPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VecTrans: LLM Transformation Framework for Better Auto-vectorization on\n  High-performance CPU"
                },
                "summary": "Large language models (LLMs) have demonstrated great capabilities in code\ngeneration, yet their effective application in compiler optimizations remains\nan open challenge due to issues such as hallucinations and a lack of\ndomain-specific reasoning. Vectorization, a crucial optimization for enhancing\ncode performance, often fails because of the compiler's inability to recognize\ncomplex code patterns, which commonly require extensive empirical expertise.\nLLMs, with their ability to capture intricate patterns, thus providing a\npromising solution to this challenge. This paper presents VecTrans, a novel\nframework that leverages LLMs to enhance compiler-based code vectorization.\nVecTrans first employs compiler analysis to identify potentially vectorizable\ncode regions. It then utilizes an LLM to refactor these regions into patterns\nthat are more amenable to the compiler's auto-vectorization. To ensure semantic\ncorrectness, VecTrans further integrates a hybrid validation mechanism at the\nintermediate representation (IR) level. With the above efforts, VecTrans\ncombines the adaptability of LLMs with the precision of compiler vectorization,\nthereby effectively opening up the vectorization opportunities. Experimental\nresults show that among all 50 TSVC functions unvectorizable by Clang, GCC, and\nBiShengCompiler, VecTrans successfully vectorizes 23 cases (46%) and achieves\nan average speedup of 2.02x, greatly surpassing state-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated great capabilities in code\ngeneration, yet their effective application in compiler optimizations remains\nan open challenge due to issues such as hallucinations and a lack of\ndomain-specific reasoning. Vectorization, a crucial optimization for enhancing\ncode performance, often fails because of the compiler's inability to recognize\ncomplex code patterns, which commonly require extensive empirical expertise.\nLLMs, with their ability to capture intricate patterns, thus providing a\npromising solution to this challenge. This paper presents VecTrans, a novel\nframework that leverages LLMs to enhance compiler-based code vectorization.\nVecTrans first employs compiler analysis to identify potentially vectorizable\ncode regions. It then utilizes an LLM to refactor these regions into patterns\nthat are more amenable to the compiler's auto-vectorization. To ensure semantic\ncorrectness, VecTrans further integrates a hybrid validation mechanism at the\nintermediate representation (IR) level. With the above efforts, VecTrans\ncombines the adaptability of LLMs with the precision of compiler vectorization,\nthereby effectively opening up the vectorization opportunities. Experimental\nresults show that among all 50 TSVC functions unvectorizable by Clang, GCC, and\nBiShengCompiler, VecTrans successfully vectorizes 23 cases (46%) and achieves\nan average speedup of 2.02x, greatly surpassing state-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Zhongchun Zheng"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Lu Li"
                    },
                    {
                        "name": "Rodrigo C. O. Rocha"
                    },
                    {
                        "name": "Tianyi Liu"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Xianwei Zhang"
                    },
                    {
                        "name": "Yaoqing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yaoqing Gao"
                },
                "author": "Yaoqing Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00092v2",
                "updated": "2025-03-25T08:24:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    8,
                    24,
                    19,
                    1,
                    84,
                    0
                ],
                "published": "2024-08-26T12:00:29Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    12,
                    0,
                    29,
                    0,
                    239,
                    0
                ],
                "title": "Large Language Model for Patent Concept Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model for Patent Concept Generation"
                },
                "summary": "In traditional innovation practices, concept and IP generation are often\niteratively integrated. Both processes demand an intricate understanding of\nadvanced technical domain knowledge. Existing large language models (LLMs),\nwhile possessing massive pre-trained knowledge, often fall short in the\ninnovative concept generation due to a lack of specialized knowledge necessary\nfor the generation. To bridge this critical gap, we propose a novel knowledge\nfinetuning (KFT) framework to endow LLM-based AI with the ability to\nautonomously mine, understand, and apply domain-specific knowledge and concepts\nfor invention generation, i.e., concept and patent generation together. Our\nproposed PatentGPT integrates knowledge injection pre-training (KPT),\ndomain-specific supervised finetuning (SFT), and reinforcement learning from\nhuman feedback (RLHF). Extensive evaluation shows that PatentGPT significantly\noutperforms the state-of-the-art models on patent-related benchmark tests. Our\nmethod not only provides new insights into data-driven innovation but also\npaves a new path to fine-tune LLMs for applications in the context of\ntechnology. We also discuss the managerial and policy implications of\nAI-generating inventions in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In traditional innovation practices, concept and IP generation are often\niteratively integrated. Both processes demand an intricate understanding of\nadvanced technical domain knowledge. Existing large language models (LLMs),\nwhile possessing massive pre-trained knowledge, often fall short in the\ninnovative concept generation due to a lack of specialized knowledge necessary\nfor the generation. To bridge this critical gap, we propose a novel knowledge\nfinetuning (KFT) framework to endow LLM-based AI with the ability to\nautonomously mine, understand, and apply domain-specific knowledge and concepts\nfor invention generation, i.e., concept and patent generation together. Our\nproposed PatentGPT integrates knowledge injection pre-training (KPT),\ndomain-specific supervised finetuning (SFT), and reinforcement learning from\nhuman feedback (RLHF). Extensive evaluation shows that PatentGPT significantly\noutperforms the state-of-the-art models on patent-related benchmark tests. Our\nmethod not only provides new insights into data-driven innovation but also\npaves a new path to fine-tune LLMs for applications in the context of\ntechnology. We also discuss the managerial and policy implications of\nAI-generating inventions in the future."
                },
                "authors": [
                    {
                        "name": "Runtao Ren"
                    },
                    {
                        "name": "Jian Ma"
                    },
                    {
                        "name": "Jianxi Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jianxi Luo"
                },
                "author": "Jianxi Luo",
                "arxiv_comment": "33 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19434v1",
                "updated": "2025-03-25T08:23:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    8,
                    23,
                    49,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T08:23:49Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    8,
                    23,
                    49,
                    1,
                    84,
                    0
                ],
                "title": "Enhanced Bloom's Educational Taxonomy for Fostering Information Literacy\n  in the Era of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Bloom's Educational Taxonomy for Fostering Information Literacy\n  in the Era of Large Language Models"
                },
                "summary": "The advent of Large Language Models (LLMs) has profoundly transformed the\nparadigms of information retrieval and problem-solving, enabling students to\naccess information acquisition more efficiently to support learning. However,\nthere is currently a lack of standardized evaluation frameworks that guide\nlearners in effectively leveraging LLMs. This paper proposes an LLM-driven\nBloom's Educational Taxonomy that aims to recognize and evaluate students'\ninformation literacy (IL) with LLMs, and to formalize and guide students\npractice-based activities of using LLMs to solve complex problems. The\nframework delineates the IL corresponding to the cognitive abilities required\nto use LLM into two distinct stages: Exploration & Action and Creation &\nMetacognition. It further subdivides these into seven phases: Perceiving,\nSearching, Reasoning, Interacting, Evaluating, Organizing, and Curating.\nThrough the case presentation, the analysis demonstrates the framework's\napplicability and feasibility, supporting its role in fostering IL among\nstudents with varying levels of prior knowledge. This framework fills the\nexisting gap in the analysis of LLM usage frameworks and provides theoretical\nsupport for guiding learners to improve IL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has profoundly transformed the\nparadigms of information retrieval and problem-solving, enabling students to\naccess information acquisition more efficiently to support learning. However,\nthere is currently a lack of standardized evaluation frameworks that guide\nlearners in effectively leveraging LLMs. This paper proposes an LLM-driven\nBloom's Educational Taxonomy that aims to recognize and evaluate students'\ninformation literacy (IL) with LLMs, and to formalize and guide students\npractice-based activities of using LLMs to solve complex problems. The\nframework delineates the IL corresponding to the cognitive abilities required\nto use LLM into two distinct stages: Exploration & Action and Creation &\nMetacognition. It further subdivides these into seven phases: Perceiving,\nSearching, Reasoning, Interacting, Evaluating, Organizing, and Curating.\nThrough the case presentation, the analysis demonstrates the framework's\napplicability and feasibility, supporting its role in fostering IL among\nstudents with varying levels of prior knowledge. This framework fills the\nexisting gap in the analysis of LLM usage frameworks and provides theoretical\nsupport for guiding learners to improve IL."
                },
                "authors": [
                    {
                        "name": "Yiming Luo"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Patrick Cheong-Iao Pang"
                    },
                    {
                        "name": "Dana McKay"
                    },
                    {
                        "name": "Ziqi Chen"
                    },
                    {
                        "name": "George Buchanan"
                    },
                    {
                        "name": "Shanton Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shanton Chang"
                },
                "author": "Shanton Chang",
                "arxiv_comment": "25 Pages, 5 figures, submitted to the journal Computers & Education,\n  currently under peer review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19426v1",
                "updated": "2025-03-25T08:16:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    8,
                    16,
                    35,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T08:16:35Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    8,
                    16,
                    35,
                    1,
                    84,
                    0
                ],
                "title": "DeCAP: Context-Adaptive Prompt Generation for Debiasing Zero-shot\n  Question Answering in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeCAP: Context-Adaptive Prompt Generation for Debiasing Zero-shot\n  Question Answering in Large Language Models"
                },
                "summary": "While Large Language Models (LLMs) excel in zero-shot Question Answering\n(QA), they tend to expose biases in their internal knowledge when faced with\nsocially sensitive questions, leading to a degradation in performance. Existing\nzero-shot methods are efficient but fail to consider context and prevent bias\npropagation in the answers. To address this, we propose DeCAP, a method for\ndebiasing LLMs using Context-Adaptive Prompt Generation. DeCAP leverages a\nQuestion Ambiguity Detection to take appropriate debiasing actions based on the\ncontext and a Neutral Answer Guidance Generation to suppress the LLMs make\nobjective judgments about the context, minimizing the propagation of bias from\ntheir internal knowledge. Our various experiments across eight LLMs show that\nDeCAP achieves state-of-the-art zero-shot debiased QA performance. This\ndemonstrates DeCAP's efficacy in enhancing the fairness and accuracy of LLMs in\ndiverse QA settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) excel in zero-shot Question Answering\n(QA), they tend to expose biases in their internal knowledge when faced with\nsocially sensitive questions, leading to a degradation in performance. Existing\nzero-shot methods are efficient but fail to consider context and prevent bias\npropagation in the answers. To address this, we propose DeCAP, a method for\ndebiasing LLMs using Context-Adaptive Prompt Generation. DeCAP leverages a\nQuestion Ambiguity Detection to take appropriate debiasing actions based on the\ncontext and a Neutral Answer Guidance Generation to suppress the LLMs make\nobjective judgments about the context, minimizing the propagation of bias from\ntheir internal knowledge. Our various experiments across eight LLMs show that\nDeCAP achieves state-of-the-art zero-shot debiased QA performance. This\ndemonstrates DeCAP's efficacy in enhancing the fairness and accuracy of LLMs in\ndiverse QA settings."
                },
                "authors": [
                    {
                        "name": "Suyoung Bae"
                    },
                    {
                        "name": "YunSeok Choi"
                    },
                    {
                        "name": "Jee-Hyong Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jee-Hyong Lee"
                },
                "author": "Jee-Hyong Lee",
                "arxiv_comment": "Accepted to NAACL 2025 main. 20 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00599v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00599v3",
                "updated": "2025-03-25T08:10:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    8,
                    10,
                    15,
                    1,
                    84,
                    0
                ],
                "published": "2024-12-31T18:56:46Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    18,
                    56,
                    46,
                    1,
                    366,
                    0
                ],
                "title": "VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with\n  Video LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with\n  Video LLM"
                },
                "summary": "Video Large Language Models (Video LLMs) have recently exhibited remarkable\ncapabilities in general video understanding. However, they mainly focus on\nholistic comprehension and struggle with capturing fine-grained spatial and\ntemporal details. Besides, the lack of high-quality object-level video\ninstruction data and a comprehensive benchmark further hinders their\nadvancements. To tackle these challenges, we introduce the VideoRefer Suite to\nempower Video LLM for finer-level spatial-temporal video understanding, i.e.,\nenabling perception and reasoning on any objects throughout the video.\nSpecially, we thoroughly develop VideoRefer Suite across three essential\naspects: dataset, model, and benchmark. Firstly, we introduce a multi-agent\ndata engine to meticulously curate a large-scale, high-quality object-level\nvideo instruction dataset, termed VideoRefer-700K. Next, we present the\nVideoRefer model, which equips a versatile spatial-temporal object encoder to\ncapture precise regional and sequential representations. Finally, we\nmeticulously create a VideoRefer-Bench to comprehensively assess the\nspatial-temporal understanding capability of a Video LLM, evaluating it across\nvarious aspects. Extensive experiments and analyses demonstrate that our\nVideoRefer model not only achieves promising performance on video referring\nbenchmarks but also facilitates general video understanding capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (Video LLMs) have recently exhibited remarkable\ncapabilities in general video understanding. However, they mainly focus on\nholistic comprehension and struggle with capturing fine-grained spatial and\ntemporal details. Besides, the lack of high-quality object-level video\ninstruction data and a comprehensive benchmark further hinders their\nadvancements. To tackle these challenges, we introduce the VideoRefer Suite to\nempower Video LLM for finer-level spatial-temporal video understanding, i.e.,\nenabling perception and reasoning on any objects throughout the video.\nSpecially, we thoroughly develop VideoRefer Suite across three essential\naspects: dataset, model, and benchmark. Firstly, we introduce a multi-agent\ndata engine to meticulously curate a large-scale, high-quality object-level\nvideo instruction dataset, termed VideoRefer-700K. Next, we present the\nVideoRefer model, which equips a versatile spatial-temporal object encoder to\ncapture precise regional and sequential representations. Finally, we\nmeticulously create a VideoRefer-Bench to comprehensively assess the\nspatial-temporal understanding capability of a Video LLM, evaluating it across\nvarious aspects. Extensive experiments and analyses demonstrate that our\nVideoRefer model not only achieves promising performance on video referring\nbenchmarks but also facilitates general video understanding capabilities."
                },
                "authors": [
                    {
                        "name": "Yuqian Yuan"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Wentong Li"
                    },
                    {
                        "name": "Zesen Cheng"
                    },
                    {
                        "name": "Boqiang Zhang"
                    },
                    {
                        "name": "Long Li"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Deli Zhao"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Yueting Zhuang"
                    },
                    {
                        "name": "Jianke Zhu"
                    },
                    {
                        "name": "Lidong Bing"
                    }
                ],
                "author_detail": {
                    "name": "Lidong Bing"
                },
                "author": "Lidong Bing",
                "arxiv_comment": "17 pages, 14 figures, technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00599v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00599v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01210v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01210v2",
                "updated": "2025-03-25T07:55:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    7,
                    55,
                    55,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-03T06:16:31Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    6,
                    16,
                    31,
                    0,
                    62,
                    0
                ],
                "title": "Every SAM Drop Counts: Embracing Semantic Priors for Multi-Modality\n  Image Fusion and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Every SAM Drop Counts: Embracing Semantic Priors for Multi-Modality\n  Image Fusion and Beyond"
                },
                "summary": "Multi-modality image fusion, particularly infrared and visible, plays a\ncrucial role in integrating diverse modalities to enhance scene understanding.\nAlthough early research prioritized visual quality, preserving fine details and\nadapting to downstream tasks remains challenging. Recent approaches attempt\ntask-specific design but rarely achieve \"The Best of Both Worlds\" due to\ninconsistent optimization goals. To address these issues, we propose a novel\nmethod that leverages the semantic knowledge from the Segment Anything Model\n(SAM) to Grow the quality of fusion results and Enable downstream task\nadaptability, namely SAGE. Specifically, we design a Semantic Persistent\nAttention (SPA) Module that efficiently maintains source information via the\npersistent repository while extracting high-level semantic priors from SAM.\nMore importantly, to eliminate the impractical dependence on SAM during\ninference, we introduce a bi-level optimization-driven distillation mechanism\nwith triplet losses, which allow the student network to effectively extract\nknowledge. Extensive experiments show that our method achieves a balance\nbetween high-quality visual results and downstream task adaptability while\nmaintaining practical deployment efficiency. The code is available at\nhttps://github.com/RollingPlain/SAGE_IVIF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modality image fusion, particularly infrared and visible, plays a\ncrucial role in integrating diverse modalities to enhance scene understanding.\nAlthough early research prioritized visual quality, preserving fine details and\nadapting to downstream tasks remains challenging. Recent approaches attempt\ntask-specific design but rarely achieve \"The Best of Both Worlds\" due to\ninconsistent optimization goals. To address these issues, we propose a novel\nmethod that leverages the semantic knowledge from the Segment Anything Model\n(SAM) to Grow the quality of fusion results and Enable downstream task\nadaptability, namely SAGE. Specifically, we design a Semantic Persistent\nAttention (SPA) Module that efficiently maintains source information via the\npersistent repository while extracting high-level semantic priors from SAM.\nMore importantly, to eliminate the impractical dependence on SAM during\ninference, we introduce a bi-level optimization-driven distillation mechanism\nwith triplet losses, which allow the student network to effectively extract\nknowledge. Extensive experiments show that our method achieves a balance\nbetween high-quality visual results and downstream task adaptability while\nmaintaining practical deployment efficiency. The code is available at\nhttps://github.com/RollingPlain/SAGE_IVIF."
                },
                "authors": [
                    {
                        "name": "Guanyao Wu"
                    },
                    {
                        "name": "Haoyu Liu"
                    },
                    {
                        "name": "Hongming Fu"
                    },
                    {
                        "name": "Yichuan Peng"
                    },
                    {
                        "name": "Jinyuan Liu"
                    },
                    {
                        "name": "Xin Fan"
                    },
                    {
                        "name": "Risheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Risheng Liu"
                },
                "author": "Risheng Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01210v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01210v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19404v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19404v2",
                "updated": "2025-03-26T03:46:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    3,
                    46,
                    53,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-25T07:24:27Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    7,
                    24,
                    27,
                    1,
                    84,
                    0
                ],
                "title": "LangBridge: Interpreting Image as a Combination of Language Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LangBridge: Interpreting Image as a Combination of Language Embeddings"
                },
                "summary": "Recent years have witnessed remarkable advances in Large Vision-Language\nModels (LVLMs), which have achieved human-level performance across various\ncomplex vision-language tasks. Following LLaVA's paradigm, mainstream LVLMs\ntypically employ a shallow MLP for visual-language alignment through a\ntwo-stage training process: pretraining for cross-modal alignment followed by\ninstruction tuning. While this approach has proven effective, the underlying\nmechanisms of how MLPs bridge the modality gap remain poorly understood.\nAlthough some research has explored how LLMs process transformed visual tokens,\nfew studies have investigated the fundamental alignment mechanism. Furthermore,\nthe MLP adapter requires retraining whenever switching LLM backbones. To\naddress these limitations, we first investigate the working principles of MLP\nadapters and discover that they learn to project visual embeddings into\nsubspaces spanned by corresponding text embeddings progressively. Based on this\ninsight, we propose LangBridge, a novel adapter that explicitly maps visual\ntokens to linear combinations of LLM vocabulary embeddings. This innovative\ndesign enables pretraining-free adapter transfer across different LLMs while\nmaintaining performance. Our experimental results demonstrate that a LangBridge\nadapter pre-trained on Qwen2-0.5B can be directly applied to larger models such\nas LLaMA3-8B or Qwen2.5-14B while maintaining competitive performance. Overall,\nLangBridge enables interpretable vision-language alignment by grounding visual\nrepresentations in LLM vocab embedding, while its plug-and-play design ensures\nefficient reuse across multiple LLMs with nearly no performance degradation.\nSee our project page at https://jiaqiliao77.github.io/LangBridge.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed remarkable advances in Large Vision-Language\nModels (LVLMs), which have achieved human-level performance across various\ncomplex vision-language tasks. Following LLaVA's paradigm, mainstream LVLMs\ntypically employ a shallow MLP for visual-language alignment through a\ntwo-stage training process: pretraining for cross-modal alignment followed by\ninstruction tuning. While this approach has proven effective, the underlying\nmechanisms of how MLPs bridge the modality gap remain poorly understood.\nAlthough some research has explored how LLMs process transformed visual tokens,\nfew studies have investigated the fundamental alignment mechanism. Furthermore,\nthe MLP adapter requires retraining whenever switching LLM backbones. To\naddress these limitations, we first investigate the working principles of MLP\nadapters and discover that they learn to project visual embeddings into\nsubspaces spanned by corresponding text embeddings progressively. Based on this\ninsight, we propose LangBridge, a novel adapter that explicitly maps visual\ntokens to linear combinations of LLM vocabulary embeddings. This innovative\ndesign enables pretraining-free adapter transfer across different LLMs while\nmaintaining performance. Our experimental results demonstrate that a LangBridge\nadapter pre-trained on Qwen2-0.5B can be directly applied to larger models such\nas LLaMA3-8B or Qwen2.5-14B while maintaining competitive performance. Overall,\nLangBridge enables interpretable vision-language alignment by grounding visual\nrepresentations in LLM vocab embedding, while its plug-and-play design ensures\nefficient reuse across multiple LLMs with nearly no performance degradation.\nSee our project page at https://jiaqiliao77.github.io/LangBridge.github.io/"
                },
                "authors": [
                    {
                        "name": "Jiaqi Liao"
                    },
                    {
                        "name": "Yuwei Niu"
                    },
                    {
                        "name": "Fanqing Meng"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Changyao Tian"
                    },
                    {
                        "name": "Yinuo Du"
                    },
                    {
                        "name": "Yuwen Xiong"
                    },
                    {
                        "name": "Dianqi Li"
                    },
                    {
                        "name": "Xizhou Zhu"
                    },
                    {
                        "name": "Li Yuan"
                    },
                    {
                        "name": "Jifeng Dai"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "The code and weights will be open-sourced. Project page:\n  https://jiaqiliao77.github.io/LangBridge.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19404v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19404v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03937v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03937v4",
                "updated": "2025-03-25T06:59:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    59,
                    40,
                    1,
                    84,
                    0
                ],
                "published": "2024-12-05T07:35:19Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    7,
                    35,
                    19,
                    3,
                    340,
                    0
                ],
                "title": "AIpparel: A Multimodal Foundation Model for Digital Garments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIpparel: A Multimodal Foundation Model for Digital Garments"
                },
                "summary": "Apparel is essential to human life, offering protection, mirroring cultural\nidentities, and showcasing personal style. Yet, the creation of garments\nremains a time-consuming process, largely due to the manual work involved in\ndesigning them. To simplify this process, we introduce AIpparel, a multimodal\nfoundation model for generating and editing sewing patterns. Our model\nfine-tunes state-of-the-art large multimodal models (LMMs) on a custom-curated\nlarge-scale dataset of over 120,000 unique garments, each with multimodal\nannotations including text, images, and sewing patterns. Additionally, we\npropose a novel tokenization scheme that concisely encodes these complex sewing\npatterns so that LLMs can learn to predict them efficiently. AIpparel achieves\nstate-of-the-art performance in single-modal tasks, including text-to-garment\nand image-to-garment prediction, and enables novel multimodal garment\ngeneration applications such as interactive garment editing. The project\nwebsite is at https://georgenakayama.github.io/AIpparel/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apparel is essential to human life, offering protection, mirroring cultural\nidentities, and showcasing personal style. Yet, the creation of garments\nremains a time-consuming process, largely due to the manual work involved in\ndesigning them. To simplify this process, we introduce AIpparel, a multimodal\nfoundation model for generating and editing sewing patterns. Our model\nfine-tunes state-of-the-art large multimodal models (LMMs) on a custom-curated\nlarge-scale dataset of over 120,000 unique garments, each with multimodal\nannotations including text, images, and sewing patterns. Additionally, we\npropose a novel tokenization scheme that concisely encodes these complex sewing\npatterns so that LLMs can learn to predict them efficiently. AIpparel achieves\nstate-of-the-art performance in single-modal tasks, including text-to-garment\nand image-to-garment prediction, and enables novel multimodal garment\ngeneration applications such as interactive garment editing. The project\nwebsite is at https://georgenakayama.github.io/AIpparel/."
                },
                "authors": [
                    {
                        "name": "Kiyohiro Nakayama"
                    },
                    {
                        "name": "Jan Ackermann"
                    },
                    {
                        "name": "Timur Levent Kesdogan"
                    },
                    {
                        "name": "Yang Zheng"
                    },
                    {
                        "name": "Maria Korosteleva"
                    },
                    {
                        "name": "Olga Sorkine-Hornung"
                    },
                    {
                        "name": "Leonidas J. Guibas"
                    },
                    {
                        "name": "Guandao Yang"
                    },
                    {
                        "name": "Gordon Wetzstein"
                    }
                ],
                "author_detail": {
                    "name": "Gordon Wetzstein"
                },
                "author": "Gordon Wetzstein",
                "arxiv_comment": "The project website is at https://georgenakayama.github.io/AIpparel/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03937v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03937v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19385v1",
                "updated": "2025-03-25T06:30:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    30,
                    45,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T06:30:45Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    30,
                    45,
                    1,
                    84,
                    0
                ],
                "title": "Inference-Time Scaling for Flow Models via Stochastic Generation and\n  Rollover Budget Forcing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Scaling for Flow Models via Stochastic Generation and\n  Rollover Budget Forcing"
                },
                "summary": "We propose an inference-time scaling approach for pretrained flow models.\nRecently, inference-time scaling has gained significant attention in LLMs and\ndiffusion models, improving sample quality or better aligning outputs with user\npreferences by leveraging additional computation. For diffusion models,\nparticle sampling has allowed more efficient scaling due to the stochasticity\nat intermediate denoising steps. On the contrary, while flow models have gained\npopularity as an alternative to diffusion models--offering faster generation\nand high-quality outputs in state-of-the-art image and video generative\nmodels--efficient inference-time scaling methods used for diffusion models\ncannot be directly applied due to their deterministic generative process. To\nenable efficient inference-time scaling for flow models, we propose three key\nideas: 1) SDE-based generation, enabling particle sampling in flow models, 2)\nInterpolant conversion, broadening the search space and enhancing sample\ndiversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of\ncomputational resources across timesteps to maximize budget utilization. Our\nexperiments show that SDE-based generation, particularly variance-preserving\n(VP) interpolant-based generation, improves the performance of particle\nsampling methods for inference-time scaling in flow models. Additionally, we\ndemonstrate that RBF with VP-SDE achieves the best performance, outperforming\nall previous inference-time scaling approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an inference-time scaling approach for pretrained flow models.\nRecently, inference-time scaling has gained significant attention in LLMs and\ndiffusion models, improving sample quality or better aligning outputs with user\npreferences by leveraging additional computation. For diffusion models,\nparticle sampling has allowed more efficient scaling due to the stochasticity\nat intermediate denoising steps. On the contrary, while flow models have gained\npopularity as an alternative to diffusion models--offering faster generation\nand high-quality outputs in state-of-the-art image and video generative\nmodels--efficient inference-time scaling methods used for diffusion models\ncannot be directly applied due to their deterministic generative process. To\nenable efficient inference-time scaling for flow models, we propose three key\nideas: 1) SDE-based generation, enabling particle sampling in flow models, 2)\nInterpolant conversion, broadening the search space and enhancing sample\ndiversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of\ncomputational resources across timesteps to maximize budget utilization. Our\nexperiments show that SDE-based generation, particularly variance-preserving\n(VP) interpolant-based generation, improves the performance of particle\nsampling methods for inference-time scaling in flow models. Additionally, we\ndemonstrate that RBF with VP-SDE achieves the best performance, outperforming\nall previous inference-time scaling approaches."
                },
                "authors": [
                    {
                        "name": "Jaihoon Kim"
                    },
                    {
                        "name": "Taehoon Yoon"
                    },
                    {
                        "name": "Jisung Hwang"
                    },
                    {
                        "name": "Minhyuk Sung"
                    }
                ],
                "author_detail": {
                    "name": "Minhyuk Sung"
                },
                "author": "Minhyuk Sung",
                "arxiv_comment": "Project page: https://flow-inference-time-scaling.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07626v2",
                "updated": "2025-03-25T06:19:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    19,
                    32,
                    1,
                    84,
                    0
                ],
                "published": "2024-12-10T16:05:56Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    5,
                    56,
                    1,
                    345,
                    0
                ],
                "title": "OmniDocBench: Benchmarking Diverse PDF Document Parsing with\n  Comprehensive Annotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDocBench: Benchmarking Diverse PDF Document Parsing with\n  Comprehensive Annotations"
                },
                "summary": "Document content extraction is a critical task in computer vision,\nunderpinning the data needs of large language models (LLMs) and\nretrieval-augmented generation (RAG) systems. Despite recent progress, current\ndocument parsing methods have not been fairly and comprehensively evaluated due\nto the narrow coverage of document types and the simplified, unrealistic\nevaluation procedures in existing benchmarks. To address these gaps, we\nintroduce OmniDocBench, a novel benchmark featuring high-quality annotations\nacross nine document sources, including academic papers, textbooks, and more\nchallenging cases such as handwritten notes and densely typeset newspapers.\nOmniDocBench supports flexible, multi-level evaluations--ranging from an\nend-to-end assessment to the task-specific and attribute--based analysis using\n19 layout categories and 15 attribute labels. We conduct a thorough evaluation\nof both pipeline-based methods and end-to-end vision-language models, revealing\ntheir strengths and weaknesses across different document types. OmniDocBench\nsets a new standard for the fair, diverse, and fine-grained evaluation in\ndocument parsing. Dataset and code are available at\nhttps://github.com/opendatalab/OmniDocBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Document content extraction is a critical task in computer vision,\nunderpinning the data needs of large language models (LLMs) and\nretrieval-augmented generation (RAG) systems. Despite recent progress, current\ndocument parsing methods have not been fairly and comprehensively evaluated due\nto the narrow coverage of document types and the simplified, unrealistic\nevaluation procedures in existing benchmarks. To address these gaps, we\nintroduce OmniDocBench, a novel benchmark featuring high-quality annotations\nacross nine document sources, including academic papers, textbooks, and more\nchallenging cases such as handwritten notes and densely typeset newspapers.\nOmniDocBench supports flexible, multi-level evaluations--ranging from an\nend-to-end assessment to the task-specific and attribute--based analysis using\n19 layout categories and 15 attribute labels. We conduct a thorough evaluation\nof both pipeline-based methods and end-to-end vision-language models, revealing\ntheir strengths and weaknesses across different document types. OmniDocBench\nsets a new standard for the fair, diverse, and fine-grained evaluation in\ndocument parsing. Dataset and code are available at\nhttps://github.com/opendatalab/OmniDocBench."
                },
                "authors": [
                    {
                        "name": "Linke Ouyang"
                    },
                    {
                        "name": "Yuan Qu"
                    },
                    {
                        "name": "Hongbin Zhou"
                    },
                    {
                        "name": "Jiawei Zhu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Qunshu Lin"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Zhiyuan Zhao"
                    },
                    {
                        "name": "Man Jiang"
                    },
                    {
                        "name": "Xiaomeng Zhao"
                    },
                    {
                        "name": "Jin Shi"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Pei Chu"
                    },
                    {
                        "name": "Minghao Liu"
                    },
                    {
                        "name": "Zhenxiang Li"
                    },
                    {
                        "name": "Chao Xu"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Zhongying Tu"
                    },
                    {
                        "name": "Conghui He"
                    }
                ],
                "author_detail": {
                    "name": "Conghui He"
                },
                "author": "Conghui He",
                "arxiv_comment": "Accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11960v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11960v3",
                "updated": "2025-03-25T06:08:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    8,
                    47,
                    1,
                    84,
                    0
                ],
                "published": "2024-04-18T07:42:46Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    7,
                    42,
                    46,
                    3,
                    109,
                    0
                ],
                "title": "MCRanker: Generating Diverse Criteria On-the-Fly to Improve Point-wise\n  LLM Rankers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCRanker: Generating Diverse Criteria On-the-Fly to Improve Point-wise\n  LLM Rankers"
                },
                "summary": "The most recent pointwise Large Language Model (LLM) rankers have achieved\nremarkable ranking results. However, these rankers are hindered by two major\ndrawbacks: (1) they fail to follow a standardized comparison guidance during\nthe ranking process, and (2) they struggle with comprehensive considerations\nwhen dealing with complicated passages. To address these shortcomings, we\npropose to build a ranker that generates ranking scores based on a set of\ncriteria from various perspectives. These criteria are intended to direct each\nperspective in providing a distinct yet synergistic evaluation. Our research,\nwhich examines eight datasets from the BEIR benchmark demonstrates that\nincorporating this multi-perspective criteria ensemble approach markedly\nenhanced the performance of pointwise LLM rankers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The most recent pointwise Large Language Model (LLM) rankers have achieved\nremarkable ranking results. However, these rankers are hindered by two major\ndrawbacks: (1) they fail to follow a standardized comparison guidance during\nthe ranking process, and (2) they struggle with comprehensive considerations\nwhen dealing with complicated passages. To address these shortcomings, we\npropose to build a ranker that generates ranking scores based on a set of\ncriteria from various perspectives. These criteria are intended to direct each\nperspective in providing a distinct yet synergistic evaluation. Our research,\nwhich examines eight datasets from the BEIR benchmark demonstrates that\nincorporating this multi-perspective criteria ensemble approach markedly\nenhanced the performance of pointwise LLM rankers."
                },
                "authors": [
                    {
                        "name": "Fang Guo"
                    },
                    {
                        "name": "Wenyu Li"
                    },
                    {
                        "name": "Honglei Zhuang"
                    },
                    {
                        "name": "Yun Luo"
                    },
                    {
                        "name": "Yafu Li"
                    },
                    {
                        "name": "Le Yan"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "arxiv_journal_ref": "WSDM 2025: Proceedings of the Eighteenth ACM International\n  Conference on Web Search and Data Mining",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11960v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11960v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19368v1",
                "updated": "2025-03-25T05:50:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    5,
                    50,
                    31,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T05:50:31Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    5,
                    50,
                    31,
                    1,
                    84,
                    0
                ],
                "title": "RIS-Assisted Passive Localization (RAPL): An Efficient Zero-Overhead\n  Framework Using Conditional Sample Mean",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIS-Assisted Passive Localization (RAPL): An Efficient Zero-Overhead\n  Framework Using Conditional Sample Mean"
                },
                "summary": "Reconfigurable Intelligent Surface (RIS) has been recognized as a promising\nsolution for enhancing localization accuracy. Traditional RIS-based\nlocalization methods typically rely on prior channel knowledge, beam scanning,\nand pilot-based assistance. These approaches often result in substantial energy\nand computational overhead, and require real-time coordination between the base\nstation (BS) and the RIS. To address these challenges, in this work, we move\nbeyond conventional methods and introduce a novel data-driven, multiple\nRISs-assisted passive localization approach (RAPL). The proposed method\nincludes two stages, the angle-of-directions (AoDs) between the RISs and the\nuser is estimated by using the conditional sample mean in the first stage, and\nthen the user's position is determined based on the estimated multiple AoD\npairs in the second stage. This approach only utilizes the existing\ncommunication signals between the user and the BS, relying solely on the\nmeasurement of received signal power at each BS antenna for a set of randomly\ngenerated phase shifts across all RISs. Moreover, by obviating the need for\nreal-time RIS phase shift optimization or user-to-BS pilot transmissions, the\nmethod introduces no additional communication overhead, making it highly\nsuitable for deployment in real-world networks. The proposed scheme is then\nextended to multi-RIS scenarios considering both parallel and cascaded RIS\ntopologies. Numerical results show that the proposed RAPL improves localization\naccuracy while significantly reducing energy and signaling overhead compared to\nconventional methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable Intelligent Surface (RIS) has been recognized as a promising\nsolution for enhancing localization accuracy. Traditional RIS-based\nlocalization methods typically rely on prior channel knowledge, beam scanning,\nand pilot-based assistance. These approaches often result in substantial energy\nand computational overhead, and require real-time coordination between the base\nstation (BS) and the RIS. To address these challenges, in this work, we move\nbeyond conventional methods and introduce a novel data-driven, multiple\nRISs-assisted passive localization approach (RAPL). The proposed method\nincludes two stages, the angle-of-directions (AoDs) between the RISs and the\nuser is estimated by using the conditional sample mean in the first stage, and\nthen the user's position is determined based on the estimated multiple AoD\npairs in the second stage. This approach only utilizes the existing\ncommunication signals between the user and the BS, relying solely on the\nmeasurement of received signal power at each BS antenna for a set of randomly\ngenerated phase shifts across all RISs. Moreover, by obviating the need for\nreal-time RIS phase shift optimization or user-to-BS pilot transmissions, the\nmethod introduces no additional communication overhead, making it highly\nsuitable for deployment in real-world networks. The proposed scheme is then\nextended to multi-RIS scenarios considering both parallel and cascaded RIS\ntopologies. Numerical results show that the proposed RAPL improves localization\naccuracy while significantly reducing energy and signaling overhead compared to\nconventional methods."
                },
                "authors": [
                    {
                        "name": "Jiawei Yao"
                    },
                    {
                        "name": "Yijie Mao"
                    },
                    {
                        "name": "Mingzhe Chen"
                    },
                    {
                        "name": "Ye Hu"
                    }
                ],
                "author_detail": {
                    "name": "Ye Hu"
                },
                "author": "Ye Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15077v2",
                "updated": "2025-03-25T05:17:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    5,
                    17,
                    19,
                    1,
                    84,
                    0
                ],
                "published": "2025-02-20T22:29:24Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    29,
                    24,
                    3,
                    51,
                    0
                ],
                "title": "Hardware-Friendly Static Quantization Method for Video Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware-Friendly Static Quantization Method for Video Diffusion\n  Transformers"
                },
                "summary": "Diffusion Transformers for video generation have gained significant research\ninterest since the impressive performance of SORA. Efficient deployment of such\ngenerative-AI models on GPUs has been demonstrated with dynamic quantization.\nHowever, resource-constrained devices cannot support dynamic quantization, and\nneed static quantization of the models for their efficient deployment on AI\nprocessors. In this paper, we propose a novel method for the post-training\nquantization of OpenSora\\cite{opensora}, a Video Diffusion Transformer, without\nrelying on dynamic quantization techniques. Our approach employs static\nquantization, achieving video quality comparable to FP16 and dynamically\nquantized ViDiT-Q methods, as measured by CLIP, and VQA metrics. In particular,\nwe utilize per-step calibration data to adequately provide a post-training\nstatically quantized model for each time step, incorporating channel-wise\nquantization for weights and tensor-wise quantization for activations. By\nfurther applying the smooth-quantization technique, we can obtain high-quality\nvideo outputs with the statically quantized models. Extensive experimental\nresults demonstrate that static quantization can be a viable alternative to\ndynamic quantization for video diffusion transformers, offering a more\nefficient approach without sacrificing performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers for video generation have gained significant research\ninterest since the impressive performance of SORA. Efficient deployment of such\ngenerative-AI models on GPUs has been demonstrated with dynamic quantization.\nHowever, resource-constrained devices cannot support dynamic quantization, and\nneed static quantization of the models for their efficient deployment on AI\nprocessors. In this paper, we propose a novel method for the post-training\nquantization of OpenSora\\cite{opensora}, a Video Diffusion Transformer, without\nrelying on dynamic quantization techniques. Our approach employs static\nquantization, achieving video quality comparable to FP16 and dynamically\nquantized ViDiT-Q methods, as measured by CLIP, and VQA metrics. In particular,\nwe utilize per-step calibration data to adequately provide a post-training\nstatically quantized model for each time step, incorporating channel-wise\nquantization for weights and tensor-wise quantization for activations. By\nfurther applying the smooth-quantization technique, we can obtain high-quality\nvideo outputs with the statically quantized models. Extensive experimental\nresults demonstrate that static quantization can be a viable alternative to\ndynamic quantization for video diffusion transformers, offering a more\nefficient approach without sacrificing performance."
                },
                "authors": [
                    {
                        "name": "Sanghyun Yi"
                    },
                    {
                        "name": "Qingfeng Liu"
                    },
                    {
                        "name": "Mostafa El-Khamy"
                    }
                ],
                "author_detail": {
                    "name": "Mostafa El-Khamy"
                },
                "author": "Mostafa El-Khamy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16861v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16861v2",
                "updated": "2025-03-25T05:12:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    5,
                    12,
                    4,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-21T05:09:46Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    9,
                    46,
                    4,
                    80,
                    0
                ],
                "title": "In-House Evaluation Is Not Enough: Towards Robust Third-Party Flaw\n  Disclosure for General-Purpose AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-House Evaluation Is Not Enough: Towards Robust Third-Party Flaw\n  Disclosure for General-Purpose AI"
                },
                "summary": "The widespread deployment of general-purpose AI (GPAI) systems introduces\nsignificant new risks. Yet the infrastructure, practices, and norms for\nreporting flaws in GPAI systems remain seriously underdeveloped, lagging far\nbehind more established fields like software security. Based on a collaboration\nbetween experts from the fields of software security, machine learning, law,\nsocial science, and policy, we identify key gaps in the evaluation and\nreporting of flaws in GPAI systems. We call for three interventions to advance\nsystem safety. First, we propose using standardized AI flaw reports and rules\nof engagement for researchers in order to ease the process of submitting,\nreproducing, and triaging flaws in GPAI systems. Second, we propose GPAI system\nproviders adopt broadly-scoped flaw disclosure programs, borrowing from bug\nbounties, with legal safe harbors to protect researchers. Third, we advocate\nfor the development of improved infrastructure to coordinate distribution of\nflaw reports across the many stakeholders who may be impacted. These\ninterventions are increasingly urgent, as evidenced by the prevalence of\njailbreaks and other flaws that can transfer across different providers' GPAI\nsystems. By promoting robust reporting and coordination in the AI ecosystem,\nthese proposals could significantly improve the safety, security, and\naccountability of GPAI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of general-purpose AI (GPAI) systems introduces\nsignificant new risks. Yet the infrastructure, practices, and norms for\nreporting flaws in GPAI systems remain seriously underdeveloped, lagging far\nbehind more established fields like software security. Based on a collaboration\nbetween experts from the fields of software security, machine learning, law,\nsocial science, and policy, we identify key gaps in the evaluation and\nreporting of flaws in GPAI systems. We call for three interventions to advance\nsystem safety. First, we propose using standardized AI flaw reports and rules\nof engagement for researchers in order to ease the process of submitting,\nreproducing, and triaging flaws in GPAI systems. Second, we propose GPAI system\nproviders adopt broadly-scoped flaw disclosure programs, borrowing from bug\nbounties, with legal safe harbors to protect researchers. Third, we advocate\nfor the development of improved infrastructure to coordinate distribution of\nflaw reports across the many stakeholders who may be impacted. These\ninterventions are increasingly urgent, as evidenced by the prevalence of\njailbreaks and other flaws that can transfer across different providers' GPAI\nsystems. By promoting robust reporting and coordination in the AI ecosystem,\nthese proposals could significantly improve the safety, security, and\naccountability of GPAI systems."
                },
                "authors": [
                    {
                        "name": "Shayne Longpre"
                    },
                    {
                        "name": "Kevin Klyman"
                    },
                    {
                        "name": "Ruth E. Appel"
                    },
                    {
                        "name": "Sayash Kapoor"
                    },
                    {
                        "name": "Rishi Bommasani"
                    },
                    {
                        "name": "Michelle Sahar"
                    },
                    {
                        "name": "Sean McGregor"
                    },
                    {
                        "name": "Avijit Ghosh"
                    },
                    {
                        "name": "Borhane Blili-Hamelin"
                    },
                    {
                        "name": "Nathan Butters"
                    },
                    {
                        "name": "Alondra Nelson"
                    },
                    {
                        "name": "Amit Elazari"
                    },
                    {
                        "name": "Andrew Sellars"
                    },
                    {
                        "name": "Casey John Ellis"
                    },
                    {
                        "name": "Dane Sherrets"
                    },
                    {
                        "name": "Dawn Song"
                    },
                    {
                        "name": "Harley Geiger"
                    },
                    {
                        "name": "Ilona Cohen"
                    },
                    {
                        "name": "Lauren McIlvenny"
                    },
                    {
                        "name": "Madhulika Srikumar"
                    },
                    {
                        "name": "Mark M. Jaycox"
                    },
                    {
                        "name": "Markus Anderljung"
                    },
                    {
                        "name": "Nadine Farid Johnson"
                    },
                    {
                        "name": "Nicholas Carlini"
                    },
                    {
                        "name": "Nicolas Miailhe"
                    },
                    {
                        "name": "Nik Marda"
                    },
                    {
                        "name": "Peter Henderson"
                    },
                    {
                        "name": "Rebecca S. Portnoff"
                    },
                    {
                        "name": "Rebecca Weiss"
                    },
                    {
                        "name": "Victoria Westerhoff"
                    },
                    {
                        "name": "Yacine Jernite"
                    },
                    {
                        "name": "Rumman Chowdhury"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Arvind Narayanan"
                    }
                ],
                "author_detail": {
                    "name": "Arvind Narayanan"
                },
                "author": "Arvind Narayanan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16861v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16861v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20021v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20021v2",
                "updated": "2025-03-25T05:11:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    5,
                    11,
                    24,
                    1,
                    84,
                    0
                ],
                "published": "2024-10-26T00:39:44Z",
                "published_parsed": [
                    2024,
                    10,
                    26,
                    0,
                    39,
                    44,
                    5,
                    300,
                    0
                ],
                "title": "Think Carefully and Check Again! Meta-Generation Unlocking LLMs for\n  Low-Resource Cross-Lingual Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Carefully and Check Again! Meta-Generation Unlocking LLMs for\n  Low-Resource Cross-Lingual Summarization"
                },
                "summary": "Cross-lingual summarization (CLS) aims to generate a summary for the source\ntext in a different target language. Currently, instruction-tuned large\nlanguage models (LLMs) excel at various English tasks. However, unlike\nlanguages such as English, Chinese or Spanish, for those relatively\nlow-resource languages with limited usage or data, recent studies have shown\nthat LLMs' performance on CLS tasks remains unsatisfactory even with few-shot\nsettings. This raises the question: Are LLMs capable of handling cross-lingual\nsummarization tasks for low-resource languages? To resolve this question, we\nfully explore the potential of large language models on cross-lingual\nsummarization task for low-resource languages through our four-step zero-shot\nmethod: Summarization, Improvement, Translation and Refinement (SITR) with\ncorrespondingly designed prompts. We test our proposed method with multiple\nLLMs on two well-known cross-lingual summarization datasets with various\nlow-resource target languages. The results show that: i) GPT-3.5 and GPT-4\nsignificantly and consistently outperform other baselines when using our\nzero-shot SITR methods. ii) By employing our proposed method, we unlock the\npotential of LLMs, enabling them to effectively handle cross-lingual\nsummarization tasks for relatively low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-lingual summarization (CLS) aims to generate a summary for the source\ntext in a different target language. Currently, instruction-tuned large\nlanguage models (LLMs) excel at various English tasks. However, unlike\nlanguages such as English, Chinese or Spanish, for those relatively\nlow-resource languages with limited usage or data, recent studies have shown\nthat LLMs' performance on CLS tasks remains unsatisfactory even with few-shot\nsettings. This raises the question: Are LLMs capable of handling cross-lingual\nsummarization tasks for low-resource languages? To resolve this question, we\nfully explore the potential of large language models on cross-lingual\nsummarization task for low-resource languages through our four-step zero-shot\nmethod: Summarization, Improvement, Translation and Refinement (SITR) with\ncorrespondingly designed prompts. We test our proposed method with multiple\nLLMs on two well-known cross-lingual summarization datasets with various\nlow-resource target languages. The results show that: i) GPT-3.5 and GPT-4\nsignificantly and consistently outperform other baselines when using our\nzero-shot SITR methods. ii) By employing our proposed method, we unlock the\npotential of LLMs, enabling them to effectively handle cross-lingual\nsummarization tasks for relatively low-resource languages."
                },
                "authors": [
                    {
                        "name": "Zhecheng Li"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Naifan Cheung"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Kai-wei Chang"
                    }
                ],
                "author_detail": {
                    "name": "Kai-wei Chang"
                },
                "author": "Kai-wei Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20021v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20021v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17993v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17993v4",
                "updated": "2025-03-25T05:10:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    5,
                    10,
                    39,
                    1,
                    84,
                    0
                ],
                "published": "2024-11-27T02:20:44Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    2,
                    20,
                    44,
                    2,
                    332,
                    0
                ],
                "title": "DRS: Deep Question Reformulation With Structured Output",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRS: Deep Question Reformulation With Structured Output"
                },
                "summary": "Question answering represents a core capability of large language models\n(LLMs). However, when individuals encounter unfamiliar knowledge in texts, they\noften formulate questions that the text itself cannot answer due to\ninsufficient understanding of the underlying information. Recent studies reveal\nthat while LLMs can detect unanswerable questions, they struggle to assist\nusers in reformulating these questions. Even advanced models like GPT-3.5\ndemonstrate limited effectiveness in this regard. To address this limitation,\nwe propose DRS: Deep Question Reformulation with Structured Output, a novel\nzero-shot method aimed at enhancing LLMs ability to assist users in\nreformulating questions to extract relevant information from new documents. DRS\ncombines the strengths of LLMs with a DFS-based algorithm to iteratively\nexplore potential entity combinations and constrain outputs using predefined\nentities. This structured approach significantly enhances the reformulation\ncapabilities of LLMs. Comprehensive experimental evaluations demonstrate that\nDRS improves the reformulation accuracy of GPT-3.5 from 23.03% to 70.42%, while\nalso enhancing the performance of open-source models, such as Gemma2-9B, from\n26.35% to 56.75%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question answering represents a core capability of large language models\n(LLMs). However, when individuals encounter unfamiliar knowledge in texts, they\noften formulate questions that the text itself cannot answer due to\ninsufficient understanding of the underlying information. Recent studies reveal\nthat while LLMs can detect unanswerable questions, they struggle to assist\nusers in reformulating these questions. Even advanced models like GPT-3.5\ndemonstrate limited effectiveness in this regard. To address this limitation,\nwe propose DRS: Deep Question Reformulation with Structured Output, a novel\nzero-shot method aimed at enhancing LLMs ability to assist users in\nreformulating questions to extract relevant information from new documents. DRS\ncombines the strengths of LLMs with a DFS-based algorithm to iteratively\nexplore potential entity combinations and constrain outputs using predefined\nentities. This structured approach significantly enhances the reformulation\ncapabilities of LLMs. Comprehensive experimental evaluations demonstrate that\nDRS improves the reformulation accuracy of GPT-3.5 from 23.03% to 70.42%, while\nalso enhancing the performance of open-source models, such as Gemma2-9B, from\n26.35% to 56.75%."
                },
                "authors": [
                    {
                        "name": "Zhecheng Li"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Wei Chang"
                },
                "author": "Kai-Wei Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17993v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17993v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20016v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20016v2",
                "updated": "2025-03-25T05:09:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    5,
                    9,
                    53,
                    1,
                    84,
                    0
                ],
                "published": "2024-10-26T00:16:08Z",
                "published_parsed": [
                    2024,
                    10,
                    26,
                    0,
                    16,
                    8,
                    5,
                    300,
                    0
                ],
                "title": "Vulnerability of LLMs to Vertically Aligned Text Manipulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vulnerability of LLMs to Vertically Aligned Text Manipulations"
                },
                "summary": "Text classification involves categorizing a given text, such as determining\nits sentiment or identifying harmful content. With the advancement of large\nlanguage models (LLMs), these models have become highly effective at performing\ntext classification tasks. However, they still show vulnerabilities to\nvariations in text formatting. Recent research demonstrates that modifying\ninput formats, such as vertically aligning words for encoder-based models, can\nsubstantially lower accuracy in text classification tasks. While easily\nunderstood by humans, these inputs can significantly mislead models, posing a\npotential risk of bypassing detection in real-world scenarios involving harmful\nor sensitive information. With the expanding application of LLMs, a crucial\nquestion arises: Do decoder-based LLMs exhibit similar vulnerabilities to\nvertically formatted text input? In this paper, we investigate the impact of\nvertical text input on the performance of various LLMs across multiple text\nclassification datasets and analyze the underlying causes. Our findings are as\nfollows: (i) Vertical text input significantly degrades the accuracy of LLMs in\ntext classification tasks. (ii) Chain of Thought (CoT) reasoning does not help\nLLMs recognize vertical input or mitigate its vulnerability, but few-shot\nlearning with careful analysis does. (iii) We explore the underlying cause of\nthe vulnerability by analyzing the inherent issues in tokenization and\nattention matrices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text classification involves categorizing a given text, such as determining\nits sentiment or identifying harmful content. With the advancement of large\nlanguage models (LLMs), these models have become highly effective at performing\ntext classification tasks. However, they still show vulnerabilities to\nvariations in text formatting. Recent research demonstrates that modifying\ninput formats, such as vertically aligning words for encoder-based models, can\nsubstantially lower accuracy in text classification tasks. While easily\nunderstood by humans, these inputs can significantly mislead models, posing a\npotential risk of bypassing detection in real-world scenarios involving harmful\nor sensitive information. With the expanding application of LLMs, a crucial\nquestion arises: Do decoder-based LLMs exhibit similar vulnerabilities to\nvertically formatted text input? In this paper, we investigate the impact of\nvertical text input on the performance of various LLMs across multiple text\nclassification datasets and analyze the underlying causes. Our findings are as\nfollows: (i) Vertical text input significantly degrades the accuracy of LLMs in\ntext classification tasks. (ii) Chain of Thought (CoT) reasoning does not help\nLLMs recognize vertical input or mitigate its vulnerability, but few-shot\nlearning with careful analysis does. (iii) We explore the underlying cause of\nthe vulnerability by analyzing the inherent issues in tokenization and\nattention matrices."
                },
                "authors": [
                    {
                        "name": "Zhecheng Li"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Zhen Xiong"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Kai-wei Chang"
                    }
                ],
                "author_detail": {
                    "name": "Kai-wei Chang"
                },
                "author": "Kai-wei Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20016v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20016v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19353v1",
                "updated": "2025-03-25T05:03:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    5,
                    3,
                    56,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T05:03:56Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    5,
                    3,
                    56,
                    1,
                    84,
                    0
                ],
                "title": "QUAD: Quantization and Parameter-Efficient Tuning of LLM with Activation\n  Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QUAD: Quantization and Parameter-Efficient Tuning of LLM with Activation\n  Decomposition"
                },
                "summary": "Large Language Models (LLMs) excel in diverse applications but suffer\ninefficiency due to massive scale. While quantization reduces computational\ncosts, existing methods degrade accuracy in medium-sized LLMs (e.g.,\nLlama-3-8B) due to activation outliers. To address this, we propose QUAD\n(Quantization with Activation Decomposition), a framework leveraging Singular\nValue Decomposition (SVD) to suppress activation outliers for effective 4-bit\nquantization. QUAD estimates activation singular vectors offline using\ncalibration data to construct an orthogonal transformation matrix P, shifting\noutliers to additional dimensions in full precision while quantizing rest\ncomponents to 4-bit. Additionally, QUAD enables parameter-efficient fine-tuning\nvia adaptable full-precision outlier weights, narrowing the accuracy gap\nbetween quantized and full-precision models. Experiments demonstrate that QUAD\nachieves 94% ~ 96% accuracy under W4A4 quantization and 98% accuracy with\nW4A4/A8 and parameter-efficient fine-tuning for Llama-3 and Qwen-2.5 models.\nOur code is available at \\href{https://github.com/hyx1999/Quad}{repository}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in diverse applications but suffer\ninefficiency due to massive scale. While quantization reduces computational\ncosts, existing methods degrade accuracy in medium-sized LLMs (e.g.,\nLlama-3-8B) due to activation outliers. To address this, we propose QUAD\n(Quantization with Activation Decomposition), a framework leveraging Singular\nValue Decomposition (SVD) to suppress activation outliers for effective 4-bit\nquantization. QUAD estimates activation singular vectors offline using\ncalibration data to construct an orthogonal transformation matrix P, shifting\noutliers to additional dimensions in full precision while quantizing rest\ncomponents to 4-bit. Additionally, QUAD enables parameter-efficient fine-tuning\nvia adaptable full-precision outlier weights, narrowing the accuracy gap\nbetween quantized and full-precision models. Experiments demonstrate that QUAD\nachieves 94% ~ 96% accuracy under W4A4 quantization and 98% accuracy with\nW4A4/A8 and parameter-efficient fine-tuning for Llama-3 and Qwen-2.5 models.\nOur code is available at \\href{https://github.com/hyx1999/Quad}{repository}."
                },
                "authors": [
                    {
                        "name": "Yuxuan Hu"
                    },
                    {
                        "name": "Xiaodong Chen"
                    },
                    {
                        "name": "Cuiping Li"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Jing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Zhang"
                },
                "author": "Jing Zhang",
                "arxiv_comment": "18 pages, 8 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19351v1",
                "updated": "2025-03-25T05:00:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    5,
                    0,
                    11,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T05:00:11Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    5,
                    0,
                    11,
                    1,
                    84,
                    0
                ],
                "title": "Multi-Object Sketch Animation by Scene Decomposition and Motion Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Object Sketch Animation by Scene Decomposition and Motion Planning"
                },
                "summary": "Sketch animation, which brings static sketches to life by generating dynamic\nvideo sequences, has found widespread applications in GIF design, cartoon\nproduction, and daily entertainment. While current sketch animation methods\nperform well in single-object sketch animation, they struggle in multi-object\nscenarios. By analyzing their failures, we summarize two challenges of\ntransitioning from single-object to multi-object sketch animation: object-aware\nmotion modeling and complex motion optimization. For multi-object sketch\nanimation, we propose MoSketch based on iterative optimization through Score\nDistillation Sampling (SDS), without any other data for training. We propose\nfour modules: LLM-based scene decomposition, LLM-based motion planning, motion\nrefinement network and compositional SDS, to tackle the two challenges in a\ndivide-and-conquer strategy. Extensive qualitative and quantitative experiments\ndemonstrate the superiority of our method over existing sketch animation\napproaches. MoSketch takes a pioneering step towards multi-object sketch\nanimation, opening new avenues for future research and applications. The code\nwill be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sketch animation, which brings static sketches to life by generating dynamic\nvideo sequences, has found widespread applications in GIF design, cartoon\nproduction, and daily entertainment. While current sketch animation methods\nperform well in single-object sketch animation, they struggle in multi-object\nscenarios. By analyzing their failures, we summarize two challenges of\ntransitioning from single-object to multi-object sketch animation: object-aware\nmotion modeling and complex motion optimization. For multi-object sketch\nanimation, we propose MoSketch based on iterative optimization through Score\nDistillation Sampling (SDS), without any other data for training. We propose\nfour modules: LLM-based scene decomposition, LLM-based motion planning, motion\nrefinement network and compositional SDS, to tackle the two challenges in a\ndivide-and-conquer strategy. Extensive qualitative and quantitative experiments\ndemonstrate the superiority of our method over existing sketch animation\napproaches. MoSketch takes a pioneering step towards multi-object sketch\nanimation, opening new avenues for future research and applications. The code\nwill be released."
                },
                "authors": [
                    {
                        "name": "Jingyu Liu"
                    },
                    {
                        "name": "Zijie Xin"
                    },
                    {
                        "name": "Yuhan Fu"
                    },
                    {
                        "name": "Ruixiang Zhao"
                    },
                    {
                        "name": "Bangxiang Lan"
                    },
                    {
                        "name": "Xirong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xirong Li"
                },
                "author": "Xirong Li",
                "arxiv_comment": "16 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17514v2",
                "updated": "2025-03-25T04:43:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    4,
                    43,
                    33,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-21T19:57:04Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    19,
                    57,
                    4,
                    4,
                    80,
                    0
                ],
                "title": "Language Models May Verbatim Complete Text They Were Not Explicitly\n  Trained On",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models May Verbatim Complete Text They Were Not Explicitly\n  Trained On"
                },
                "summary": "An important question today is whether a given text was used to train a large\nlanguage model (LLM). A \\emph{completion} test is often employed: check if the\nLLM completes a sufficiently complex text. This, however, requires a\nground-truth definition of membership; most commonly, it is defined as a member\nbased on the $n$-gram overlap between the target text and any text in the\ndataset. In this work, we demonstrate that this $n$-gram based membership\ndefinition can be effectively gamed. We study scenarios where sequences are\n\\emph{non-members} for a given $n$ and we find that completion tests still\nsucceed. We find many natural cases of this phenomenon by retraining LLMs from\nscratch after removing all training samples that were completed; these cases\ninclude exact duplicates, near-duplicates, and even short overlaps. They\nshowcase that it is difficult to find a single viable choice of $n$ for\nmembership definitions. Using these insights, we design adversarial datasets\nthat can cause a given target sequence to be completed without containing it,\nfor any reasonable choice of $n$. Our findings highlight the inadequacy of\n$n$-gram membership, suggesting membership definitions fail to account for\nauxiliary information available to the training algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An important question today is whether a given text was used to train a large\nlanguage model (LLM). A \\emph{completion} test is often employed: check if the\nLLM completes a sufficiently complex text. This, however, requires a\nground-truth definition of membership; most commonly, it is defined as a member\nbased on the $n$-gram overlap between the target text and any text in the\ndataset. In this work, we demonstrate that this $n$-gram based membership\ndefinition can be effectively gamed. We study scenarios where sequences are\n\\emph{non-members} for a given $n$ and we find that completion tests still\nsucceed. We find many natural cases of this phenomenon by retraining LLMs from\nscratch after removing all training samples that were completed; these cases\ninclude exact duplicates, near-duplicates, and even short overlaps. They\nshowcase that it is difficult to find a single viable choice of $n$ for\nmembership definitions. Using these insights, we design adversarial datasets\nthat can cause a given target sequence to be completed without containing it,\nfor any reasonable choice of $n$. Our findings highlight the inadequacy of\n$n$-gram membership, suggesting membership definitions fail to account for\nauxiliary information available to the training algorithm."
                },
                "authors": [
                    {
                        "name": "Ken Ziyu Liu"
                    },
                    {
                        "name": "Christopher A. Choquette-Choo"
                    },
                    {
                        "name": "Matthew Jagielski"
                    },
                    {
                        "name": "Peter Kairouz"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Nicolas Papernot"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Papernot"
                },
                "author": "Nicolas Papernot",
                "arxiv_comment": "Main text: 9 pages, 7 figures, 1 table. Appendix: 29 pages, 20\n  tables, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20916v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20916v2",
                "updated": "2025-03-25T04:12:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    4,
                    12,
                    27,
                    1,
                    84,
                    0
                ],
                "published": "2025-02-28T10:17:07Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    10,
                    17,
                    7,
                    4,
                    59,
                    0
                ],
                "title": "COCOA: a compact Compton camera for astrophysical observation of\n  MeV-scale gamma rays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COCOA: a compact Compton camera for astrophysical observation of\n  MeV-scale gamma rays"
                },
                "summary": "COCOA (COmpact COmpton cAmera) is a next-generation, cost-effective gamma-ray\ntelescope designed for astrophysical observations in the MeV energy range. The\ndetector comprises a scatterer volume employing the LiquidO detection\ntechnology and an array of scintillating crystals acting as absorber.\nSurrounding plastic scintillator panels serve as a veto system for charged\nparticles. The detector's compact, scalable design enables flexible deployment\non microsatellites or high-altitude balloons. Gamma rays at MeV energies have\nnot been well explored historically (the so-called \"MeV gap\") and COCOA has the\npotential to improve the sensitivity in this energy band by more than one order\nof magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COCOA (COmpact COmpton cAmera) is a next-generation, cost-effective gamma-ray\ntelescope designed for astrophysical observations in the MeV energy range. The\ndetector comprises a scatterer volume employing the LiquidO detection\ntechnology and an array of scintillating crystals acting as absorber.\nSurrounding plastic scintillator panels serve as a veto system for charged\nparticles. The detector's compact, scalable design enables flexible deployment\non microsatellites or high-altitude balloons. Gamma rays at MeV energies have\nnot been well explored historically (the so-called \"MeV gap\") and COCOA has the\npotential to improve the sensitivity in this energy band by more than one order\nof magnitude."
                },
                "authors": [
                    {
                        "name": "LiquidO Collaboration"
                    },
                    {
                        "name": "S. R. Soleti"
                    },
                    {
                        "name": "J. J. Gómez-Cadenas"
                    },
                    {
                        "name": "J. Apilluelo"
                    },
                    {
                        "name": "L. Asquith"
                    },
                    {
                        "name": "E. F. Bannister"
                    },
                    {
                        "name": "N. P. Barradas"
                    },
                    {
                        "name": "C. L. Baylis"
                    },
                    {
                        "name": "J. L. Beney"
                    },
                    {
                        "name": "M. Berberan e Santos"
                    },
                    {
                        "name": "X. de la Bernardie"
                    },
                    {
                        "name": "T. J. C. Bezerra"
                    },
                    {
                        "name": "M. Bongrand"
                    },
                    {
                        "name": "C. Bourgeois"
                    },
                    {
                        "name": "D. Breton"
                    },
                    {
                        "name": "J. Busto"
                    },
                    {
                        "name": "K. Burns"
                    },
                    {
                        "name": "A. Cabrera"
                    },
                    {
                        "name": "A. Cadiou"
                    },
                    {
                        "name": "E. Calvo"
                    },
                    {
                        "name": "M. de Carlos Generowicz"
                    },
                    {
                        "name": "E. Chauveau"
                    },
                    {
                        "name": "B. J. Cattermole"
                    },
                    {
                        "name": "M. Chen"
                    },
                    {
                        "name": "P. Chimenti"
                    },
                    {
                        "name": "D. F. Cowen"
                    },
                    {
                        "name": "S. Kr. Das"
                    },
                    {
                        "name": "S. Dusini"
                    },
                    {
                        "name": "A. Earle"
                    },
                    {
                        "name": "M. Felizardo"
                    },
                    {
                        "name": "C. Frigerio Martins"
                    },
                    {
                        "name": "J. Galán"
                    },
                    {
                        "name": "J. A. García"
                    },
                    {
                        "name": "R. Gazzini"
                    },
                    {
                        "name": "A. Gibson-Foster"
                    },
                    {
                        "name": "C. Girard-Carillo"
                    },
                    {
                        "name": "W. C. Griffith"
                    },
                    {
                        "name": "M. Guitière"
                    },
                    {
                        "name": "F. Haddad"
                    },
                    {
                        "name": "J. Hartnell"
                    },
                    {
                        "name": "A. Holin"
                    },
                    {
                        "name": "I. G. Irastorza"
                    },
                    {
                        "name": "I. Jovanovic"
                    },
                    {
                        "name": "A. Kling"
                    },
                    {
                        "name": "L. Koch"
                    },
                    {
                        "name": "P. Lasorak"
                    },
                    {
                        "name": "J. F. Le Du"
                    },
                    {
                        "name": "F. Lefevre"
                    },
                    {
                        "name": "P. Loaiza"
                    },
                    {
                        "name": "J. A. Lock"
                    },
                    {
                        "name": "G. Luzón"
                    },
                    {
                        "name": "J. Maalmi"
                    },
                    {
                        "name": "J. P. Malhado"
                    },
                    {
                        "name": "F. Mantovani"
                    },
                    {
                        "name": "J. G. Marques"
                    },
                    {
                        "name": "C. Marquet"
                    },
                    {
                        "name": "M. Martínez"
                    },
                    {
                        "name": "D. Navas-Nicolás"
                    },
                    {
                        "name": "H. Nunokawa"
                    },
                    {
                        "name": "J. P. Ochoa-Ricoux"
                    },
                    {
                        "name": "T. Palmeira"
                    },
                    {
                        "name": "C. Palomares"
                    },
                    {
                        "name": "D. Petyt"
                    },
                    {
                        "name": "P. Pillot"
                    },
                    {
                        "name": "A. Pin"
                    },
                    {
                        "name": "J. C. C. Porter"
                    },
                    {
                        "name": "M. S. Pravikoff"
                    },
                    {
                        "name": "S. Richards"
                    },
                    {
                        "name": "N. Rodrigues"
                    },
                    {
                        "name": "M. Roche"
                    },
                    {
                        "name": "R. Rosero"
                    },
                    {
                        "name": "B. Roskovec"
                    },
                    {
                        "name": "N. Roy"
                    },
                    {
                        "name": "M. L. Sarsa"
                    },
                    {
                        "name": "A. Serafini"
                    },
                    {
                        "name": "C. Shepherd-Themistocleous"
                    },
                    {
                        "name": "W. Shorrock"
                    },
                    {
                        "name": "M. Silva"
                    },
                    {
                        "name": "L. Simard"
                    },
                    {
                        "name": "D. Stocco"
                    },
                    {
                        "name": "V. Strati"
                    },
                    {
                        "name": "J. S. Stutzmann"
                    },
                    {
                        "name": "F. Suekane"
                    },
                    {
                        "name": "N. Tuccori"
                    },
                    {
                        "name": "A. Verdugo"
                    },
                    {
                        "name": "B. Viaud"
                    },
                    {
                        "name": "S. M. Wakely"
                    },
                    {
                        "name": "G. Wendel"
                    },
                    {
                        "name": "A. S. Wilhelm"
                    },
                    {
                        "name": "A. W. R. Wong"
                    },
                    {
                        "name": "M. Yeh"
                    },
                    {
                        "name": "F. Yermia"
                    }
                ],
                "author_detail": {
                    "name": "F. Yermia"
                },
                "author": "F. Yermia",
                "arxiv_comment": "12 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20916v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20916v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19338v1",
                "updated": "2025-03-25T04:11:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    4,
                    11,
                    47,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T04:11:47Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    4,
                    11,
                    47,
                    1,
                    84,
                    0
                ],
                "title": "Membership Inference Attacks on Large-Scale Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership Inference Attacks on Large-Scale Models: A Survey"
                },
                "summary": "The adoption of the Large Language Model (LLM) has accelerated dramatically\nsince the ChatGPT from OpenAI went online in November 2022. Recent advances in\nLarge Multimodal Models (LMMs), which process diverse data types and enable\ninteraction through various channels, have expanded beyond the text-to-text\nlimitations of early LLMs, attracting significant and concurrent attention from\nboth researchers and industry. While LLMs and LMMs are starting to spread\nwidely, concerns about their privacy risks are increasing as well. Membership\nInference Attacks (MIAs), techniques used to determine whether a particular\ndata point was part of a model's training set, serve as a key metric for\nassessing the privacy vulnerabilities of machine learning models. Hu et al.\nshow that various machine learning algorithms are vulnerable to MIA. Despite\nextensive studies on MIAs in traditional models, there remains a lack of\nsystematic surveys addressing their effectiveness and implications in modern\nlarge-scale models like LLMs and LMMs. In this paper, we systematically\nreviewed recent studies of MIA against LLMs and LMMs. We analyzed and\ncategorized each attack based on their methodology and scenario and discussed\nthe limitations in existing research. Additionally, we examine privacy concerns\nassociated with the fine-tuning process. Finally, we provided some suggestions\nfor future research in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of the Large Language Model (LLM) has accelerated dramatically\nsince the ChatGPT from OpenAI went online in November 2022. Recent advances in\nLarge Multimodal Models (LMMs), which process diverse data types and enable\ninteraction through various channels, have expanded beyond the text-to-text\nlimitations of early LLMs, attracting significant and concurrent attention from\nboth researchers and industry. While LLMs and LMMs are starting to spread\nwidely, concerns about their privacy risks are increasing as well. Membership\nInference Attacks (MIAs), techniques used to determine whether a particular\ndata point was part of a model's training set, serve as a key metric for\nassessing the privacy vulnerabilities of machine learning models. Hu et al.\nshow that various machine learning algorithms are vulnerable to MIA. Despite\nextensive studies on MIAs in traditional models, there remains a lack of\nsystematic surveys addressing their effectiveness and implications in modern\nlarge-scale models like LLMs and LMMs. In this paper, we systematically\nreviewed recent studies of MIA against LLMs and LMMs. We analyzed and\ncategorized each attack based on their methodology and scenario and discussed\nthe limitations in existing research. Additionally, we examine privacy concerns\nassociated with the fine-tuning process. Finally, we provided some suggestions\nfor future research in this direction."
                },
                "authors": [
                    {
                        "name": "Hengyu Wu"
                    },
                    {
                        "name": "Yang Cao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Cao"
                },
                "author": "Yang Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13420v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13420v2",
                "updated": "2025-03-25T03:43:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    3,
                    43,
                    57,
                    1,
                    84,
                    0
                ],
                "published": "2025-01-23T06:48:48Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    6,
                    48,
                    48,
                    3,
                    23,
                    0
                ],
                "title": "LVFace: Progressive Cluster Optimization for Large Vision Models in Face\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LVFace: Progressive Cluster Optimization for Large Vision Models in Face\n  Recognition"
                },
                "summary": "Vision Transformers (ViTs) have revolutionized large-scale visual modeling,\nyet remain underexplored in face recognition (FR) where CNNs still dominate. We\nidentify a critical bottleneck: CNN-inspired training paradigms fail to unlock\nViT's potential, leading to suboptimal performance and convergence\ninstability.To address this challenge, we propose LVFace, a ViT-based FR model\nthat integrates Progressive Cluster Optimization (PCO) to achieve superior\nresults. Specifically, PCO sequentially applies negative class sub-sampling\n(NCS) for robust and fast feature alignment from random initialization, feature\nexpectation penalties for centroid stabilization, performing cluster boundary\nrefinement through full-batch training without NCS constraints. LVFace\nestablishes a new state-of-the-art face recognition baseline, surpassing\nleading approaches such as UniFace and TopoFR across multiple benchmarks.\nExtensive experiments demonstrate that LVFace delivers consistent performance\ngains, while exhibiting scalability to large-scale datasets and compatibility\nwith mainstream VLMs and LLMs. Notably, LVFace secured 1st place in the ICCV\n2021 Masked Face Recognition (MFR)-Ongoing Challenge (March 2025), proving its\nefficacy in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Transformers (ViTs) have revolutionized large-scale visual modeling,\nyet remain underexplored in face recognition (FR) where CNNs still dominate. We\nidentify a critical bottleneck: CNN-inspired training paradigms fail to unlock\nViT's potential, leading to suboptimal performance and convergence\ninstability.To address this challenge, we propose LVFace, a ViT-based FR model\nthat integrates Progressive Cluster Optimization (PCO) to achieve superior\nresults. Specifically, PCO sequentially applies negative class sub-sampling\n(NCS) for robust and fast feature alignment from random initialization, feature\nexpectation penalties for centroid stabilization, performing cluster boundary\nrefinement through full-batch training without NCS constraints. LVFace\nestablishes a new state-of-the-art face recognition baseline, surpassing\nleading approaches such as UniFace and TopoFR across multiple benchmarks.\nExtensive experiments demonstrate that LVFace delivers consistent performance\ngains, while exhibiting scalability to large-scale datasets and compatibility\nwith mainstream VLMs and LLMs. Notably, LVFace secured 1st place in the ICCV\n2021 Masked Face Recognition (MFR)-Ongoing Challenge (March 2025), proving its\nefficacy in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Jinghan You"
                    },
                    {
                        "name": "Shanglin Li"
                    },
                    {
                        "name": "Yuanrui Sun"
                    },
                    {
                        "name": "Jiangchuan Wei"
                    },
                    {
                        "name": "Mingyu Guo"
                    },
                    {
                        "name": "Chao Feng"
                    },
                    {
                        "name": "Jiao Ran"
                    }
                ],
                "author_detail": {
                    "name": "Jiao Ran"
                },
                "author": "Jiao Ran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13420v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13420v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19326v1",
                "updated": "2025-03-25T03:43:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    3,
                    43,
                    11,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T03:43:11Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    3,
                    43,
                    11,
                    1,
                    84,
                    0
                ],
                "title": "Process or Result? Manipulated Ending Tokens Can Mislead Reasoning LLMs\n  to Ignore the Correct Reasoning Steps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process or Result? Manipulated Ending Tokens Can Mislead Reasoning LLMs\n  to Ignore the Correct Reasoning Steps"
                },
                "summary": "Recent reasoning large language models (LLMs) have demonstrated remarkable\nimprovements in mathematical reasoning capabilities through long\nChain-of-Thought. The reasoning tokens of these models enable self-correction\nwithin reasoning chains, enhancing robustness. This motivates our exploration:\nhow vulnerable are reasoning LLMs to subtle errors in their input reasoning\nchains? We introduce \"Compromising Thought\" (CPT), a vulnerability where models\npresented with reasoning tokens containing manipulated calculation results tend\nto ignore correct reasoning steps and adopt incorrect results instead. Through\nsystematic evaluation across multiple reasoning LLMs, we design three\nincreasingly explicit prompting methods to measure CPT resistance, revealing\nthat models struggle significantly to identify and correct these manipulations.\nNotably, contrary to existing research suggesting structural alterations affect\nmodel performance more than content modifications, we find that local ending\ntoken manipulations have greater impact on reasoning outcomes than structural\nchanges. Moreover, we discover a security vulnerability in DeepSeek-R1 where\ntampered reasoning tokens can trigger complete reasoning cessation. Our work\nenhances understanding of reasoning robustness and highlights security\nconsiderations for reasoning-intensive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent reasoning large language models (LLMs) have demonstrated remarkable\nimprovements in mathematical reasoning capabilities through long\nChain-of-Thought. The reasoning tokens of these models enable self-correction\nwithin reasoning chains, enhancing robustness. This motivates our exploration:\nhow vulnerable are reasoning LLMs to subtle errors in their input reasoning\nchains? We introduce \"Compromising Thought\" (CPT), a vulnerability where models\npresented with reasoning tokens containing manipulated calculation results tend\nto ignore correct reasoning steps and adopt incorrect results instead. Through\nsystematic evaluation across multiple reasoning LLMs, we design three\nincreasingly explicit prompting methods to measure CPT resistance, revealing\nthat models struggle significantly to identify and correct these manipulations.\nNotably, contrary to existing research suggesting structural alterations affect\nmodel performance more than content modifications, we find that local ending\ntoken manipulations have greater impact on reasoning outcomes than structural\nchanges. Moreover, we discover a security vulnerability in DeepSeek-R1 where\ntampered reasoning tokens can trigger complete reasoning cessation. Our work\nenhances understanding of reasoning robustness and highlights security\nconsiderations for reasoning-intensive applications."
                },
                "authors": [
                    {
                        "name": "Yu Cui"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Yiwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yiwei Wang"
                },
                "author": "Yiwei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08795v2",
                "updated": "2025-03-25T03:19:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    3,
                    19,
                    51,
                    1,
                    84,
                    0
                ],
                "published": "2024-12-11T22:01:30Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    22,
                    1,
                    30,
                    2,
                    346,
                    0
                ],
                "title": "Coverage-based Fairness in Multi-document Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coverage-based Fairness in Multi-document Summarization"
                },
                "summary": "Fairness in multi-document summarization (MDS) measures whether a system can\ngenerate a summary fairly representing information from documents with\ndifferent social attribute values. Fairness in MDS is crucial since a fair\nsummary can offer readers a comprehensive view. Previous works focus on\nquantifying summary-level fairness using Proportional Representation, a\nfairness measure based on Statistical Parity. However, Proportional\nRepresentation does not consider redundancy in input documents and overlooks\ncorpus-level unfairness. In this work, we propose a new summary-level fairness\nmeasure, Equal Coverage, which is based on coverage of documents with different\nsocial attribute values and considers the redundancy within documents. To\ndetect the corpus-level unfairness, we propose a new corpus-level measure,\nCoverage Parity. Our human evaluations show that our measures align more with\nour definition of fairness. Using our measures, we evaluate the fairness of\nthirteen different LLMs. We find that Claude3-sonnet is the fairest among all\nevaluated LLMs. We also find that almost all LLMs overrepresent different\nsocial attribute values. The code is available at\nhttps://github.com/leehaoyuan/coverage_fairness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fairness in multi-document summarization (MDS) measures whether a system can\ngenerate a summary fairly representing information from documents with\ndifferent social attribute values. Fairness in MDS is crucial since a fair\nsummary can offer readers a comprehensive view. Previous works focus on\nquantifying summary-level fairness using Proportional Representation, a\nfairness measure based on Statistical Parity. However, Proportional\nRepresentation does not consider redundancy in input documents and overlooks\ncorpus-level unfairness. In this work, we propose a new summary-level fairness\nmeasure, Equal Coverage, which is based on coverage of documents with different\nsocial attribute values and considers the redundancy within documents. To\ndetect the corpus-level unfairness, we propose a new corpus-level measure,\nCoverage Parity. Our human evaluations show that our measures align more with\nour definition of fairness. Using our measures, we evaluate the fairness of\nthirteen different LLMs. We find that Claude3-sonnet is the fairest among all\nevaluated LLMs. We also find that almost all LLMs overrepresent different\nsocial attribute values. The code is available at\nhttps://github.com/leehaoyuan/coverage_fairness."
                },
                "authors": [
                    {
                        "name": "Haoyuan Li"
                    },
                    {
                        "name": "Yusen Zhang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Snigdha Chaturvedi"
                    }
                ],
                "author_detail": {
                    "name": "Snigdha Chaturvedi"
                },
                "author": "Snigdha Chaturvedi",
                "arxiv_comment": "accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19312v1",
                "updated": "2025-03-25T03:18:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    3,
                    18,
                    46,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T03:18:46Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    3,
                    18,
                    46,
                    1,
                    84,
                    0
                ],
                "title": "ImageGen-CoT: Enhancing Text-to-Image In-context Learning with\n  Chain-of-Thought Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ImageGen-CoT: Enhancing Text-to-Image In-context Learning with\n  Chain-of-Thought Reasoning"
                },
                "summary": "In this work, we study the problem of Text-to-Image In-Context Learning\n(T2I-ICL). While Unified Multimodal LLMs (MLLMs) have advanced rapidly in\nrecent years, they struggle with contextual reasoning in T2I-ICL scenarios. To\naddress this limitation, we propose a novel framework that incorporates a\nthought process called ImageGen-CoT prior to image generation. To avoid\ngenerating unstructured ineffective reasoning steps, we develop an automatic\npipeline to curate a high-quality ImageGen-CoT dataset. We then fine-tune MLLMs\nusing this dataset to enhance their contextual reasoning capabilities. To\nfurther enhance performance, we explore test-time scale-up strategies and\npropose a novel hybrid scaling approach. This approach first generates multiple\nImageGen-CoT chains and then produces multiple images for each chain via\nsampling. Extensive experiments demonstrate the effectiveness of our proposed\nmethod. Notably, fine-tuning with the ImageGen-CoT dataset leads to a\nsubstantial 80\\% performance gain for SEED-X on T2I-ICL tasks. See our project\npage at https://ImageGen-CoT.github.io/. Code and model weights will be\nopen-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we study the problem of Text-to-Image In-Context Learning\n(T2I-ICL). While Unified Multimodal LLMs (MLLMs) have advanced rapidly in\nrecent years, they struggle with contextual reasoning in T2I-ICL scenarios. To\naddress this limitation, we propose a novel framework that incorporates a\nthought process called ImageGen-CoT prior to image generation. To avoid\ngenerating unstructured ineffective reasoning steps, we develop an automatic\npipeline to curate a high-quality ImageGen-CoT dataset. We then fine-tune MLLMs\nusing this dataset to enhance their contextual reasoning capabilities. To\nfurther enhance performance, we explore test-time scale-up strategies and\npropose a novel hybrid scaling approach. This approach first generates multiple\nImageGen-CoT chains and then produces multiple images for each chain via\nsampling. Extensive experiments demonstrate the effectiveness of our proposed\nmethod. Notably, fine-tuning with the ImageGen-CoT dataset leads to a\nsubstantial 80\\% performance gain for SEED-X on T2I-ICL tasks. See our project\npage at https://ImageGen-CoT.github.io/. Code and model weights will be\nopen-sourced."
                },
                "authors": [
                    {
                        "name": "Jiaqi Liao"
                    },
                    {
                        "name": "Zhengyuan Yang"
                    },
                    {
                        "name": "Linjie Li"
                    },
                    {
                        "name": "Dianqi Li"
                    },
                    {
                        "name": "Kevin Lin"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Lijuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lijuan Wang"
                },
                "author": "Lijuan Wang",
                "arxiv_comment": "Project Page: https://ImageGen-CoT.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19309v1",
                "updated": "2025-03-25T03:14:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    3,
                    14,
                    53,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T03:14:53Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    3,
                    14,
                    53,
                    1,
                    84,
                    0
                ],
                "title": "Iterative Hypothesis Generation for Scientific Discovery with Monte\n  Carlo Nash Equilibrium Self-Refining Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative Hypothesis Generation for Scientific Discovery with Monte\n  Carlo Nash Equilibrium Self-Refining Trees"
                },
                "summary": "Scientific hypothesis generation is a fundamentally challenging task in\nresearch, requiring the synthesis of novel and empirically grounded insights.\nTraditional approaches rely on human intuition and domain expertise, while\npurely large language model (LLM) based methods often struggle to produce\nhypotheses that are both innovative and reliable. To address these limitations,\nwe propose the Monte Carlo Nash Equilibrium Self-Refine Tree (MC-NEST), a novel\nframework that integrates Monte Carlo Tree Search with Nash Equilibrium\nstrategies to iteratively refine and validate hypotheses. MC-NEST dynamically\nbalances exploration and exploitation through adaptive sampling strategies,\nwhich prioritize high-potential hypotheses while maintaining diversity in the\nsearch space. We demonstrate the effectiveness of MC-NEST through comprehensive\nexperiments across multiple domains, including biomedicine, social science, and\ncomputer science. MC-NEST achieves average scores of 2.65, 2.74, and 2.80 (on a\n1-3 scale) for novelty, clarity, significance, and verifiability metrics on the\nsocial science, computer science, and biomedicine datasets, respectively,\noutperforming state-of-the-art prompt-based methods, which achieve 2.36, 2.51,\nand 2.52 on the same datasets. These results underscore MC-NEST's ability to\ngenerate high-quality, empirically grounded hypotheses across diverse domains.\nFurthermore, MC-NEST facilitates structured human-AI collaboration, ensuring\nthat LLMs augment human creativity rather than replace it. By addressing key\nchallenges such as iterative refinement and the exploration-exploitation\nbalance, MC-NEST sets a new benchmark in automated hypothesis generation.\nAdditionally, MC-NEST's ethical design enables responsible AI use, emphasizing\ntransparency and human supervision in hypothesis generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific hypothesis generation is a fundamentally challenging task in\nresearch, requiring the synthesis of novel and empirically grounded insights.\nTraditional approaches rely on human intuition and domain expertise, while\npurely large language model (LLM) based methods often struggle to produce\nhypotheses that are both innovative and reliable. To address these limitations,\nwe propose the Monte Carlo Nash Equilibrium Self-Refine Tree (MC-NEST), a novel\nframework that integrates Monte Carlo Tree Search with Nash Equilibrium\nstrategies to iteratively refine and validate hypotheses. MC-NEST dynamically\nbalances exploration and exploitation through adaptive sampling strategies,\nwhich prioritize high-potential hypotheses while maintaining diversity in the\nsearch space. We demonstrate the effectiveness of MC-NEST through comprehensive\nexperiments across multiple domains, including biomedicine, social science, and\ncomputer science. MC-NEST achieves average scores of 2.65, 2.74, and 2.80 (on a\n1-3 scale) for novelty, clarity, significance, and verifiability metrics on the\nsocial science, computer science, and biomedicine datasets, respectively,\noutperforming state-of-the-art prompt-based methods, which achieve 2.36, 2.51,\nand 2.52 on the same datasets. These results underscore MC-NEST's ability to\ngenerate high-quality, empirically grounded hypotheses across diverse domains.\nFurthermore, MC-NEST facilitates structured human-AI collaboration, ensuring\nthat LLMs augment human creativity rather than replace it. By addressing key\nchallenges such as iterative refinement and the exploration-exploitation\nbalance, MC-NEST sets a new benchmark in automated hypothesis generation.\nAdditionally, MC-NEST's ethical design enables responsible AI use, emphasizing\ntransparency and human supervision in hypothesis generation."
                },
                "authors": [
                    {
                        "name": "Gollam Rabby"
                    },
                    {
                        "name": "Diyana Muhammed"
                    },
                    {
                        "name": "Prasenjit Mitra"
                    },
                    {
                        "name": "Sören Auer"
                    }
                ],
                "author_detail": {
                    "name": "Sören Auer"
                },
                "author": "Sören Auer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18559v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18559v2",
                "updated": "2025-03-25T02:43:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    2,
                    43,
                    16,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-24T11:13:33Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    13,
                    33,
                    0,
                    83,
                    0
                ],
                "title": "AMD-Hummingbird: Towards an Efficient Text-to-Video Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AMD-Hummingbird: Towards an Efficient Text-to-Video Model"
                },
                "summary": "Text-to-Video (T2V) generation has attracted significant attention for its\nability to synthesize realistic videos from textual descriptions. However,\nexisting models struggle to balance computational efficiency and high visual\nquality, particularly on resource-limited devices, e.g.,iGPUs and mobile\nphones. Most prior work prioritizes visual fidelity while overlooking the need\nfor smaller, more efficient models suitable for real-world deployment. To\naddress this challenge, we propose a lightweight T2V framework, termed\nHummingbird, which prunes existing models and enhances visual quality through\nvisual feedback learning. Our approach reduces the size of the U-Net from 1.4\nbillion to 0.7 billion parameters, significantly improving efficiency while\npreserving high-quality video generation. Additionally, we introduce a novel\ndata processing pipeline that leverages Large Language Models (LLMs) and Video\nQuality Assessment (VQA) models to enhance the quality of both text prompts and\nvideo data. To support user-driven training and style customization, we\npublicly release the full training code, including data processing and model\ntraining. Extensive experiments show that our method achieves a 31X speedup\ncompared to state-of-the-art models such as VideoCrafter2, while also attaining\nthe highest overall score on VBench. Moreover, our method supports the\ngeneration of videos with up to 26 frames, addressing the limitations of\nexisting U-Net-based methods in long video generation. Notably, the entire\ntraining process requires only four GPUs, yet delivers performance competitive\nwith existing leading methods. Hummingbird presents a practical and efficient\nsolution for T2V generation, combining high performance, scalability, and\nflexibility for real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-Video (T2V) generation has attracted significant attention for its\nability to synthesize realistic videos from textual descriptions. However,\nexisting models struggle to balance computational efficiency and high visual\nquality, particularly on resource-limited devices, e.g.,iGPUs and mobile\nphones. Most prior work prioritizes visual fidelity while overlooking the need\nfor smaller, more efficient models suitable for real-world deployment. To\naddress this challenge, we propose a lightweight T2V framework, termed\nHummingbird, which prunes existing models and enhances visual quality through\nvisual feedback learning. Our approach reduces the size of the U-Net from 1.4\nbillion to 0.7 billion parameters, significantly improving efficiency while\npreserving high-quality video generation. Additionally, we introduce a novel\ndata processing pipeline that leverages Large Language Models (LLMs) and Video\nQuality Assessment (VQA) models to enhance the quality of both text prompts and\nvideo data. To support user-driven training and style customization, we\npublicly release the full training code, including data processing and model\ntraining. Extensive experiments show that our method achieves a 31X speedup\ncompared to state-of-the-art models such as VideoCrafter2, while also attaining\nthe highest overall score on VBench. Moreover, our method supports the\ngeneration of videos with up to 26 frames, addressing the limitations of\nexisting U-Net-based methods in long video generation. Notably, the entire\ntraining process requires only four GPUs, yet delivers performance competitive\nwith existing leading methods. Hummingbird presents a practical and efficient\nsolution for T2V generation, combining high performance, scalability, and\nflexibility for real-world applications."
                },
                "authors": [
                    {
                        "name": "Takashi Isobe"
                    },
                    {
                        "name": "He Cui"
                    },
                    {
                        "name": "Dong Zhou"
                    },
                    {
                        "name": "Mengmeng Ge"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "arxiv_comment": "Homepage:\n  https://www.amd.com/en/developer/resources/technical-articles/amd-hummingbird-0-9b-text-to-video-diffusion-model-with-4-step-inferencing.html|\n  GitHub: https://github.com/AMD-AIG-AIMA/AMD-Hummingbird-T2V",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18559v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18559v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]