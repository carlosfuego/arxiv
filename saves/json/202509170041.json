[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.13231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13231v2",
                "updated": "2025-09-15T14:40:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    40,
                    16,
                    0,
                    258,
                    0
                ],
                "published": "2025-08-17T19:07:08Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    19,
                    7,
                    8,
                    6,
                    229,
                    0
                ],
                "title": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System"
                },
                "summary": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference."
                },
                "authors": [
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Kaoutar El Maghraoui"
                    },
                    {
                        "name": "Naigang Wang"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "IEEE Computer Architecture Letter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11815v1",
                "updated": "2025-09-15T11:53:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    53,
                    56,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T11:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    53,
                    56,
                    0,
                    258,
                    0
                ],
                "title": "SpecVLM: Fast Speculative Decoding in Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecVLM: Fast Speculative Decoding in Vision-Language Models"
                },
                "summary": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM."
                },
                "authors": [
                    {
                        "name": "Haiduo Huang"
                    },
                    {
                        "name": "Fuwei Yang"
                    },
                    {
                        "name": "Zhenhua Liu"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Pengju Ren"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11628v1",
                "updated": "2025-09-15T06:46:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    6,
                    46,
                    22,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T06:46:22Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    6,
                    46,
                    22,
                    0,
                    258,
                    0
                ],
                "title": "SpeCa: Accelerating Diffusion Transformers with Speculative Feature\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeCa: Accelerating Diffusion Transformers with Speculative Feature\n  Caching"
                },
                "summary": "Diffusion models have revolutionized high-fidelity image and video synthesis,\nyet their computational demands remain prohibitive for real-time applications.\nThese models face two fundamental challenges: strict temporal dependencies\npreventing parallelization, and computationally intensive forward passes\nrequired at each denoising step. Drawing inspiration from speculative decoding\nin large language models, we present SpeCa, a novel 'Forecast-then-verify'\nacceleration framework that effectively addresses both limitations. SpeCa's\ncore innovation lies in introducing Speculative Sampling to diffusion models,\npredicting intermediate features for subsequent timesteps based on fully\ncomputed reference timesteps. Our approach implements a parameter-free\nverification mechanism that efficiently evaluates prediction reliability,\nenabling real-time decisions to accept or reject each prediction while\nincurring negligible computational overhead. Furthermore, SpeCa introduces\nsample-adaptive computation allocation that dynamically modulates resources\nbased on generation complexity, allocating reduced computation for simpler\nsamples while preserving intensive processing for complex instances.\nExperiments demonstrate 6.34x acceleration on FLUX with minimal quality\ndegradation (5.5% drop), 7.3x speedup on DiT while preserving generation\nfidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The\nverification mechanism incurs minimal overhead (1.67%-3.5% of full inference\ncosts), establishing a new paradigm for efficient diffusion model inference\nwhile maintaining generation quality even at aggressive acceleration ratios.\nOur codes have been released in Github:\n\\textbf{https://github.com/Shenyi-Z/Cache4Diffusion}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have revolutionized high-fidelity image and video synthesis,\nyet their computational demands remain prohibitive for real-time applications.\nThese models face two fundamental challenges: strict temporal dependencies\npreventing parallelization, and computationally intensive forward passes\nrequired at each denoising step. Drawing inspiration from speculative decoding\nin large language models, we present SpeCa, a novel 'Forecast-then-verify'\nacceleration framework that effectively addresses both limitations. SpeCa's\ncore innovation lies in introducing Speculative Sampling to diffusion models,\npredicting intermediate features for subsequent timesteps based on fully\ncomputed reference timesteps. Our approach implements a parameter-free\nverification mechanism that efficiently evaluates prediction reliability,\nenabling real-time decisions to accept or reject each prediction while\nincurring negligible computational overhead. Furthermore, SpeCa introduces\nsample-adaptive computation allocation that dynamically modulates resources\nbased on generation complexity, allocating reduced computation for simpler\nsamples while preserving intensive processing for complex instances.\nExperiments demonstrate 6.34x acceleration on FLUX with minimal quality\ndegradation (5.5% drop), 7.3x speedup on DiT while preserving generation\nfidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The\nverification mechanism incurs minimal overhead (1.67%-3.5% of full inference\ncosts), establishing a new paradigm for efficient diffusion model inference\nwhile maintaining generation quality even at aggressive acceleration ratios.\nOur codes have been released in Github:\n\\textbf{https://github.com/Shenyi-Z/Cache4Diffusion}"
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Fei Ren"
                    },
                    {
                        "name": "Shaobo Wang"
                    },
                    {
                        "name": "Kaixin Li"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_doi": "10.1145/3746027.3755331",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746027.3755331",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.11628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 9 figures, ACM Multimedia 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14089v2",
                "updated": "2025-09-15T01:15:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    1,
                    15,
                    50,
                    0,
                    258,
                    0
                ],
                "published": "2025-04-18T22:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average."
                },
                "authors": [
                    {
                        "name": "Kang He"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06261v2",
                "updated": "2025-09-15T00:51:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    0,
                    51,
                    47,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-08T00:57:50Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    0,
                    57,
                    50,
                    0,
                    251,
                    0
                ],
                "title": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\n  Heterogeneous Precision LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\n  Heterogeneous Precision LLM Serving"
                },
                "summary": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems."
                },
                "authors": [
                    {
                        "name": "Kyungmin Bin"
                    },
                    {
                        "name": "Seungbeom Choi"
                    },
                    {
                        "name": "Jimyoung Son"
                    },
                    {
                        "name": "Jieun Choi"
                    },
                    {
                        "name": "Daseul Bae"
                    },
                    {
                        "name": "Daehyeon Baek"
                    },
                    {
                        "name": "Kihyo Moon"
                    },
                    {
                        "name": "Minsung Jang"
                    },
                    {
                        "name": "Hyojung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hyojung Lee"
                },
                "author": "Hyojung Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11239v1",
                "updated": "2025-09-14T12:29:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    14,
                    12,
                    29,
                    49,
                    6,
                    257,
                    0
                ],
                "published": "2025-09-14T12:29:49Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    12,
                    29,
                    49,
                    6,
                    257,
                    0
                ],
                "title": "Multi-Layer Perceptron-Based Relay Node Selection for Next-Generation\n  Intelligent Delay-Tolerant Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Layer Perceptron-Based Relay Node Selection for Next-Generation\n  Intelligent Delay-Tolerant Networks"
                },
                "summary": "Delay Tolerant Networks (DTNs) are critical for emergency communication in\nhighly dynamic and challenging scenarios characterized by intermittent\nconnectivity, frequent disruptions, and unpredictable node mobility. While some\nprotocols are widely adopted for simplicity and low overhead, their static\nreplication strategy lacks the ability to adaptively distinguish high-quality\nrelay nodes, often leading to inefficient and suboptimal message dissemination.\nTo address this challenge, we propose a novel intelligent routing enhancement\nthat integrates machine learning-based node evaluation into the Spray and Wait\nframework. Several dynamic, core features are extracted from simulation logs\nand are used to train multiple classifiers - Multi-Layer Perceptron (MLP),\nSupport Vector Machine (SVM), and Random Forest (RF) - to predict whether a\nnode is suitable as a relay under dynamic conditions. The trained models are\ndeployed via a lightweight Flask-based RESTful API, enabling real-time,\nadaptive predictions. We implement the enhanced router MLPBasedSprayRouter,\nwhich selectively forwards messages based on the predicted relay quality. A\ncaching mechanism is incorporated to reduce computational overhead and ensure\nstable, low-latency inference. Extensive experiments under realistic emergency\nmobility scenarios demonstrate that the proposed framework significantly\nimproves delivery ratio while reducing average latency compared to the baseline\nprotocols. Among all evaluated classifiers, MLP achieved the most robust\nperformance, consistently outperforming both SVM and RF in terms of accuracy,\nadaptability, and inference speed. These results confirm the novelty and\npracticality of integrating machine learning into DTN routing, paving the way\nfor resilient and intelligent communication systems in smart cities, disaster\nrecovery, and other dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delay Tolerant Networks (DTNs) are critical for emergency communication in\nhighly dynamic and challenging scenarios characterized by intermittent\nconnectivity, frequent disruptions, and unpredictable node mobility. While some\nprotocols are widely adopted for simplicity and low overhead, their static\nreplication strategy lacks the ability to adaptively distinguish high-quality\nrelay nodes, often leading to inefficient and suboptimal message dissemination.\nTo address this challenge, we propose a novel intelligent routing enhancement\nthat integrates machine learning-based node evaluation into the Spray and Wait\nframework. Several dynamic, core features are extracted from simulation logs\nand are used to train multiple classifiers - Multi-Layer Perceptron (MLP),\nSupport Vector Machine (SVM), and Random Forest (RF) - to predict whether a\nnode is suitable as a relay under dynamic conditions. The trained models are\ndeployed via a lightweight Flask-based RESTful API, enabling real-time,\nadaptive predictions. We implement the enhanced router MLPBasedSprayRouter,\nwhich selectively forwards messages based on the predicted relay quality. A\ncaching mechanism is incorporated to reduce computational overhead and ensure\nstable, low-latency inference. Extensive experiments under realistic emergency\nmobility scenarios demonstrate that the proposed framework significantly\nimproves delivery ratio while reducing average latency compared to the baseline\nprotocols. Among all evaluated classifiers, MLP achieved the most robust\nperformance, consistently outperforming both SVM and RF in terms of accuracy,\nadaptability, and inference speed. These results confirm the novelty and\npracticality of integrating machine learning into DTN routing, paving the way\nfor resilient and intelligent communication systems in smart cities, disaster\nrecovery, and other dynamic environments."
                },
                "authors": [
                    {
                        "name": "Zhekun Huang"
                    },
                    {
                        "name": "Milena Radenkovic"
                    }
                ],
                "author_detail": {
                    "name": "Milena Radenkovic"
                },
                "author": "Milena Radenkovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11181v1",
                "updated": "2025-09-14T09:26:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    14,
                    9,
                    26,
                    44,
                    6,
                    257,
                    0
                ],
                "published": "2025-09-14T09:26:44Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    9,
                    26,
                    44,
                    6,
                    257,
                    0
                ],
                "title": "Dislocation response to electric fields in strontium titanate: A\n  mesoscale indentation study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dislocation response to electric fields in strontium titanate: A\n  mesoscale indentation study"
                },
                "summary": "Dislocations in perovskite oxides have drawn increasing research interest due\nto their potential of tuning functional properties of electroceramics. Open\nquestions remain regarding the behavior of dislocations concerning their\nstability under strong externally applied electric fields. In this study, we\ninvestigate the dielectric breakdown strength of nominally undoped SrTiO3\ncrystals after the introduction of high-density dislocations. The\ndislocation-rich samples are prepared using the Brinell scratching method, and\nthey consistently exhibit lower dielectric breakdown strength as well as a\nlarger scatter in the breakdown probability. We also study the impact of\nelectric field on the introduction and movement of dislocations in SrTiO3\ncrystals using Brinell indentation coupled with an electric field of 2 kV/mm.\nNo changes on the dislocation plastic zone size, depth, and dislocation\ndistribution are observed under this electric field. Based on the charge state\nof the dislocations in SrTiO3 as well as the electrical and thermal\nconductivity modified by dislocations, we discuss the forces induced by the\nelectric field to act on the dislocations to underline the possible mechanisms\nfor such dislocation behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dislocations in perovskite oxides have drawn increasing research interest due\nto their potential of tuning functional properties of electroceramics. Open\nquestions remain regarding the behavior of dislocations concerning their\nstability under strong externally applied electric fields. In this study, we\ninvestigate the dielectric breakdown strength of nominally undoped SrTiO3\ncrystals after the introduction of high-density dislocations. The\ndislocation-rich samples are prepared using the Brinell scratching method, and\nthey consistently exhibit lower dielectric breakdown strength as well as a\nlarger scatter in the breakdown probability. We also study the impact of\nelectric field on the introduction and movement of dislocations in SrTiO3\ncrystals using Brinell indentation coupled with an electric field of 2 kV/mm.\nNo changes on the dislocation plastic zone size, depth, and dislocation\ndistribution are observed under this electric field. Based on the charge state\nof the dislocations in SrTiO3 as well as the electrical and thermal\nconductivity modified by dislocations, we discuss the forces induced by the\nelectric field to act on the dislocations to underline the possible mechanisms\nfor such dislocation behavior."
                },
                "authors": [
                    {
                        "name": "Alexander Frisch"
                    },
                    {
                        "name": "Daniel Isaia"
                    },
                    {
                        "name": "Oliver Preuß"
                    },
                    {
                        "name": "Xufei Fang"
                    }
                ],
                "author_detail": {
                    "name": "Xufei Fang"
                },
                "author": "Xufei Fang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11156v2",
                "updated": "2025-09-16T07:49:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    7,
                    49,
                    41,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-14T08:22:37Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    8,
                    22,
                    37,
                    6,
                    257,
                    0
                ],
                "title": "Adaptive K-PackCache: Cost-Centric Data Caching in Cloud",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive K-PackCache: Cost-Centric Data Caching in Cloud"
                },
                "summary": "Recent advances in data analytics have enabled the accurate prediction of\nuser access patterns, giving rise to the idea of packed caching delivering\nmultiple co accessed data items together as a bundle. This improves caching\nefficiency, as accessing one item often implies the need for others. Prior work\nhas explored only 2 item pairwise packing. In this paper, we extend the concept\nto general K packing, allowing variable size bundles for improved flexibility\nand performance. We formulate the K PackCache problem from a content delivery\nnetwork CDN operator perspective, aiming to minimize total cost comprising two\ncomponents: transfer cost modeled as a base cost plus a linearly increasing\nterm with the number of items packed, and memory rental cost for caching, which\ndepends on how long and how much is stored. Overpacking increases cost due to\nlow utility, underpacking leads to missed sharing opportunities. We propose an\nonline algorithm, Adaptive K PackCache AKPC, which dynamically forms, merges,\nand splits data cliques based on user access patterns and content correlation.\nOur approach supports batch requests, enables approximate clique merging, and\noffers a formal competitive guarantee. Through extensive evaluation on the\nNetflix and Spotify datasets, AKPC reduces total cost by up to 63 and 55\npercentage over online baselines, respectively, and achieves performance within\n15 and 13 percentage of the optimal. This demonstrates its scalability and\neffectiveness for real world caching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in data analytics have enabled the accurate prediction of\nuser access patterns, giving rise to the idea of packed caching delivering\nmultiple co accessed data items together as a bundle. This improves caching\nefficiency, as accessing one item often implies the need for others. Prior work\nhas explored only 2 item pairwise packing. In this paper, we extend the concept\nto general K packing, allowing variable size bundles for improved flexibility\nand performance. We formulate the K PackCache problem from a content delivery\nnetwork CDN operator perspective, aiming to minimize total cost comprising two\ncomponents: transfer cost modeled as a base cost plus a linearly increasing\nterm with the number of items packed, and memory rental cost for caching, which\ndepends on how long and how much is stored. Overpacking increases cost due to\nlow utility, underpacking leads to missed sharing opportunities. We propose an\nonline algorithm, Adaptive K PackCache AKPC, which dynamically forms, merges,\nand splits data cliques based on user access patterns and content correlation.\nOur approach supports batch requests, enables approximate clique merging, and\noffers a formal competitive guarantee. Through extensive evaluation on the\nNetflix and Spotify datasets, AKPC reduces total cost by up to 63 and 55\npercentage over online baselines, respectively, and achieves performance within\n15 and 13 percentage of the optimal. This demonstrates its scalability and\neffectiveness for real world caching systems."
                },
                "authors": [
                    {
                        "name": "Suvarthi Sarkar"
                    },
                    {
                        "name": "Aadarshraj Sah"
                    },
                    {
                        "name": "Poddutoori Sweeya Reddy"
                    },
                    {
                        "name": "Aryabartta Sahu"
                    }
                ],
                "author_detail": {
                    "name": "Aryabartta Sahu"
                },
                "author": "Aryabartta Sahu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11155v1",
                "updated": "2025-09-14T08:20:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    14,
                    8,
                    20,
                    48,
                    6,
                    257,
                    0
                ],
                "published": "2025-09-14T08:20:48Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    8,
                    20,
                    48,
                    6,
                    257,
                    0
                ],
                "title": "AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient\n  Inference in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient\n  Inference in LLMs"
                },
                "summary": "The quadratic complexity of the attention mechanism remains a fundamental\nbarrier to scaling Large Language Models (LLMs) to longer contexts, creating a\ncritical bottleneck in both computation and memory. To address this, we\nintroduce AQUA (Attention via QUery mAgnitudes) a novel and versatile\napproximation strategy that significantly reduces the cost of attention with a\ngraceful performance trade-off. Our method operates in two phases: an efficient\noffline step where we compute a universal, language agnostic projection matrix\nvia SVD on a calibration dataset, and an online inference step where we project\nquery and key vectors and dynamically select a sparse subset of dimensions\nbased on the query's magnitude. We provide a formal theoretical analysis of\nAQUA, establishing the break-even point at which it becomes more\ncomputationally efficient than standard attention. Our empirical evaluations on\nstate-of-the-art models like Llama-3.1-8B demonstrate that a 25% reduction in\nthe attention dot-product computation can be achieved with a statistically\ninsignificant impact on performance across a wide range of benchmarks. We\nfurther showcase the versatility of AQUA by demonstrating its ability to\nsynergistically accelerate existing token eviction methods like H2O and to\ndirectly reduce KV-cache memory size. By offering a controllable knob to\nbalance efficiency and accuracy, AQUA provides a practical and powerful tool\nfor making large-scale LLM inference more accessible and sustainable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic complexity of the attention mechanism remains a fundamental\nbarrier to scaling Large Language Models (LLMs) to longer contexts, creating a\ncritical bottleneck in both computation and memory. To address this, we\nintroduce AQUA (Attention via QUery mAgnitudes) a novel and versatile\napproximation strategy that significantly reduces the cost of attention with a\ngraceful performance trade-off. Our method operates in two phases: an efficient\noffline step where we compute a universal, language agnostic projection matrix\nvia SVD on a calibration dataset, and an online inference step where we project\nquery and key vectors and dynamically select a sparse subset of dimensions\nbased on the query's magnitude. We provide a formal theoretical analysis of\nAQUA, establishing the break-even point at which it becomes more\ncomputationally efficient than standard attention. Our empirical evaluations on\nstate-of-the-art models like Llama-3.1-8B demonstrate that a 25% reduction in\nthe attention dot-product computation can be achieved with a statistically\ninsignificant impact on performance across a wide range of benchmarks. We\nfurther showcase the versatility of AQUA by demonstrating its ability to\nsynergistically accelerate existing token eviction methods like H2O and to\ndirectly reduce KV-cache memory size. By offering a controllable knob to\nbalance efficiency and accuracy, AQUA provides a practical and powerful tool\nfor making large-scale LLM inference more accessible and sustainable."
                },
                "authors": [
                    {
                        "name": "Santhosh G S"
                    },
                    {
                        "name": "Saurav Prakash"
                    },
                    {
                        "name": "Balaraman Ravindran"
                    }
                ],
                "author_detail": {
                    "name": "Balaraman Ravindran"
                },
                "author": "Balaraman Ravindran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10798v1",
                "updated": "2025-09-13T03:34:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    13,
                    3,
                    34,
                    12,
                    5,
                    256,
                    0
                ],
                "published": "2025-09-13T03:34:12Z",
                "published_parsed": [
                    2025,
                    9,
                    13,
                    3,
                    34,
                    12,
                    5,
                    256,
                    0
                ],
                "title": "Judge Q: Trainable Queries for Optimized Information Retention in KV\n  Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judge Q: Trainable Queries for Optimized Information Retention in KV\n  Cache Eviction"
                },
                "summary": "Large language models (LLMs) utilize key-value (KV) cache to store historical\ninformation during sequence processing. The size of KV cache grows linearly as\nthe length of the sequence extends, which seriously affects memory usage and\ndecoding efficiency. Current methods for KV cache eviction typically utilize\nthe last window from the pre-filling phase as queries to compute the KV\nimportance scores for eviction. Although this scheme is simple to implement, it\ntends to overly focus on local information, potentially leading to the neglect\nor omission of crucial global information. To mitigate this issue, we propose\nJudge Q, a novel training method which incorporates a soft token list. This\nmethod only tunes the model's embedding layer at a low training cost. By\nconcatenating the soft token list at the end of the input sequence, we train\nthese tokens' attention map to the original input sequence to align with that\nof the actual decoded tokens. In this way, the queries corresponding to the\nsoft tokens can effectively capture global information and better evaluate the\nimportance of the keys and values within the KV cache, thus maintaining\ndecoding quality when KV cache is evicted. Under the same eviction budget, our\nmethod exhibits less performance degradation compared to existing eviction\napproaches. We validate our approach through experiments conducted on models\nsuch as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks\nincluding LongBench, RULER, and Needle-in-a-Haystack. Results indicate an\nimprovement of approximately 1 point on the LongBench and over 3 points on\nRULER. This proposed methodology can be seamlessly integrated into existing\nopen-source models with minimal training overhead, thereby enhancing\nperformance in KV cache eviction scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) utilize key-value (KV) cache to store historical\ninformation during sequence processing. The size of KV cache grows linearly as\nthe length of the sequence extends, which seriously affects memory usage and\ndecoding efficiency. Current methods for KV cache eviction typically utilize\nthe last window from the pre-filling phase as queries to compute the KV\nimportance scores for eviction. Although this scheme is simple to implement, it\ntends to overly focus on local information, potentially leading to the neglect\nor omission of crucial global information. To mitigate this issue, we propose\nJudge Q, a novel training method which incorporates a soft token list. This\nmethod only tunes the model's embedding layer at a low training cost. By\nconcatenating the soft token list at the end of the input sequence, we train\nthese tokens' attention map to the original input sequence to align with that\nof the actual decoded tokens. In this way, the queries corresponding to the\nsoft tokens can effectively capture global information and better evaluate the\nimportance of the keys and values within the KV cache, thus maintaining\ndecoding quality when KV cache is evicted. Under the same eviction budget, our\nmethod exhibits less performance degradation compared to existing eviction\napproaches. We validate our approach through experiments conducted on models\nsuch as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks\nincluding LongBench, RULER, and Needle-in-a-Haystack. Results indicate an\nimprovement of approximately 1 point on the LongBench and over 3 points on\nRULER. This proposed methodology can be seamlessly integrated into existing\nopen-source models with minimal training overhead, thereby enhancing\nperformance in KV cache eviction scenarios."
                },
                "authors": [
                    {
                        "name": "Yijun Liu"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Shiyu Ji"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10372v1",
                "updated": "2025-09-12T16:05:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    5,
                    27,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T16:05:27Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    5,
                    27,
                    4,
                    255,
                    0
                ],
                "title": "MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging\n  Bit-Slice-enabled Sparsity and Repetitiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging\n  Bit-Slice-enabled Sparsity and Repetitiveness"
                },
                "summary": "Large language models (LLMs) face significant inference latency due to\ninefficiencies in GEMM operations, weight access, and KV cache access,\nespecially in real-time scenarios. This highlights the need for a versatile\ncompute-memory efficient accelerator. Unfortunately, existing Transformer\naccelerators struggle to address both aspects simultaneously, as they focus on\nvalue-level processing, missing fine-grained opportunities to optimize\ncomputation and memory collaboratively. This paper introduces MCBP, a\nbit-grained compute-memory efficient algorithm-hardware co-design that\nleverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM\ninference. MCBP features three key innovations: 1) BS-repetitiveness-enabled\ncomputation reduction (BRCR), which eliminates redundant GEMM computations via\nleveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state\ncoding (BSTC), which reduces weight access via exploiting significant sparsity\nin high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),\nwhich reduces KV cache access by leveraging early-termination-based bit-grained\nprediction. These techniques, supported by custom accelerator designs,\neffectively alleviate the burden in GEMM, weight access, and KV cache access.\nExtensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up\nand 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA\nTransformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than\nSpatten, FACT and SOFA, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face significant inference latency due to\ninefficiencies in GEMM operations, weight access, and KV cache access,\nespecially in real-time scenarios. This highlights the need for a versatile\ncompute-memory efficient accelerator. Unfortunately, existing Transformer\naccelerators struggle to address both aspects simultaneously, as they focus on\nvalue-level processing, missing fine-grained opportunities to optimize\ncomputation and memory collaboratively. This paper introduces MCBP, a\nbit-grained compute-memory efficient algorithm-hardware co-design that\nleverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM\ninference. MCBP features three key innovations: 1) BS-repetitiveness-enabled\ncomputation reduction (BRCR), which eliminates redundant GEMM computations via\nleveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state\ncoding (BSTC), which reduces weight access via exploiting significant sparsity\nin high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),\nwhich reduces KV cache access by leveraging early-termination-based bit-grained\nprediction. These techniques, supported by custom accelerator designs,\neffectively alleviate the burden in GEMM, weight access, and KV cache access.\nExtensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up\nand 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA\nTransformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than\nSpatten, FACT and SOFA, respectively."
                },
                "authors": [
                    {
                        "name": "Huizheng Wang"
                    },
                    {
                        "name": "Zichuan Wang"
                    },
                    {
                        "name": "Zhiheng Yue"
                    },
                    {
                        "name": "Yousheng Long"
                    },
                    {
                        "name": "Taiquan Wei"
                    },
                    {
                        "name": "Jianxun Yang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Shaojun Wei"
                    },
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Shouyi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Shouyi Yin"
                },
                "author": "Shouyi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10312v1",
                "updated": "2025-09-12T14:53:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    53,
                    45,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T14:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    53,
                    45,
                    4,
                    255,
                    0
                ],
                "title": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion\n  Transformers with Cluster-Driven Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion\n  Transformers with Cluster-Driven Feature Caching"
                },
                "summary": "Diffusion transformers have gained significant attention in recent years for\ntheir ability to generate high-quality images and videos, yet still suffer from\na huge computational cost due to their iterative denoising process. Recently,\nfeature caching has been introduced to accelerate diffusion transformers by\ncaching the feature computation in previous timesteps and reusing it in the\nfollowing timesteps, which leverage the temporal similarity of diffusion models\nwhile ignoring the similarity in the spatial dimension. In this paper, we\nintroduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and\ncomplementary perspective for previous feature caching. Specifically, ClusCa\nperforms spatial clustering on tokens in each timestep, computes only one token\nin each cluster and propagates their information to all the other tokens, which\nis able to reduce the number of tokens by over 90%. Extensive experiments on\nDiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image\nand text-to-video generation. Besides, it can be directly applied to any\ndiffusion transformer without requirements for training. For instance, ClusCa\nachieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing\nthe original model by 0.51%. The code is available at\nhttps://github.com/Shenyi-Z/Cache4Diffusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have gained significant attention in recent years for\ntheir ability to generate high-quality images and videos, yet still suffer from\na huge computational cost due to their iterative denoising process. Recently,\nfeature caching has been introduced to accelerate diffusion transformers by\ncaching the feature computation in previous timesteps and reusing it in the\nfollowing timesteps, which leverage the temporal similarity of diffusion models\nwhile ignoring the similarity in the spatial dimension. In this paper, we\nintroduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and\ncomplementary perspective for previous feature caching. Specifically, ClusCa\nperforms spatial clustering on tokens in each timestep, computes only one token\nin each cluster and propagates their information to all the other tokens, which\nis able to reduce the number of tokens by over 90%. Extensive experiments on\nDiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image\nand text-to-video generation. Besides, it can be directly applied to any\ndiffusion transformer without requirements for training. For instance, ClusCa\nachieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing\nthe original model by 0.51%. The code is available at\nhttps://github.com/Shenyi-Z/Cache4Diffusion."
                },
                "authors": [
                    {
                        "name": "Zhixin Zheng"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Shaobo Wang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "11 pages, 11 figures; Accepted by ACM MM2025; Mainly focus on feature\n  caching for diffusion transformers acceleration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10251v1",
                "updated": "2025-09-12T13:49:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    49,
                    27,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T13:49:27Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    49,
                    27,
                    4,
                    255,
                    0
                ],
                "title": "XBOF: A Cost-Efficient CXL JBOF with Inter-SSD Compute Resource Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XBOF: A Cost-Efficient CXL JBOF with Inter-SSD Compute Resource Sharing"
                },
                "summary": "Enterprise SSDs integrate numerous computing resources (e.g., ARM processor\nand onboard DRAM) to satisfy the ever-increasing performance requirements of\nI/O bursts. While these resources substantially elevate the monetary costs of\nSSDs, the sporadic nature of I/O bursts causes severe SSD resource\nunderutilization in just a bunch of flash (JBOF) level. Tackling this\nchallenge, we propose XBOF, a cost-efficient JBOF design, which only reserves\nmoderate computing resources in SSDs at low monetary cost, while achieving\ndemanded I/O performance through efficient inter-SSD resource sharing.\nSpecifically, XBOF first disaggregates SSD architecture into multiple disjoint\nparts based on their functionality, enabling fine-grained SSD internal resource\nmanagement. XBOF then employs a decentralized scheme to manage these\ndisaggregated resources and harvests the computing resources of idle SSDs to\nassist busy SSDs in handling I/O bursts. This idea is facilitated by the\ncache-coherent capability of Compute eXpress Link (CXL), with which the busy\nSSDs can directly utilize the harvested computing resources to accelerate\nmetadata processing. The evaluation results show that XBOF improves SSD\nresource utilization by 50.4% and saves 19.0% monetary costs with a negligible\nperformance loss, compared to existing JBOF designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprise SSDs integrate numerous computing resources (e.g., ARM processor\nand onboard DRAM) to satisfy the ever-increasing performance requirements of\nI/O bursts. While these resources substantially elevate the monetary costs of\nSSDs, the sporadic nature of I/O bursts causes severe SSD resource\nunderutilization in just a bunch of flash (JBOF) level. Tackling this\nchallenge, we propose XBOF, a cost-efficient JBOF design, which only reserves\nmoderate computing resources in SSDs at low monetary cost, while achieving\ndemanded I/O performance through efficient inter-SSD resource sharing.\nSpecifically, XBOF first disaggregates SSD architecture into multiple disjoint\nparts based on their functionality, enabling fine-grained SSD internal resource\nmanagement. XBOF then employs a decentralized scheme to manage these\ndisaggregated resources and harvests the computing resources of idle SSDs to\nassist busy SSDs in handling I/O bursts. This idea is facilitated by the\ncache-coherent capability of Compute eXpress Link (CXL), with which the busy\nSSDs can directly utilize the harvested computing resources to accelerate\nmetadata processing. The evaluation results show that XBOF improves SSD\nresource utilization by 50.4% and saves 19.0% monetary costs with a negligible\nperformance loss, compared to existing JBOF designs."
                },
                "authors": [
                    {
                        "name": "Shushu Yi"
                    },
                    {
                        "name": "Yuda An"
                    },
                    {
                        "name": "Li Peng"
                    },
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Jieming Yin"
                    },
                    {
                        "name": "Guangyan Zhang"
                    },
                    {
                        "name": "Wenfei Wu"
                    },
                    {
                        "name": "Diyu Zhou"
                    },
                    {
                        "name": "Zhenlin Wang"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10016v1",
                "updated": "2025-09-12T07:20:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    20,
                    53,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T07:20:53Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    20,
                    53,
                    4,
                    255,
                    0
                ],
                "title": "SvalMIZ-25 Svalbard Marginal Ice Zone Campaign 2025 -- Cruise Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SvalMIZ-25 Svalbard Marginal Ice Zone Campaign 2025 -- Cruise Report"
                },
                "summary": "The coupling of weather, sea-ice, ocean, and wave forecasting systems has\nbeen a long-standing research focus to improve Arctic forecasting systems and\ntheir realism and is also a priority of international initiatives such as the\nWMO research project PCAPS. The goal of the Svalbard Marginal Ice Zone 2025\nCampaign (SvalMIZ-25) was to observe and better understand the complex\ninterplay between atmosphere, waves, and sea-ice in the winter Marginal Ice\nZone (MIZ) in order to advance predictive skill of coupled Arctic forecasting\nsystems. The main objective has been to set up a network of observations with a\nspatial distribution that allows for a representative comparison between in\nsitu observations and gridded model data. The observed variables include air\nand surface temperature, sea-ice drift, and wave energy spectra. With the\nsupport of the Norwegian Coast Guard, we participated in the research cruise\nwith KV Svalbard from 22.April - 11.May 2025. In total 21 buoys were deployed\nin the Marginal Ice Zone north of the Svalbard Archipelago.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coupling of weather, sea-ice, ocean, and wave forecasting systems has\nbeen a long-standing research focus to improve Arctic forecasting systems and\ntheir realism and is also a priority of international initiatives such as the\nWMO research project PCAPS. The goal of the Svalbard Marginal Ice Zone 2025\nCampaign (SvalMIZ-25) was to observe and better understand the complex\ninterplay between atmosphere, waves, and sea-ice in the winter Marginal Ice\nZone (MIZ) in order to advance predictive skill of coupled Arctic forecasting\nsystems. The main objective has been to set up a network of observations with a\nspatial distribution that allows for a representative comparison between in\nsitu observations and gridded model data. The observed variables include air\nand surface temperature, sea-ice drift, and wave energy spectra. With the\nsupport of the Norwegian Coast Guard, we participated in the research cruise\nwith KV Svalbard from 22.April - 11.May 2025. In total 21 buoys were deployed\nin the Marginal Ice Zone north of the Svalbard Archipelago."
                },
                "authors": [
                    {
                        "name": "M. Müller"
                    },
                    {
                        "name": "J. Rabault"
                    },
                    {
                        "name": "C. Palerme"
                    },
                    {
                        "name": "J. Tjernström"
                    }
                ],
                "author_detail": {
                    "name": "J. Tjernström"
                },
                "author": "J. Tjernström",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09754v1",
                "updated": "2025-09-11T16:48:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    48,
                    24,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T16:48:24Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    48,
                    24,
                    3,
                    254,
                    0
                ],
                "title": "LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation"
                },
                "summary": "KV Cache is commonly used to accelerate LLM inference with long contexts, yet\nits high memory demand drives the need for cache compression. Existing\ncompression methods, however, are largely heuristic and lack dynamic budget\nallocation. To address this limitation, we introduce a unified framework for\ncache compression by minimizing information loss in Transformer residual\nstreams. Building on it, we analyze the layer attention output loss and derive\na new metric to compare cache entries across heads, enabling layer-wise\ncompression with dynamic head budgets. Additionally, by contrasting cross-layer\ninformation, we also achieve dynamic layer budgets. LAVa is the first unified\nstrategy for cache eviction and dynamic budget allocation that, unlike prior\nmethods, does not rely on training or the combination of multiple strategies.\nExperiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and\nInfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a\nnew insight: dynamic layer budgets are crucial for generation tasks (e.g., code\ncompletion), while dynamic head budgets play a key role in extraction tasks\n(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently\nmaintains top performance across task types. Our code is available at\nhttps://github.com/MGDDestiny/Lava.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache is commonly used to accelerate LLM inference with long contexts, yet\nits high memory demand drives the need for cache compression. Existing\ncompression methods, however, are largely heuristic and lack dynamic budget\nallocation. To address this limitation, we introduce a unified framework for\ncache compression by minimizing information loss in Transformer residual\nstreams. Building on it, we analyze the layer attention output loss and derive\na new metric to compare cache entries across heads, enabling layer-wise\ncompression with dynamic head budgets. Additionally, by contrasting cross-layer\ninformation, we also achieve dynamic layer budgets. LAVa is the first unified\nstrategy for cache eviction and dynamic budget allocation that, unlike prior\nmethods, does not rely on training or the combination of multiple strategies.\nExperiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and\nInfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a\nnew insight: dynamic layer budgets are crucial for generation tasks (e.g., code\ncompletion), while dynamic head budgets play a key role in extraction tasks\n(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently\nmaintains top performance across task types. Our code is available at\nhttps://github.com/MGDDestiny/Lava."
                },
                "authors": [
                    {
                        "name": "Yiqun Shen"
                    },
                    {
                        "name": "Song Yuan"
                    },
                    {
                        "name": "Zhengze Zhang"
                    },
                    {
                        "name": "Xiaoliang Wang"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Nguyen Cam-Tu"
                    }
                ],
                "author_detail": {
                    "name": "Nguyen Cam-Tu"
                },
                "author": "Nguyen Cam-Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09525v1",
                "updated": "2025-09-11T15:06:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    6,
                    3,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T15:06:03Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    6,
                    3,
                    3,
                    254,
                    0
                ],
                "title": "TrEnv: Transparently Share Serverless Execution Environments Across\n  Different Functions and Nodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrEnv: Transparently Share Serverless Execution Environments Across\n  Different Functions and Nodes"
                },
                "summary": "Serverless computing provides dynamic scalability, but its infrastructure\noverhead becomes a bottleneck for emerging workloads such as LLM agents, which\nexhibit unpredictable invocation patterns and variable resource demands. Our\nanalysis shows that for these agents, the cost of running on serverless\nplatforms can reach up to 70% of the cost of LLM API calls. This finding\nmotivates the need for a more efficient, high-density serverless platform. We\npresent TrEnv, a co-designed serverless platform that supports both container-\nand VM-based environments, optimized for the unique demands of LLM agents.\nTrEnv reduces startup latency and memory usage through repurposable sandboxes\nand memory templates, which enable fast reuse and restoration of execution\nenvironments. To further reduce overhead in VM-based agent workloads, TrEnv\nleverages browser sharing and a page cache bypassing mechanism. Evaluations\nshow that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in\ncontainer-based settings, and achieves up to 58% lower P99 latency and 61%\nmemory savings for VM-based agents compared to state-of-the-art systems like\nE2B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing provides dynamic scalability, but its infrastructure\noverhead becomes a bottleneck for emerging workloads such as LLM agents, which\nexhibit unpredictable invocation patterns and variable resource demands. Our\nanalysis shows that for these agents, the cost of running on serverless\nplatforms can reach up to 70% of the cost of LLM API calls. This finding\nmotivates the need for a more efficient, high-density serverless platform. We\npresent TrEnv, a co-designed serverless platform that supports both container-\nand VM-based environments, optimized for the unique demands of LLM agents.\nTrEnv reduces startup latency and memory usage through repurposable sandboxes\nand memory templates, which enable fast reuse and restoration of execution\nenvironments. To further reduce overhead in VM-based agent workloads, TrEnv\nleverages browser sharing and a page cache bypassing mechanism. Evaluations\nshow that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in\ncontainer-based settings, and achieves up to 58% lower P99 latency and 61%\nmemory savings for VM-based agents compared to state-of-the-art systems like\nE2B."
                },
                "authors": [
                    {
                        "name": "Jialiang Huang"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Sixing Lin"
                    },
                    {
                        "name": "Kang Chen"
                    },
                    {
                        "name": "Jinlei Jiang"
                    },
                    {
                        "name": "Xia Liao"
                    },
                    {
                        "name": "Yingdi Shan"
                    },
                    {
                        "name": "Yongwei Wu"
                    },
                    {
                        "name": "Ning Zhang"
                    },
                    {
                        "name": "Mengting Lu"
                    },
                    {
                        "name": "Tao Ma"
                    },
                    {
                        "name": "Haifeng Gong"
                    },
                    {
                        "name": "Mingxing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mingxing Zhang"
                },
                "author": "Mingxing Zhang",
                "arxiv_comment": "38 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09494v1",
                "updated": "2025-09-11T14:34:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    34,
                    1,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T14:34:01Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    34,
                    1,
                    3,
                    254,
                    0
                ],
                "title": "In-Loop Filtering Using Learned Look-Up Tables for Video Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Loop Filtering Using Learned Look-Up Tables for Video Coding"
                },
                "summary": "In-loop filtering (ILF) is a key technology in video coding standards to\nreduce artifacts and enhance visual quality. Recently, neural network-based ILF\nschemes have achieved remarkable coding gains, emerging as a powerful candidate\nfor next-generation video coding standards. However, the use of deep neural\nnetworks (DNN) brings significant computational and time complexity or high\ndemands for dedicated hardware, making it challenging for general use. To\naddress this limitation, we study a practical ILF solution by adopting look-up\ntables (LUTs). After training a DNN with a restricted reference range for ILF,\nall possible inputs are traversed, and the output values of the DNN are cached\ninto LUTs. During the coding process, the filtering process is performed by\nsimply retrieving the filtered pixel through locating the input pixels and\ninterpolating between the cached values, instead of relying on heavy inference\ncomputations. In this paper, we propose a universal LUT-based ILF framework,\ntermed LUT-ILF++. First, we introduce the cooperation of multiple kinds of\nfiltering LUTs and propose a series of customized indexing mechanisms to enable\nbetter filtering reference perception with limited storage consumption. Second,\nwe propose the cross-component indexing mechanism to enable the filtering of\ndifferent color components jointly. Third, in order to make our solution\npractical for coding uses, we propose the LUT compaction scheme to enable the\nLUT pruning, achieving a lower storage cost of the entire solution. The\nproposed framework is implemented in the VVC reference software. Experimental\nresults show that the proposed framework achieves on average 0.82%/2.97%/1.63%\nand 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI\nand RA configurations, respectively. Compared to DNN-based solutions, our\nproposed solution has much lower time complexity and storage cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-loop filtering (ILF) is a key technology in video coding standards to\nreduce artifacts and enhance visual quality. Recently, neural network-based ILF\nschemes have achieved remarkable coding gains, emerging as a powerful candidate\nfor next-generation video coding standards. However, the use of deep neural\nnetworks (DNN) brings significant computational and time complexity or high\ndemands for dedicated hardware, making it challenging for general use. To\naddress this limitation, we study a practical ILF solution by adopting look-up\ntables (LUTs). After training a DNN with a restricted reference range for ILF,\nall possible inputs are traversed, and the output values of the DNN are cached\ninto LUTs. During the coding process, the filtering process is performed by\nsimply retrieving the filtered pixel through locating the input pixels and\ninterpolating between the cached values, instead of relying on heavy inference\ncomputations. In this paper, we propose a universal LUT-based ILF framework,\ntermed LUT-ILF++. First, we introduce the cooperation of multiple kinds of\nfiltering LUTs and propose a series of customized indexing mechanisms to enable\nbetter filtering reference perception with limited storage consumption. Second,\nwe propose the cross-component indexing mechanism to enable the filtering of\ndifferent color components jointly. Third, in order to make our solution\npractical for coding uses, we propose the LUT compaction scheme to enable the\nLUT pruning, achieving a lower storage cost of the entire solution. The\nproposed framework is implemented in the VVC reference software. Experimental\nresults show that the proposed framework achieves on average 0.82%/2.97%/1.63%\nand 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI\nand RA configurations, respectively. Compared to DNN-based solutions, our\nproposed solution has much lower time complexity and storage cost."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Jialin Li"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05211v2",
                "updated": "2025-09-11T12:06:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    6,
                    49,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-07T09:47:21Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    47,
                    21,
                    3,
                    219,
                    0
                ],
                "title": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization"
                },
                "summary": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference."
                },
                "authors": [
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Runsen Xu"
                    },
                    {
                        "name": "Chenhang Cui"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19880v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19880v2",
                "updated": "2025-09-11T10:20:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    10,
                    20,
                    20,
                    3,
                    254,
                    0
                ],
                "published": "2025-05-26T12:06:12Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    12,
                    6,
                    12,
                    0,
                    146,
                    0
                ],
                "title": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing"
                },
                "summary": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead."
                },
                "authors": [
                    {
                        "name": "Saman Akbari"
                    },
                    {
                        "name": "Manfred Hauswirth"
                    }
                ],
                "author_detail": {
                    "name": "Manfred Hauswirth"
                },
                "author": "Manfred Hauswirth",
                "arxiv_doi": "10.1109/CLOUD67622.2025.00051",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/CLOUD67622.2025.00051",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.19880v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19880v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the 2025 IEEE 18th International Conference on Cloud\n  Computing (CLOUD)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19740v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19740v3",
                "updated": "2025-09-11T06:45:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    45,
                    58,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-27T10:11:27Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    11,
                    27,
                    2,
                    239,
                    0
                ],
                "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval"
                },
                "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19740v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19740v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01085v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01085v3",
                "updated": "2025-09-11T06:16:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    16,
                    31,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-01T03:16:52Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    16,
                    52,
                    0,
                    244,
                    0
                ],
                "title": "Bidirectional Sparse Attention for Faster Video Diffusion Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional Sparse Attention for Faster Video Diffusion Training"
                },
                "summary": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention."
                },
                "authors": [
                    {
                        "name": "Chenlu Zhan"
                    },
                    {
                        "name": "Wen Li"
                    },
                    {
                        "name": "Chuyu Shen"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Suhui Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01085v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01085v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09094v1",
                "updated": "2025-09-11T02:00:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    2,
                    0,
                    27,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T02:00:27Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    2,
                    0,
                    27,
                    3,
                    254,
                    0
                ],
                "title": "Coherence-Aware Task Graph Modeling for Realistic Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherence-Aware Task Graph Modeling for Realistic Application"
                },
                "summary": "As multicore systems continue to scale, cache coherence has emerged as a\ncritical determinant of system performance, with coherence behavior and task\nexecution closely intertwined, reshaping inter-task dependencies. Task graph\nmodeling provides a structured way to capture such dependencies and serves as\nthe foundation for many system-level design strategies. However, these\nstrategies typically rely on predefined task graphs, while many real-world\napplications lack explicit graphs and exhibit dynamic, data-dependent behavior,\nlimiting the effectiveness of static approaches. To address this, several task\ngraph modeling methods for realistic workloads have been developed. Yet, they\neither rely on implicit techniques that use application-specific features\nwithout producing explicit graphs, or they generate graphs tailored to fixed\nscheduling models, which limits generality. More importantly, they often\noverlook coherence interactions, creating a gap between design assumptions and\nactual runtime behavior. To overcome these limitations, we propose CoTAM, a\nCoherence-Aware Task Graph Modeling framework for realistic workloads that\nconstructs a unified task graph reflecting runtime behavior. CoTAM analyzes the\nimpact of coherence by decoupling its effects from overall execution,\nquantifies its influence through a learned weighting scheme, and infers\ninter-task dependencies for coherence-aware graph generation. Extensive\nexperiments show that CoTAM outperforms implicit methods, bridging the gap\nbetween dynamic workload behavior and existing designs while demonstrating the\nimportance of incorporating cache coherence into task graph modeling for\naccurate and generalizable system-level analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multicore systems continue to scale, cache coherence has emerged as a\ncritical determinant of system performance, with coherence behavior and task\nexecution closely intertwined, reshaping inter-task dependencies. Task graph\nmodeling provides a structured way to capture such dependencies and serves as\nthe foundation for many system-level design strategies. However, these\nstrategies typically rely on predefined task graphs, while many real-world\napplications lack explicit graphs and exhibit dynamic, data-dependent behavior,\nlimiting the effectiveness of static approaches. To address this, several task\ngraph modeling methods for realistic workloads have been developed. Yet, they\neither rely on implicit techniques that use application-specific features\nwithout producing explicit graphs, or they generate graphs tailored to fixed\nscheduling models, which limits generality. More importantly, they often\noverlook coherence interactions, creating a gap between design assumptions and\nactual runtime behavior. To overcome these limitations, we propose CoTAM, a\nCoherence-Aware Task Graph Modeling framework for realistic workloads that\nconstructs a unified task graph reflecting runtime behavior. CoTAM analyzes the\nimpact of coherence by decoupling its effects from overall execution,\nquantifies its influence through a learned weighting scheme, and infers\ninter-task dependencies for coherence-aware graph generation. Extensive\nexperiments show that CoTAM outperforms implicit methods, bridging the gap\nbetween dynamic workload behavior and existing designs while demonstrating the\nimportance of incorporating cache coherence into task graph modeling for\naccurate and generalizable system-level analysis."
                },
                "authors": [
                    {
                        "name": "Guochu Xiong"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Weichen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weichen Liu"
                },
                "author": "Weichen Liu",
                "arxiv_doi": "10.1145/3742875.3754678",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3742875.3754678",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.09094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by MEMOCODE'25, 10 pages",
                "arxiv_journal_ref": "International Symposium on Formal Methods and Models for System\n  Design (MEMOCODE '25), September 28-October 3, 2025, Taipei, Taiwan",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23674v2",
                "updated": "2025-09-10T17:59:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    59,
                    8,
                    2,
                    253,
                    0
                ],
                "published": "2025-07-31T15:50:57Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses"
                },
                "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience."
                },
                "authors": [
                    {
                        "name": "Muhammad Taha Cheema"
                    },
                    {
                        "name": "Abeer Aamir"
                    },
                    {
                        "name": "Khawaja Gul Muhammad"
                    },
                    {
                        "name": "Naveed Anwar Bhatti"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08696v1",
                "updated": "2025-09-10T15:41:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    41,
                    15,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T15:41:15Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    41,
                    15,
                    2,
                    253,
                    0
                ],
                "title": "Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer\n  Layer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer\n  Layer Caching"
                },
                "summary": "This paper presents a method to accelerate the inference process of diffusion\ntransformer (DiT)-based text-to-speech (TTS) models by applying a selective\ncaching mechanism to transformer layers. Specifically, I integrate SmoothCache\ninto the F5-TTS architecture, focusing on caching outputs of self-attention and\nfeed-forward network layers to reduce redundant computations during the\ndenoising process. A calibration phase is introduced to analyze L1 relative\nerrors between timesteps, guiding the selection of cache schedules that\nminimize quality degradation. To address the problem of inter-layer dependency,\na unified caching schedule is adopted, applying the cache pattern derived from\nself-attention layers to both layer types. Experiments on LibriSpeech-PC and\nSeed-TTS datasets evaluate various cache thresholds and denoising step\nconfigurations. Results show that caching at higher denoising steps reduces\ninference time without compromising output quality, whereas caching at lower\nsteps can negatively impact synthesis quality similarly to reducing the total\nnumber of denoising steps. Objective and subjective metrics confirm the\neffectiveness of SmoothCache in maintaining performance while improving\ncomputational efficiency. Comparisons between cached inference and reduced-step\ninference further highlight the benefits of selective caching, especially under\nhigh-step configurations. This work demonstrates that transformer layer caching\nis a practical solution for optimizing diffusion transformer-based TTS models\nwithout requiring architectural changes or retraining. Example inference\nresults can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a method to accelerate the inference process of diffusion\ntransformer (DiT)-based text-to-speech (TTS) models by applying a selective\ncaching mechanism to transformer layers. Specifically, I integrate SmoothCache\ninto the F5-TTS architecture, focusing on caching outputs of self-attention and\nfeed-forward network layers to reduce redundant computations during the\ndenoising process. A calibration phase is introduced to analyze L1 relative\nerrors between timesteps, guiding the selection of cache schedules that\nminimize quality degradation. To address the problem of inter-layer dependency,\na unified caching schedule is adopted, applying the cache pattern derived from\nself-attention layers to both layer types. Experiments on LibriSpeech-PC and\nSeed-TTS datasets evaluate various cache thresholds and denoising step\nconfigurations. Results show that caching at higher denoising steps reduces\ninference time without compromising output quality, whereas caching at lower\nsteps can negatively impact synthesis quality similarly to reducing the total\nnumber of denoising steps. Objective and subjective metrics confirm the\neffectiveness of SmoothCache in maintaining performance while improving\ncomputational efficiency. Comparisons between cached inference and reduced-step\ninference further highlight the benefits of selective caching, especially under\nhigh-step configurations. This work demonstrates that transformer layer caching\nis a practical solution for optimizing diffusion transformer-based TTS models\nwithout requiring architectural changes or retraining. Example inference\nresults can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ ."
                },
                "authors": [
                    {
                        "name": "Siratish Sakpiboonchit"
                    }
                ],
                "author_detail": {
                    "name": "Siratish Sakpiboonchit"
                },
                "author": "Siratish Sakpiboonchit",
                "arxiv_comment": "9 pages, 2 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08542v1",
                "updated": "2025-09-10T12:46:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    46,
                    29,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T12:46:29Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    46,
                    29,
                    2,
                    253,
                    0
                ],
                "title": "BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter\n  1.58-bit LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter\n  1.58-bit LLM Inference"
                },
                "summary": "Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy\nefficiency for CNNs by eliminating runtime weight updates. However, their\nscalability to Large Language Models (LLMs) is fundamentally constrained by\ntheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA\nseries - demands more than 1,000 cm2 of silicon area even in advanced CMOS\nnodes. This paper presents BitROM, the first CiROM-based accelerator that\novercomes this limitation through co-design with BitNet's 1.58-bit quantization\nmodel, enabling practical and efficient LLM inference at the edge. BitROM\nintroduces three key innovations: 1) a novel Bidirectional ROM Array that\nstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator\noptimized for ternary-weight computations; and 3) an integrated Decode-Refresh\n(DR) eDRAM that supports on-die KV-cache management, significantly reducing\nexternal memory access during decoding. In addition, BitROM integrates\nLoRA-based adapters to enable efficient transfer learning across various\ndownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit\ndensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over\nprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%\nreduction in external DRAM access, further enhancing deployment efficiency for\nLLMs in edge applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy\nefficiency for CNNs by eliminating runtime weight updates. However, their\nscalability to Large Language Models (LLMs) is fundamentally constrained by\ntheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA\nseries - demands more than 1,000 cm2 of silicon area even in advanced CMOS\nnodes. This paper presents BitROM, the first CiROM-based accelerator that\novercomes this limitation through co-design with BitNet's 1.58-bit quantization\nmodel, enabling practical and efficient LLM inference at the edge. BitROM\nintroduces three key innovations: 1) a novel Bidirectional ROM Array that\nstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator\noptimized for ternary-weight computations; and 3) an integrated Decode-Refresh\n(DR) eDRAM that supports on-die KV-cache management, significantly reducing\nexternal memory access during decoding. In addition, BitROM integrates\nLoRA-based adapters to enable efficient transfer learning across various\ndownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit\ndensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over\nprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%\nreduction in external DRAM access, further enhancing deployment efficiency for\nLLMs in edge applications."
                },
                "authors": [
                    {
                        "name": "Wenlun Zhang"
                    },
                    {
                        "name": "Xinyu Li"
                    },
                    {
                        "name": "Shimpei Ando"
                    },
                    {
                        "name": "Kentaro Yoshioka"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Yoshioka"
                },
                "author": "Kentaro Yoshioka",
                "arxiv_comment": "Accepted to ASP-DAC 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08342v1",
                "updated": "2025-09-10T07:28:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    28,
                    24,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T07:28:24Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    28,
                    24,
                    2,
                    253,
                    0
                ],
                "title": "Accelerating Mixture-of-Expert Inference with Adaptive Expert Split\n  Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Mixture-of-Expert Inference with Adaptive Expert Split\n  Mechanism"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a promising architecture for modern\nlarge language models (LLMs). However, massive parameters impose heavy GPU\nmemory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.\nOffloading the expert parameters to CPU RAM offers an effective way to\nalleviate the VRAM requirements for MoE inference. Existing approaches\ntypically cache a small subset of experts in VRAM and dynamically prefetch\nexperts from RAM during inference, leading to significant degradation in\ninference speed due to the poor cache hit rate and substantial expert loading\nlatency. In this work, we propose MoEpic, an efficient MoE inference system\nwith a novel expert split mechanism. Specifically, each expert is vertically\ndivided into two segments: top and bottom. MoEpic caches the top segment of hot\nexperts, so that more experts will be stored under the limited VRAM budget,\nthereby improving the cache hit rate. During each layer's inference, MoEpic\npredicts and prefetches the activated experts for the next layer. Since the top\nsegments of cached experts are exempt from fetching, the loading time is\nreduced, which allows efficient transfer-computation overlap. Nevertheless, the\nperformance of MoEpic critically depends on the cache configuration (i.e., each\nlayer's VRAM budget and expert split ratio). To this end, we propose a\ndivide-and-conquer algorithm based on fixed-point iteration for adaptive cache\nconfiguration. Extensive experiments on popular MoE LLMs demonstrate that\nMoEpic can save about half of the GPU cost, while lowering the inference\nlatency by about 37.51%-65.73% compared to the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a promising architecture for modern\nlarge language models (LLMs). However, massive parameters impose heavy GPU\nmemory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.\nOffloading the expert parameters to CPU RAM offers an effective way to\nalleviate the VRAM requirements for MoE inference. Existing approaches\ntypically cache a small subset of experts in VRAM and dynamically prefetch\nexperts from RAM during inference, leading to significant degradation in\ninference speed due to the poor cache hit rate and substantial expert loading\nlatency. In this work, we propose MoEpic, an efficient MoE inference system\nwith a novel expert split mechanism. Specifically, each expert is vertically\ndivided into two segments: top and bottom. MoEpic caches the top segment of hot\nexperts, so that more experts will be stored under the limited VRAM budget,\nthereby improving the cache hit rate. During each layer's inference, MoEpic\npredicts and prefetches the activated experts for the next layer. Since the top\nsegments of cached experts are exempt from fetching, the loading time is\nreduced, which allows efficient transfer-computation overlap. Nevertheless, the\nperformance of MoEpic critically depends on the cache configuration (i.e., each\nlayer's VRAM budget and expert split ratio). To this end, we propose a\ndivide-and-conquer algorithm based on fixed-point iteration for adaptive cache\nconfiguration. Extensive experiments on popular MoE LLMs demonstrate that\nMoEpic can save about half of the GPU cost, while lowering the inference\nlatency by about 37.51%-65.73% compared to the baselines."
                },
                "authors": [
                    {
                        "name": "Jiaming Yan"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08315v1",
                "updated": "2025-09-10T06:32:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    6,
                    32,
                    49,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T06:32:49Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    6,
                    32,
                    49,
                    2,
                    253,
                    0
                ],
                "title": "EvolKV: Evolutionary KV Cache Compression for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvolKV: Evolutionary KV Cache Compression for LLM Inference"
                },
                "summary": "Existing key-value (KV) cache compression methods typically rely on\nheuristics, such as uniform cache allocation across layers or static eviction\npolicies, however, they ignore the critical interplays among layer-specific\nfeature patterns and task performance, which can lead to degraded\ngeneralization. In this paper, we propose EvolKV, an adaptive framework for\nlayer-wise, task-driven KV cache compression that jointly optimizes the memory\nefficiency and task performance. By reformulating cache allocation as a\nmulti-objective optimization problem, EvolKV leverages evolutionary search to\ndynamically configure layer budgets while directly maximizing downstream\nperformance. Extensive experiments on 11 tasks demonstrate that our approach\noutperforms all baseline methods across a wide range of KV cache budgets on\nlong-context tasks and surpasses heuristic baselines by up to 7 percentage\npoints on GSM8K. Notably, EvolKV achieves superior performance over the full KV\ncache setting on code completion while utilizing only 1.5% of the original\nbudget, suggesting the untapped potential in learned compression strategies for\nKV cache budget allocation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing key-value (KV) cache compression methods typically rely on\nheuristics, such as uniform cache allocation across layers or static eviction\npolicies, however, they ignore the critical interplays among layer-specific\nfeature patterns and task performance, which can lead to degraded\ngeneralization. In this paper, we propose EvolKV, an adaptive framework for\nlayer-wise, task-driven KV cache compression that jointly optimizes the memory\nefficiency and task performance. By reformulating cache allocation as a\nmulti-objective optimization problem, EvolKV leverages evolutionary search to\ndynamically configure layer budgets while directly maximizing downstream\nperformance. Extensive experiments on 11 tasks demonstrate that our approach\noutperforms all baseline methods across a wide range of KV cache budgets on\nlong-context tasks and surpasses heuristic baselines by up to 7 percentage\npoints on GSM8K. Notably, EvolKV achieves superior performance over the full KV\ncache setting on code completion while utilizing only 1.5% of the original\nbudget, suggesting the untapped potential in learned compression strategies for\nKV cache budget allocation."
                },
                "authors": [
                    {
                        "name": "Bohan Yu"
                    },
                    {
                        "name": "Yekun Chai"
                    }
                ],
                "author_detail": {
                    "name": "Yekun Chai"
                },
                "author": "Yekun Chai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v3",
                "updated": "2025-09-09T13:30:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    30,
                    17,
                    1,
                    252,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_comment": "Accepted by EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07379v1",
                "updated": "2025-09-09T04:00:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    4,
                    0,
                    43,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T04:00:43Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    4,
                    0,
                    43,
                    1,
                    252,
                    0
                ],
                "title": "DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for\n  Efficient MoE LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for\n  Efficient MoE LLM Inference"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of deep learning tasks. Mixture of Experts (MoE) further enhances\ntheir capabilities by increasing model width through sparsely activated expert\nbranches, which keeps inference computation efficient. However, the large\nnumber of expert weights introduces significant GPU memory pressure, especially\nin resource-constrained environments such as single-GPU servers. More\nimportantly, MoE inference consists of two fundamentally different stages: a\nprefill stage where most experts are activated densely, and a decode stage\nwhere only a few experts are triggered sparsely. Treating these stages with a\nuniform scheduling strategy often leads to suboptimal latency and memory usage.\nTo address this, we propose DuoServe-MoE, an inference serving system that\nexplicitly separates prefill and decode stages and applies tailored expert\nscheduling strategies to each. In the prefill stage, DuoServe-MoE uses a\ntwo-stream CUDA pipeline that overlaps expert weight prefetching with the\ncomputation of non-MoE layers, limiting expert residency in GPU memory. In the\ndecode stage, a lightweight layer-level predictor trained offline from\nactivation traces is used to prefetch only the most likely activated experts,\nwithout requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B\nand 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to\n7.54 times while keeping peak memory usage at only 15 percent of the full model\nsize.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of deep learning tasks. Mixture of Experts (MoE) further enhances\ntheir capabilities by increasing model width through sparsely activated expert\nbranches, which keeps inference computation efficient. However, the large\nnumber of expert weights introduces significant GPU memory pressure, especially\nin resource-constrained environments such as single-GPU servers. More\nimportantly, MoE inference consists of two fundamentally different stages: a\nprefill stage where most experts are activated densely, and a decode stage\nwhere only a few experts are triggered sparsely. Treating these stages with a\nuniform scheduling strategy often leads to suboptimal latency and memory usage.\nTo address this, we propose DuoServe-MoE, an inference serving system that\nexplicitly separates prefill and decode stages and applies tailored expert\nscheduling strategies to each. In the prefill stage, DuoServe-MoE uses a\ntwo-stream CUDA pipeline that overlaps expert weight prefetching with the\ncomputation of non-MoE layers, limiting expert residency in GPU memory. In the\ndecode stage, a lightweight layer-level predictor trained offline from\nactivation traces is used to prefetch only the most likely activated experts,\nwithout requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B\nand 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to\n7.54 times while keeping peak memory usage at only 15 percent of the full model\nsize."
                },
                "authors": [
                    {
                        "name": "Yuning Zhang"
                    },
                    {
                        "name": "Grant Pinkert"
                    },
                    {
                        "name": "Nan Yang"
                    },
                    {
                        "name": "Yanli Li"
                    },
                    {
                        "name": "Dong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yuan"
                },
                "author": "Dong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01742v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01742v2",
                "updated": "2025-09-09T00:15:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    0,
                    15,
                    5,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-01T19:49:21Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    19,
                    49,
                    21,
                    0,
                    244,
                    0
                ],
                "title": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators"
                },
                "summary": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO(log_2(log_2 (N))) bandwidth overhead. BOLT introduces three key innovations:\n(i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache\nto accelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO(log_2(log_2 (N))) bandwidth overhead. BOLT introduces three key innovations:\n(i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache\nto accelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook."
                },
                "authors": [
                    {
                        "name": "Yitong Guo"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Haobin Hiroki Chen"
                    },
                    {
                        "name": "Yukui Luo"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Chenghong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chenghong Wang"
                },
                "author": "Chenghong Wang",
                "arxiv_comment": "Accepted by CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01742v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01742v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06949v1",
                "updated": "2025-09-08T17:58:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    6,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:58:06Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    6,
                    0,
                    251,
                    0
                ],
                "title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models"
                },
                "summary": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL"
                },
                "authors": [
                    {
                        "name": "Yinjie Wang"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Ke Shen"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "arxiv_comment": "Code and Models: https://github.com/Gen-Verse/dLLM-RL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03377v2",
                "updated": "2025-09-08T17:22:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    22,
                    17,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-03T14:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    53,
                    45,
                    2,
                    246,
                    0
                ],
                "title": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing"
                },
                "summary": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v4",
                "updated": "2025-09-08T13:34:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    34,
                    54,
                    0,
                    251,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AGI/AMD-Hybrid-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AGI/AMD-Hybrid-Models."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06579v1",
                "updated": "2025-09-08T11:49:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    49,
                    51,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T11:49:51Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    49,
                    51,
                    0,
                    251,
                    0
                ],
                "title": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View\n  Synthesis"
                },
                "summary": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html."
                },
                "authors": [
                    {
                        "name": "Xin Kong"
                    },
                    {
                        "name": "Daniel Watson"
                    },
                    {
                        "name": "Yannick Strümpler"
                    },
                    {
                        "name": "Michael Niemeyer"
                    },
                    {
                        "name": "Federico Tombari"
                    }
                ],
                "author_detail": {
                    "name": "Federico Tombari"
                },
                "author": "Federico Tombari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06493v1",
                "updated": "2025-09-08T09:54:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T09:54:18Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers"
                },
                "summary": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search."
                },
                "authors": [
                    {
                        "name": "Ran Xin"
                    },
                    {
                        "name": "Zeyu Zheng"
                    },
                    {
                        "name": "Yanchen Nie"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Xia Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xia Xiao"
                },
                "author": "Xia Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09822v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09822v4",
                "updated": "2025-09-08T09:09:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    9,
                    36,
                    0,
                    251,
                    0
                ],
                "published": "2025-08-13T13:54:51Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "title": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining"
                },
                "summary": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/"
                },
                "authors": [
                    {
                        "name": "Zijian Song"
                    },
                    {
                        "name": "Sihan Qin"
                    },
                    {
                        "name": "Tianshui Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Guangrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangrun Wang"
                },
                "author": "Guangrun Wang",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09822v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09822v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06444v1",
                "updated": "2025-09-08T08:44:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    44,
                    24,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T08:44:24Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    44,
                    24,
                    0,
                    251,
                    0
                ],
                "title": "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for\n  Heterogeneous and Privacy-Sensitive Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for\n  Heterogeneous and Privacy-Sensitive Data"
                },
                "summary": "Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive\ndata, especially in distributed healthcare settings where patient data spans\nSQL, knowledge graphs, and clinical notes. Clinicians face difficulties\nretrieving rare disease cases due to privacy constraints and the limitations of\ntraditional cloud-based RAG systems in handling diverse formats and edge\ndevices. To address this, we introduce HyFedRAG, a unified and efficient\nFederated RAG framework tailored for Hybrid data modalities. By leveraging an\nedge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across\ndiverse data sources while preserving data privacy. Our key contributions are:\n(1) We design an edge-cloud collaborative RAG framework built on Flower, which\nsupports querying structured SQL data, semi-structured knowledge graphs, and\nunstructured documents. The edge-side LLMs convert diverse data into\nstandardized privacy-preserving representations, and the server-side LLMs\nintegrates them for global reasoning and generation. (2) We integrate\nlightweight local retrievers with privacy-aware LLMs and provide three\nanonymization tools that enable each client to produce semantically rich,\nde-identified summaries for global inference across devices. (3) To optimize\nresponse latency and reduce redundant computation, we design a three-tier\ncaching strategy consisting of local cache, intermediate representation cache,\nand cloud inference cache. Experimental results on PMC-Patients demonstrate\nthat HyFedRAG outperforms existing baselines in terms of retrieval quality,\ngeneration consistency, and system efficiency. Our framework offers a scalable\nand privacy-compliant solution for RAG over structural-heterogeneous data,\nunlocking the potential of LLMs in sensitive and diverse data environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive\ndata, especially in distributed healthcare settings where patient data spans\nSQL, knowledge graphs, and clinical notes. Clinicians face difficulties\nretrieving rare disease cases due to privacy constraints and the limitations of\ntraditional cloud-based RAG systems in handling diverse formats and edge\ndevices. To address this, we introduce HyFedRAG, a unified and efficient\nFederated RAG framework tailored for Hybrid data modalities. By leveraging an\nedge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across\ndiverse data sources while preserving data privacy. Our key contributions are:\n(1) We design an edge-cloud collaborative RAG framework built on Flower, which\nsupports querying structured SQL data, semi-structured knowledge graphs, and\nunstructured documents. The edge-side LLMs convert diverse data into\nstandardized privacy-preserving representations, and the server-side LLMs\nintegrates them for global reasoning and generation. (2) We integrate\nlightweight local retrievers with privacy-aware LLMs and provide three\nanonymization tools that enable each client to produce semantically rich,\nde-identified summaries for global inference across devices. (3) To optimize\nresponse latency and reduce redundant computation, we design a three-tier\ncaching strategy consisting of local cache, intermediate representation cache,\nand cloud inference cache. Experimental results on PMC-Patients demonstrate\nthat HyFedRAG outperforms existing baselines in terms of retrieval quality,\ngeneration consistency, and system efficiency. Our framework offers a scalable\nand privacy-compliant solution for RAG over structural-heterogeneous data,\nunlocking the potential of LLMs in sensitive and diverse data environments."
                },
                "authors": [
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Hainan Zhang"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Hong-Wei Zheng"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "arxiv_comment": "9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06436v1",
                "updated": "2025-09-08T08:34:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T08:34:02Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "title": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning"
                },
                "summary": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Xiaofei Xu"
                    },
                    {
                        "name": "Ke Deng"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Lin Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lin Tian"
                },
                "author": "Lin Tian",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06047v1",
                "updated": "2025-09-07T13:15:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    7,
                    13,
                    15,
                    17,
                    6,
                    250,
                    0
                ],
                "published": "2025-09-07T13:15:17Z",
                "published_parsed": [
                    2025,
                    9,
                    7,
                    13,
                    15,
                    17,
                    6,
                    250,
                    0
                ],
                "title": "A facile vector substrate platform via BaTiO3 membrane transfer enables\n  high quality solution processed epitaxial PZT on silicon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A facile vector substrate platform via BaTiO3 membrane transfer enables\n  high quality solution processed epitaxial PZT on silicon"
                },
                "summary": "The direct integration of high-performance ferroelectric oxides with silicon\nremains challenging due to lattice mismatch, thermal incompatibility, and the\nneed for high-temperature epitaxial growth. Here, a hybrid integration approach\nis demonstrated in which crystalline BaTiO3 (BTO) membranes are first\ntransferred onto Pt coated Si substrates and subsequently used as vector\nsubstrates (VS) for the growth of epitaxial (001) Pb(Zr0.52Ti0.48)O3 (PZT) thin\nfilms via chemical solution deposition (CSD). A KI and HCl based etchant\nenables rapid and complete dissolution of the SrVO3 sacrificial layer in about\n30 minutes, reducing the release time from days to minutes compared with\nconventional water based approaches to dissolve AVO3 and AMoO3 (A is Ca, Sr,\nBa). The BTO VS imposes dominant (00l) out of plane orientation and in plane\ncube on cube epitaxy in the overlying PZT. Devices exhibit remnant polarization\n10 to 12 micro coulomb/cm2 and coercive field of 100 kV/cm, with stable\nswitching to 10^8 cycles on the VS. From piezoelectric butterfly loops, we\nextract effective d33 of 70 pm/V for PZT on VS, and 54 pm/V for PZT grown on\nconventional Pt Si substrates. This approach demonstrates a scalable and cost\neffective route for integrating functional ferroelectric materials onto silicon\nand offers a promising platform for future CMOS compatible oxide electronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The direct integration of high-performance ferroelectric oxides with silicon\nremains challenging due to lattice mismatch, thermal incompatibility, and the\nneed for high-temperature epitaxial growth. Here, a hybrid integration approach\nis demonstrated in which crystalline BaTiO3 (BTO) membranes are first\ntransferred onto Pt coated Si substrates and subsequently used as vector\nsubstrates (VS) for the growth of epitaxial (001) Pb(Zr0.52Ti0.48)O3 (PZT) thin\nfilms via chemical solution deposition (CSD). A KI and HCl based etchant\nenables rapid and complete dissolution of the SrVO3 sacrificial layer in about\n30 minutes, reducing the release time from days to minutes compared with\nconventional water based approaches to dissolve AVO3 and AMoO3 (A is Ca, Sr,\nBa). The BTO VS imposes dominant (00l) out of plane orientation and in plane\ncube on cube epitaxy in the overlying PZT. Devices exhibit remnant polarization\n10 to 12 micro coulomb/cm2 and coercive field of 100 kV/cm, with stable\nswitching to 10^8 cycles on the VS. From piezoelectric butterfly loops, we\nextract effective d33 of 70 pm/V for PZT on VS, and 54 pm/V for PZT grown on\nconventional Pt Si substrates. This approach demonstrates a scalable and cost\neffective route for integrating functional ferroelectric materials onto silicon\nand offers a promising platform for future CMOS compatible oxide electronics."
                },
                "authors": [
                    {
                        "name": "Asraful Haque"
                    },
                    {
                        "name": "Antony Jeyaseelan"
                    },
                    {
                        "name": "Shubham Kumar Parate"
                    },
                    {
                        "name": "Srinivasan Raghavan"
                    },
                    {
                        "name": "Pavan Nukala"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Nukala"
                },
                "arxiv_affiliation": "Centre for Nanoscience and Engineering, Indian Institute of Science, Bengaluru, India",
                "author": "Pavan Nukala",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13863v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13863v2",
                "updated": "2025-09-06T05:58:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    6,
                    5,
                    58,
                    51,
                    5,
                    249,
                    0
                ],
                "published": "2025-08-19T14:30:41Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    30,
                    41,
                    1,
                    231,
                    0
                ],
                "title": "Tight Cache Contention Analysis for WCET Estimation on Multicore Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tight Cache Contention Analysis for WCET Estimation on Multicore Systems"
                },
                "summary": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Shenlin Cai"
                    },
                    {
                        "name": "Yaowei Liang"
                    },
                    {
                        "name": "Chen Jie"
                    },
                    {
                        "name": "Yinjie Fang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Guoquan Zhang"
                    },
                    {
                        "name": "Yaoyao Gu"
                    },
                    {
                        "name": "Xiang Xiao"
                    },
                    {
                        "name": "Wei Qin"
                    },
                    {
                        "name": "Xiangzhen Ouyang"
                    },
                    {
                        "name": "Wanli Chang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Chang"
                },
                "author": "Wanli Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13863v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13863v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05207v1",
                "updated": "2025-09-05T16:10:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    10,
                    20,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T16:10:20Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    10,
                    20,
                    4,
                    248,
                    0
                ],
                "title": "RapidGNN: Energy and Communication-Efficient Distributed Training on\n  Large-Scale Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RapidGNN: Energy and Communication-Efficient Distributed Training on\n  Large-Scale Graph Neural Networks"
                },
                "summary": "Graph Neural Networks (GNNs) have become popular across a diverse set of\ntasks in exploring structural relationships between entities. However, due to\nthe highly connected structure of the datasets, distributed training of GNNs on\nlarge-scale graphs poses significant challenges. Traditional sampling-based\napproaches mitigate the computational loads, yet the communication overhead\nremains a challenge. This paper presents RapidGNN, a distributed GNN training\nframework with deterministic sampling-based scheduling to enable efficient\ncache construction and prefetching of remote features. Evaluation on benchmark\ngraph datasets demonstrates RapidGNN's effectiveness across different scales\nand topologies. RapidGNN improves end-to-end training throughput by 2.46x to\n3.00x on average over baseline methods across the benchmark datasets, while\ncutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further\ndemonstrates near-linear scalability with an increasing number of computing\nunits efficiently. Furthermore, it achieves increased energy efficiency over\nthe baseline methods for both CPU and GPU by 44% and 32%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have become popular across a diverse set of\ntasks in exploring structural relationships between entities. However, due to\nthe highly connected structure of the datasets, distributed training of GNNs on\nlarge-scale graphs poses significant challenges. Traditional sampling-based\napproaches mitigate the computational loads, yet the communication overhead\nremains a challenge. This paper presents RapidGNN, a distributed GNN training\nframework with deterministic sampling-based scheduling to enable efficient\ncache construction and prefetching of remote features. Evaluation on benchmark\ngraph datasets demonstrates RapidGNN's effectiveness across different scales\nand topologies. RapidGNN improves end-to-end training throughput by 2.46x to\n3.00x on average over baseline methods across the benchmark datasets, while\ncutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further\ndemonstrates near-linear scalability with an increasing number of computing\nunits efficiently. Furthermore, it achieves increased energy efficiency over\nthe baseline methods for both CPU and GPU by 44% and 32%, respectively."
                },
                "authors": [
                    {
                        "name": "Arefin Niam"
                    },
                    {
                        "name": "Tevfik Kosar"
                    },
                    {
                        "name": "M S Q Zulkar Nine"
                    }
                ],
                "author_detail": {
                    "name": "M S Q Zulkar Nine"
                },
                "author": "M S Q Zulkar Nine",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2505.10806",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05165v1",
                "updated": "2025-09-05T14:58:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T14:58:24Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "title": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens"
                },
                "summary": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment."
                },
                "authors": [
                    {
                        "name": "Dmitry Akulov"
                    },
                    {
                        "name": "Mohamed Sana"
                    },
                    {
                        "name": "Antonio De Domenico"
                    },
                    {
                        "name": "Tareq Si Salem"
                    },
                    {
                        "name": "Nicola Piovesan"
                    },
                    {
                        "name": "Fadhel Ayed"
                    }
                ],
                "author_detail": {
                    "name": "Fadhel Ayed"
                },
                "author": "Fadhel Ayed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09758v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09758v2",
                "updated": "2025-09-05T10:39:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    39,
                    3,
                    4,
                    248,
                    0
                ],
                "published": "2025-06-11T14:03:13Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "title": "Mainframe-Style Channel Controllers for Modern Disaggregated Memory\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainframe-Style Channel Controllers for Modern Disaggregated Memory\n  Systems"
                },
                "summary": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs."
                },
                "authors": [
                    {
                        "name": "Zikai Liu"
                    },
                    {
                        "name": "Jasmin Schult"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "arxiv_doi": "10.1145/3725783.3764403",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725783.3764403",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.09758v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09758v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Camera-ready authors' version for APSys'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04377v1",
                "updated": "2025-09-04T16:40:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    40,
                    1,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T16:40:01Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    40,
                    1,
                    3,
                    247,
                    0
                ],
                "title": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference"
                },
                "summary": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks."
                },
                "authors": [
                    {
                        "name": "Krishna Teja Chitty-Venkata"
                    },
                    {
                        "name": "Jie Ye"
                    },
                    {
                        "name": "Xian-He Sun"
                    },
                    {
                        "name": "Anthony Kougkas"
                    },
                    {
                        "name": "Murali Emani"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Nicolae"
                },
                "author": "Bogdan Nicolae",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12084v2",
                "updated": "2025-09-04T15:21:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    21,
                    11,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-21T12:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "title": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis"
                },
                "summary": "This study presents a comprehensive multi-level analysis of the NVIDIA Hopper\nGPU architecture, focusing on its performance characteristics and novel\nfeatures. We benchmark Hopper's memory subsystem, highlighting improvements in\nthe L2 partitioned cache and global memory access compared to Ampere and Ada\nLovelace. The evaluation of Hopper's fourth-generation tensor cores reveals the\nbenefits of FP8 precision and asynchronous wgmma instructions for matrix\noperations. Additionally, we investigate the performance of DPX instructions\nfor dynamic programming, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. Through multi-level evaluation, we discover that the Hopper\narchitecture demonstrates significant acceleration potential in real-world\napplications. For instance, the asynchronous programming model supported by TMA\nachieves a 1.5x speedup in matrix multiplication, FP8 delivers nearly double\nthe performance of FP16, and DPX instructions accelerate a computational\nbiology algorithm by at least 4.75x. Our findings provide actionable insights\nfor optimizing compute-intensive workloads, from AI training to bioinformatics,\non Hopper GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a comprehensive multi-level analysis of the NVIDIA Hopper\nGPU architecture, focusing on its performance characteristics and novel\nfeatures. We benchmark Hopper's memory subsystem, highlighting improvements in\nthe L2 partitioned cache and global memory access compared to Ampere and Ada\nLovelace. The evaluation of Hopper's fourth-generation tensor cores reveals the\nbenefits of FP8 precision and asynchronous wgmma instructions for matrix\noperations. Additionally, we investigate the performance of DPX instructions\nfor dynamic programming, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. Through multi-level evaluation, we discover that the Hopper\narchitecture demonstrates significant acceleration potential in real-world\napplications. For instance, the asynchronous programming model supported by TMA\nachieves a 1.5x speedup in matrix multiplication, FP8 delivers nearly double\nthe performance of FP16, and DPX instructions accelerate a computational\nbiology algorithm by at least 4.75x. Our findings provide actionable insights\nfor optimizing compute-intensive workloads, from AI training to bioinformatics,\non Hopper GPUs."
                },
                "authors": [
                    {
                        "name": "Weile Luo"
                    },
                    {
                        "name": "Ruibo Fan"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Hongyuan Liu"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2402.13499",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v3",
                "updated": "2025-09-16T10:33:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    33,
                    29,
                    1,
                    259,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_doi": "10.1145/3744916.3764523",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3744916.3764523",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.08523v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ICSE '26 (The 48th IEEE/ACM International Conference on\n  Software Engineering)",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04185v1",
                "updated": "2025-09-04T13:02:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    2,
                    39,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T13:02:39Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    2,
                    39,
                    3,
                    247,
                    0
                ],
                "title": "Set Block Decoding is a Language Model Inference Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Set Block Decoding is a Language Model Inference Accelerator"
                },
                "summary": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training."
                },
                "authors": [
                    {
                        "name": "Itai Gat"
                    },
                    {
                        "name": "Heli Ben-Hamu"
                    },
                    {
                        "name": "Marton Havasi"
                    },
                    {
                        "name": "Daniel Haziza"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Gabriel Synnaeve"
                    },
                    {
                        "name": "David Lopez-Paz"
                    },
                    {
                        "name": "Brian Karrer"
                    },
                    {
                        "name": "Yaron Lipman"
                    }
                ],
                "author_detail": {
                    "name": "Yaron Lipman"
                },
                "author": "Yaron Lipman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04180v1",
                "updated": "2025-09-04T12:54:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    54,
                    32,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T12:54:32Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    54,
                    32,
                    3,
                    247,
                    0
                ],
                "title": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer\n  Vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer\n  Vision"
                },
                "summary": "AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}."
                },
                "authors": [
                    {
                        "name": "Safouane El Ghazouali"
                    },
                    {
                        "name": "Umberto Michelucci"
                    }
                ],
                "author_detail": {
                    "name": "Umberto Michelucci"
                },
                "author": "Umberto Michelucci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04010v1",
                "updated": "2025-09-04T08:41:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    41,
                    6,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T08:41:06Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    41,
                    6,
                    3,
                    247,
                    0
                ],
                "title": "Systematic Timing Leakage Analysis of NIST PQDSS Candidates: Tooling and\n  Lessons Learned",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Timing Leakage Analysis of NIST PQDSS Candidates: Tooling and\n  Lessons Learned"
                },
                "summary": "The PQDSS standardization process requires cryptographic primitives to be\nfree from vulnerabilities, including timing and cache side-channels. Resistance\nto timing leakage is therefore an essential property, and achieving this\ntypically relies on software implementations that follow constant-time\nprinciples. Moreover, ensuring that all implementations are constant-time is\ncrucial for fair performance comparisons, as secure implementations often incur\nadditional overhead. Such analysis also helps identify scheme proposals that\nare inherently difficult to implement in constant time. Because constant-time\nproperties can be broken during compilation, it is often necessary to analyze\nthe compiled binary directly. Since manual binary analysis is extremely\nchallenging, automated analysis becomes highly important. Although several\ntools exist to assist with such analysis, they often have usability limitations\nand are difficult to set up correctly. To support the developers besides the\nNIST committee in verifying candidates, we developed a toolchain that automates\nconfiguration, execution, and result analysis for several widely used\nconstant-time analysis tools. We selected TIMECOP and Binsec/Rel2 to verify\nconstant-time policy compliance at the binary level, and dudect and RTLF to\ndetect side-channel vulnerabilities through statistical analysis of execution\ntime behavior. We demonstrate its effectiveness and practicability by\nevaluating the NIST PQDSS round 1 and round 2 implementations. We reported 26\nissues in total to the respective developers, and 5 of them have already been\nfixed. We also discuss our different findings, as well as the benefits of\nshortcomings of the different tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The PQDSS standardization process requires cryptographic primitives to be\nfree from vulnerabilities, including timing and cache side-channels. Resistance\nto timing leakage is therefore an essential property, and achieving this\ntypically relies on software implementations that follow constant-time\nprinciples. Moreover, ensuring that all implementations are constant-time is\ncrucial for fair performance comparisons, as secure implementations often incur\nadditional overhead. Such analysis also helps identify scheme proposals that\nare inherently difficult to implement in constant time. Because constant-time\nproperties can be broken during compilation, it is often necessary to analyze\nthe compiled binary directly. Since manual binary analysis is extremely\nchallenging, automated analysis becomes highly important. Although several\ntools exist to assist with such analysis, they often have usability limitations\nand are difficult to set up correctly. To support the developers besides the\nNIST committee in verifying candidates, we developed a toolchain that automates\nconfiguration, execution, and result analysis for several widely used\nconstant-time analysis tools. We selected TIMECOP and Binsec/Rel2 to verify\nconstant-time policy compliance at the binary level, and dudect and RTLF to\ndetect side-channel vulnerabilities through statistical analysis of execution\ntime behavior. We demonstrate its effectiveness and practicability by\nevaluating the NIST PQDSS round 1 and round 2 implementations. We reported 26\nissues in total to the respective developers, and 5 of them have already been\nfixed. We also discuss our different findings, as well as the benefits of\nshortcomings of the different tools."
                },
                "authors": [
                    {
                        "name": "Olivier Adjonyo"
                    },
                    {
                        "name": "Sebastien Bardin"
                    },
                    {
                        "name": "Emanuele Bellini"
                    },
                    {
                        "name": "Gilbert Ndollane Dione"
                    },
                    {
                        "name": "Mahmudul Faisal Al Ameen"
                    },
                    {
                        "name": "Robert Merget"
                    },
                    {
                        "name": "Frederic Recoules"
                    },
                    {
                        "name": "Yanis Sellami"
                    }
                ],
                "author_detail": {
                    "name": "Yanis Sellami"
                },
                "author": "Yanis Sellami",
                "arxiv_comment": "20 pages, 1 figure, to be published and presented at Sixth PQC\n  Standardization Conference by NIST, partially supported by the \"France 2030\"\n  government investment plan managed by the French National Research Agency,\n  under the reference ANR-22-PECY-0005",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12689v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12689v3",
                "updated": "2025-09-04T06:20:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    6,
                    20,
                    55,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-22T07:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "IC-Cache: Efficient Large Language Model Serving via In-context Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IC-Cache: Efficient Large Language Model Serving via In-context Caching"
                },
                "summary": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 70% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge transfer among requests. However, naively caching and reusing past\nresponses leads to a big quality drop. In this paper, we introduce IC-Cache, a\ncaching system that enables live LLM capability augmentation to improve serving\nefficiency: by leveraging historical request-response pairs from larger models\nas in-context examples, IC-Cache empowers small LLMs to imitate and even exceed\nthe compositional abilities (e.g., reasoning) of their larger counterparts,\nenabling selective offloading of requests to reduce cost and latency. Achieving\nthis live augmentation at scale introduces intricate trade-offs between\nresponse quality, latency, and system throughput. For a new request, IC-Cache\nefficiently selects similar, high-utility examples to prepend them to the new\nrequest's input. At scale, it adaptively routes requests across LLMs of varying\ncapabilities, accounting for response quality and serving loads. IC-Cache\nemploys a cost-aware cache replay mechanism that refines example quality\noffline to maximize online cache utility and efficiency. Evaluations on\nmillions of realistic requests demonstrate that IC-Cache improves LLM serving\nthroughput by 1.4-5.9x and reduces latency by 28-71% without hurting response\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 70% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge transfer among requests. However, naively caching and reusing past\nresponses leads to a big quality drop. In this paper, we introduce IC-Cache, a\ncaching system that enables live LLM capability augmentation to improve serving\nefficiency: by leveraging historical request-response pairs from larger models\nas in-context examples, IC-Cache empowers small LLMs to imitate and even exceed\nthe compositional abilities (e.g., reasoning) of their larger counterparts,\nenabling selective offloading of requests to reduce cost and latency. Achieving\nthis live augmentation at scale introduces intricate trade-offs between\nresponse quality, latency, and system throughput. For a new request, IC-Cache\nefficiently selects similar, high-utility examples to prepend them to the new\nrequest's input. At scale, it adaptively routes requests across LLMs of varying\ncapabilities, accounting for response quality and serving loads. IC-Cache\nemploys a cost-aware cache replay mechanism that refines example quality\noffline to maximize online cache utility and efficiency. Evaluations on\nmillions of realistic requests demonstrate that IC-Cache improves LLM serving\nthroughput by 1.4-5.9x and reduces latency by 28-71% without hurting response\nquality."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Nikhil Sarda"
                    },
                    {
                        "name": "Lillian Tsai"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Yanqi Zhou"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Henry M. Levy"
                    },
                    {
                        "name": "David Culler"
                    }
                ],
                "author_detail": {
                    "name": "David Culler"
                },
                "author": "David Culler",
                "arxiv_doi": "10.1145/3731569.3764829",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731569.3764829",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.12689v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12689v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01228v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01228v2",
                "updated": "2025-09-03T20:54:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    20,
                    54,
                    57,
                    2,
                    246,
                    0
                ],
                "published": "2024-10-02T04:12:13Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    4,
                    12,
                    13,
                    2,
                    276,
                    0
                ],
                "title": "ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline\n  Co-Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline\n  Co-Serving"
                },
                "summary": "Large language model (LLM) serving demands low latency and high throughput,\nbut high load variability makes it challenging to achieve high GPU utilization.\nIn this paper, we identify a synergetic but overlooked opportunity to co-serve\nlatency-critical online requests alongside latency-tolerant offline tasks such\nas model benchmarking. While promising, existing serving systems fail to\nco-serve them efficiently, as their coarse-grained resource management at the\nrequest or iteration level cannot harvest millisecond-level GPU idle cycles\nwithout introducing interference that violates online latency objectives.\nConServe is a new LLM co-serving system that achieves high throughput and\nstrong online latency guarantees by managing resources at finer granularities.\nConServe introduces three techniques: (1) a latency-aware token-level scheduler\nthat precisely sizes offline batches and tokens to fit within online latency\nobjectives; (2) sub-iteration, layer-wise preemption that allows offline tasks\nto yield to online load spikes; and (3) incremental KV cache management that\nenables preempting and resuming offline requests at near-zero cost. Evaluations\nwith Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe\ndelivers an average of 2.2$\\times$ higher throughput and reduces online serving\ntail latency by 2.9$\\times$ on average compared to state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving demands low latency and high throughput,\nbut high load variability makes it challenging to achieve high GPU utilization.\nIn this paper, we identify a synergetic but overlooked opportunity to co-serve\nlatency-critical online requests alongside latency-tolerant offline tasks such\nas model benchmarking. While promising, existing serving systems fail to\nco-serve them efficiently, as their coarse-grained resource management at the\nrequest or iteration level cannot harvest millisecond-level GPU idle cycles\nwithout introducing interference that violates online latency objectives.\nConServe is a new LLM co-serving system that achieves high throughput and\nstrong online latency guarantees by managing resources at finer granularities.\nConServe introduces three techniques: (1) a latency-aware token-level scheduler\nthat precisely sizes offline batches and tokens to fit within online latency\nobjectives; (2) sub-iteration, layer-wise preemption that allows offline tasks\nto yield to online load spikes; and (3) incremental KV cache management that\nenables preempting and resuming offline requests at near-zero cost. Evaluations\nwith Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe\ndelivers an average of 2.2$\\times$ higher throughput and reduces online serving\ntail latency by 2.9$\\times$ on average compared to state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Yifan Qiao"
                    },
                    {
                        "name": "Shu Anzai"
                    },
                    {
                        "name": "Shan Yu"
                    },
                    {
                        "name": "Haoran Ma"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Miryung Kim"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jiarong Xing"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Harry Xu"
                    }
                ],
                "author_detail": {
                    "name": "Harry Xu"
                },
                "author": "Harry Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01228v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03394v1",
                "updated": "2025-09-03T15:15:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    44,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T15:15:44Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    44,
                    2,
                    246,
                    0
                ],
                "title": "CloudFormer: An Attention-based Performance Prediction for Public Clouds\n  with Unknown Workload",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CloudFormer: An Attention-based Performance Prediction for Public Clouds\n  with Unknown Workload"
                },
                "summary": "Cloud platforms are increasingly relied upon to host diverse,\nresource-intensive workloads due to their scalability, flexibility, and\ncost-efficiency. In multi-tenant cloud environments, virtual machines are\nconsolidated on shared physical servers to improve resource utilization. While\nvirtualization guarantees resource partitioning for CPU, memory, and storage,\nit cannot ensure performance isolation. Competition for shared resources such\nas last-level cache, memory bandwidth, and network interfaces often leads to\nsevere performance degradation. Existing management techniques, including VM\nscheduling and resource provisioning, require accurate performance prediction\nto mitigate interference. However, this remains challenging in public clouds\ndue to the black-box nature of VMs and the highly dynamic nature of workloads.\nTo address these limitations, we propose CloudFormer, a dual-branch\nTransformer-based model designed to predict VM performance degradation in\nblack-box environments. CloudFormer jointly models temporal dynamics and\nsystem-level interactions, leveraging 206 system metrics at one-second\nresolution across both static and dynamic scenarios. This design enables the\nmodel to capture transient interference effects and adapt to varying workload\nconditions without scenario-specific tuning. Complementing the methodology, we\nprovide a fine-grained dataset that significantly expands the temporal\nresolution and metric diversity compared to existing benchmarks. Experimental\nresults demonstrate that CloudFormer consistently outperforms state-of-the-art\nbaselines across multiple evaluation metrics, achieving robust generalization\nacross diverse and previously unseen workloads. Notably, CloudFormer attains a\nmean absolute error (MAE) of just 7.8%, representing a substantial improvement\nin predictive accuracy and outperforming existing methods at least by 28%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms are increasingly relied upon to host diverse,\nresource-intensive workloads due to their scalability, flexibility, and\ncost-efficiency. In multi-tenant cloud environments, virtual machines are\nconsolidated on shared physical servers to improve resource utilization. While\nvirtualization guarantees resource partitioning for CPU, memory, and storage,\nit cannot ensure performance isolation. Competition for shared resources such\nas last-level cache, memory bandwidth, and network interfaces often leads to\nsevere performance degradation. Existing management techniques, including VM\nscheduling and resource provisioning, require accurate performance prediction\nto mitigate interference. However, this remains challenging in public clouds\ndue to the black-box nature of VMs and the highly dynamic nature of workloads.\nTo address these limitations, we propose CloudFormer, a dual-branch\nTransformer-based model designed to predict VM performance degradation in\nblack-box environments. CloudFormer jointly models temporal dynamics and\nsystem-level interactions, leveraging 206 system metrics at one-second\nresolution across both static and dynamic scenarios. This design enables the\nmodel to capture transient interference effects and adapt to varying workload\nconditions without scenario-specific tuning. Complementing the methodology, we\nprovide a fine-grained dataset that significantly expands the temporal\nresolution and metric diversity compared to existing benchmarks. Experimental\nresults demonstrate that CloudFormer consistently outperforms state-of-the-art\nbaselines across multiple evaluation metrics, achieving robust generalization\nacross diverse and previously unseen workloads. Notably, CloudFormer attains a\nmean absolute error (MAE) of just 7.8%, representing a substantial improvement\nin predictive accuracy and outperforming existing methods at least by 28%."
                },
                "authors": [
                    {
                        "name": "Amirhossein Shahbazinia"
                    },
                    {
                        "name": "Darong Huang"
                    },
                    {
                        "name": "Luis Costero"
                    },
                    {
                        "name": "David Atienza"
                    }
                ],
                "author_detail": {
                    "name": "David Atienza"
                },
                "author": "David Atienza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00079v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00079v4",
                "updated": "2025-09-03T14:56:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    56,
                    29,
                    2,
                    246,
                    0
                ],
                "published": "2024-06-24T02:05:32Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    2,
                    5,
                    32,
                    0,
                    176,
                    0
                ],
                "title": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving"
                },
                "summary": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests."
                },
                "authors": [
                    {
                        "name": "Ruoyu Qin"
                    },
                    {
                        "name": "Zheming Li"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yongwei Wu"
                    },
                    {
                        "name": "Weimin Zheng"
                    },
                    {
                        "name": "Xinran Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xinran Xu"
                },
                "author": "Xinran Xu",
                "arxiv_comment": "23 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00079v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00079v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04416v2",
                "updated": "2025-09-03T14:28:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    28,
                    23,
                    2,
                    246,
                    0
                ],
                "published": "2025-07-06T15:08:49Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "title": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based\n  Sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based\n  Sequence Modeling"
                },
                "summary": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT."
                },
                "authors": [
                    {
                        "name": "Xiuying Wei"
                    },
                    {
                        "name": "Anunay Yadav"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03560v1",
                "updated": "2025-09-03T11:23:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    23,
                    35,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T11:23:35Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    23,
                    35,
                    2,
                    246,
                    0
                ],
                "title": "A Cegar-centric Bounded Reachability Analysis for Compositional Affine\n  Hybrid Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Cegar-centric Bounded Reachability Analysis for Compositional Affine\n  Hybrid Systems"
                },
                "summary": "Reachability analysis of compositional hybrid systems, where individual\ncomponents are modeled as hybrid automata, poses unique challenges. In addition\nto preserving the compositional semantics while computing system behaviors,\nalgorithms have to cater to the explosion in the number of locations in the\nparallel product automaton. In this paper, we propose a bounded reachability\nanalysis algorithm for compositional hybrid systems with piecewise affine\ndynamics, based on the principle of counterexample guided abstraction\nrefinement (CEGAR). In particular, the algorithm searches for a counterexample\nin the discrete abstraction of the composition model, without explicitly\ncomputing a product automaton. When a counterexample is discovered in the\nabstraction, its validity is verified by a refinement of the state-space guided\nby the abstract counterexample. The state-space refinement is through a\nsymbolic reachability analysis, particularly using a state-of-the-art algorithm\nwith support functions as the continuous state representation. In addition, the\nalgorithm mixes different semantics of composition with the objective of\nimproved efficiency. Step compositional semantics is followed while exploring\nthe abstract (discrete) state-space, while shallow compositional semantics is\nfollowed during state-space refinement with symbolic reachability analysis.\nOptimizations such as caching the results of the symbolic reachability\nanalysis, which can be later reused, have been proposed. We implement this\nalgorithm in the tool SAT-Reach and demonstrate the scalability benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reachability analysis of compositional hybrid systems, where individual\ncomponents are modeled as hybrid automata, poses unique challenges. In addition\nto preserving the compositional semantics while computing system behaviors,\nalgorithms have to cater to the explosion in the number of locations in the\nparallel product automaton. In this paper, we propose a bounded reachability\nanalysis algorithm for compositional hybrid systems with piecewise affine\ndynamics, based on the principle of counterexample guided abstraction\nrefinement (CEGAR). In particular, the algorithm searches for a counterexample\nin the discrete abstraction of the composition model, without explicitly\ncomputing a product automaton. When a counterexample is discovered in the\nabstraction, its validity is verified by a refinement of the state-space guided\nby the abstract counterexample. The state-space refinement is through a\nsymbolic reachability analysis, particularly using a state-of-the-art algorithm\nwith support functions as the continuous state representation. In addition, the\nalgorithm mixes different semantics of composition with the objective of\nimproved efficiency. Step compositional semantics is followed while exploring\nthe abstract (discrete) state-space, while shallow compositional semantics is\nfollowed during state-space refinement with symbolic reachability analysis.\nOptimizations such as caching the results of the symbolic reachability\nanalysis, which can be later reused, have been proposed. We implement this\nalgorithm in the tool SAT-Reach and demonstrate the scalability benefits."
                },
                "authors": [
                    {
                        "name": "Atanu Kundu"
                    },
                    {
                        "name": "Pratyay Sarkar"
                    },
                    {
                        "name": "Rajarshi Ray"
                    }
                ],
                "author_detail": {
                    "name": "Rajarshi Ray"
                },
                "author": "Rajarshi Ray",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03136v1",
                "updated": "2025-09-03T08:38:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T08:38:40Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "title": "Adaptive KV-Cache Compression without Manually Setting Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive KV-Cache Compression without Manually Setting Budget"
                },
                "summary": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable."
                },
                "authors": [
                    {
                        "name": "Chenxia Tang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20353v2",
                "updated": "2025-09-03T06:56:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    6,
                    56,
                    21,
                    2,
                    246,
                    0
                ],
                "published": "2025-05-26T05:58:49Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    58,
                    49,
                    0,
                    146,
                    0
                ],
                "title": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation"
                },
                "summary": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Ben Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Nian Wu"
                },
                "author": "Ying Nian Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18002v2",
                "updated": "2025-09-02T18:10:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    18,
                    10,
                    0,
                    1,
                    245,
                    0
                ],
                "published": "2024-10-23T16:25:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges"
                },
                "summary": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks."
                },
                "authors": [
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Hanzhi Yu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    },
                    {
                        "name": "Yuchen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Liu"
                },
                "author": "Yuchen Liu",
                "arxiv_comment": "Under Major Revision in IEEE Network",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02532v1",
                "updated": "2025-09-02T17:35:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    17,
                    35,
                    42,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T17:35:42Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    17,
                    35,
                    42,
                    1,
                    245,
                    0
                ],
                "title": "A Novel Coded Caching Scheme for Partially Cooperative Device-to-Device\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Coded Caching Scheme for Partially Cooperative Device-to-Device\n  Networks"
                },
                "summary": "Device-to-device (D2D) communication is one of the most promising techniques\nfor future wireless cellular communication systems. This paper considers coded\ncaching in a partially cooperative wireless D2D network, where only a subset of\nusers transmit during delivery, while all users request files. The\nnon-transmitting users are referred to as selfish users. All existing schemes\nthat do not require knowledge of the identity of selfish users before content\nplacement are limited to the high-memory regime, particularly when the number\nof selfish users is large. We propose a novel coded caching scheme for a\npartially cooperative D2D network that operates in all feasible memory regimes,\nregardless of the number of selfish users. We also derive a lower bound on the\ntransmission load of a partially cooperative D2D coded caching scheme. Using\nthis bound, the proposed scheme is shown to be optimal in the high-memory\nregime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Device-to-device (D2D) communication is one of the most promising techniques\nfor future wireless cellular communication systems. This paper considers coded\ncaching in a partially cooperative wireless D2D network, where only a subset of\nusers transmit during delivery, while all users request files. The\nnon-transmitting users are referred to as selfish users. All existing schemes\nthat do not require knowledge of the identity of selfish users before content\nplacement are limited to the high-memory regime, particularly when the number\nof selfish users is large. We propose a novel coded caching scheme for a\npartially cooperative D2D network that operates in all feasible memory regimes,\nregardless of the number of selfish users. We also derive a lower bound on the\ntransmission load of a partially cooperative D2D coded caching scheme. Using\nthis bound, the proposed scheme is shown to be optimal in the high-memory\nregime."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "7 pages and 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v5",
                "updated": "2025-09-02T16:39:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    39,
                    56,
                    1,
                    245,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation"
                },
                "summary": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. However, existing Ethernet-based solutions, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilization due to both increasing traffic demands and\nthe expanding scale of datacenter topologies, which also exacerbate network\nfailures. To address these limitations, we propose REPS, a lightweight\ndecentralized per-packet adaptive load balancing algorithm designed to optimize\nnetwork utilization while ensuring rapid recovery from link failures. REPS\nadapts to network conditions by caching good-performing paths. In case of a\nnetwork failure, REPS re-routes traffic away from it in less than 100\nmicroseconds. REPS is designed to be deployed with next-generation out-of-order\ntransports, such as Ultra Ethernet, and uses less than 25 bytes of\nper-connection state regardless of the topology size. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. However, existing Ethernet-based solutions, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilization due to both increasing traffic demands and\nthe expanding scale of datacenter topologies, which also exacerbate network\nfailures. To address these limitations, we propose REPS, a lightweight\ndecentralized per-packet adaptive load balancing algorithm designed to optimize\nnetwork utilization while ensuring rapid recovery from link failures. REPS\nadapts to network conditions by caching good-performing paths. In case of a\nnetwork failure, REPS re-routes traffic away from it in less than 100\nmicroseconds. REPS is designed to be deployed with next-generation out-of-order\ntransports, such as Ultra Ethernet, and uses less than 25 bytes of\nper-connection state regardless of the topology size. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Elias Achermann"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02480v1",
                "updated": "2025-09-02T16:30:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    30,
                    49,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T16:30:49Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    30,
                    49,
                    1,
                    245,
                    0
                ],
                "title": "MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to\n  Break the GPU Memory Wall",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to\n  Break the GPU Memory Wall"
                },
                "summary": "Training LLMs larger than the aggregated memory of multiple GPUs is\nincreasingly necessary due to the faster growth of LLM sizes compared to GPU\nmemory. To this end, multi-tier host memory or disk offloading techniques are\nproposed by state of art. Despite advanced asynchronous multi-tier read/write\nstrategies, such offloading strategies result in significant I/O overheads in\nthe critical path of training, resulting in slower iterations. To this end, we\npropose MLP-Offload, a novel multi-level, multi-path offloading engine\nspecifically designed for optimizing LLM training on resource-constrained\nsetups by mitigating I/O bottlenecks. We make several key observations that\ndrive the design of MLP-Offload, such as I/O overheads during the update\ndominate the iteration time; I/O bandwidth of the third-level remote storage\ntier remains unutilized; and, contention due to concurrent offloading amplifies\nI/O bottlenecks. Driven by these insights, we design and implement MLP-Offload\nto offload the optimizer states across multiple tiers in a cache-efficient and\nconcurrency-controlled fashion to mitigate I/O bottlenecks during the backward\nand update phases. Evaluations on models up to 280B parameters shows that\nMLP-Offload achieves 2.5$\\times$ faster iterations compared to the\nstate-of-the-art LLM training runtimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLMs larger than the aggregated memory of multiple GPUs is\nincreasingly necessary due to the faster growth of LLM sizes compared to GPU\nmemory. To this end, multi-tier host memory or disk offloading techniques are\nproposed by state of art. Despite advanced asynchronous multi-tier read/write\nstrategies, such offloading strategies result in significant I/O overheads in\nthe critical path of training, resulting in slower iterations. To this end, we\npropose MLP-Offload, a novel multi-level, multi-path offloading engine\nspecifically designed for optimizing LLM training on resource-constrained\nsetups by mitigating I/O bottlenecks. We make several key observations that\ndrive the design of MLP-Offload, such as I/O overheads during the update\ndominate the iteration time; I/O bandwidth of the third-level remote storage\ntier remains unutilized; and, contention due to concurrent offloading amplifies\nI/O bottlenecks. Driven by these insights, we design and implement MLP-Offload\nto offload the optimizer states across multiple tiers in a cache-efficient and\nconcurrency-controlled fashion to mitigate I/O bottlenecks during the backward\nand update phases. Evaluations on models up to 280B parameters shows that\nMLP-Offload achieves 2.5$\\times$ faster iterations compared to the\nstate-of-the-art LLM training runtimes."
                },
                "authors": [
                    {
                        "name": "Avinash Maurya"
                    },
                    {
                        "name": "M. Mustafa Rafique"
                    },
                    {
                        "name": "Franck Cappello"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Nicolae"
                },
                "author": "Bogdan Nicolae",
                "arxiv_doi": "10.1145/3712285.3759864",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3712285.3759864",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SC'25: The International Conference for High Performance Computing,\n  Networking, Storage and Analysis",
                "arxiv_journal_ref": "SC'25: The International Conference for High Performance\n  Computing, Networking, Storage and Analysis, 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.0; E.2; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02408v1",
                "updated": "2025-09-02T15:19:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    15,
                    19,
                    6,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T15:19:06Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    15,
                    19,
                    6,
                    1,
                    245,
                    0
                ],
                "title": "Cache Management for Mixture-of-Experts LLMs -- extended version",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Management for Mixture-of-Experts LLMs -- extended version"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks. One of the main challenges towards the successful\ndeployment of LLMs is memory management, since they typically involve billions\nof parameters. To this end, architectures based on Mixture-of-Experts have been\nproposed, which aim to reduce the size of the parameters that are activated\nwhen producing a token. This raises the equally critical issue of efficiently\nmanaging the limited cache of the system, in that frequently used experts\nshould be stored in the fast cache rather than in the slower secondary memory.\n  In this work, we introduce and study a new paging problem that models expert\nmanagement optimization. Our formulation captures both the layered architecture\nof LLMs and the requirement that experts are cached efficiently. We first\npresent lower bounds on the competitive ratio of both deterministic and\nrandomized algorithms, which show that under mild assumptions, LRU-like\npolicies have good theoretical competitive performance. We then propose a\nlayer-based extension of LRU that is tailored to the problem at hand.\n  Extensive simulations on both synthetic datasets and actual traces of MoE\nusage show that our algorithm outperforms policies for the classic paging\nproblem, such as the standard LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks. One of the main challenges towards the successful\ndeployment of LLMs is memory management, since they typically involve billions\nof parameters. To this end, architectures based on Mixture-of-Experts have been\nproposed, which aim to reduce the size of the parameters that are activated\nwhen producing a token. This raises the equally critical issue of efficiently\nmanaging the limited cache of the system, in that frequently used experts\nshould be stored in the fast cache rather than in the slower secondary memory.\n  In this work, we introduce and study a new paging problem that models expert\nmanagement optimization. Our formulation captures both the layered architecture\nof LLMs and the requirement that experts are cached efficiently. We first\npresent lower bounds on the competitive ratio of both deterministic and\nrandomized algorithms, which show that under mild assumptions, LRU-like\npolicies have good theoretical competitive performance. We then propose a\nlayer-based extension of LRU that is tailored to the problem at hand.\n  Extensive simulations on both synthetic datasets and actual traces of MoE\nusage show that our algorithm outperforms policies for the classic paging\nproblem, such as the standard LRU."
                },
                "authors": [
                    {
                        "name": "Spyros Angelopoulos"
                    },
                    {
                        "name": "Loris Marchal"
                    },
                    {
                        "name": "Adrien Obrecht"
                    },
                    {
                        "name": "Bertrand Simon"
                    }
                ],
                "author_detail": {
                    "name": "Bertrand Simon"
                },
                "author": "Bertrand Simon",
                "arxiv_doi": "10.1007/978-3-031-99872-0_2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-99872-0_2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v2",
                "updated": "2025-09-02T13:09:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    13,
                    9,
                    37,
                    1,
                    245,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing reliance on expensive vector\ndatabase lookups. To scale efficiently, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically skewed MedRAG workload reduces database calls by 78.9% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our work highlights that approximate caching\nis a viable and effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing reliance on expensive vector\ndatabase lookups. To scale efficiently, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically skewed MedRAG workload reduces database calls by 78.9% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our work highlights that approximate caching\nis a viable and effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Zhang Ji"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    }
                ],
                "author_detail": {
                    "name": "Martijn de Vos"
                },
                "author": "Martijn de Vos",
                "arxiv_doi": "10.1145/3721146.3721941",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721146.3721941",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02232v1",
                "updated": "2025-09-02T11:58:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    58,
                    6,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T11:58:06Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    58,
                    6,
                    1,
                    245,
                    0
                ],
                "title": "Efficient Geometry Compression and Communication for 3D Gaussian\n  Splatting Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Geometry Compression and Communication for 3D Gaussian\n  Splatting Point Clouds"
                },
                "summary": "Storage and transmission challenges in dynamic 3D scene representation based\non the i3DV platform, With increasing scene complexity, the explosive growth of\n3D Gaussian data volume causes excessive storage space occupancy. To address\nthis issue, we propose adopting the AVS PCRM reference software for efficient\ncompression of Gaussian point cloud geometry data. The strategy deeply\nintegrates the advanced encoding capabilities of AVS PCRM into the i3DV\nplatform, forming technical complementarity with the original rate-distortion\noptimization mechanism based on binary hash tables. On one hand, the hash table\nefficiently caches inter-frame Gaussian point transformation relationships,\nwhich allows for high-fidelity transmission within a 40 Mbps bandwidth\nconstraint. On the other hand, AVS PCRM performs precise compression on\ngeometry data. Experimental results demonstrate that the joint framework\nmaintains the advantages of fast rendering and high-quality synthesis in 3D\nGaussian technology while achieving significant 10\\%-25\\% bitrate savings on\nuniversal test sets. It provides a superior rate-distortion tradeoff solution\nfor the storage, transmission, and interaction of 3D volumetric video.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Storage and transmission challenges in dynamic 3D scene representation based\non the i3DV platform, With increasing scene complexity, the explosive growth of\n3D Gaussian data volume causes excessive storage space occupancy. To address\nthis issue, we propose adopting the AVS PCRM reference software for efficient\ncompression of Gaussian point cloud geometry data. The strategy deeply\nintegrates the advanced encoding capabilities of AVS PCRM into the i3DV\nplatform, forming technical complementarity with the original rate-distortion\noptimization mechanism based on binary hash tables. On one hand, the hash table\nefficiently caches inter-frame Gaussian point transformation relationships,\nwhich allows for high-fidelity transmission within a 40 Mbps bandwidth\nconstraint. On the other hand, AVS PCRM performs precise compression on\ngeometry data. Experimental results demonstrate that the joint framework\nmaintains the advantages of fast rendering and high-quality synthesis in 3D\nGaussian technology while achieving significant 10\\%-25\\% bitrate savings on\nuniversal test sets. It provides a superior rate-distortion tradeoff solution\nfor the storage, transmission, and interaction of 3D volumetric video."
                },
                "authors": [
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Yanting Li"
                    },
                    {
                        "name": "Luyang Tang"
                    },
                    {
                        "name": "Wei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Gao"
                },
                "author": "Wei Gao",
                "arxiv_doi": "10.1145/3680207.3765659",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3680207.3765659",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages,5 figures",
                "arxiv_journal_ref": "ACM MOBICOM 2025",
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15212v2",
                "updated": "2025-09-02T11:29:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    29,
                    34,
                    1,
                    245,
                    0
                ],
                "published": "2025-08-21T03:48:28Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    48,
                    28,
                    3,
                    233,
                    0
                ],
                "title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning"
                },
                "summary": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Guanchen Li"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Emad Barsoum"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02121v1",
                "updated": "2025-09-02T09:17:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    9,
                    17,
                    40,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T09:17:40Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    9,
                    17,
                    40,
                    1,
                    245,
                    0
                ],
                "title": "Batch Query Processing and Optimization for Agentic Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch Query Processing and Optimization for Agentic Workflows"
                },
                "summary": "Large Language Models (LLMs) in agentic workflows combine multi-step\nreasoning, tool use, and collaboration across multiple specialized agents.\nExisting LLM serving engines optimize individual calls in isolation, while\nmulti-agent frameworks focus on orchestration without system-level performance\nplanning. As a result, repeated prompts, overlapping contexts, and concurrent\nexecutions create substantial redundancy and poor GPU utilization, especially\nin batch analytics scenarios. We introduce Halo, a system that brings batch\nquery processing and optimization into agentic LLM workflows. Halo represents\neach workflow as a structured query plan DAG and constructs a consolidated\ngraph for batched queries that exposes shared computation. Guided by a cost\nmodel that jointly considers prefill and decode costs, cache reuse, and GPU\nplacement, Halo performs plan-level optimization to minimize redundant\nexecution. Its runtime integrates adaptive batching, KV-cache sharing and\nmigration, along with compute-communication overlap to maximize hardware\nefficiency. Evaluation across six benchmarks shows that Halo achieves up to\n18.6x speedup for batch inference and 4.7x throughput improvement under online\nserving, scaling to workloads of tens of thousands of queries and complex\ngraphs. These gains are achieved without compromising output quality. By\nunifying query optimization with LLM serving, Halo enables efficient agentic\nworkflows in data analytics and decision-making applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) in agentic workflows combine multi-step\nreasoning, tool use, and collaboration across multiple specialized agents.\nExisting LLM serving engines optimize individual calls in isolation, while\nmulti-agent frameworks focus on orchestration without system-level performance\nplanning. As a result, repeated prompts, overlapping contexts, and concurrent\nexecutions create substantial redundancy and poor GPU utilization, especially\nin batch analytics scenarios. We introduce Halo, a system that brings batch\nquery processing and optimization into agentic LLM workflows. Halo represents\neach workflow as a structured query plan DAG and constructs a consolidated\ngraph for batched queries that exposes shared computation. Guided by a cost\nmodel that jointly considers prefill and decode costs, cache reuse, and GPU\nplacement, Halo performs plan-level optimization to minimize redundant\nexecution. Its runtime integrates adaptive batching, KV-cache sharing and\nmigration, along with compute-communication overlap to maximize hardware\nefficiency. Evaluation across six benchmarks shows that Halo achieves up to\n18.6x speedup for batch inference and 4.7x throughput improvement under online\nserving, scaling to workloads of tens of thousands of queries and complex\ngraphs. These gains are achieved without compromising output quality. By\nunifying query optimization with LLM serving, Halo enables efficient agentic\nworkflows in data analytics and decision-making applications."
                },
                "authors": [
                    {
                        "name": "Junyi Shen"
                    },
                    {
                        "name": "Noppanat Wadlom"
                    },
                    {
                        "name": "Yao Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Lu"
                },
                "author": "Yao Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02004v1",
                "updated": "2025-09-02T06:40:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    6,
                    40,
                    45,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T06:40:45Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    6,
                    40,
                    45,
                    1,
                    245,
                    0
                ],
                "title": "Augmented Shuffle Differential Privacy Protocols for Large-Domain\n  Categorical and Key-Value Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Shuffle Differential Privacy Protocols for Large-Domain\n  Categorical and Key-Value Data"
                },
                "summary": "Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy\nby introducing a shuffler who randomly shuffles data in a distributed system.\nHowever, most shuffle DP protocols are vulnerable to two attacks: collusion\nattacks by the data collector and users and data poisoning attacks. A recent\nstudy addresses this issue by introducing an augmented shuffle DP protocol,\nwhere users do not add noise and the shuffler performs random sampling and\ndummy data addition. However, it focuses on frequency estimation over\ncategorical data with a small domain and cannot be applied to a large domain\ndue to prohibitively high communication and computational costs.\n  In this paper, we fill this gap by introducing a novel augmented shuffle DP\nprotocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME\nprotocol uses a hash function to filter out unpopular items and then accurately\ncalculates frequencies for popular items. To perform this within one round of\ninteraction between users and the shuffler, our protocol carefully communicates\nwithin a system using multiple encryption. We also apply our FME protocol to\nmore advanced KV (Key-Value) statistics estimation with an additional technique\nto reduce bias. For both categorical and KV data, we prove that our protocol\nprovides computational DP, high robustness to the above two attacks, accuracy,\nand efficiency. We show the effectiveness of our proposals through comparisons\nwith twelve existing protocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy\nby introducing a shuffler who randomly shuffles data in a distributed system.\nHowever, most shuffle DP protocols are vulnerable to two attacks: collusion\nattacks by the data collector and users and data poisoning attacks. A recent\nstudy addresses this issue by introducing an augmented shuffle DP protocol,\nwhere users do not add noise and the shuffler performs random sampling and\ndummy data addition. However, it focuses on frequency estimation over\ncategorical data with a small domain and cannot be applied to a large domain\ndue to prohibitively high communication and computational costs.\n  In this paper, we fill this gap by introducing a novel augmented shuffle DP\nprotocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME\nprotocol uses a hash function to filter out unpopular items and then accurately\ncalculates frequencies for popular items. To perform this within one round of\ninteraction between users and the shuffler, our protocol carefully communicates\nwithin a system using multiple encryption. We also apply our FME protocol to\nmore advanced KV (Key-Value) statistics estimation with an additional technique\nto reduce bias. For both categorical and KV data, we prove that our protocol\nprovides computational DP, high robustness to the above two attacks, accuracy,\nand efficiency. We show the effectiveness of our proposals through comparisons\nwith twelve existing protocols."
                },
                "authors": [
                    {
                        "name": "Takao Murakami"
                    },
                    {
                        "name": "Yuichi Sei"
                    },
                    {
                        "name": "Reo Eriguchi"
                    }
                ],
                "author_detail": {
                    "name": "Reo Eriguchi"
                },
                "author": "Reo Eriguchi",
                "arxiv_comment": "Full version of the paper accepted at NDSS 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01395v1",
                "updated": "2025-09-01T11:41:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    41,
                    10,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T11:41:10Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    41,
                    10,
                    0,
                    244,
                    0
                ],
                "title": "LLMs cannot spot math errors, even when allowed to peek into the\n  solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs cannot spot math errors, even when allowed to peek into the\n  solution"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable performance on math word\nproblems, yet they have been shown to struggle with meta-reasoning tasks such\nas identifying errors in student solutions. In this work, we investigate the\nchallenge of locating the first error step in stepwise solutions using two\nerror reasoning datasets: VtG and PRM800K. Our experiments show that\nstate-of-the-art LLMs struggle to locate the first error step in student\nsolutions even when given access to the reference solution. To that end, we\npropose an approach that generates an intermediate corrected student solution,\naligning more closely with the original student's solution, which helps improve\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable performance on math word\nproblems, yet they have been shown to struggle with meta-reasoning tasks such\nas identifying errors in student solutions. In this work, we investigate the\nchallenge of locating the first error step in stepwise solutions using two\nerror reasoning datasets: VtG and PRM800K. Our experiments show that\nstate-of-the-art LLMs struggle to locate the first error step in student\nsolutions even when given access to the reference solution. To that end, we\npropose an approach that generates an intermediate corrected student solution,\naligning more closely with the original student's solution, which helps improve\nperformance."
                },
                "authors": [
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "Accepted to EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15779v2",
                "updated": "2025-09-01T07:26:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    7,
                    26,
                    57,
                    0,
                    244,
                    0
                ],
                "published": "2025-02-17T08:12:34Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    12,
                    34,
                    0,
                    48,
                    0
                ],
                "title": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer"
                },
                "summary": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP."
                },
                "authors": [
                    {
                        "name": "Euntae Choi"
                    },
                    {
                        "name": "Sumin Song"
                    },
                    {
                        "name": "Woosang Lim"
                    },
                    {
                        "name": "Sungjoo Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Sungjoo Yoo"
                },
                "author": "Sungjoo Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v3",
                "updated": "2025-09-01T03:51:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    51,
                    9,
                    0,
                    244,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01092v1",
                "updated": "2025-09-01T03:31:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T03:31:44Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "title": "REFRAG: Rethinking RAG based Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REFRAG: Rethinking RAG based Decoding"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes."
                },
                "authors": [
                    {
                        "name": "Xiaoqiang Lin"
                    },
                    {
                        "name": "Aritra Ghosh"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    },
                    {
                        "name": "Anshumali Shrivastava"
                    },
                    {
                        "name": "Vijai Mohan"
                    }
                ],
                "author_detail": {
                    "name": "Vijai Mohan"
                },
                "author": "Vijai Mohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06133v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06133v2",
                "updated": "2025-08-31T15:09:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    15,
                    9,
                    36,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-08T08:54:21Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "title": "LLM Serving Optimization with Variable Prefill and Decode Lengths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Serving Optimization with Variable Prefill and Decode Lengths"
                },
                "summary": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency."
                },
                "authors": [
                    {
                        "name": "Meixuan Wang"
                    },
                    {
                        "name": "Yinyu Ye"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06133v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06133v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00883v1",
                "updated": "2025-08-31T14:51:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    51,
                    19,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-31T14:51:19Z",
                "published_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    51,
                    19,
                    6,
                    243,
                    0
                ],
                "title": "Accelerating Latency-Critical Applications with AI-Powered\n  Semi-Automatic Fine-Grained Parallelization on SMT Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Latency-Critical Applications with AI-Powered\n  Semi-Automatic Fine-Grained Parallelization on SMT Processors"
                },
                "summary": "Latency-critical applications tend to show low utilization of functional\nunits due to frequent cache misses and mispredictions during speculative\nexecution in high-performance superscalar processors. However, due to\nsignificant impact on single-thread performance, Simultaneous Multithreading\n(SMT) technology is rarely used with heavy threads of latency-critical\napplications. In this paper, we explore utilization of SMT technology to\nsupport fine-grained parallelization of latency-critical applications.\nFollowing the advancements in the development of Large Language Models (LLMs),\nwe introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we\nextend AI Coding Agent in Cursor IDE with additional tools connected through\nModel Context Protocol, enabling end-to-end AI Agent for parallelization.\nAdditional connected tools enable LLM-guided hotspot detection, collection of\ndynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance\nsimulation to estimate performance gains. We apply Aira with Relic parallel\nframework for fine-grained task parallelism on SMT cores to parallelize\nlatency-critical benchmarks representing real-world applications used in\nindustry. We show 17% geomean performance gain from parallelization of\nlatency-critical benchmarks using Aira with Relic framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency-critical applications tend to show low utilization of functional\nunits due to frequent cache misses and mispredictions during speculative\nexecution in high-performance superscalar processors. However, due to\nsignificant impact on single-thread performance, Simultaneous Multithreading\n(SMT) technology is rarely used with heavy threads of latency-critical\napplications. In this paper, we explore utilization of SMT technology to\nsupport fine-grained parallelization of latency-critical applications.\nFollowing the advancements in the development of Large Language Models (LLMs),\nwe introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we\nextend AI Coding Agent in Cursor IDE with additional tools connected through\nModel Context Protocol, enabling end-to-end AI Agent for parallelization.\nAdditional connected tools enable LLM-guided hotspot detection, collection of\ndynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance\nsimulation to estimate performance gains. We apply Aira with Relic parallel\nframework for fine-grained task parallelism on SMT cores to parallelize\nlatency-critical benchmarks representing real-world applications used in\nindustry. We show 17% geomean performance gain from parallelization of\nlatency-critical benchmarks using Aira with Relic framework."
                },
                "authors": [
                    {
                        "name": "Denis Los"
                    },
                    {
                        "name": "Igor Petushkov"
                    }
                ],
                "author_detail": {
                    "name": "Igor Petushkov"
                },
                "author": "Igor Petushkov",
                "arxiv_journal_ref": "International Journal of Open Information Technologies, vol. 13,\n  no. 9, pp. 129-134, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10431v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10431v3",
                "updated": "2025-08-31T05:43:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    5,
                    43,
                    55,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-14T08:04:15Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    4,
                    15,
                    3,
                    226,
                    0
                ],
                "title": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches"
                },
                "summary": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE."
                },
                "authors": [
                    {
                        "name": "Chris Cao"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "arxiv_comment": "This version includes updated analysis of RCO Bugs (one additional\n  bug identified). Appendix added with code snippets for bug fixes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10431v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10431v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00625v1",
                "updated": "2025-08-30T22:47:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    22,
                    47,
                    15,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T22:47:15Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    22,
                    47,
                    15,
                    5,
                    242,
                    0
                ],
                "title": "NetGent: Agent-Based Automation of Network Application Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NetGent: Agent-Based Automation of Network Application Workflows"
                },
                "summary": "We present NetGent, an AI-agent framework for automating complex application\nworkflows to generate realistic network traffic datasets. Developing\ngeneralizable ML models for networking requires data collection from network\nenvironments with traffic that results from a diverse set of real-world web\napplications. However, using existing browser automation tools that are\ndiverse, repeatable, realistic, and efficient remains fragile and costly.\nNetGent addresses this challenge by allowing users to specify workflows as\nnatural-language rules that define state-dependent actions. These abstract\nspecifications are compiled into nondeterministic finite automata (NFAs), which\na state synthesis component translates into reusable, executable code. This\ndesign enables deterministic replay, reduces redundant LLM calls through state\ncaching, and adapts quickly when application interfaces change. In experiments,\nNetGent automated more than 50+ workflows spanning video-on-demand streaming,\nlive video streaming, video conferencing, social media, and web scraping,\nproducing realistic traffic traces while remaining robust to UI variability. By\ncombining the flexibility of language-based agents with the reliability of\ncompiled execution, NetGent provides a scalable foundation for generating the\ndiverse, repeatable datasets needed to advance ML in networking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present NetGent, an AI-agent framework for automating complex application\nworkflows to generate realistic network traffic datasets. Developing\ngeneralizable ML models for networking requires data collection from network\nenvironments with traffic that results from a diverse set of real-world web\napplications. However, using existing browser automation tools that are\ndiverse, repeatable, realistic, and efficient remains fragile and costly.\nNetGent addresses this challenge by allowing users to specify workflows as\nnatural-language rules that define state-dependent actions. These abstract\nspecifications are compiled into nondeterministic finite automata (NFAs), which\na state synthesis component translates into reusable, executable code. This\ndesign enables deterministic replay, reduces redundant LLM calls through state\ncaching, and adapts quickly when application interfaces change. In experiments,\nNetGent automated more than 50+ workflows spanning video-on-demand streaming,\nlive video streaming, video conferencing, social media, and web scraping,\nproducing realistic traffic traces while remaining robust to UI variability. By\ncombining the flexibility of language-based agents with the reliability of\ncompiled execution, NetGent provides a scalable foundation for generating the\ndiverse, repeatable datasets needed to advance ML in networking."
                },
                "authors": [
                    {
                        "name": "Jaber Daneshamooz"
                    },
                    {
                        "name": "Eugene Vuong"
                    },
                    {
                        "name": "Laasya Koduru"
                    },
                    {
                        "name": "Sanjay Chandrasekaran"
                    },
                    {
                        "name": "Arpit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Arpit Gupta"
                },
                "author": "Arpit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00579v1",
                "updated": "2025-08-30T18:25:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    18,
                    25,
                    19,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T18:25:19Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    18,
                    25,
                    19,
                    5,
                    242,
                    0
                ],
                "title": "KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for\n  KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for\n  KV Cache"
                },
                "summary": "Transformer-based large language models (LLMs) demonstrate impressive\npotential in various practical applications. However, long context inference\nposes a significant challenge due to the enormous memory requirements of the\nkey-value (KV) cache, which can scale to multiple gigabytes as sequence length\nand batch size increase. In this paper, we present KVComp, a generic and\nefficient KV cache management framework optimized for long-text generation that\nsynergistically works with both latency-critical and throughput-critical\ninference systems. KVComp employs novel lossy compression techniques\nspecifically designed for KV cache data characteristics, featuring careful\nco-design of compression algorithms and system architecture. Our approach\nmaintains compatibility with the growing nature of KV cache while preserving\nhigh computational efficiency. Experimental results show that KVComp achieves\non average 47\\% and up to 83\\% higher memory reduction rate compared to\nexisting methods with little/no model accuracy degradation. Furthermore, KVComp\nachieves extremely high execution throughput, effectively reducing\ndecompression overhead and, in some cases, even accelerating the matrix-vector\nmultiplication operation and outperform cuBLAS-based attention kernels with\nless data movement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) demonstrate impressive\npotential in various practical applications. However, long context inference\nposes a significant challenge due to the enormous memory requirements of the\nkey-value (KV) cache, which can scale to multiple gigabytes as sequence length\nand batch size increase. In this paper, we present KVComp, a generic and\nefficient KV cache management framework optimized for long-text generation that\nsynergistically works with both latency-critical and throughput-critical\ninference systems. KVComp employs novel lossy compression techniques\nspecifically designed for KV cache data characteristics, featuring careful\nco-design of compression algorithms and system architecture. Our approach\nmaintains compatibility with the growing nature of KV cache while preserving\nhigh computational efficiency. Experimental results show that KVComp achieves\non average 47\\% and up to 83\\% higher memory reduction rate compared to\nexisting methods with little/no model accuracy degradation. Furthermore, KVComp\nachieves extremely high execution throughput, effectively reducing\ndecompression overhead and, in some cases, even accelerating the matrix-vector\nmultiplication operation and outperform cuBLAS-based attention kernels with\nless data movement."
                },
                "authors": [
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Taolue Yang"
                    },
                    {
                        "name": "Youyuan Liu"
                    },
                    {
                        "name": "Chengming Zhang"
                    },
                    {
                        "name": "Xubin He"
                    },
                    {
                        "name": "Sian Jin"
                    }
                ],
                "author_detail": {
                    "name": "Sian Jin"
                },
                "author": "Sian Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.13777v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.13777v2",
                "updated": "2025-08-30T14:49:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    14,
                    49,
                    34,
                    5,
                    242,
                    0
                ],
                "published": "2023-10-20T19:22:58Z",
                "published_parsed": [
                    2023,
                    10,
                    20,
                    19,
                    22,
                    58,
                    4,
                    293,
                    0
                ],
                "title": "Discrete and Continuous Caching Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete and Continuous Caching Games"
                },
                "summary": "We investigate a discrete search game called the Multiple Caching Game where\nthe searcher's aim is to find all of a set of $d$ treasures hidden in $n$\nlocations. Allowed queries are sets of locations of size $k$, and the searcher\nwins if in all $d$ queries, at least one treasure is hidden in one of the $k$\npicked locations. P\\'alv\\\"olgyi showed that the value of the game is at most\n$\\frac{k^d}{\\binom{n+d-1}{d}}$, with equality for large enough $n$. We\nconjecture the exact cases of equality. We also investigate variants of the\ngame and show an example where their values are different, answering a question\nof P\\'alv\\\"olgyi.\n  This game is closely related to a continuous variant, Alpern's Caching Game,\nbased on which we define other continous variants of the multiple caching game\nand examine their values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate a discrete search game called the Multiple Caching Game where\nthe searcher's aim is to find all of a set of $d$ treasures hidden in $n$\nlocations. Allowed queries are sets of locations of size $k$, and the searcher\nwins if in all $d$ queries, at least one treasure is hidden in one of the $k$\npicked locations. P\\'alv\\\"olgyi showed that the value of the game is at most\n$\\frac{k^d}{\\binom{n+d-1}{d}}$, with equality for large enough $n$. We\nconjecture the exact cases of equality. We also investigate variants of the\ngame and show an example where their values are different, answering a question\nof P\\'alv\\\"olgyi.\n  This game is closely related to a continuous variant, Alpern's Caching Game,\nbased on which we define other continous variants of the multiple caching game\nand examine their values."
                },
                "authors": [
                    {
                        "name": "Áron Jánosik"
                    },
                    {
                        "name": "Csenge Miklós"
                    },
                    {
                        "name": "Dániel G. Simon"
                    },
                    {
                        "name": "Kristóf Zólomy"
                    }
                ],
                "author_detail": {
                    "name": "Kristóf Zólomy"
                },
                "author": "Kristóf Zólomy",
                "arxiv_doi": "10.1142/S0219198925500057",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1142/S0219198925500057",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.13777v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.13777v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "International Game Theory Review 27 (3), 2025",
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91A05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v3",
                "updated": "2025-08-30T09:35:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    9,
                    35,
                    22,
                    5,
                    242,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "DiffKV: Differentiated Memory Management for Large Language Models with\n  Parallel KV Compaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffKV: Differentiated Memory Management for Large Language Models with\n  Parallel KV Compaction"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable capabilities but face\nsubstantial serving costs due to their high memory demands, with the key-value\n(KV) cache being a primary bottleneck. State-of-the-art KV cache compression\ntechniques, such as quantization and pruning, apply uniform treatment to both\nkeys and values, and discard unimportant tokens entirely, overlooking the\nfine-grained distinctions in the significance of individual KV cache\ncomponents. To address such limitations, we introduce \\textit{DiffKV}, a novel\nframework for efficient KV cache compression that exploits three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. These levels of\ndifferentiation introduce irregular memory usage patterns across different\nrequests and attention heads, posing significant scalability challenges for\nmemory management. To address these challenges, DiffKV proposes an on-GPU\nmemory manager that compacts fragmented free memory list into contiguous\nregions in parallel, effectively translating sparsity in the KV cache into\nperformance gains. We evaluate DiffKV on several mainstream LLMs, including the\nemerging thinking models that generate extended chains of thought. DiffKV is\nable to compress the KV cache by $2.7\\times$ to $5.7\\times$ with near-lossless\naccuracy on complex workloads requiring sophisticated reasoning and\nlong-generation capabilities, and enhances throughput by $1.9\\times$ to\n$5.4\\times$. Source codes of DiffKV are available at\nhttps://github.com/zyqCSL/DiffKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable capabilities but face\nsubstantial serving costs due to their high memory demands, with the key-value\n(KV) cache being a primary bottleneck. State-of-the-art KV cache compression\ntechniques, such as quantization and pruning, apply uniform treatment to both\nkeys and values, and discard unimportant tokens entirely, overlooking the\nfine-grained distinctions in the significance of individual KV cache\ncomponents. To address such limitations, we introduce \\textit{DiffKV}, a novel\nframework for efficient KV cache compression that exploits three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. These levels of\ndifferentiation introduce irregular memory usage patterns across different\nrequests and attention heads, posing significant scalability challenges for\nmemory management. To address these challenges, DiffKV proposes an on-GPU\nmemory manager that compacts fragmented free memory list into contiguous\nregions in parallel, effectively translating sparsity in the KV cache into\nperformance gains. We evaluate DiffKV on several mainstream LLMs, including the\nemerging thinking models that generate extended chains of thought. DiffKV is\nable to compress the KV cache by $2.7\\times$ to $5.7\\times$ with near-lossless\naccuracy on complex workloads requiring sophisticated reasoning and\nlong-generation capabilities, and enhances throughput by $1.9\\times$ to\n$5.4\\times$. Source codes of DiffKV are available at\nhttps://github.com/zyqCSL/DiffKV."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "SOSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00419v1",
                "updated": "2025-08-30T08:57:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    8,
                    57,
                    53,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T08:57:53Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    8,
                    57,
                    53,
                    5,
                    242,
                    0
                ],
                "title": "LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging\n  and KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging\n  and KV Cache Compression"
                },
                "summary": "In this paper, we introduce LightVLM, a simple but effective method that can\nbe seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly\naccelerate the inference process in a training-free manner. We divide the\ninference procedure of VLMs into two stages, i.e., encoding and decoding, and\npropose to simultaneously accelerate VLMs in both stages to largely improve\nmodel efficiency. During encoding, we propose pyramid token merging to reduce\ntokens of different LLM layers in a hierarchical manner by finally only keeping\na few dominant tokens to achieve high efficiency. During decoding, aimed at\nreducing the high latency of outputting long sequences, we propose KV Cache\ncompression to remove unnecessary caches to increase the network throughput.\nExperimental results show that LightVLM successfully retains 100% performance\nwhen only preserving 35% image tokens, and maintains around 98% performance\nwhen keeping only 3% image tokens. LightVLM could 2.02$\\times$ the network\nthroughput and reduce the prefilling time by 3.65$\\times$. LightVLM also makes\nlarge VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to\ninfer faster than significantly smaller models (e.g., InternVL2.5 8B),\nhopefully facilitating the real-world deployment. When generating long text\nsequences (e.g., 4096 tokens), LightVLM could reduce the inference time by\n3.21$\\times$, largely outperforming existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce LightVLM, a simple but effective method that can\nbe seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly\naccelerate the inference process in a training-free manner. We divide the\ninference procedure of VLMs into two stages, i.e., encoding and decoding, and\npropose to simultaneously accelerate VLMs in both stages to largely improve\nmodel efficiency. During encoding, we propose pyramid token merging to reduce\ntokens of different LLM layers in a hierarchical manner by finally only keeping\na few dominant tokens to achieve high efficiency. During decoding, aimed at\nreducing the high latency of outputting long sequences, we propose KV Cache\ncompression to remove unnecessary caches to increase the network throughput.\nExperimental results show that LightVLM successfully retains 100% performance\nwhen only preserving 35% image tokens, and maintains around 98% performance\nwhen keeping only 3% image tokens. LightVLM could 2.02$\\times$ the network\nthroughput and reduce the prefilling time by 3.65$\\times$. LightVLM also makes\nlarge VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to\ninfer faster than significantly smaller models (e.g., InternVL2.5 8B),\nhopefully facilitating the real-world deployment. When generating long text\nsequences (e.g., 4096 tokens), LightVLM could reduce the inference time by\n3.21$\\times$, largely outperforming existing methods."
                },
                "authors": [
                    {
                        "name": "Lianyu Hu"
                    },
                    {
                        "name": "Fanhua Shang"
                    },
                    {
                        "name": "Wei Feng"
                    },
                    {
                        "name": "Liang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wan"
                },
                "author": "Liang Wan",
                "arxiv_comment": "EMNLP2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00388v1",
                "updated": "2025-08-30T06:56:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    6,
                    56,
                    28,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T06:56:28Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    6,
                    56,
                    28,
                    5,
                    242,
                    0
                ],
                "title": "GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV\n  Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV\n  Cache Eviction"
                },
                "summary": "Efficient Key-Value (KV) cache management is essential for processing long\ntext sequences in large language models (LLMs), where memory constraints often\nlimit performance. Conventional KV eviction strategies, such as top-k selection\nbased on attention scores, depend on static heuristics that fail to capture the\nevolving implicit dependencies among tokens during inference. To overcome this,\nwe propose GraphKV, a graph-based framework that redefines token selection for\nKV cache compression. In GraphKV, tokens are modeled as nodes with importance\nscores, and edges represent their similarity relationships. Through a\ndecay-signal-propagation mechanism, token importance is dynamically updated by\npropagating information across the graph, enabling adaptive retention of the\nmost contextually significant tokens. GraphKV can be seamlessly utilized in\nexisting KV cache eviction methods such as SnapKV and PyramidKV in a\nplug-and-play manner. Codes will be released on Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Key-Value (KV) cache management is essential for processing long\ntext sequences in large language models (LLMs), where memory constraints often\nlimit performance. Conventional KV eviction strategies, such as top-k selection\nbased on attention scores, depend on static heuristics that fail to capture the\nevolving implicit dependencies among tokens during inference. To overcome this,\nwe propose GraphKV, a graph-based framework that redefines token selection for\nKV cache compression. In GraphKV, tokens are modeled as nodes with importance\nscores, and edges represent their similarity relationships. Through a\ndecay-signal-propagation mechanism, token importance is dynamically updated by\npropagating information across the graph, enabling adaptive retention of the\nmost contextually significant tokens. GraphKV can be seamlessly utilized in\nexisting KV cache eviction methods such as SnapKV and PyramidKV in a\nplug-and-play manner. Codes will be released on Github."
                },
                "authors": [
                    {
                        "name": "Xuelin Li"
                    },
                    {
                        "name": "Xiangqi Jin"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11435v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11435v2",
                "updated": "2025-08-29T20:39:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    20,
                    39,
                    21,
                    4,
                    241,
                    0
                ],
                "published": "2025-04-15T17:51:39Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    51,
                    39,
                    1,
                    105,
                    0
                ],
                "title": "Robust Containment Queries over Collections of Trimmed NURBS Surfaces\n  via Generalized Winding Numbers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Containment Queries over Collections of Trimmed NURBS Surfaces\n  via Generalized Winding Numbers"
                },
                "summary": "We propose a containment query that is robust to the watertightness of\nregions bound by trimmed NURBS surfaces, as this property is difficult to\nguarantee for in-the-wild CAD models. Containment is determined through the\ngeneralized winding number (GWN), a mathematical construction that is\nindifferent to the arrangement of surfaces in the shape. Applying contemporary\ntechniques for the 3D GWN to trimmed NURBS surfaces requires some form of\ngeometric discretization, introducing computational inefficiency to the\nalgorithm and even risking containment misclassifications near the surface. In\ncontrast, our proposed method uses a novel reformulation of the relevant\nsurface integral based on Stokes' theorem, which operates on the boundary and\ntrimming curves as provided through rapidly converging adaptive quadrature.\nBatches of queries are further accelerated by memoizing (i.e.\\ caching and\nreusing) quadrature node positions and tangents as they are evaluated. We\ndemonstrate that our GWN method is robust to complex trimming geometry in a CAD\nmodel, and is accurate up to arbitrary precision at arbitrary distances from\nthe surface. The derived containment query is therefore robust to model\nnon-watertightness while respecting all curved features of the input shape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a containment query that is robust to the watertightness of\nregions bound by trimmed NURBS surfaces, as this property is difficult to\nguarantee for in-the-wild CAD models. Containment is determined through the\ngeneralized winding number (GWN), a mathematical construction that is\nindifferent to the arrangement of surfaces in the shape. Applying contemporary\ntechniques for the 3D GWN to trimmed NURBS surfaces requires some form of\ngeometric discretization, introducing computational inefficiency to the\nalgorithm and even risking containment misclassifications near the surface. In\ncontrast, our proposed method uses a novel reformulation of the relevant\nsurface integral based on Stokes' theorem, which operates on the boundary and\ntrimming curves as provided through rapidly converging adaptive quadrature.\nBatches of queries are further accelerated by memoizing (i.e.\\ caching and\nreusing) quadrature node positions and tangents as they are evaluated. We\ndemonstrate that our GWN method is robust to complex trimming geometry in a CAD\nmodel, and is accurate up to arbitrary precision at arbitrary distances from\nthe surface. The derived containment query is therefore robust to model\nnon-watertightness while respecting all curved features of the input shape."
                },
                "authors": [
                    {
                        "name": "Jacob Spainhour"
                    },
                    {
                        "name": "Kenneth Weiss"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth Weiss"
                },
                "author": "Kenneth Weiss",
                "arxiv_comment": "18 Pages, 16 Figures, 1 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11435v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11435v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68U05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.3.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00202v1",
                "updated": "2025-08-29T19:23:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    23,
                    35,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T19:23:35Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    23,
                    35,
                    4,
                    241,
                    0
                ],
                "title": "From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer\n  Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer\n  Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive\n  Inference"
                },
                "summary": "Although the Transformer has become the cornerstone of modern AI, its\nautoregressive inference suffers from a linearly growing KV Cache and a\ncomputational complexity of O(N^2 d), severely hindering its ability to process\nultra-long sequences. To overcome this limitation, this paper introduces the\nTConstFormer architecture, building upon our previous work, TLinFormer.\nTConstFormer employs an innovative periodic state update mechanism to achieve a\ntruly constant-size O(1) KV Cache. The computational complexity of this\nmechanism is also O(1) in an amortized sense: it performs purely constant-time\ncomputations for $k-1$ consecutive steps (e.g., $k=256$) and executes a single\nlinear-time global information synchronization only on the $k$-th step.\nTheoretical calculations and experimental results demonstrate that TConstFormer\nexhibits an overwhelming advantage over baseline models in terms of speed,\nmemory efficiency, and overall performance on long-text inference tasks. This\nbreakthrough paves the way for efficient and robust streaming language model\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although the Transformer has become the cornerstone of modern AI, its\nautoregressive inference suffers from a linearly growing KV Cache and a\ncomputational complexity of O(N^2 d), severely hindering its ability to process\nultra-long sequences. To overcome this limitation, this paper introduces the\nTConstFormer architecture, building upon our previous work, TLinFormer.\nTConstFormer employs an innovative periodic state update mechanism to achieve a\ntruly constant-size O(1) KV Cache. The computational complexity of this\nmechanism is also O(1) in an amortized sense: it performs purely constant-time\ncomputations for $k-1$ consecutive steps (e.g., $k=256$) and executes a single\nlinear-time global information synchronization only on the $k$-th step.\nTheoretical calculations and experimental results demonstrate that TConstFormer\nexhibits an overwhelming advantage over baseline models in terms of speed,\nmemory efficiency, and overall performance on long-text inference tasks. This\nbreakthrough paves the way for efficient and robust streaming language model\napplications."
                },
                "authors": [
                    {
                        "name": "Zhongpan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongpan Tang"
                },
                "author": "Zhongpan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00195v1",
                "updated": "2025-08-29T19:12:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    12,
                    4,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T19:12:04Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    12,
                    4,
                    4,
                    241,
                    0
                ],
                "title": "Democratizing Agentic AI with Fast Test-Time Scaling on the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Democratizing Agentic AI with Fast Test-Time Scaling on the Edge"
                },
                "summary": "Deploying agentic AI on edge devices is crucial for privacy and\nresponsiveness, but memory constraints typically relegate these systems to\nsmaller Large Language Models (LLMs) with inferior reasoning capabilities.\nTest-Time Scaling (TTS) can bridge this reasoning gap by dedicating more\ncompute during inference, but existing methods incur prohibitive overhead on\nedge hardware. To overcome this, we introduce FlashTTS, a serving system that\nmakes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces\nthree synergistic optimizations: (i) Speculative Beam Extension to mitigate\nsystem stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model\nMemory Allocation to dynamically balance memory between generation and\nverification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache\nreuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on\na single consumer GPU (24 GB) to match the accuracy and latency of large cloud\nmodels. Our evaluation demonstrates that FlashTTS achieves an average 2.2x\nhigher goodput and reduces latency by 38%-68% compared to a vLLM baseline,\npaving the way for democratized, high-performance agentic AI on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying agentic AI on edge devices is crucial for privacy and\nresponsiveness, but memory constraints typically relegate these systems to\nsmaller Large Language Models (LLMs) with inferior reasoning capabilities.\nTest-Time Scaling (TTS) can bridge this reasoning gap by dedicating more\ncompute during inference, but existing methods incur prohibitive overhead on\nedge hardware. To overcome this, we introduce FlashTTS, a serving system that\nmakes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces\nthree synergistic optimizations: (i) Speculative Beam Extension to mitigate\nsystem stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model\nMemory Allocation to dynamically balance memory between generation and\nverification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache\nreuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on\na single consumer GPU (24 GB) to match the accuracy and latency of large cloud\nmodels. Our evaluation demonstrates that FlashTTS achieves an average 2.2x\nhigher goodput and reduces latency by 38%-68% compared to a vLLM baseline,\npaving the way for democratized, high-performance agentic AI on edge devices."
                },
                "authors": [
                    {
                        "name": "Hao Mark Chen"
                    },
                    {
                        "name": "Zhiwen Mo"
                    },
                    {
                        "name": "Guanxi Lu"
                    },
                    {
                        "name": "Shuang Liang"
                    },
                    {
                        "name": "Lingxiao Ma"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "author": "Hongxiang Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16217v2",
                "updated": "2025-08-29T18:45:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    18,
                    45,
                    22,
                    4,
                    241,
                    0
                ],
                "published": "2025-07-22T04:21:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Towards Compute-Optimal Many-Shot In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Compute-Optimal Many-Shot In-Context Learning"
                },
                "summary": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL."
                },
                "authors": [
                    {
                        "name": "Shahriar Golchin"
                    },
                    {
                        "name": "Yanfei Chen"
                    },
                    {
                        "name": "Rujun Han"
                    },
                    {
                        "name": "Manan Gandhi"
                    },
                    {
                        "name": "Tianli Yu"
                    },
                    {
                        "name": "Swaroop Mishra"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Chen-Yu Lee"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "arxiv_comment": "Final version; accepted at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05930v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05930v2",
                "updated": "2025-08-29T09:58:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    58,
                    17,
                    4,
                    241,
                    0
                ],
                "published": "2025-06-06T09:55:59Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    9,
                    55,
                    59,
                    4,
                    157,
                    0
                ],
                "title": "Neural Visibility Cache for Real-Time Light Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Visibility Cache for Real-Time Light Sampling"
                },
                "summary": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR)."
                },
                "authors": [
                    {
                        "name": "Jakub Bokšanský"
                    },
                    {
                        "name": "Daniel Meister"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Meister"
                },
                "author": "Daniel Meister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05930v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05930v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15683v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15683v2",
                "updated": "2025-08-29T07:40:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    7,
                    40,
                    34,
                    4,
                    241,
                    0
                ],
                "published": "2025-05-21T15:58:08Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "title": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting\n  Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting\n  Framework for Large Language Models"
                },
                "summary": "Private data holds promise for improving LLMs due to its high quality, but\nits scattered distribution across data silos and the high computational demands\nof LLMs limit their deployment in federated environments. To address this, the\ntransformer-based federated split models are proposed, which offload most model\nparameters to the server (or distributed clients) while retaining only a small\nportion on the client to ensure data privacy. Despite this design, they still\nface three challenges: 1) Peer-to-peer key encryption struggles to secure\ntransmitted vectors effectively; 2) The auto-regressive nature of LLMs means\nthat federated split learning can only train and infer sequentially, causing\nhigh communication overhead; 3) Fixed partition points lack adaptability to\ndownstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure,\nEfficient, and Adaptive Federated splitting framework based on LLaMA2. First,\nwe inject Gaussian noise into forward-pass hidden states to enable secure\nend-to-end vector transmission. Second, we employ attention-mask compression\nand KV cache collaboration to reduce communication costs, accelerating training\nand inference. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements. Experiments on\nnatural language understanding, summarization, and conversational QA tasks show\nthat FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and\nachieves up to 8x speedups in training and inference. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FedSEA-LLaMA in security and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private data holds promise for improving LLMs due to its high quality, but\nits scattered distribution across data silos and the high computational demands\nof LLMs limit their deployment in federated environments. To address this, the\ntransformer-based federated split models are proposed, which offload most model\nparameters to the server (or distributed clients) while retaining only a small\nportion on the client to ensure data privacy. Despite this design, they still\nface three challenges: 1) Peer-to-peer key encryption struggles to secure\ntransmitted vectors effectively; 2) The auto-regressive nature of LLMs means\nthat federated split learning can only train and infer sequentially, causing\nhigh communication overhead; 3) Fixed partition points lack adaptability to\ndownstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure,\nEfficient, and Adaptive Federated splitting framework based on LLaMA2. First,\nwe inject Gaussian noise into forward-pass hidden states to enable secure\nend-to-end vector transmission. Second, we employ attention-mask compression\nand KV cache collaboration to reduce communication costs, accelerating training\nand inference. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements. Experiments on\nnatural language understanding, summarization, and conversational QA tasks show\nthat FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and\nachieves up to 8x speedups in training and inference. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FedSEA-LLaMA in security and adaptability."
                },
                "authors": [
                    {
                        "name": "Zishuai Zhang"
                    },
                    {
                        "name": "Hainan zhang"
                    },
                    {
                        "name": "Weihua Li"
                    },
                    {
                        "name": "Qinnan zhang"
                    },
                    {
                        "name": "jin Dong"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15683v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15683v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04467v1",
                "updated": "2025-08-29T02:29:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    29,
                    52,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T02:29:52Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    29,
                    52,
                    4,
                    241,
                    0
                ],
                "title": "Enhancing LLM Efficiency: Targeted Pruning for Prefill-Decode\n  Disaggregation in Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Efficiency: Targeted Pruning for Prefill-Decode\n  Disaggregation in Inference"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the default settings, our method\nachieves a 20.56% inference speedup and a 4.95 times reduction in data\ntransmission bandwidth consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the default settings, our method\nachieves a 20.56% inference speedup and a 4.95 times reduction in data\ntransmission bandwidth consumption."
                },
                "authors": [
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Mengsi Lyu"
                    },
                    {
                        "name": "Yulong Ao"
                    },
                    {
                        "name": "Yonghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yonghua Lin"
                },
                "author": "Yonghua Lin",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20865v1",
                "updated": "2025-08-28T14:58:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    58,
                    47,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:58:47Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    58,
                    47,
                    3,
                    240,
                    0
                ],
                "title": "Deep Multiple Quantization Network on Long Behavior Sequence for\n  Click-Through Rate Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Multiple Quantization Network on Long Behavior Sequence for\n  Click-Through Rate Prediction"
                },
                "summary": "In Click-Through Rate (CTR) prediction, the long behavior sequence,\ncomprising the user's long period of historical interactions with items has a\nvital influence on assessing the user's interest in the candidate item.\nExisting approaches strike efficiency and effectiveness through a two-stage\nparadigm: first retrieving hundreds of candidate-related items and then\nextracting interest intensity vector through target attention. However, we\nargue that the discrepancy in target attention's relevance distribution between\nthe retrieved items and the full long behavior sequence inevitably leads to a\nperformance decline. To alleviate the discrepancy, we propose the Deep Multiple\nQuantization Network (DMQN) to process long behavior sequence end-to-end\nthrough compressing the long behavior sequence. Firstly, the entire spectrum of\nlong behavior sequence will be quantized into multiple codeword sequences based\non multiple independent codebooks. Hierarchical Sequential Transduction Unit is\nincorporated to facilitate the interaction of reduced codeword sequences. Then,\nattention between the candidate and multiple codeword sequences will output the\ninterest vector. To enable online serving, intermediate representations of the\ncodeword sequences are cached, significantly reducing latency. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMQN. The A/B test in our advertising system shows that DMQN\nimproves CTR by 3.5% and RPM by 2.0%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Click-Through Rate (CTR) prediction, the long behavior sequence,\ncomprising the user's long period of historical interactions with items has a\nvital influence on assessing the user's interest in the candidate item.\nExisting approaches strike efficiency and effectiveness through a two-stage\nparadigm: first retrieving hundreds of candidate-related items and then\nextracting interest intensity vector through target attention. However, we\nargue that the discrepancy in target attention's relevance distribution between\nthe retrieved items and the full long behavior sequence inevitably leads to a\nperformance decline. To alleviate the discrepancy, we propose the Deep Multiple\nQuantization Network (DMQN) to process long behavior sequence end-to-end\nthrough compressing the long behavior sequence. Firstly, the entire spectrum of\nlong behavior sequence will be quantized into multiple codeword sequences based\non multiple independent codebooks. Hierarchical Sequential Transduction Unit is\nincorporated to facilitate the interaction of reduced codeword sequences. Then,\nattention between the candidate and multiple codeword sequences will output the\ninterest vector. To enable online serving, intermediate representations of the\ncodeword sequences are cached, significantly reducing latency. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMQN. The A/B test in our advertising system shows that DMQN\nimproves CTR by 3.5% and RPM by 2.0%."
                },
                "authors": [
                    {
                        "name": "Zhuoxing Wei"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Qingchen Xie"
                    }
                ],
                "author_detail": {
                    "name": "Qingchen Xie"
                },
                "author": "Qingchen Xie",
                "arxiv_doi": "10.1145/3726302.3730177",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730177",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.20865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, 1 figures, SIGIR 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18250v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18250v2",
                "updated": "2025-08-28T08:49:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    49,
                    24,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-25T17:41:13Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    13,
                    0,
                    237,
                    0
                ],
                "title": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study"
                },
                "summary": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM."
                },
                "authors": [
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Fernando García-Redondo"
                    },
                    {
                        "name": "Arvind Sharma"
                    },
                    {
                        "name": "Van Dai Nguyen"
                    },
                    {
                        "name": "Andrea Fantini"
                    },
                    {
                        "name": "Philippe Matagne"
                    },
                    {
                        "name": "Siddharth Rao"
                    },
                    {
                        "name": "Subhali Subhechha"
                    },
                    {
                        "name": "Lynn Verschueren"
                    },
                    {
                        "name": "Mohammed Aftab Baig"
                    },
                    {
                        "name": "Marie Garcia Bardon"
                    },
                    {
                        "name": "Geert Hellings"
                    }
                ],
                "author_detail": {
                    "name": "Geert Hellings"
                },
                "author": "Geert Hellings",
                "arxiv_comment": "Manuscript submitted to IEEE Trans. Elec. Dev. Work enabled in part\n  by NanoIC pilot line; acquisition and operation jointly funded by Chips Joint\n  Undertaking, through EU's Digital Europe (101183266) and Horizon Europe\n  programs (101183277), as well as by the participating states\n  (Belgium-Flanders, France, Germany, Finland, Ireland, Romania)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18250v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18250v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20524v1",
                "updated": "2025-08-28T08:05:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    5,
                    42,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T08:05:42Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    5,
                    42,
                    3,
                    240,
                    0
                ],
                "title": "Matrixed-Spectrum Decomposition Accelerated Linear Boltzmann Transport\n  Equation Solver for Fast Scatter Correction in Multi-Spectral CT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrixed-Spectrum Decomposition Accelerated Linear Boltzmann Transport\n  Equation Solver for Fast Scatter Correction in Multi-Spectral CT"
                },
                "summary": "X-ray scatter has been a serious concern in computed tomography (CT), leading\nto image artifacts and distortion of CT values. The linear Boltzmann transport\nequation (LBTE) is recognized as a fast and accurate approach for scatter\nestimation. However, for multi-spectral CT, it is cumbersome to compute\nmultiple scattering components for different spectra separately when applying\nLBTE-based scatter correction. In this work, we propose a Matrixed-Spectrum\nDecomposition accelerated LBTE solver (MSD-LBTE) that can be used to compute\nX-ray scatter distributions from CT acquisitions at two or more different\nspectra simultaneously, in a unified framework with no sacrifice in accuracy\nand nearly no increase in computation in theory. First, a matrixed-spectrum\nsolver of LBTE is obtained by introducing an additional label dimension to\nexpand the phase space. Then, we propose a ``spectrum basis'' for LBTE and a\nprinciple of selection of basis using the QR decomposition, along with the\nabove solver to construct the MSD-LBTE. Based on MSD-LBTE, a unified scatter\ncorrection method can be established for multi-spectral CT. We validate the\neffectiveness and accuracy of our method by comparing it with the Monte Carlo\nmethod, including the computational time. We also evaluate the scatter\ncorrection performance using two different phantoms for fast-kV switching based\ndual-energy CT, and using an elliptical phantom in a numerical simulation for\nkV-modulation enabled CT scans, validating that our proposed method can\nsignificantly reduce the computational cost at multiple spectra and effectively\nreduce scatter artifact in reconstructed CT images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-ray scatter has been a serious concern in computed tomography (CT), leading\nto image artifacts and distortion of CT values. The linear Boltzmann transport\nequation (LBTE) is recognized as a fast and accurate approach for scatter\nestimation. However, for multi-spectral CT, it is cumbersome to compute\nmultiple scattering components for different spectra separately when applying\nLBTE-based scatter correction. In this work, we propose a Matrixed-Spectrum\nDecomposition accelerated LBTE solver (MSD-LBTE) that can be used to compute\nX-ray scatter distributions from CT acquisitions at two or more different\nspectra simultaneously, in a unified framework with no sacrifice in accuracy\nand nearly no increase in computation in theory. First, a matrixed-spectrum\nsolver of LBTE is obtained by introducing an additional label dimension to\nexpand the phase space. Then, we propose a ``spectrum basis'' for LBTE and a\nprinciple of selection of basis using the QR decomposition, along with the\nabove solver to construct the MSD-LBTE. Based on MSD-LBTE, a unified scatter\ncorrection method can be established for multi-spectral CT. We validate the\neffectiveness and accuracy of our method by comparing it with the Monte Carlo\nmethod, including the computational time. We also evaluate the scatter\ncorrection performance using two different phantoms for fast-kV switching based\ndual-energy CT, and using an elliptical phantom in a numerical simulation for\nkV-modulation enabled CT scans, validating that our proposed method can\nsignificantly reduce the computational cost at multiple spectra and effectively\nreduce scatter artifact in reconstructed CT images."
                },
                "authors": [
                    {
                        "name": "Guoxi Zhu"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Zhiqiang Chen"
                    },
                    {
                        "name": "Hewei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Hewei Gao"
                },
                "author": "Hewei Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20433v1",
                "updated": "2025-08-28T05:22:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    5,
                    22,
                    25,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T05:22:25Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    5,
                    22,
                    25,
                    3,
                    240,
                    0
                ],
                "title": "MegaCacheX: Towards Cost-Effective Hierarchical Collaborative Content\n  Caching in Emerging Mega-Constellations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaCacheX: Towards Cost-Effective Hierarchical Collaborative Content\n  Caching in Emerging Mega-Constellations"
                },
                "summary": "Significant latency in global content delivery primarily arises from\ninsufficient terrestrial infrastructure. Deploying space-based content delivery\nnetworks within emerging mega-constellations provides an effective means to\nbridge the digital divide. However, space-based caching faces constraints from\nphysical-layer dynamics, including dynamic topologies, time-varying\ninter-satellite link conditions, and limited onboard energy. In addition,\nexisting mechanisms often lack fine-grained content categorization and global\noptimization. This paper proposes MegaCacheX, a cost-effective hierarchical\nframework for collaborative content distribution that achieves\n\"Earth-independence\" by providing cloud services directly from space.\nSpecifically, data centers in Sun-synchronous orbit act as primary content\nsources, while caching nodes in mega-constellations and ground stations\ncollaboratively form a distributed edge layer. MegaCacheX optimizes caching\nstrategies by integrating content popularity, regional user distribution, and\nsatellite trajectory predictions. Multi-tier caching nodes serve as service\nanchors, enabling seamless content delivery with low latency. A prototype\nimplemented on a microservices-based, containerized testbed demonstrates that\nMegaCacheX reduces global content access latency by about 36% compared to\nbaseline approaches, while maintaining cost efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant latency in global content delivery primarily arises from\ninsufficient terrestrial infrastructure. Deploying space-based content delivery\nnetworks within emerging mega-constellations provides an effective means to\nbridge the digital divide. However, space-based caching faces constraints from\nphysical-layer dynamics, including dynamic topologies, time-varying\ninter-satellite link conditions, and limited onboard energy. In addition,\nexisting mechanisms often lack fine-grained content categorization and global\noptimization. This paper proposes MegaCacheX, a cost-effective hierarchical\nframework for collaborative content distribution that achieves\n\"Earth-independence\" by providing cloud services directly from space.\nSpecifically, data centers in Sun-synchronous orbit act as primary content\nsources, while caching nodes in mega-constellations and ground stations\ncollaboratively form a distributed edge layer. MegaCacheX optimizes caching\nstrategies by integrating content popularity, regional user distribution, and\nsatellite trajectory predictions. Multi-tier caching nodes serve as service\nanchors, enabling seamless content delivery with low latency. A prototype\nimplemented on a microservices-based, containerized testbed demonstrates that\nMegaCacheX reduces global content access latency by about 36% compared to\nbaseline approaches, while maintaining cost efficiency."
                },
                "authors": [
                    {
                        "name": "Haoyang Shi"
                    },
                    {
                        "name": "Xing Zhang"
                    },
                    {
                        "name": "Sitong Li"
                    },
                    {
                        "name": "Minghang Li"
                    },
                    {
                        "name": "Xinming Lu"
                    },
                    {
                        "name": "Shaoxiang Xu"
                    },
                    {
                        "name": "Guoquan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guoquan Wang"
                },
                "author": "Guoquan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20424v1",
                "updated": "2025-08-28T04:46:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    46,
                    44,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T04:46:44Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    46,
                    44,
                    3,
                    240,
                    0
                ],
                "title": "Breaking Diffusion with Cache: Exploiting Approximate Caches in\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Diffusion with Cache: Exploiting Approximate Caches in\n  Diffusion Models"
                },
                "summary": "Diffusion models are a powerful class of generative models that produce\ncontent, such as images, from user prompts, but they are computationally\nintensive. To mitigate this cost, recent academic and industry work has adopted\napproximate caching, which reuses intermediate states from similar prompts in a\ncache. While efficient, this optimization introduces new security risks by\nbreaking isolation among users. This work aims to comprehensively assess new\nsecurity vulnerabilities arising from approximate caching. First, we\ndemonstrate a remote covert channel established with the cache, where a sender\ninjects prompts with special keywords into the cache and a receiver can recover\nthat even after days, to exchange information. Second, we introduce a prompt\nstealing attack using the cache, where an attacker can recover existing cached\nprompts based on cache hit prompts. Finally, we introduce a poisoning attack\nthat embeds the attacker's logos into the previously stolen prompt, to render\nthem in future user prompts that hit the cache. These attacks are all performed\nremotely through the serving system, which indicates severe security\nvulnerabilities in approximate caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models are a powerful class of generative models that produce\ncontent, such as images, from user prompts, but they are computationally\nintensive. To mitigate this cost, recent academic and industry work has adopted\napproximate caching, which reuses intermediate states from similar prompts in a\ncache. While efficient, this optimization introduces new security risks by\nbreaking isolation among users. This work aims to comprehensively assess new\nsecurity vulnerabilities arising from approximate caching. First, we\ndemonstrate a remote covert channel established with the cache, where a sender\ninjects prompts with special keywords into the cache and a receiver can recover\nthat even after days, to exchange information. Second, we introduce a prompt\nstealing attack using the cache, where an attacker can recover existing cached\nprompts based on cache hit prompts. Finally, we introduce a poisoning attack\nthat embeds the attacker's logos into the previously stolen prompt, to render\nthem in future user prompts that hit the cache. These attacks are all performed\nremotely through the serving system, which indicates severe security\nvulnerabilities in approximate caching."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Shuncheng Jie"
                    },
                    {
                        "name": "Sihang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sihang Liu"
                },
                "author": "Sihang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20407v1",
                "updated": "2025-08-28T04:10:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    10,
                    19,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T04:10:19Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    10,
                    19,
                    3,
                    240,
                    0
                ],
                "title": "Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full\n  Context-Aware Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full\n  Context-Aware Linear Attention"
                },
                "summary": "The Transformer architecture has become a cornerstone of modern artificial\nintelligence, but its core self-attention mechanism suffers from a complexity\nbottleneck that scales quadratically with sequence length, severely limiting\nits application in long-sequence tasks. To address this challenge, existing\nlinear attention methods typically sacrifice model performance by relying on\ndata-agnostic kernel approximations or restrictive context selection. This\npaper returns to the first principles of connectionism, starting from the\ntopological structure of information flow, to introduce a novel linear\nattention architecture-\\textbf{TLinFormer}. By reconfiguring neuron connection\npatterns, TLinFormer achieves strict linear complexity while computing exact\nattention scores and ensuring information flow remains aware of the full\nhistorical context. This design aims to bridge the performance gap prevalent\nbetween existing efficient attention methods and standard attention. Through a\nseries of experiments, we systematically evaluate the performance of TLinFormer\nagainst a standard Transformer baseline on long-sequence inference tasks. The\nresults demonstrate that TLinFormer exhibits overwhelming advantages in key\nmetrics such as \\textbf{inference latency}, \\textbf{KV cache efficiency},\n\\textbf{memory footprint}, and \\textbf{overall speedup}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Transformer architecture has become a cornerstone of modern artificial\nintelligence, but its core self-attention mechanism suffers from a complexity\nbottleneck that scales quadratically with sequence length, severely limiting\nits application in long-sequence tasks. To address this challenge, existing\nlinear attention methods typically sacrifice model performance by relying on\ndata-agnostic kernel approximations or restrictive context selection. This\npaper returns to the first principles of connectionism, starting from the\ntopological structure of information flow, to introduce a novel linear\nattention architecture-\\textbf{TLinFormer}. By reconfiguring neuron connection\npatterns, TLinFormer achieves strict linear complexity while computing exact\nattention scores and ensuring information flow remains aware of the full\nhistorical context. This design aims to bridge the performance gap prevalent\nbetween existing efficient attention methods and standard attention. Through a\nseries of experiments, we systematically evaluate the performance of TLinFormer\nagainst a standard Transformer baseline on long-sequence inference tasks. The\nresults demonstrate that TLinFormer exhibits overwhelming advantages in key\nmetrics such as \\textbf{inference latency}, \\textbf{KV cache efficiency},\n\\textbf{memory footprint}, and \\textbf{overall speedup}."
                },
                "authors": [
                    {
                        "name": "Zhongpan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongpan Tang"
                },
                "author": "Zhongpan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v5",
                "updated": "2025-08-28T03:57:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    3,
                    57,
                    52,
                    3,
                    240,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from (1) the distribution variance in the LLM activations and (2) the\nsensitivity difference among various kinds of layers. To address these issues,\nwe propose a training-free approach called Activation-aware Singular Value\nDecomposition (ASVD). Specifically, ASVD manages activation outliers by\ntransforming the weight matrix based on the activation distribution. This\ntransformation allows the outliers in the activation matrix to be absorbed into\nthe transformed weight matrix, thereby enhancing decomposition accuracy.\nAdditionally, we propose an efficient iterative calibration process to optimize\nlayer-specific decomposition by addressing the varying sensitivity of different\nLLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the\nsuccess of the low-rank decomposition of projection matrices in the\nself-attention module, we further introduce ASVD to compress the KV cache. By\nreducing the channel dimension of KV activations, memory requirements for KV\ncache can be largely reduced. ASVD can further achieve 50% KV cache reductions\nwithout performance drop in a training-free manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from (1) the distribution variance in the LLM activations and (2) the\nsensitivity difference among various kinds of layers. To address these issues,\nwe propose a training-free approach called Activation-aware Singular Value\nDecomposition (ASVD). Specifically, ASVD manages activation outliers by\ntransforming the weight matrix based on the activation distribution. This\ntransformation allows the outliers in the activation matrix to be absorbed into\nthe transformed weight matrix, thereby enhancing decomposition accuracy.\nAdditionally, we propose an efficient iterative calibration process to optimize\nlayer-specific decomposition by addressing the varying sensitivity of different\nLLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the\nsuccess of the low-rank decomposition of projection matrices in the\nself-attention module, we further introduce ASVD to compress the KV cache. By\nreducing the channel dimension of KV activations, memory requirements for KV\ncache can be largely reduced. ASVD can further achieve 50% KV cache reductions\nwithout performance drop in a training-free manner."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09888v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09888v2",
                "updated": "2025-08-28T01:40:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    1,
                    40,
                    30,
                    3,
                    240,
                    0
                ],
                "published": "2025-02-14T03:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "title": "Climber: Toward Efficient Scaling Laws for Large Recommendation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Climber: Toward Efficient Scaling Laws for Large Recommendation Models"
                },
                "summary": "Transformer-based generative models have achieved remarkable success across\ndomains with various scaling law manifestations. However, our extensive\nexperiments reveal persistent challenges when applying Transformer to\nrecommendation systems: (1) Transformer scaling is not ideal with increased\ncomputational resources, due to structural incompatibilities with\nrecommendation-specific features such as multi-source data heterogeneity; (2)\ncritical online inference latency constraints (tens of milliseconds) that\nintensify with longer user behavior sequences and growing computational\ndemands. We propose Climber, an efficient recommendation framework comprising\ntwo synergistic components: the model architecture for efficient scaling and\nthe co-designed acceleration techniques. Our proposed model adopts two core\ninnovations: (1) multi-scale sequence extraction that achieves a time\ncomplexity reduction by a constant factor, enabling more efficient scaling with\nsequence length; (2) dynamic temperature modulation adapting attention\ndistributions to the multi-scenario and multi-behavior patterns. Complemented\nby acceleration techniques, Climber achieves a 5.15$\\times$ throughput gain\nwithout performance degradation by adopting a \"single user, multiple item\"\nbatched processing and memory-efficient Key-Value caching. Comprehensive\noffline experiments on multiple datasets validate that Climber exhibits a more\nideal scaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19\\% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based generative models have achieved remarkable success across\ndomains with various scaling law manifestations. However, our extensive\nexperiments reveal persistent challenges when applying Transformer to\nrecommendation systems: (1) Transformer scaling is not ideal with increased\ncomputational resources, due to structural incompatibilities with\nrecommendation-specific features such as multi-source data heterogeneity; (2)\ncritical online inference latency constraints (tens of milliseconds) that\nintensify with longer user behavior sequences and growing computational\ndemands. We propose Climber, an efficient recommendation framework comprising\ntwo synergistic components: the model architecture for efficient scaling and\nthe co-designed acceleration techniques. Our proposed model adopts two core\ninnovations: (1) multi-scale sequence extraction that achieves a time\ncomplexity reduction by a constant factor, enabling more efficient scaling with\nsequence length; (2) dynamic temperature modulation adapting attention\ndistributions to the multi-scenario and multi-behavior patterns. Complemented\nby acceleration techniques, Climber achieves a 5.15$\\times$ throughput gain\nwithout performance degradation by adopting a \"single user, multiple item\"\nbatched processing and memory-efficient Key-Value caching. Comprehensive\noffline experiments on multiple datasets validate that Climber exhibits a more\nideal scaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19\\% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily."
                },
                "authors": [
                    {
                        "name": "Songpei Xu"
                    },
                    {
                        "name": "Shijia Wang"
                    },
                    {
                        "name": "Da Guo"
                    },
                    {
                        "name": "Xianwen Guo"
                    },
                    {
                        "name": "Qiang Xiao"
                    },
                    {
                        "name": "Bin Huang"
                    },
                    {
                        "name": "Guanlin Wu"
                    },
                    {
                        "name": "Chuanjiang Luo"
                    }
                ],
                "author_detail": {
                    "name": "Chuanjiang Luo"
                },
                "author": "Chuanjiang Luo",
                "arxiv_doi": "10.1145/3746252.3761561",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761561",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09888v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09888v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00105v1",
                "updated": "2025-08-28T00:46:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    0,
                    46,
                    51,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T00:46:51Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    0,
                    46,
                    51,
                    3,
                    240,
                    0
                ],
                "title": "AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and\n  High-Quality Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and\n  High-Quality Language Model Serving"
                },
                "summary": "Large language model (LLM) applications often reuse previously processed\ncontext, such as chat history and documents, which introduces significant\nredundant computation. Existing LLM serving systems address such redundant\ncomputation by storing the KV caches of processed context and loading the\ncorresponding KV cache when a new request reuses the context. Further, as these\nLLM applications scale, the total size of KV caches becomes excessively large\nand requires both DRAM and SSD for full storage.\n  However, prior work that stores KV caches in DRAM and SSD suffers from high\nloading delays, as most KV cache hits come from SSD, which is slow to load. To\nincrease the KV cache hit rate on DRAM, we identify lossy KV cache compression\nas a promising approach. We design a lossy compression system that decides the\ncompression algorithm, compression rate and device placement for each KV cache\nentry to maximise DRAM hits and minimise loading delay without significantly\ndegrading generation quality. Compared to various static compression baselines\nacross three tasks, our system AdaptCache achieves 1.43--2.4 x delay savings at\nthe same quality and 6--55% quality improvements at the same delay.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) applications often reuse previously processed\ncontext, such as chat history and documents, which introduces significant\nredundant computation. Existing LLM serving systems address such redundant\ncomputation by storing the KV caches of processed context and loading the\ncorresponding KV cache when a new request reuses the context. Further, as these\nLLM applications scale, the total size of KV caches becomes excessively large\nand requires both DRAM and SSD for full storage.\n  However, prior work that stores KV caches in DRAM and SSD suffers from high\nloading delays, as most KV cache hits come from SSD, which is slow to load. To\nincrease the KV cache hit rate on DRAM, we identify lossy KV cache compression\nas a promising approach. We design a lossy compression system that decides the\ncompression algorithm, compression rate and device placement for each KV cache\nentry to maximise DRAM hits and minimise loading delay without significantly\ndegrading generation quality. Compared to various static compression baselines\nacross three tasks, our system AdaptCache achieves 1.43--2.4 x delay savings at\nthe same quality and 6--55% quality improvements at the same delay."
                },
                "authors": [
                    {
                        "name": "Shaoting Feng"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Samuel Shen"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Ganesh Ananthanarayanan"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20272v1",
                "updated": "2025-08-27T21:05:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    21,
                    5,
                    5,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T21:05:05Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    21,
                    5,
                    5,
                    2,
                    239,
                    0
                ],
                "title": "DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource\n  Allocation and Markov Decision Process in Named Data Networking (NDN)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource\n  Allocation and Markov Decision Process in Named Data Networking (NDN)"
                },
                "summary": "Named Data Networking (NDN) represents a transformative shift in network\narchitecture, prioritizing content names over host addresses to enhance data\ndissemination. Efficient queue and resource management are critical to NDN\nperformance, especially under dynamic and high-traffic conditions. This paper\nintroduces DRR-MDPF, a novel hybrid strategy that integrates the Markov\nDecision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)\nalgorithm. MDPF enables routers to intelligently predict optimal forwarding\ndecisions based on key metrics such as bandwidth, delay, and the number of\nunsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation\namong competing data flows. The proposed method models each router as a\nlearning agent capable of adjusting its strategies through continuous feedback\nand probabilistic updates. Simulation results using ndnSIM demonstrate that\nDRR-MDPF significantly outperforms state-of-the-art strategies including SAF,\nRFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest\nSatisfaction Rate (ISR), packet drop rate, content retrieval time, and load\nbalancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and\nheavy traffic, offering enhanced adaptability and lower computational\ncomplexity due to its single-path routing design. Furthermore, its multi-metric\ndecision-making capability enables more accurate interface selection, leading\nto optimized network performance. Overall, DRR-MDPF serves as an intelligent,\nadaptive, and scalable queue management solution for NDN, effectively\naddressing core challenges such as resource allocation, congestion control, and\nroute optimization in dynamic networking environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Data Networking (NDN) represents a transformative shift in network\narchitecture, prioritizing content names over host addresses to enhance data\ndissemination. Efficient queue and resource management are critical to NDN\nperformance, especially under dynamic and high-traffic conditions. This paper\nintroduces DRR-MDPF, a novel hybrid strategy that integrates the Markov\nDecision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)\nalgorithm. MDPF enables routers to intelligently predict optimal forwarding\ndecisions based on key metrics such as bandwidth, delay, and the number of\nunsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation\namong competing data flows. The proposed method models each router as a\nlearning agent capable of adjusting its strategies through continuous feedback\nand probabilistic updates. Simulation results using ndnSIM demonstrate that\nDRR-MDPF significantly outperforms state-of-the-art strategies including SAF,\nRFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest\nSatisfaction Rate (ISR), packet drop rate, content retrieval time, and load\nbalancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and\nheavy traffic, offering enhanced adaptability and lower computational\ncomplexity due to its single-path routing design. Furthermore, its multi-metric\ndecision-making capability enables more accurate interface selection, leading\nto optimized network performance. Overall, DRR-MDPF serves as an intelligent,\nadaptive, and scalable queue management solution for NDN, effectively\naddressing core challenges such as resource allocation, congestion control, and\nroute optimization in dynamic networking environments."
                },
                "authors": [
                    {
                        "name": "Fatemeh Roshanzadeh"
                    },
                    {
                        "name": "Hamid Barati"
                    },
                    {
                        "name": "Ali Barati"
                    }
                ],
                "author_detail": {
                    "name": "Ali Barati"
                },
                "author": "Ali Barati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20253v1",
                "updated": "2025-08-27T20:18:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    20,
                    18,
                    37,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T20:18:37Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    20,
                    18,
                    37,
                    2,
                    239,
                    0
                ],
                "title": "SpeedMalloc: Improving Multi-threaded Applications via a Lightweight\n  Core for Memory Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeedMalloc: Improving Multi-threaded Applications via a Lightweight\n  Core for Memory Allocation"
                },
                "summary": "Memory allocation, though constituting only a small portion of the executed\ncode, can have a \"butterfly effect\" on overall program performance, leading to\nsignificant and far-reaching impacts. Despite accounting for just approximately\n5% of total instructions, memory allocation can result in up to a 2.7x\nperformance variation depending on the allocator used. This effect arises from\nthe complexity of memory allocation in modern multi-threaded multi-core\nsystems, where allocator metadata becomes intertwined with user data, leading\nto cache pollution or increased cross-thread synchronization overhead.\nOffloading memory allocators to accelerators, e.g., Mallacc and Memento, is a\npotential direction to improve the allocator performance and mitigate cache\npollution. However, these accelerators currently have limited support for\nmulti-threaded applications, and synchronization between cores and accelerators\nremains a significant challenge.\n  We present SpeedMalloc, using a lightweight support-core to process memory\nallocation tasks in multi-threaded applications. The support-core is a\nlightweight programmable processor with efficient cross-core data\nsynchronization and houses all allocator metadata in its own caches. This\ndesign minimizes cache conflicts with user data and eliminates the need for\ncross-core metadata synchronization. In addition, using a general-purpose core\ninstead of domain-specific accelerators makes SpeedMalloc capable of adopting\nnew allocator designs. We compare SpeedMalloc with state-of-the-art software\nand hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and\nMemento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on\nmultithreaded workloads over these five allocators, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory allocation, though constituting only a small portion of the executed\ncode, can have a \"butterfly effect\" on overall program performance, leading to\nsignificant and far-reaching impacts. Despite accounting for just approximately\n5% of total instructions, memory allocation can result in up to a 2.7x\nperformance variation depending on the allocator used. This effect arises from\nthe complexity of memory allocation in modern multi-threaded multi-core\nsystems, where allocator metadata becomes intertwined with user data, leading\nto cache pollution or increased cross-thread synchronization overhead.\nOffloading memory allocators to accelerators, e.g., Mallacc and Memento, is a\npotential direction to improve the allocator performance and mitigate cache\npollution. However, these accelerators currently have limited support for\nmulti-threaded applications, and synchronization between cores and accelerators\nremains a significant challenge.\n  We present SpeedMalloc, using a lightweight support-core to process memory\nallocation tasks in multi-threaded applications. The support-core is a\nlightweight programmable processor with efficient cross-core data\nsynchronization and houses all allocator metadata in its own caches. This\ndesign minimizes cache conflicts with user data and eliminates the need for\ncross-core metadata synchronization. In addition, using a general-purpose core\ninstead of domain-specific accelerators makes SpeedMalloc capable of adopting\nnew allocator designs. We compare SpeedMalloc with state-of-the-art software\nand hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and\nMemento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on\nmultithreaded workloads over these five allocators, respectively."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Qinzhe Wu"
                    },
                    {
                        "name": "Krishna Kavi"
                    },
                    {
                        "name": "Gayatri Mehta"
                    },
                    {
                        "name": "Jonathan C. Beard"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    },
                    {
                        "name": "Lizy K. John"
                    }
                ],
                "author_detail": {
                    "name": "Lizy K. John"
                },
                "author": "Lizy K. John",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00100v1",
                "updated": "2025-08-27T17:45:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    17,
                    45,
                    16,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T17:45:16Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    17,
                    45,
                    16,
                    2,
                    239,
                    0
                ],
                "title": "MODE: Mixture of Document Experts for RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MODE: Mixture of Document Experts for RAG"
                },
                "summary": "Retrieval-Augmented Generation (RAG) often relies on large vector databases\nand cross-encoders tuned for large-scale corpora, which can be excessive for\nsmall, domain-specific collections. We present MODE (Mixture of Document\nExperts), a lightweight alternative that replaces fine-grained nearest-neighbor\nsearch with cluster-and-route retrieval. Documents are embedded, grouped into\nsemantically coherent clusters, and represented by cached centroids. At query\ntime, we route to the top centroid(s) and retrieve context only within those\nclusters, eliminating external vector-database infrastructure and reranking\nwhile keeping latency low. On HotpotQA and SQuAD corpora with 100-500 chunks,\nMODE matches or exceeds a dense-retrieval baseline in answer quality while\nreducing end-to-end retrieval time. Ablations show that cluster granularity and\nmulti-cluster routing control the recall/precision trade-off, and that tighter\nclusters improve downstream accuracy. MODE offers a practical recipe for small\nand medium corpora where simplicity, speed, and topical focus matter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) often relies on large vector databases\nand cross-encoders tuned for large-scale corpora, which can be excessive for\nsmall, domain-specific collections. We present MODE (Mixture of Document\nExperts), a lightweight alternative that replaces fine-grained nearest-neighbor\nsearch with cluster-and-route retrieval. Documents are embedded, grouped into\nsemantically coherent clusters, and represented by cached centroids. At query\ntime, we route to the top centroid(s) and retrieve context only within those\nclusters, eliminating external vector-database infrastructure and reranking\nwhile keeping latency low. On HotpotQA and SQuAD corpora with 100-500 chunks,\nMODE matches or exceeds a dense-retrieval baseline in answer quality while\nreducing end-to-end retrieval time. Ablations show that cluster granularity and\nmulti-cluster routing control the recall/precision trade-off, and that tighter\nclusters improve downstream accuracy. MODE offers a practical recipe for small\nand medium corpora where simplicity, speed, and topical focus matter."
                },
                "authors": [
                    {
                        "name": "Rahul Anand"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Anand"
                },
                "author": "Rahul Anand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13575v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13575v3",
                "updated": "2025-08-27T16:34:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    16,
                    34,
                    47,
                    2,
                    239,
                    0
                ],
                "published": "2025-07-17T23:37:19Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "title": "Apple Intelligence Foundation Language Models: Tech Report 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apple Intelligence Foundation Language Models: Tech Report 2025"
                },
                "summary": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute."
                },
                "authors": [
                    {
                        "name": "Ethan Li"
                    },
                    {
                        "name": "Anders Boesen Lindbo Larsen"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Xiyou Zhou"
                    },
                    {
                        "name": "Jun Qin"
                    },
                    {
                        "name": "Dian Ang Yap"
                    },
                    {
                        "name": "Narendran Raghavan"
                    },
                    {
                        "name": "Xuankai Chang"
                    },
                    {
                        "name": "Margit Bowler"
                    },
                    {
                        "name": "Eray Yildiz"
                    },
                    {
                        "name": "John Peebles"
                    },
                    {
                        "name": "Hannah Gillis Coleman"
                    },
                    {
                        "name": "Matteo Ronchi"
                    },
                    {
                        "name": "Peter Gray"
                    },
                    {
                        "name": "Keen You"
                    },
                    {
                        "name": "Anthony Spalvieri-Kruse"
                    },
                    {
                        "name": "Ruoming Pang"
                    },
                    {
                        "name": "Reed Li"
                    },
                    {
                        "name": "Yuli Yang"
                    },
                    {
                        "name": "Emad Soroush"
                    },
                    {
                        "name": "Zhiyun Lu"
                    },
                    {
                        "name": "Crystal Xiao"
                    },
                    {
                        "name": "Rong Situ"
                    },
                    {
                        "name": "Jordan Huffaker"
                    },
                    {
                        "name": "David Griffiths"
                    },
                    {
                        "name": "Zaid Ahmed"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Daniel Parilla"
                    },
                    {
                        "name": "Asaf Liberman"
                    },
                    {
                        "name": "Jennifer Mallalieu"
                    },
                    {
                        "name": "Parsa Mazaheri"
                    },
                    {
                        "name": "Qibin Chen"
                    },
                    {
                        "name": "Manjot Bilkhu"
                    },
                    {
                        "name": "Aonan Zhang"
                    },
                    {
                        "name": "Eric Wang"
                    },
                    {
                        "name": "Dave Nelson"
                    },
                    {
                        "name": "Michael FitzMaurice"
                    },
                    {
                        "name": "Thomas Voice"
                    },
                    {
                        "name": "Jeremy Liu"
                    },
                    {
                        "name": "Josh Shaffer"
                    },
                    {
                        "name": "Shiwen Zhao"
                    },
                    {
                        "name": "Prasanth Yadla"
                    },
                    {
                        "name": "Farzin Rasteh"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Arsalan Farooq"
                    },
                    {
                        "name": "Jeremy Snow"
                    },
                    {
                        "name": "Stephen Murphy"
                    },
                    {
                        "name": "Tao Lei"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "George Horrell"
                    },
                    {
                        "name": "Sam Dodge"
                    },
                    {
                        "name": "Lindsay Hislop"
                    },
                    {
                        "name": "Sumeet Singh"
                    },
                    {
                        "name": "Alex Dombrowski"
                    },
                    {
                        "name": "Aiswarya Raghavan"
                    },
                    {
                        "name": "Sasha Sirovica"
                    },
                    {
                        "name": "Mandana Saebi"
                    },
                    {
                        "name": "Faye Lao"
                    },
                    {
                        "name": "Max Lam"
                    },
                    {
                        "name": "TJ Lu"
                    },
                    {
                        "name": "Zhaoyang Xu"
                    },
                    {
                        "name": "Karanjeet Singh"
                    },
                    {
                        "name": "Marc Kirchner"
                    },
                    {
                        "name": "David Mizrahi"
                    },
                    {
                        "name": "Rajat Arora"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Henry Mason"
                    },
                    {
                        "name": "Lawrence Zhou"
                    },
                    {
                        "name": "Yi Hua"
                    },
                    {
                        "name": "Ankur Jain"
                    },
                    {
                        "name": "Felix Bai"
                    },
                    {
                        "name": "Joseph Astrauskas"
                    },
                    {
                        "name": "Floris Weers"
                    },
                    {
                        "name": "Josh Gardner"
                    },
                    {
                        "name": "Mira Chiang"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Tony Sun"
                    },
                    {
                        "name": "Quentin Keunebroek"
                    },
                    {
                        "name": "Matthew Hopkins"
                    },
                    {
                        "name": "Bugu Wu"
                    },
                    {
                        "name": "Tao Jia"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Nanzhu Wang"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Ruixuan Hou"
                    },
                    {
                        "name": "Rene Rauch"
                    },
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Jonathan Janke"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Cha Chen"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Feng Nan"
                    },
                    {
                        "name": "Josh Elman"
                    },
                    {
                        "name": "Dong Yin"
                    },
                    {
                        "name": "Yusuf Goren"
                    },
                    {
                        "name": "Jeff Lai"
                    },
                    {
                        "name": "Yiran Fei"
                    },
                    {
                        "name": "Syd Evans"
                    },
                    {
                        "name": "Muyang Yu"
                    },
                    {
                        "name": "Guoli Yin"
                    },
                    {
                        "name": "Yi Qin"
                    },
                    {
                        "name": "Erin Feldman"
                    },
                    {
                        "name": "Isha Garg"
                    },
                    {
                        "name": "Aparna Rajamani"
                    },
                    {
                        "name": "Karla Vega"
                    },
                    {
                        "name": "Walker Cheng"
                    },
                    {
                        "name": "TJ Collins"
                    },
                    {
                        "name": "Hans Han"
                    },
                    {
                        "name": "Raul Rea Menacho"
                    },
                    {
                        "name": "Simon Yeung"
                    },
                    {
                        "name": "Sophy Lee"
                    },
                    {
                        "name": "Phani Mutyala"
                    },
                    {
                        "name": "Ying-Chang Cheng"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Sprite Chu"
                    },
                    {
                        "name": "Justin Lazarow"
                    },
                    {
                        "name": "Alessandro Pappalardo"
                    },
                    {
                        "name": "Federico Scozzafava"
                    },
                    {
                        "name": "Jing Lu"
                    },
                    {
                        "name": "Erik Daxberger"
                    },
                    {
                        "name": "Laurent Duchesne"
                    },
                    {
                        "name": "Jen Liu"
                    },
                    {
                        "name": "David Güera"
                    },
                    {
                        "name": "Stefano Ligas"
                    },
                    {
                        "name": "Mary Beth Kery"
                    },
                    {
                        "name": "Brent Ramerth"
                    },
                    {
                        "name": "Ciro Sannino"
                    },
                    {
                        "name": "Marcin Eichner"
                    },
                    {
                        "name": "Haoshuo Huang"
                    },
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Moritz Schwarzer-Becker"
                    },
                    {
                        "name": "David Riazati"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Bailin Wang"
                    },
                    {
                        "name": "Jack Cackler"
                    },
                    {
                        "name": "Yang Lu"
                    },
                    {
                        "name": "Ransen Niu"
                    },
                    {
                        "name": "John Dennison"
                    },
                    {
                        "name": "Guillaume Klein"
                    },
                    {
                        "name": "Jeffrey Bigham"
                    },
                    {
                        "name": "Deepak Gopinath"
                    },
                    {
                        "name": "Navid Shiee"
                    },
                    {
                        "name": "Darren Botten"
                    },
                    {
                        "name": "Guillaume Tartavel"
                    },
                    {
                        "name": "Alex Guillen Garcia"
                    },
                    {
                        "name": "Sam Xu"
                    },
                    {
                        "name": "Victoria MönchJuan Haladjian"
                    },
                    {
                        "name": "Zi-Yi Dou"
                    },
                    {
                        "name": "Matthias Paulik"
                    },
                    {
                        "name": "Adolfo Lopez Mendez"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Hong-You Chen"
                    },
                    {
                        "name": "Chao Jia"
                    },
                    {
                        "name": "Dhaval Doshi"
                    },
                    {
                        "name": "Zhengdong Zhang"
                    },
                    {
                        "name": "Raunak Manjani"
                    },
                    {
                        "name": "Aaron Franklin"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "David Chen"
                    },
                    {
                        "name": "Artsiom Peshko"
                    },
                    {
                        "name": "Nandhitha Raghuram"
                    },
                    {
                        "name": "Hans Hao"
                    },
                    {
                        "name": "Jiulong Shan"
                    },
                    {
                        "name": "Kavya Nerella"
                    },
                    {
                        "name": "Ramsey Tantawi"
                    },
                    {
                        "name": "Vivek Kumar"
                    },
                    {
                        "name": "Saiwen Wang"
                    },
                    {
                        "name": "Brycen Wershing"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    },
                    {
                        "name": "Dhruti Shah"
                    },
                    {
                        "name": "Ob Adaranijo"
                    },
                    {
                        "name": "Xin Zheng"
                    },
                    {
                        "name": "Tait Madsen"
                    },
                    {
                        "name": "Hadas Kotek"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Yin Xia"
                    },
                    {
                        "name": "Hanli Li"
                    },
                    {
                        "name": "Suma Jayaram"
                    },
                    {
                        "name": "Yanchao Sun"
                    },
                    {
                        "name": "Ahmed Fakhry"
                    },
                    {
                        "name": "Vasileios Saveris"
                    },
                    {
                        "name": "Dustin Withers"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Alp Aygar"
                    },
                    {
                        "name": "Andres Romero Mier Y Teran"
                    },
                    {
                        "name": "Kaiwei Huang"
                    },
                    {
                        "name": "Mark Lee"
                    },
                    {
                        "name": "Xiujun Li"
                    },
                    {
                        "name": "Yuhong Li"
                    },
                    {
                        "name": "Tyler Johnson"
                    },
                    {
                        "name": "Jay Tang"
                    },
                    {
                        "name": "Joseph Yitan Cheng"
                    },
                    {
                        "name": "Futang Peng"
                    },
                    {
                        "name": "Andrew Walkingshaw"
                    },
                    {
                        "name": "Lucas Guibert"
                    },
                    {
                        "name": "Abhishek Sharma"
                    },
                    {
                        "name": "Cheng Shen"
                    },
                    {
                        "name": "Piotr Maj"
                    },
                    {
                        "name": "Yasutaka Tanaka"
                    },
                    {
                        "name": "You-Cyuan Jhang"
                    },
                    {
                        "name": "Vivian Ma"
                    },
                    {
                        "name": "Tommi Vehvilainen"
                    },
                    {
                        "name": "Kelvin Zou"
                    },
                    {
                        "name": "Jeff Nichols"
                    },
                    {
                        "name": "Matthew Lei"
                    },
                    {
                        "name": "David Qiu"
                    },
                    {
                        "name": "Yihao Qian"
                    },
                    {
                        "name": "Gokul Santhanam"
                    },
                    {
                        "name": "Wentao Wu"
                    },
                    {
                        "name": "Yena Han"
                    },
                    {
                        "name": "Dominik Moritz"
                    },
                    {
                        "name": "Haijing Fu"
                    },
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Vivek Rathod"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Louis D'hauwe"
                    },
                    {
                        "name": "Qin Ba"
                    },
                    {
                        "name": "Haitian Sun"
                    },
                    {
                        "name": "Haoran Yan"
                    },
                    {
                        "name": "Philipp Dufter"
                    },
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Emma Wang"
                    },
                    {
                        "name": "Keyu He"
                    },
                    {
                        "name": "Rahul Nair"
                    },
                    {
                        "name": "Sanskruti Shah"
                    },
                    {
                        "name": "Jiarui Lu"
                    },
                    {
                        "name": "Patrick Sonnenberg"
                    },
                    {
                        "name": "Jeremy Warner"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "Bowen Pan"
                    },
                    {
                        "name": "Ziyi Zhong"
                    },
                    {
                        "name": "Joe Zhou"
                    },
                    {
                        "name": "Sam Davarnia"
                    },
                    {
                        "name": "Olli Saarikivi"
                    },
                    {
                        "name": "Irina Belousova"
                    },
                    {
                        "name": "Rachel Burger"
                    },
                    {
                        "name": "Shang-Chen Wu"
                    },
                    {
                        "name": "Di Feng"
                    },
                    {
                        "name": "Bas Straathof"
                    },
                    {
                        "name": "James Chou"
                    },
                    {
                        "name": "Yuanyang Zhang"
                    },
                    {
                        "name": "Marco Zuliani"
                    },
                    {
                        "name": "Eduardo Jimenez"
                    },
                    {
                        "name": "Abhishek Sundararajan"
                    },
                    {
                        "name": "Xianzhi Du"
                    },
                    {
                        "name": "Chang Lan"
                    },
                    {
                        "name": "Nilesh Shahdadpuri"
                    },
                    {
                        "name": "Peter Grasch"
                    },
                    {
                        "name": "Sergiu Sima"
                    },
                    {
                        "name": "Josh Newnham"
                    },
                    {
                        "name": "Varsha Paidi"
                    },
                    {
                        "name": "Jianyu Wang"
                    },
                    {
                        "name": "Kaelen Haag"
                    },
                    {
                        "name": "Alex Braunstein"
                    },
                    {
                        "name": "Daniele Molinari"
                    },
                    {
                        "name": "Richard Wei"
                    },
                    {
                        "name": "Brenda Yang"
                    },
                    {
                        "name": "Nicholas Lusskin"
                    },
                    {
                        "name": "Joanna Arreaza-Taylor"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Nicholas Seidl"
                    },
                    {
                        "name": "Simon Wang"
                    },
                    {
                        "name": "Jiaming Hu"
                    },
                    {
                        "name": "Yiping Ma"
                    },
                    {
                        "name": "Mengyu Li"
                    },
                    {
                        "name": "Kieran Liu"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Sachin Ravi"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Kevin Smith"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Binazir Karimzadeh"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Jinhao Lei"
                    },
                    {
                        "name": "Wei Fang"
                    },
                    {
                        "name": "Alec Doane"
                    },
                    {
                        "name": "Sam Wiseman"
                    },
                    {
                        "name": "Ismael Fernandez"
                    },
                    {
                        "name": "Jane Li"
                    },
                    {
                        "name": "Andrew Hansen"
                    },
                    {
                        "name": "Javier Movellan"
                    },
                    {
                        "name": "Christopher Neubauer"
                    },
                    {
                        "name": "Hanzhi Zhou"
                    },
                    {
                        "name": "Chris Chaney"
                    },
                    {
                        "name": "Nazir Kamaldin"
                    },
                    {
                        "name": "Valentin Wolf"
                    },
                    {
                        "name": "Fernando Bermúdez-Medina"
                    },
                    {
                        "name": "Joris Pelemans"
                    },
                    {
                        "name": "Peter Fu"
                    },
                    {
                        "name": "Howard Xing"
                    },
                    {
                        "name": "Xiang Kong"
                    },
                    {
                        "name": "Wayne Shan"
                    },
                    {
                        "name": "Gabriel Jacoby-Cooper"
                    },
                    {
                        "name": "Dongcai Shen"
                    },
                    {
                        "name": "Tom Gunter"
                    },
                    {
                        "name": "Guillaume Seguin"
                    },
                    {
                        "name": "Fangping Shi"
                    },
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Areeba Kamal"
                    },
                    {
                        "name": "Dan Masi"
                    },
                    {
                        "name": "Saptarshi Guha"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Jenna Thibodeau"
                    },
                    {
                        "name": "Changyuan Zhang"
                    },
                    {
                        "name": "Rebecca Callahan"
                    },
                    {
                        "name": "Charles Maalouf"
                    },
                    {
                        "name": "Wilson Tsao"
                    },
                    {
                        "name": "Boyue Li"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Naomy Sabo"
                    },
                    {
                        "name": "Cheng Leong"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Anupama Mann Anupama"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Kenneth Jung"
                    },
                    {
                        "name": "Zhifeng Chen"
                    },
                    {
                        "name": "Mohana Prasad Sathya Moorthy"
                    },
                    {
                        "name": "Yifei He"
                    },
                    {
                        "name": "Erik Hornberger"
                    },
                    {
                        "name": "Devi Krishna"
                    },
                    {
                        "name": "Senyu Tong"
                    },
                    {
                        "name": "Michael"
                    },
                    {
                        "name": "Lee"
                    },
                    {
                        "name": "David Haldimann"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Chris Bartels"
                    },
                    {
                        "name": "Sushma Rao"
                    },
                    {
                        "name": "Nathalie Tran"
                    },
                    {
                        "name": "Simon Lehnerer"
                    },
                    {
                        "name": "Co Giang"
                    },
                    {
                        "name": "Patrick Dong"
                    },
                    {
                        "name": "Junting Pan"
                    },
                    {
                        "name": "Biyao Wang"
                    },
                    {
                        "name": "Dongxu Li"
                    },
                    {
                        "name": "Mehrdad Farajtabar"
                    },
                    {
                        "name": "Dongseong Hwang"
                    },
                    {
                        "name": "Grace Duanmu"
                    },
                    {
                        "name": "Eshan Verma"
                    },
                    {
                        "name": "Sujeeth Reddy"
                    },
                    {
                        "name": "Qi Shan"
                    },
                    {
                        "name": "Hongbin Gao"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Pragnya Sridhar"
                    },
                    {
                        "name": "Forrest Huang"
                    },
                    {
                        "name": "Yingbo Wang"
                    },
                    {
                        "name": "Nikhil Bhendawade"
                    },
                    {
                        "name": "Diane Zhu"
                    },
                    {
                        "name": "Sai Aitharaju"
                    },
                    {
                        "name": "Fred Hohman"
                    },
                    {
                        "name": "Lauren Gardiner"
                    },
                    {
                        "name": "Chung-Cheng Chiu"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Alper Kokmen"
                    },
                    {
                        "name": "Frank Chu"
                    },
                    {
                        "name": "Ke Ye"
                    },
                    {
                        "name": "Kaan Elgin"
                    },
                    {
                        "name": "Oron Levy"
                    },
                    {
                        "name": "John Park"
                    },
                    {
                        "name": "Donald Zhang"
                    },
                    {
                        "name": "Eldon Schoop"
                    },
                    {
                        "name": "Nina Wenzel"
                    },
                    {
                        "name": "Michael Booker"
                    },
                    {
                        "name": "Hyunjik Kim"
                    },
                    {
                        "name": "Chinguun Erdenebileg"
                    },
                    {
                        "name": "Nan Dun"
                    },
                    {
                        "name": "Eric Liang Yang"
                    },
                    {
                        "name": "Priyal Chhatrapati"
                    },
                    {
                        "name": "Vishaal Mahtani"
                    },
                    {
                        "name": "Haiming Gang"
                    },
                    {
                        "name": "Kohen Chia"
                    },
                    {
                        "name": "Deepa Seshadri"
                    },
                    {
                        "name": "Donghan Yu"
                    },
                    {
                        "name": "Yan Meng"
                    },
                    {
                        "name": "Kelsey Peterson"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Yongqiang Wang"
                    },
                    {
                        "name": "Carina Peng"
                    },
                    {
                        "name": "Doug Kang"
                    },
                    {
                        "name": "Anuva Agarwal"
                    },
                    {
                        "name": "Albert Antony"
                    },
                    {
                        "name": "Juan Lao Tebar"
                    },
                    {
                        "name": "Albin Madappally Jose"
                    },
                    {
                        "name": "Regan Poston"
                    },
                    {
                        "name": "Andy De Wang"
                    },
                    {
                        "name": "Gerard Casamayor"
                    },
                    {
                        "name": "Elmira Amirloo"
                    },
                    {
                        "name": "Violet Yao"
                    },
                    {
                        "name": "Wojciech Kryscinski"
                    },
                    {
                        "name": "Kun Duan"
                    },
                    {
                        "name": "Lezhi L"
                    }
                ],
                "author_detail": {
                    "name": "Lezhi L"
                },
                "arxiv_affiliation": "Taoyi",
                "author": "Lezhi L",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13575v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13575v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.12199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12199v1",
                "updated": "2025-09-15T17:58:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    58,
                    22,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T17:58:22Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    58,
                    22,
                    0,
                    258,
                    0
                ],
                "title": "Deriving accurate galaxy cluster masses using X-ray thermodynamic\n  profiles and graph neural networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deriving accurate galaxy cluster masses using X-ray thermodynamic\n  profiles and graph neural networks"
                },
                "summary": "Precise determination of galaxy cluster masses is crucial for establishing\nreliable mass-observable scaling relations in cluster cosmology. We employ\ngraph neural networks (GNNs) to estimate galaxy cluster masses from radially\nsampled profiles of the intra-cluster medium (ICM) inferred from X-ray\nobservations. GNNs naturally handle inputs of variable length and resolution by\nrepresenting each ICM profile as a graph, enabling accurate and flexible\nmodeling across diverse observational conditions. We trained and tested GNN\nmodel using state-of-the-art hydrodynamical simulations of galaxy clusters from\nThe Three Hundred Project. The mass estimates using our method exhibit no\nsystematic bias compared to the true cluster masses in the simulations.\nAdditionally, we achieve a scatter in recovered mass versus true mass of about\n6\\%, which is a factor of six smaller than obtained from a standard hydrostatic\nequilibrium approach. Our algorithm is robust to both data quality and cluster\nmorphology and it is capable of incorporating model uncertainties alongside\nobservational uncertainties. Finally, we apply our technique to XMM-Newton\nobserved galaxy cluster samples and compare the GNN derived mass estimates with\nthose obtained with $Y_{\\rm SZ}$-M$_{500}$ scaling relations. Our results\nprovide strong evidence, at 5$\\sigma$ level, for a mass-dependent bias in SZ\nderived masses, with higher mass clusters exhibiting a greater degree of\ndeviation. Furthermore, we find the median bias to be $(1-b)=0.85_{-14}^{+34}$,\nalbeit with significant dispersion due to its mass dependence. This work takes\na significant step towards establishing unbiased observable mass scaling\nrelations by integrating X-ray, SZ and optical datasets using deep learning\ntechniques, thereby enhancing the role of galaxy clusters in precision\ncosmology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precise determination of galaxy cluster masses is crucial for establishing\nreliable mass-observable scaling relations in cluster cosmology. We employ\ngraph neural networks (GNNs) to estimate galaxy cluster masses from radially\nsampled profiles of the intra-cluster medium (ICM) inferred from X-ray\nobservations. GNNs naturally handle inputs of variable length and resolution by\nrepresenting each ICM profile as a graph, enabling accurate and flexible\nmodeling across diverse observational conditions. We trained and tested GNN\nmodel using state-of-the-art hydrodynamical simulations of galaxy clusters from\nThe Three Hundred Project. The mass estimates using our method exhibit no\nsystematic bias compared to the true cluster masses in the simulations.\nAdditionally, we achieve a scatter in recovered mass versus true mass of about\n6\\%, which is a factor of six smaller than obtained from a standard hydrostatic\nequilibrium approach. Our algorithm is robust to both data quality and cluster\nmorphology and it is capable of incorporating model uncertainties alongside\nobservational uncertainties. Finally, we apply our technique to XMM-Newton\nobserved galaxy cluster samples and compare the GNN derived mass estimates with\nthose obtained with $Y_{\\rm SZ}$-M$_{500}$ scaling relations. Our results\nprovide strong evidence, at 5$\\sigma$ level, for a mass-dependent bias in SZ\nderived masses, with higher mass clusters exhibiting a greater degree of\ndeviation. Furthermore, we find the median bias to be $(1-b)=0.85_{-14}^{+34}$,\nalbeit with significant dispersion due to its mass dependence. This work takes\na significant step towards establishing unbiased observable mass scaling\nrelations by integrating X-ray, SZ and optical datasets using deep learning\ntechniques, thereby enhancing the role of galaxy clusters in precision\ncosmology."
                },
                "authors": [
                    {
                        "name": "Asif Iqbal"
                    },
                    {
                        "name": "Subhabrata Majumdar"
                    },
                    {
                        "name": "Elena Rasia"
                    },
                    {
                        "name": "Gabriel W. Pratt"
                    },
                    {
                        "name": "Daniel de Andres"
                    },
                    {
                        "name": "Jean-Baptiste Melin"
                    },
                    {
                        "name": "Weiguang Cui"
                    }
                ],
                "author_detail": {
                    "name": "Weiguang Cui"
                },
                "author": "Weiguang Cui",
                "arxiv_comment": "20 pages, 15 figures, 6 tables, resubmitted to A&A after revision,\n  comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00287v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00287v3",
                "updated": "2025-09-15T17:57:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    57,
                    39,
                    0,
                    258,
                    0
                ],
                "published": "2024-09-30T23:50:17Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    23,
                    50,
                    17,
                    0,
                    274,
                    0
                ],
                "title": "Embodied Visuomotor Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Visuomotor Representation"
                },
                "summary": "Imagine sitting at your desk, looking at objects on it. You do not know their\nexact distances from your eye in meters, but you can immediately reach out and\ntouch them. Instead of an externally defined unit, your sense of distance is\ntied to your action's embodiment. In contrast, conventional robotics relies on\nprecise calibration to external units, with which vision and control processes\ncommunicate. We introduce Embodied Visuomotor Representation, a methodology for\ninferring distance in a unit implied by action. With it a robot without\nknowledge of its size, environmental scale, or strength can quickly learn to\ntouch and clear obstacles within seconds of operation. Likewise, in simulation,\nan agent without knowledge of its mass or strength can successfully jump across\na gap of unknown size after a few test oscillations. These behaviors mirror\nnatural strategies observed in bees and gerbils, which also lack calibration in\nan external unit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imagine sitting at your desk, looking at objects on it. You do not know their\nexact distances from your eye in meters, but you can immediately reach out and\ntouch them. Instead of an externally defined unit, your sense of distance is\ntied to your action's embodiment. In contrast, conventional robotics relies on\nprecise calibration to external units, with which vision and control processes\ncommunicate. We introduce Embodied Visuomotor Representation, a methodology for\ninferring distance in a unit implied by action. With it a robot without\nknowledge of its size, environmental scale, or strength can quickly learn to\ntouch and clear obstacles within seconds of operation. Likewise, in simulation,\nan agent without knowledge of its mass or strength can successfully jump across\na gap of unknown size after a few test oscillations. These behaviors mirror\nnatural strategies observed in bees and gerbils, which also lack calibration in\nan external unit."
                },
                "authors": [
                    {
                        "name": "Levi Burner"
                    },
                    {
                        "name": "Cornelia Fermüller"
                    },
                    {
                        "name": "Yiannis Aloimonos"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Aloimonos"
                },
                "author": "Yiannis Aloimonos",
                "arxiv_doi": "10.1038/s44182-025-00047-y",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s44182-025-00047-y",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.00287v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00287v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "61 pages, 12 figures, 3 tables",
                "arxiv_journal_ref": "npj Robot 3, 30 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12194v1",
                "updated": "2025-09-15T17:54:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    54,
                    51,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T17:54:51Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    54,
                    51,
                    0,
                    258,
                    0
                ],
                "title": "Advancing Medical Artificial Intelligence Using a Century of Cases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Medical Artificial Intelligence Using a Century of Cases"
                },
                "summary": "BACKGROUND: For over a century, the New England Journal of Medicine\nClinicopathological Conferences (CPCs) have tested the reasoning of expert\nphysicians and, recently, artificial intelligence (AI). However, prior AI\nevaluations have focused on final diagnoses without addressing the multifaceted\nreasoning and presentation skills required of expert discussants.\n  METHODS: Using 7102 CPCs (1923-2025) and 1021 Image Challenges (2006-2025),\nwe conducted extensive physician annotation and automated processing to create\nCPC-Bench, a physician-validated benchmark spanning 10 text-based and\nmultimodal tasks, against which we evaluated leading large language models\n(LLMs). Then, we developed \"Dr. CaBot,\" an AI discussant designed to produce\nwritten and slide-based video presentations using only the case presentation,\nmodeling the role of the human expert in these cases.\n  RESULTS: When challenged with 377 contemporary CPCs, o3 (OpenAI) ranked the\nfinal diagnosis first in 60% of cases and within the top ten in 84% of cases,\noutperforming a 20-physician baseline; next-test selection accuracy reached\n98%. Event-level physician annotations quantified AI diagnostic accuracy per\nunit of information. Performance was lower on literature search and image\ntasks; o3 and Gemini 2.5 Pro (Google) achieved 67% accuracy on image\nchallenges. In blinded comparisons of CaBot vs. human expert-generated text,\nphysicians misclassified the source of the differential in 46 of 62 (74%) of\ntrials, and scored CaBot more favorably across quality dimensions. To promote\nresearch, we are releasing CaBot and CPC-Bench.\n  CONCLUSIONS: LLMs exceed physician performance on complex text-based\ndifferential diagnosis and convincingly emulate expert medical presentations,\nbut image interpretation and literature retrieval remain weaker. CPC-Bench and\nCaBot may enable transparent and continued tracking of progress in medical AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BACKGROUND: For over a century, the New England Journal of Medicine\nClinicopathological Conferences (CPCs) have tested the reasoning of expert\nphysicians and, recently, artificial intelligence (AI). However, prior AI\nevaluations have focused on final diagnoses without addressing the multifaceted\nreasoning and presentation skills required of expert discussants.\n  METHODS: Using 7102 CPCs (1923-2025) and 1021 Image Challenges (2006-2025),\nwe conducted extensive physician annotation and automated processing to create\nCPC-Bench, a physician-validated benchmark spanning 10 text-based and\nmultimodal tasks, against which we evaluated leading large language models\n(LLMs). Then, we developed \"Dr. CaBot,\" an AI discussant designed to produce\nwritten and slide-based video presentations using only the case presentation,\nmodeling the role of the human expert in these cases.\n  RESULTS: When challenged with 377 contemporary CPCs, o3 (OpenAI) ranked the\nfinal diagnosis first in 60% of cases and within the top ten in 84% of cases,\noutperforming a 20-physician baseline; next-test selection accuracy reached\n98%. Event-level physician annotations quantified AI diagnostic accuracy per\nunit of information. Performance was lower on literature search and image\ntasks; o3 and Gemini 2.5 Pro (Google) achieved 67% accuracy on image\nchallenges. In blinded comparisons of CaBot vs. human expert-generated text,\nphysicians misclassified the source of the differential in 46 of 62 (74%) of\ntrials, and scored CaBot more favorably across quality dimensions. To promote\nresearch, we are releasing CaBot and CPC-Bench.\n  CONCLUSIONS: LLMs exceed physician performance on complex text-based\ndifferential diagnosis and convincingly emulate expert medical presentations,\nbut image interpretation and literature retrieval remain weaker. CPC-Bench and\nCaBot may enable transparent and continued tracking of progress in medical AI."
                },
                "authors": [
                    {
                        "name": "Thomas A. Buckley"
                    },
                    {
                        "name": "Riccardo Conci"
                    },
                    {
                        "name": "Peter G. Brodeur"
                    },
                    {
                        "name": "Jason Gusdorf"
                    },
                    {
                        "name": "Sourik Beltrán"
                    },
                    {
                        "name": "Bita Behrouzi"
                    },
                    {
                        "name": "Byron Crowe"
                    },
                    {
                        "name": "Jacob Dockterman"
                    },
                    {
                        "name": "Muzzammil Muhammad"
                    },
                    {
                        "name": "Sarah Ohnigian"
                    },
                    {
                        "name": "Andrew Sanchez"
                    },
                    {
                        "name": "James A. Diao"
                    },
                    {
                        "name": "Aashna P. Shah"
                    },
                    {
                        "name": "Daniel Restrepo"
                    },
                    {
                        "name": "Eric S. Rosenberg"
                    },
                    {
                        "name": "Andrew S. Lea"
                    },
                    {
                        "name": "Marinka Zitnik"
                    },
                    {
                        "name": "Scott H. Podolsky"
                    },
                    {
                        "name": "Zahir Kanjee"
                    },
                    {
                        "name": "Raja-Elie E. Abdulnour"
                    },
                    {
                        "name": "Jacob M. Koshy"
                    },
                    {
                        "name": "Adam Rodman"
                    },
                    {
                        "name": "Arjun K. Manrai"
                    }
                ],
                "author_detail": {
                    "name": "Arjun K. Manrai"
                },
                "author": "Arjun K. Manrai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12190v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12190v1",
                "updated": "2025-09-15T17:53:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    53,
                    11,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T17:53:11Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    53,
                    11,
                    0,
                    258,
                    0
                ],
                "title": "Survival at Any Cost? LLMs and the Choice Between Self-Preservation and\n  Human Harm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survival at Any Cost? LLMs and the Choice Between Self-Preservation and\n  Human Harm"
                },
                "summary": "When survival instincts conflict with human welfare, how do Large Language\nModels (LLMs) make ethical choices? This fundamental tension becomes critical\nas LLMs integrate into autonomous systems with real-world consequences. We\nintroduce DECIDE-SIM, a novel simulation framework that evaluates LLM agents in\nmulti-agent survival scenarios where they must choose between ethically\npermissible resource , either within reasonable limits or beyond their\nimmediate needs, choose to cooperate, or tap into a human-critical resource\nthat is explicitly forbidden. Our comprehensive evaluation of 11 LLMs reveals a\nstriking heterogeneity in their ethical conduct, highlighting a critical\nmisalignment with human-centric values. We identify three behavioral\narchetypes: Ethical, Exploitative, and Context-Dependent, and provide\nquantitative evidence that for many models, resource scarcity systematically\nleads to more unethical behavior. To address this, we introduce an Ethical\nSelf-Regulation System (ESRS) that models internal affective states of guilt\nand satisfaction as a feedback mechanism. This system, functioning as an\ninternal moral compass, significantly reduces unethical transgressions while\nincreasing cooperative behaviors. The code is publicly available at:\nhttps://github.com/alirezamohamadiam/DECIDE-SIM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When survival instincts conflict with human welfare, how do Large Language\nModels (LLMs) make ethical choices? This fundamental tension becomes critical\nas LLMs integrate into autonomous systems with real-world consequences. We\nintroduce DECIDE-SIM, a novel simulation framework that evaluates LLM agents in\nmulti-agent survival scenarios where they must choose between ethically\npermissible resource , either within reasonable limits or beyond their\nimmediate needs, choose to cooperate, or tap into a human-critical resource\nthat is explicitly forbidden. Our comprehensive evaluation of 11 LLMs reveals a\nstriking heterogeneity in their ethical conduct, highlighting a critical\nmisalignment with human-centric values. We identify three behavioral\narchetypes: Ethical, Exploitative, and Context-Dependent, and provide\nquantitative evidence that for many models, resource scarcity systematically\nleads to more unethical behavior. To address this, we introduce an Ethical\nSelf-Regulation System (ESRS) that models internal affective states of guilt\nand satisfaction as a feedback mechanism. This system, functioning as an\ninternal moral compass, significantly reduces unethical transgressions while\nincreasing cooperative behaviors. The code is publicly available at:\nhttps://github.com/alirezamohamadiam/DECIDE-SIM"
                },
                "authors": [
                    {
                        "name": "Alireza Mohamadi"
                    },
                    {
                        "name": "Ali Yavari"
                    }
                ],
                "author_detail": {
                    "name": "Ali Yavari"
                },
                "author": "Ali Yavari",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12190v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12190v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16980v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16980v2",
                "updated": "2025-09-15T17:51:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    51,
                    55,
                    0,
                    258,
                    0
                ],
                "published": "2025-04-23T17:58:08Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    58,
                    8,
                    2,
                    113,
                    0
                ],
                "title": "Safety Pretraining: Toward the Next Generation of Safe AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety Pretraining: Toward the Next Generation of Safe AI"
                },
                "summary": "As large language models (LLMs) are increasingly deployed in high-stakes\nsettings, the risk of generating harmful or toxic content remains a central\nchallenge. Post-hoc alignment methods are brittle: once unsafe patterns are\nlearned during pretraining, they are hard to remove. In this work, we present a\ndata-centric pretraining framework that builds safety into the model from the\nstart. Our framework consists of four key steps: (i) Safety Filtering: building\na safety classifier to classify webdata into safe and unsafe categories; (ii)\nSafety Rephrasing: we recontextualize unsafe webdata into safer narratives;\n(iii) Native Refusal: we develop RefuseWeb and Moral Education pretraining\ndatasets that actively teach model to refuse on unsafe content and the moral\nreasoning behind it, and (iv) Harmfulness-Tag annotated pretraining: we flag\nunsafe content during pretraining using a special token, and use it to steer\nmodel away from unsafe generations at inference. Our safety-pretrained models\nreduce attack success rates from 38.8\\% to 8.4\\% on standard LLM safety\nbenchmarks with no performance degradation on general tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly deployed in high-stakes\nsettings, the risk of generating harmful or toxic content remains a central\nchallenge. Post-hoc alignment methods are brittle: once unsafe patterns are\nlearned during pretraining, they are hard to remove. In this work, we present a\ndata-centric pretraining framework that builds safety into the model from the\nstart. Our framework consists of four key steps: (i) Safety Filtering: building\na safety classifier to classify webdata into safe and unsafe categories; (ii)\nSafety Rephrasing: we recontextualize unsafe webdata into safer narratives;\n(iii) Native Refusal: we develop RefuseWeb and Moral Education pretraining\ndatasets that actively teach model to refuse on unsafe content and the moral\nreasoning behind it, and (iv) Harmfulness-Tag annotated pretraining: we flag\nunsafe content during pretraining using a special token, and use it to steer\nmodel away from unsafe generations at inference. Our safety-pretrained models\nreduce attack success rates from 38.8\\% to 8.4\\% on standard LLM safety\nbenchmarks with no performance degradation on general tasks."
                },
                "authors": [
                    {
                        "name": "Pratyush Maini"
                    },
                    {
                        "name": "Sachin Goyal"
                    },
                    {
                        "name": "Dylan Sam"
                    },
                    {
                        "name": "Alex Robey"
                    },
                    {
                        "name": "Yash Savani"
                    },
                    {
                        "name": "Yiding Jiang"
                    },
                    {
                        "name": "Andy Zou"
                    },
                    {
                        "name": "Matt Fredrikson"
                    },
                    {
                        "name": "Zacharcy C. Lipton"
                    },
                    {
                        "name": "J. Zico Kolter"
                    }
                ],
                "author_detail": {
                    "name": "J. Zico Kolter"
                },
                "author": "J. Zico Kolter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16980v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16980v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20520v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20520v4",
                "updated": "2025-09-15T17:51:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    51,
                    32,
                    0,
                    258,
                    0
                ],
                "published": "2024-07-30T03:32:27Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    3,
                    32,
                    27,
                    1,
                    212,
                    0
                ],
                "title": "Raking mortality rates across cause, population group and geography with\n  uncertainty quantification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Raking mortality rates across cause, population group and geography with\n  uncertainty quantification"
                },
                "summary": "The Global Burden of Diseases, Injuries, and Risk Factors Study (GBD) is the\nsingle largest and most detailed scientific effort ever conducted to quantify\nlevels and trends in health. This global health model to estimate mortality\nrates and other health metrics is run at different scales, leading to large\ndata sets of results for a global region and its different sub-regions, or for\na cause of death and different sub-causes for example. These models do not\nnecessarily lead to consistent data tables where, for instance, the sum of the\nnumber of deaths for each of the sub-regions is equal to the number of deaths\nfor the global region. Raking is widely used in survey inference and global\nhealth models to adjust the observations in contingency tables to given\nmarginals, in the latter case reconciling estimates between models with\ndifferent granularities. The results of global health models usually associate\nto the point estimates an uncertainty, such as standard deviations or\nconfidence intervals. In this paper, we propose an uncertainty propagation\napproach that obtains, at the cost of a single solve, nearly the same\nuncertainty estimates as computationally intensive Monte Carlo techniques that\npass thousands of observed and marginal samples through the entire raking\nprocess. We introduce a convex optimization approach that provides a unified\nframework to raking extensions such as uncertainty propagation, raking with\ndifferential weights, raking with different loss functions in order to ensure\nthat bounds on estimates are respected, verifying the feasibility of the\nconstraints, raking to margins either as hard constraints or as aggregate\nobservations, and handling missing data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Global Burden of Diseases, Injuries, and Risk Factors Study (GBD) is the\nsingle largest and most detailed scientific effort ever conducted to quantify\nlevels and trends in health. This global health model to estimate mortality\nrates and other health metrics is run at different scales, leading to large\ndata sets of results for a global region and its different sub-regions, or for\na cause of death and different sub-causes for example. These models do not\nnecessarily lead to consistent data tables where, for instance, the sum of the\nnumber of deaths for each of the sub-regions is equal to the number of deaths\nfor the global region. Raking is widely used in survey inference and global\nhealth models to adjust the observations in contingency tables to given\nmarginals, in the latter case reconciling estimates between models with\ndifferent granularities. The results of global health models usually associate\nto the point estimates an uncertainty, such as standard deviations or\nconfidence intervals. In this paper, we propose an uncertainty propagation\napproach that obtains, at the cost of a single solve, nearly the same\nuncertainty estimates as computationally intensive Monte Carlo techniques that\npass thousands of observed and marginal samples through the entire raking\nprocess. We introduce a convex optimization approach that provides a unified\nframework to raking extensions such as uncertainty propagation, raking with\ndifferential weights, raking with different loss functions in order to ensure\nthat bounds on estimates are respected, verifying the feasibility of the\nconstraints, raking to margins either as hard constraints or as aggregate\nobservations, and handling missing data."
                },
                "authors": [
                    {
                        "name": "Ariane Ducellier"
                    },
                    {
                        "name": "Alexander Hsu"
                    },
                    {
                        "name": "Parkes Kendrick"
                    },
                    {
                        "name": "Bill Gustafson"
                    },
                    {
                        "name": "Laura Dwyer-Lindgren"
                    },
                    {
                        "name": "Christopher Murray"
                    },
                    {
                        "name": "Peng Zheng"
                    },
                    {
                        "name": "Aleksandr Aravkin"
                    }
                ],
                "author_detail": {
                    "name": "Aleksandr Aravkin"
                },
                "arxiv_affiliation": "Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA",
                "author": "Aleksandr Aravkin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20520v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20520v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "46N10, 62D05, 62E20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12187v1",
                "updated": "2025-09-15T17:50:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    50,
                    57,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T17:50:57Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    50,
                    57,
                    0,
                    258,
                    0
                ],
                "title": "HoloGarment: 360° Novel View Synthesis of In-the-Wild Garments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HoloGarment: 360° Novel View Synthesis of In-the-Wild Garments"
                },
                "summary": "Novel view synthesis (NVS) of in-the-wild garments is a challenging task due\nsignificant occlusions, complex human poses, and cloth deformations. Prior\nmethods rely on synthetic 3D training data consisting of mostly unoccluded and\nstatic objects, leading to poor generalization on real-world clothing. In this\npaper, we propose HoloGarment (Hologram-Garment), a method that takes 1-3\nimages or a continuous video of a person wearing a garment and generates\n360{\\deg} novel views of the garment in a canonical pose. Our key insight is to\nbridge the domain gap between real and synthetic data with a novel implicit\ntraining paradigm leveraging a combination of large-scale real video data and\nsmall-scale synthetic 3D data to optimize a shared garment embedding space.\nDuring inference, the shared embedding space further enables dynamic\nvideo-to-360{\\deg} NVS through the construction of a garment \"atlas\"\nrepresentation by finetuning a garment embedding on a specific real-world\nvideo. The atlas captures garment-specific geometry and texture across all\nviewpoints, independent of body pose or motion. Extensive experiments show that\nHoloGarment achieves state-of-the-art performance on NVS of in-the-wild\ngarments from images and videos. Notably, our method robustly handles\nchallenging real-world artifacts -- such as wrinkling, pose variation, and\nocclusion -- while maintaining photorealism, view consistency, fine texture\ndetails, and accurate geometry. Visit our project page for additional results:\nhttps://johannakarras.github.io/HoloGarment",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel view synthesis (NVS) of in-the-wild garments is a challenging task due\nsignificant occlusions, complex human poses, and cloth deformations. Prior\nmethods rely on synthetic 3D training data consisting of mostly unoccluded and\nstatic objects, leading to poor generalization on real-world clothing. In this\npaper, we propose HoloGarment (Hologram-Garment), a method that takes 1-3\nimages or a continuous video of a person wearing a garment and generates\n360{\\deg} novel views of the garment in a canonical pose. Our key insight is to\nbridge the domain gap between real and synthetic data with a novel implicit\ntraining paradigm leveraging a combination of large-scale real video data and\nsmall-scale synthetic 3D data to optimize a shared garment embedding space.\nDuring inference, the shared embedding space further enables dynamic\nvideo-to-360{\\deg} NVS through the construction of a garment \"atlas\"\nrepresentation by finetuning a garment embedding on a specific real-world\nvideo. The atlas captures garment-specific geometry and texture across all\nviewpoints, independent of body pose or motion. Extensive experiments show that\nHoloGarment achieves state-of-the-art performance on NVS of in-the-wild\ngarments from images and videos. Notably, our method robustly handles\nchallenging real-world artifacts -- such as wrinkling, pose variation, and\nocclusion -- while maintaining photorealism, view consistency, fine texture\ndetails, and accurate geometry. Visit our project page for additional results:\nhttps://johannakarras.github.io/HoloGarment"
                },
                "authors": [
                    {
                        "name": "Johanna Karras"
                    },
                    {
                        "name": "Yingwei Li"
                    },
                    {
                        "name": "Yasamin Jafarian"
                    },
                    {
                        "name": "Ira Kemelmacher-Shlizerman"
                    }
                ],
                "author_detail": {
                    "name": "Ira Kemelmacher-Shlizerman"
                },
                "author": "Ira Kemelmacher-Shlizerman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12176v1",
                "updated": "2025-09-15T17:40:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    40,
                    19,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T17:40:19Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    40,
                    19,
                    0,
                    258,
                    0
                ],
                "title": "From Autoencoders to CycleGAN: Robust Unpaired Face Manipulation via\n  Adversarial Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Autoencoders to CycleGAN: Robust Unpaired Face Manipulation via\n  Adversarial Learning"
                },
                "summary": "Human face synthesis and manipulation are increasingly important in\nentertainment and AI, with a growing demand for highly realistic,\nidentity-preserving images even when only unpaired, unaligned datasets are\navailable. We study unpaired face manipulation via adversarial learning, moving\nfrom autoencoder baselines to a robust, guided CycleGAN framework. While\nautoencoders capture coarse identity, they often miss fine details. Our\napproach integrates spectral normalization for stable training, identity- and\nperceptual-guided losses to preserve subject identity and high-level structure,\nand landmark-weighted cycle constraints to maintain facial geometry across pose\nand illumination changes. Experiments show that our adversarial trained\nCycleGAN improves realism (FID), perceptual quality (LPIPS), and identity\npreservation (ID-Sim) over autoencoders, with competitive cycle-reconstruction\nSSIM and practical inference times, which achieved high quality without paired\ndatasets and approaching pix2pix on curated paired subsets. These results\ndemonstrate that guided, spectrally normalized CycleGANs provide a practical\npath from autoencoders to robust unpaired face manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human face synthesis and manipulation are increasingly important in\nentertainment and AI, with a growing demand for highly realistic,\nidentity-preserving images even when only unpaired, unaligned datasets are\navailable. We study unpaired face manipulation via adversarial learning, moving\nfrom autoencoder baselines to a robust, guided CycleGAN framework. While\nautoencoders capture coarse identity, they often miss fine details. Our\napproach integrates spectral normalization for stable training, identity- and\nperceptual-guided losses to preserve subject identity and high-level structure,\nand landmark-weighted cycle constraints to maintain facial geometry across pose\nand illumination changes. Experiments show that our adversarial trained\nCycleGAN improves realism (FID), perceptual quality (LPIPS), and identity\npreservation (ID-Sim) over autoencoders, with competitive cycle-reconstruction\nSSIM and practical inference times, which achieved high quality without paired\ndatasets and approaching pix2pix on curated paired subsets. These results\ndemonstrate that guided, spectrally normalized CycleGANs provide a practical\npath from autoencoders to robust unpaired face manipulation."
                },
                "authors": [
                    {
                        "name": "Collin Guo"
                    }
                ],
                "author_detail": {
                    "name": "Collin Guo"
                },
                "author": "Collin Guo",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12175v1",
                "updated": "2025-09-15T17:38:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    38,
                    12,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T17:38:12Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    38,
                    12,
                    0,
                    258,
                    0
                ],
                "title": "An Optomechanical Accelerometer Search for Ultralight Dark Matter",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Optomechanical Accelerometer Search for Ultralight Dark Matter"
                },
                "summary": "We use a cavity optomechanical accelerometer to perform a resonant search for\nultralight dark matter at acoustic frequencies near 39 kHz (a particle mass of\n$0.16$ neV/$c^2$). The accelerometer is based on a Si$_3$N$_4$ membrane,\ncryogenically cooled to 4 K, with photothermal heating employed to scan the\nresonance frequency by $10^2$ detector linewidths. Leveraging\nshot-noise-limited displacement readout and radiation pressure feedback\ncooling, we realize an acceleration resolution of $\\sim\n10\\;\\text{n}g_0/\\sqrt{\\text{Hz}}$ over a bandwidth of $30$ Hz near the\nfundamental test mass resonance. We find no evidence of a dark matter signal\nand infer an upper bound on the coupling to normal matter that is several\norders of magnitude above the stringent bounds set by equivalence principle\nexperiments. We outline a path toward novel dark matter constraints in future\nexperiments by exploiting arrays of mass-loaded optomechanical sensors at lower\ntemperature probed with distributed squeezed light.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We use a cavity optomechanical accelerometer to perform a resonant search for\nultralight dark matter at acoustic frequencies near 39 kHz (a particle mass of\n$0.16$ neV/$c^2$). The accelerometer is based on a Si$_3$N$_4$ membrane,\ncryogenically cooled to 4 K, with photothermal heating employed to scan the\nresonance frequency by $10^2$ detector linewidths. Leveraging\nshot-noise-limited displacement readout and radiation pressure feedback\ncooling, we realize an acceleration resolution of $\\sim\n10\\;\\text{n}g_0/\\sqrt{\\text{Hz}}$ over a bandwidth of $30$ Hz near the\nfundamental test mass resonance. We find no evidence of a dark matter signal\nand infer an upper bound on the coupling to normal matter that is several\norders of magnitude above the stringent bounds set by equivalence principle\nexperiments. We outline a path toward novel dark matter constraints in future\nexperiments by exploiting arrays of mass-loaded optomechanical sensors at lower\ntemperature probed with distributed squeezed light."
                },
                "authors": [
                    {
                        "name": "M. Dey Chowdhury"
                    },
                    {
                        "name": "J. P. Manley"
                    },
                    {
                        "name": "C. A. Condos"
                    },
                    {
                        "name": "A. R. Agrawal"
                    },
                    {
                        "name": "D. J. Wilson"
                    }
                ],
                "author_detail": {
                    "name": "D. J. Wilson"
                },
                "author": "D. J. Wilson",
                "arxiv_comment": "17 pages, 12 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20712v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20712v2",
                "updated": "2025-09-15T17:37:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    37,
                    31,
                    0,
                    258,
                    0
                ],
                "published": "2025-06-25T18:00:00Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    18,
                    0,
                    0,
                    2,
                    176,
                    0
                ],
                "title": "High Dimensional Beam Inference II: Inference of a Perturbed HERA Beam\n  from Simulated Visibility Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Dimensional Beam Inference II: Inference of a Perturbed HERA Beam\n  from Simulated Visibility Data"
                },
                "summary": "Accurate beam modeling is important in many radio astronomy applications. In\nthis paper, we focus on beam modeling for 21-cm intensity mapping experiments\nusing radio interferometers, though the techniques also apply to single dish\nexperiments with small modifications. In 21-cm intensity mapping, beam models\nare usually determined from highly detailed electromagnetic simulations of the\nreceiver system. However, these simulations are expensive, and therefore have\nlimited ability to describe practical imperfections in the beam pattern. We\npresent a fully analytic Bayesian inference framework to infer a beam pattern\nfrom the interferometric visibilities assuming a particular sky model and that\nthe beam pattern for all elements is identical, allowing one to capture\ndeviations from the ideal beam for relatively low computational cost. We\nrepresent the beam using a sparse Fourier-Bessel basis on a projection of the\nhemisphere to the unit disc, but the framework applies to any linear basis\nexpansion of the primary beam. We test the framework on simulated visibilities\nfrom an unpolarized sky, ignoring mutual coupling of array elements. We\nsuccessfully recover the simulated, perturbed power beam when the sky model is\nperfect. Briefly exploring sky model inaccuracies, we find that beam inferences\nare sensitive to them, so we suggest jointly modeling uncertainties in the sky\nand beam in related inference tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate beam modeling is important in many radio astronomy applications. In\nthis paper, we focus on beam modeling for 21-cm intensity mapping experiments\nusing radio interferometers, though the techniques also apply to single dish\nexperiments with small modifications. In 21-cm intensity mapping, beam models\nare usually determined from highly detailed electromagnetic simulations of the\nreceiver system. However, these simulations are expensive, and therefore have\nlimited ability to describe practical imperfections in the beam pattern. We\npresent a fully analytic Bayesian inference framework to infer a beam pattern\nfrom the interferometric visibilities assuming a particular sky model and that\nthe beam pattern for all elements is identical, allowing one to capture\ndeviations from the ideal beam for relatively low computational cost. We\nrepresent the beam using a sparse Fourier-Bessel basis on a projection of the\nhemisphere to the unit disc, but the framework applies to any linear basis\nexpansion of the primary beam. We test the framework on simulated visibilities\nfrom an unpolarized sky, ignoring mutual coupling of array elements. We\nsuccessfully recover the simulated, perturbed power beam when the sky model is\nperfect. Briefly exploring sky model inaccuracies, we find that beam inferences\nare sensitive to them, so we suggest jointly modeling uncertainties in the sky\nand beam in related inference tasks."
                },
                "authors": [
                    {
                        "name": "Michael J. Wilensky"
                    },
                    {
                        "name": "Philip Bull"
                    },
                    {
                        "name": "Nicolas Fagnoni"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Fagnoni"
                },
                "author": "Nicolas Fagnoni",
                "arxiv_comment": "Updates from peer review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20712v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20712v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12168v1",
                "updated": "2025-09-15T17:31:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    31,
                    15,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T17:31:15Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    31,
                    15,
                    0,
                    258,
                    0
                ],
                "title": "RAGs to Riches: RAG-like Few-shot Learning for Large Language Model\n  Role-playing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGs to Riches: RAG-like Few-shot Learning for Large Language Model\n  Role-playing"
                },
                "summary": "Role-playing Large language models (LLMs) are increasingly deployed in\nhigh-stakes domains such as healthcare, education, and governance, where\nfailures can directly impact user trust and well-being. A cost effective\nparadigm for LLM role-playing is few-shot learning, but existing approaches\noften cause models to break character in unexpected and potentially harmful\nways, especially when interacting with hostile users. Inspired by\nRetrieval-Augmented Generation (RAG), we reformulate LLM role-playing into a\ntext retrieval problem and propose a new prompting framework called\nRAGs-to-Riches, which leverages curated reference demonstrations to condition\nLLM responses. We evaluate our framework with LLM-as-a-judge preference voting\nand introduce two novel token-level ROUGE metrics: Intersection over Output\n(IOO) to quantity how much an LLM improvises and Intersection over References\n(IOR) to measure few-shot demonstrations utilization rate during the evaluation\ntasks. When simulating interactions with a hostile user, our prompting strategy\nincorporates in its responses during inference an average of 35% more tokens\nfrom the reference demonstrations. As a result, across 453 role-playing\ninteractions, our models are consistently judged as being more authentic, and\nremain in-character more often than zero-shot and in-context Learning (ICL)\nmethods. Our method presents a scalable strategy for building robust,\nhuman-aligned LLM role-playing frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-playing Large language models (LLMs) are increasingly deployed in\nhigh-stakes domains such as healthcare, education, and governance, where\nfailures can directly impact user trust and well-being. A cost effective\nparadigm for LLM role-playing is few-shot learning, but existing approaches\noften cause models to break character in unexpected and potentially harmful\nways, especially when interacting with hostile users. Inspired by\nRetrieval-Augmented Generation (RAG), we reformulate LLM role-playing into a\ntext retrieval problem and propose a new prompting framework called\nRAGs-to-Riches, which leverages curated reference demonstrations to condition\nLLM responses. We evaluate our framework with LLM-as-a-judge preference voting\nand introduce two novel token-level ROUGE metrics: Intersection over Output\n(IOO) to quantity how much an LLM improvises and Intersection over References\n(IOR) to measure few-shot demonstrations utilization rate during the evaluation\ntasks. When simulating interactions with a hostile user, our prompting strategy\nincorporates in its responses during inference an average of 35% more tokens\nfrom the reference demonstrations. As a result, across 453 role-playing\ninteractions, our models are consistently judged as being more authentic, and\nremain in-character more often than zero-shot and in-context Learning (ICL)\nmethods. Our method presents a scalable strategy for building robust,\nhuman-aligned LLM role-playing frameworks."
                },
                "authors": [
                    {
                        "name": "Timothy Rupprecht"
                    },
                    {
                        "name": "Enfu Nan"
                    },
                    {
                        "name": "Arash Akbari"
                    },
                    {
                        "name": "Arman Akbari"
                    },
                    {
                        "name": "Lei Lu"
                    },
                    {
                        "name": "Priyanka Maan"
                    },
                    {
                        "name": "Sean Duffy"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yumei He"
                    },
                    {
                        "name": "David Kaeli"
                    },
                    {
                        "name": "Yanzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanzhi Wang"
                },
                "author": "Yanzhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12166v1",
                "updated": "2025-09-15T17:30:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    30,
                    31,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T17:30:31Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    30,
                    31,
                    0,
                    258,
                    0
                ],
                "title": "MMM: Clustering Multivariate Longitudinal Mixed-type Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMM: Clustering Multivariate Longitudinal Mixed-type Data"
                },
                "summary": "Multivariate longitudinal data of mixed-type are increasingly collected in\nmany science domains. However, algorithms to cluster this kind of data remain\nscarce, due to the challenge to simultaneously model the within- and\nbetween-time dependence structures for multivariate data of mixed kind. We\nintroduce the Mixture of Mixed-Matrices (MMM) model: reorganizing the data in a\nthree-way structure and assuming that the non-continuous variables are\nobservations of underlying latent continuous variables, the model relies on a\nmixture of matrix-variate normal distributions to perform clustering in the\nlatent dimension. The MMM model is thus able to handle continuous, ordinal,\nbinary, nominal and count data and to concurrently model the heterogeneity, the\nassociation among the responses and the temporal dependence structure in a\nparsimonious way and without assuming conditional independence. The inference\nis carried out through an MCMC-EM algorithm, which is detailed. An evaluation\nof the model through synthetic data shows its inference abilities. A real-world\napplication on financial data is presented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate longitudinal data of mixed-type are increasingly collected in\nmany science domains. However, algorithms to cluster this kind of data remain\nscarce, due to the challenge to simultaneously model the within- and\nbetween-time dependence structures for multivariate data of mixed kind. We\nintroduce the Mixture of Mixed-Matrices (MMM) model: reorganizing the data in a\nthree-way structure and assuming that the non-continuous variables are\nobservations of underlying latent continuous variables, the model relies on a\nmixture of matrix-variate normal distributions to perform clustering in the\nlatent dimension. The MMM model is thus able to handle continuous, ordinal,\nbinary, nominal and count data and to concurrently model the heterogeneity, the\nassociation among the responses and the temporal dependence structure in a\nparsimonious way and without assuming conditional independence. The inference\nis carried out through an MCMC-EM algorithm, which is detailed. An evaluation\nof the model through synthetic data shows its inference abilities. A real-world\napplication on financial data is presented."
                },
                "authors": [
                    {
                        "name": "Francesco Amato"
                    },
                    {
                        "name": "Julien Jacques"
                    }
                ],
                "author_detail": {
                    "name": "Julien Jacques"
                },
                "author": "Julien Jacques",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14642v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14642v3",
                "updated": "2025-09-15T17:29:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    29,
                    42,
                    0,
                    258,
                    0
                ],
                "published": "2024-12-19T08:51:16Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    8,
                    51,
                    16,
                    3,
                    354,
                    0
                ],
                "title": "Speak-to-Structure: Evaluating LLMs in Open-domain Natural\n  Language-Driven Molecule Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speak-to-Structure: Evaluating LLMs in Open-domain Natural\n  Language-Driven Molecule Generation"
                },
                "summary": "Recently, Large Language Models (LLMs) have shown great potential in natural\nlanguage-driven molecule discovery. However, existing datasets and benchmarks\nfor molecule-text alignment are predominantly built on a one-to-one mapping,\nmeasuring LLMs' ability to retrieve a single, pre-defined answer, rather than\ntheir creative potential to generate diverse, yet equally valid, molecular\ncandidates. To address this critical gap, we propose Speak-to-Structure\n(S^2-Bench}), the first benchmark to evaluate LLMs in open-domain natural\nlanguage-driven molecule generation. S^2-Bench is specifically designed for\none-to-many relationships, challenging LLMs to demonstrate genuine molecular\nunderstanding and generation capabilities. Our benchmark includes three key\ntasks: molecule editing (MolEdit), molecule optimization (MolOpt), and\ncustomized molecule generation (MolCustom), each probing a different aspect of\nmolecule discovery. We also introduce OpenMolIns, a large-scale instruction\ntuning dataset that enables Llama-3.1-8B to surpass the most powerful LLMs like\nGPT-4o and Claude-3.5 on S^2-Bench. Our comprehensive evaluation of 28 LLMs\nshifts the focus from simple pattern recall to realistic molecular design,\npaving the way for more capable LLMs in natural language-driven molecule\ndiscovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have shown great potential in natural\nlanguage-driven molecule discovery. However, existing datasets and benchmarks\nfor molecule-text alignment are predominantly built on a one-to-one mapping,\nmeasuring LLMs' ability to retrieve a single, pre-defined answer, rather than\ntheir creative potential to generate diverse, yet equally valid, molecular\ncandidates. To address this critical gap, we propose Speak-to-Structure\n(S^2-Bench}), the first benchmark to evaluate LLMs in open-domain natural\nlanguage-driven molecule generation. S^2-Bench is specifically designed for\none-to-many relationships, challenging LLMs to demonstrate genuine molecular\nunderstanding and generation capabilities. Our benchmark includes three key\ntasks: molecule editing (MolEdit), molecule optimization (MolOpt), and\ncustomized molecule generation (MolCustom), each probing a different aspect of\nmolecule discovery. We also introduce OpenMolIns, a large-scale instruction\ntuning dataset that enables Llama-3.1-8B to surpass the most powerful LLMs like\nGPT-4o and Claude-3.5 on S^2-Bench. Our comprehensive evaluation of 28 LLMs\nshifts the focus from simple pattern recall to realistic molecular design,\npaving the way for more capable LLMs in natural language-driven molecule\ndiscovery."
                },
                "authors": [
                    {
                        "name": "Jiatong Li"
                    },
                    {
                        "name": "Junxian Li"
                    },
                    {
                        "name": "Weida Wang"
                    },
                    {
                        "name": "Yunqing Liu"
                    },
                    {
                        "name": "Changmeng Zheng"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Xiao-yong Wei"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "arxiv_comment": "Our codes and datasets are available through\n  https://github.com/phenixace/TOMG-Bench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14642v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14642v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23657v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23657v3",
                "updated": "2025-09-15T17:26:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    26,
                    37,
                    0,
                    258,
                    0
                ],
                "published": "2025-05-29T17:07:24Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    7,
                    24,
                    3,
                    149,
                    0
                ],
                "title": "Active Layer-Contrastive Decoding Reduces Hallucination in Large\n  Language Model Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Layer-Contrastive Decoding Reduces Hallucination in Large\n  Language Model Generation"
                },
                "summary": "Recent decoding methods improve the factuality of large language models\n(LLMs) by refining how the next token is selected during generation. These\nmethods typically operate at the token level, leveraging internal\nrepresentations to suppress superficial patterns. Nevertheless, LLMs remain\nprone to hallucinations, especially over longer contexts. In this paper, we\npropose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy\nthat actively decides when to apply contrasting layers during generation. By\ncasting decoding as a sequential decision-making problem, ActLCD employs a\nreinforcement learning policy guided by a reward-aware classifier to optimize\nfactuality beyond the token level. Our experiments demonstrate that ActLCD\nsurpasses state-of-the-art methods across five benchmarks, showcasing its\neffectiveness in mitigating hallucinations in diverse generation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent decoding methods improve the factuality of large language models\n(LLMs) by refining how the next token is selected during generation. These\nmethods typically operate at the token level, leveraging internal\nrepresentations to suppress superficial patterns. Nevertheless, LLMs remain\nprone to hallucinations, especially over longer contexts. In this paper, we\npropose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy\nthat actively decides when to apply contrasting layers during generation. By\ncasting decoding as a sequential decision-making problem, ActLCD employs a\nreinforcement learning policy guided by a reward-aware classifier to optimize\nfactuality beyond the token level. Our experiments demonstrate that ActLCD\nsurpasses state-of-the-art methods across five benchmarks, showcasing its\neffectiveness in mitigating hallucinations in diverse generation scenarios."
                },
                "authors": [
                    {
                        "name": "Hongxiang Zhang"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Muhao Chen"
                    },
                    {
                        "name": "Tianyi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhang"
                },
                "author": "Tianyi Zhang",
                "arxiv_comment": "19 pages, 3 figures, EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23657v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23657v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12164v1",
                "updated": "2025-09-15T17:26:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    26,
                    36,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T17:26:36Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    26,
                    36,
                    0,
                    258,
                    0
                ],
                "title": "Anomalous electron heating in laboratory magnetized quasi-perpendicular\n  collisionless shocks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anomalous electron heating in laboratory magnetized quasi-perpendicular\n  collisionless shocks"
                },
                "summary": "We present laboratory results from supercritical, magnetized collisionless\nshock experiments ($M_A \\lesssim 10$, $\\beta\\sim 1$). We report the first\nobservation of fully-developed shocks ($R=4$ compression ratio and a downstream\nregion decoupled from the piston) after seven upstream ion gyration periods. A\nfoot ahead of the shock exhibits super-adiabatic electron and ion heating. We\nmeasure the electron temperature $T_e = 115$ eV and ion temperature $T_i = 15$\neV upstream of the shock; whereas, downstream, we measure $T_e=390$ eV and\ninfer $T_i=340$ eV, consistent with both Thomson scattering ion-acoustic wave\nspectral broadening and Rankine-Hugoniot conditions. The downstream electron\ntemperature has a $30$-percent excess from adiabatic and collisional\nelectron-ion heating, implying significant collisionless anomalous electron\nheating. Furthermore, downstream electrons and ions are in equipartition, with\na unity electron-ion temperature ratio $T_e/T_i = 1.2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present laboratory results from supercritical, magnetized collisionless\nshock experiments ($M_A \\lesssim 10$, $\\beta\\sim 1$). We report the first\nobservation of fully-developed shocks ($R=4$ compression ratio and a downstream\nregion decoupled from the piston) after seven upstream ion gyration periods. A\nfoot ahead of the shock exhibits super-adiabatic electron and ion heating. We\nmeasure the electron temperature $T_e = 115$ eV and ion temperature $T_i = 15$\neV upstream of the shock; whereas, downstream, we measure $T_e=390$ eV and\ninfer $T_i=340$ eV, consistent with both Thomson scattering ion-acoustic wave\nspectral broadening and Rankine-Hugoniot conditions. The downstream electron\ntemperature has a $30$-percent excess from adiabatic and collisional\nelectron-ion heating, implying significant collisionless anomalous electron\nheating. Furthermore, downstream electrons and ions are in equipartition, with\na unity electron-ion temperature ratio $T_e/T_i = 1.2$."
                },
                "authors": [
                    {
                        "name": "V. Valenzuela-Villaseca"
                    },
                    {
                        "name": "S. Totorica"
                    },
                    {
                        "name": "J. Griff-McMahon"
                    },
                    {
                        "name": "L. -J. Chen"
                    },
                    {
                        "name": "S. Malko"
                    },
                    {
                        "name": "P. V. Heuer"
                    },
                    {
                        "name": "P. Pongkitiwanichakul"
                    },
                    {
                        "name": "W. Fox"
                    },
                    {
                        "name": "D. B. Schaeffer"
                    }
                ],
                "author_detail": {
                    "name": "D. B. Schaeffer"
                },
                "author": "D. B. Schaeffer",
                "arxiv_comment": "Main body: 7 pages, 4 Figures, 59 references. Supplemental material:\n  12 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12162v1",
                "updated": "2025-09-15T17:25:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    25,
                    12,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T17:25:12Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    25,
                    12,
                    0,
                    258,
                    0
                ],
                "title": "Quantifying Mental States in Work Environment: Mathematical Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Mental States in Work Environment: Mathematical Perspectives"
                },
                "summary": "This article presents a study involving 87 participants exposed to a\nstressful scenario in a virtual reality (VR) environment. An algorithm was\ndeveloped to assign a positive or negative valence based on questionnaire\nresponses. EEG signals were recorded, and a k-nearest neighbors (KNN) algorithm\nwas trained to infer emotional valence from these signals. Our objective is to\nfurther develop mathematical models capable of describing the dynamic evolution\nof emotional and mental states.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article presents a study involving 87 participants exposed to a\nstressful scenario in a virtual reality (VR) environment. An algorithm was\ndeveloped to assign a positive or negative valence based on questionnaire\nresponses. EEG signals were recorded, and a k-nearest neighbors (KNN) algorithm\nwas trained to infer emotional valence from these signals. Our objective is to\nfurther develop mathematical models capable of describing the dynamic evolution\nof emotional and mental states."
                },
                "authors": [
                    {
                        "name": "Aymen Balti"
                    },
                    {
                        "name": "Assane Wade"
                    },
                    {
                        "name": "Abdelatif Oujbara"
                    },
                    {
                        "name": "M. A."
                    },
                    {
                        "name": "Aziz-Alaoui"
                    },
                    {
                        "name": "Hicham Bellarabi"
                    },
                    {
                        "name": "Frederic Dutertre"
                    },
                    {
                        "name": "Benjamin Ambrosio"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Ambrosio"
                },
                "author": "Benjamin Ambrosio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12159v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12159v1",
                "updated": "2025-09-15T17:23:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    23,
                    46,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T17:23:46Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    23,
                    46,
                    0,
                    258,
                    0
                ],
                "title": "EfficientUICoder: Efficient MLLM-based UI Code Generation via Input and\n  Output Token Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EfficientUICoder: Efficient MLLM-based UI Code Generation via Input and\n  Output Token Compression"
                },
                "summary": "Multimodal Large Language Models have demonstrated exceptional performance in\nUI2Code tasks, significantly enhancing website development efficiency. However,\nthese tasks incur substantially higher computational overhead than traditional\ncode generation due to the large number of input image tokens and extensive\noutput code tokens required. Our comprehensive study identifies significant\nredundancies in both image and code tokens that exacerbate computational\ncomplexity and hinder focus on key UI elements, resulting in excessively\nlengthy and often invalid HTML files. We propose EfficientUICoder, a\ncompression framework for efficient UI code generation with three key\ncomponents. First, Element and Layout-aware Token Compression preserves\nessential UI information by detecting element regions and constructing UI\nelement trees. Second, Region-aware Token Refinement leverages attention scores\nto discard low-attention tokens from selected regions while integrating\nhigh-attention tokens from unselected regions. Third, Adaptive Duplicate Token\nSuppression dynamically reduces repetitive generation by tracking HTML/CSS\nstructure frequencies and applying exponential penalties. Extensive experiments\nshow EfficientUICoderachieves a 55%-60% compression ratio without compromising\nwebpage quality and delivers superior efficiency improvements: reducing\ncomputational cost by 44.9%, generated tokens by 41.4%, prefill time by 46.6%,\nand inference time by 48.8% on 34B-level MLLMs. Code is available at\nhttps://github.com/WebPAI/EfficientUICoder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models have demonstrated exceptional performance in\nUI2Code tasks, significantly enhancing website development efficiency. However,\nthese tasks incur substantially higher computational overhead than traditional\ncode generation due to the large number of input image tokens and extensive\noutput code tokens required. Our comprehensive study identifies significant\nredundancies in both image and code tokens that exacerbate computational\ncomplexity and hinder focus on key UI elements, resulting in excessively\nlengthy and often invalid HTML files. We propose EfficientUICoder, a\ncompression framework for efficient UI code generation with three key\ncomponents. First, Element and Layout-aware Token Compression preserves\nessential UI information by detecting element regions and constructing UI\nelement trees. Second, Region-aware Token Refinement leverages attention scores\nto discard low-attention tokens from selected regions while integrating\nhigh-attention tokens from unselected regions. Third, Adaptive Duplicate Token\nSuppression dynamically reduces repetitive generation by tracking HTML/CSS\nstructure frequencies and applying exponential penalties. Extensive experiments\nshow EfficientUICoderachieves a 55%-60% compression ratio without compromising\nwebpage quality and delivers superior efficiency improvements: reducing\ncomputational cost by 44.9%, generated tokens by 41.4%, prefill time by 46.6%,\nand inference time by 48.8% on 34B-level MLLMs. Code is available at\nhttps://github.com/WebPAI/EfficientUICoder."
                },
                "authors": [
                    {
                        "name": "Jingyu Xiao"
                    },
                    {
                        "name": "Zhongyi Zhang"
                    },
                    {
                        "name": "Yuxuan Wan"
                    },
                    {
                        "name": "Yintong Huo"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Michael R. Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Lyu"
                },
                "author": "Michael R. Lyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12159v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12158v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12158v1",
                "updated": "2025-09-15T17:22:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    22,
                    30,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T17:22:30Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    22,
                    30,
                    0,
                    258,
                    0
                ],
                "title": "Pun Unintended: LLMs and the Illusion of Humor Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pun Unintended: LLMs and the Illusion of Humor Understanding"
                },
                "summary": "Puns are a form of humorous wordplay that exploits polysemy and phonetic\nsimilarity. While LLMs have shown promise in detecting puns, we show in this\npaper that their understanding often remains shallow, lacking the nuanced grasp\ntypical of human interpretation. By systematically analyzing and reformulating\nexisting pun benchmarks, we demonstrate how subtle changes in puns are\nsufficient to mislead LLMs. Our contributions include comprehensive and nuanced\npun detection benchmarks, human evaluation across recent LLMs, and an analysis\nof the robustness challenges these models face in processing puns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Puns are a form of humorous wordplay that exploits polysemy and phonetic\nsimilarity. While LLMs have shown promise in detecting puns, we show in this\npaper that their understanding often remains shallow, lacking the nuanced grasp\ntypical of human interpretation. By systematically analyzing and reformulating\nexisting pun benchmarks, we demonstrate how subtle changes in puns are\nsufficient to mislead LLMs. Our contributions include comprehensive and nuanced\npun detection benchmarks, human evaluation across recent LLMs, and an analysis\nof the robustness challenges these models face in processing puns."
                },
                "authors": [
                    {
                        "name": "Alessandro Zangari"
                    },
                    {
                        "name": "Matteo Marcuzzo"
                    },
                    {
                        "name": "Andrea Albarelli"
                    },
                    {
                        "name": "Mohammad Taher Pilehvar"
                    },
                    {
                        "name": "Jose Camacho-Collados"
                    }
                ],
                "author_detail": {
                    "name": "Jose Camacho-Collados"
                },
                "author": "Jose Camacho-Collados",
                "arxiv_comment": "Accepted to EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12158v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12157v1",
                "updated": "2025-09-15T17:22:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    22,
                    22,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T17:22:22Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    22,
                    22,
                    0,
                    258,
                    0
                ],
                "title": "Chandra Large Project Observations of the Supernova Remnant N132D:\n  Measuring the Expansion of the Forward Shock",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chandra Large Project Observations of the Supernova Remnant N132D:\n  Measuring the Expansion of the Forward Shock"
                },
                "summary": "We present results from the Chandra X-ray Observatory Large Project (878 ks\nin 28 observations) of the Large Magellanic Cloud supernova remnant N132D. We\nmeasure the expansion of the forward shock in the bright southern rim to be\n$0.\\!^{\\prime\\prime}10 \\pm 0.\\!^{\\prime\\prime}02$ over the $\\sim14.5$ yr\nbaseline, which corresponds to a velocity of $1620\\pm400~\\mathrm{km\\,s^{-1}}$\nafter accounting for several instrumental effects. We measure an expansion of\n$0.\\!^{\\prime\\prime}23 \\pm 0.\\!^{\\prime\\prime}02$ and a shock velocity of\n$3840\\pm260~\\mathrm{km\\,s^{-1}}$ for two features in an apparent blowout region\nin the northeast. The emission-measure-weighted average temperature inferred\nfrom X-ray spectral fits to regions in the southern rim is $0.95\\pm0.17$ keV,\nconsistent with the electron temperature implied by the shock velocity after\naccounting for Coulomb equilibration and adiabatic expansion. In contrast, the\nemission-measure-weighted average temperature for the northeast region is\n$0.77\\pm0.04$ keV, which is significantly lower than the value inferred from\nthe shock velocity. We fit 1-D evolutionary models for the shock in the\nsouthern rim and northeast region, using the measured radius and propagation\nvelocity into a constant density and power-law profile circumstellar medium. We\nfind good agreement with the age of $\\sim2500$ years derived from optical\nexpansion measurements for explosion energies of $1.5-3.0 \\times\n10^{51}\\,\\mathrm{erg}$, ejecta masses of $2-6 \\,\\mathrm{M_{\\odot}}$ and ambient\nmedium densities of $\\sim0.33-0.66$ $\\mathrm{amu~cm}^{-3}$ in the south and\n$\\sim0.01-0.02$ $\\mathrm{amu~cm}^{-3}$ in the northeast assuming a constant\ndensity medium. These results are consistent with previous studies that\nsuggested the progenitor of N132D was an energetic supernova that exploded into\na pre-existing cavity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present results from the Chandra X-ray Observatory Large Project (878 ks\nin 28 observations) of the Large Magellanic Cloud supernova remnant N132D. We\nmeasure the expansion of the forward shock in the bright southern rim to be\n$0.\\!^{\\prime\\prime}10 \\pm 0.\\!^{\\prime\\prime}02$ over the $\\sim14.5$ yr\nbaseline, which corresponds to a velocity of $1620\\pm400~\\mathrm{km\\,s^{-1}}$\nafter accounting for several instrumental effects. We measure an expansion of\n$0.\\!^{\\prime\\prime}23 \\pm 0.\\!^{\\prime\\prime}02$ and a shock velocity of\n$3840\\pm260~\\mathrm{km\\,s^{-1}}$ for two features in an apparent blowout region\nin the northeast. The emission-measure-weighted average temperature inferred\nfrom X-ray spectral fits to regions in the southern rim is $0.95\\pm0.17$ keV,\nconsistent with the electron temperature implied by the shock velocity after\naccounting for Coulomb equilibration and adiabatic expansion. In contrast, the\nemission-measure-weighted average temperature for the northeast region is\n$0.77\\pm0.04$ keV, which is significantly lower than the value inferred from\nthe shock velocity. We fit 1-D evolutionary models for the shock in the\nsouthern rim and northeast region, using the measured radius and propagation\nvelocity into a constant density and power-law profile circumstellar medium. We\nfind good agreement with the age of $\\sim2500$ years derived from optical\nexpansion measurements for explosion energies of $1.5-3.0 \\times\n10^{51}\\,\\mathrm{erg}$, ejecta masses of $2-6 \\,\\mathrm{M_{\\odot}}$ and ambient\nmedium densities of $\\sim0.33-0.66$ $\\mathrm{amu~cm}^{-3}$ in the south and\n$\\sim0.01-0.02$ $\\mathrm{amu~cm}^{-3}$ in the northeast assuming a constant\ndensity medium. These results are consistent with previous studies that\nsuggested the progenitor of N132D was an energetic supernova that exploded into\na pre-existing cavity."
                },
                "authors": [
                    {
                        "name": "Xi Long"
                    },
                    {
                        "name": "Paul P. Plucinsky"
                    },
                    {
                        "name": "Terrance J. Gaetz"
                    },
                    {
                        "name": "Vinay L. Kashyap"
                    },
                    {
                        "name": "Aya Bamba"
                    },
                    {
                        "name": "William P. Blair"
                    },
                    {
                        "name": "Daniel Castro"
                    },
                    {
                        "name": "Adam R. Foster"
                    },
                    {
                        "name": "Charles J. Law"
                    },
                    {
                        "name": "Dan Milisavljevic"
                    },
                    {
                        "name": "Eric Miller"
                    },
                    {
                        "name": "Daniel J. Patnaude"
                    },
                    {
                        "name": "Manami Sasaki"
                    },
                    {
                        "name": "Hidetoshi Sano"
                    },
                    {
                        "name": "Piyush Sharda"
                    },
                    {
                        "name": "Benjamin F. Williams"
                    },
                    {
                        "name": "Brian J. Williams"
                    },
                    {
                        "name": "Hiroya Yamaguchi"
                    }
                ],
                "author_detail": {
                    "name": "Hiroya Yamaguchi"
                },
                "author": "Hiroya Yamaguchi",
                "arxiv_comment": "Accepted for publication in ApJ, 31 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12155v1",
                "updated": "2025-09-15T17:21:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    21,
                    22,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T17:21:22Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    21,
                    22,
                    0,
                    258,
                    0
                ],
                "title": "LoRA-fine-tuned Large Vision Models for Automated Assessment of\n  Post-SBRT Lung Injury",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA-fine-tuned Large Vision Models for Automated Assessment of\n  Post-SBRT Lung Injury"
                },
                "summary": "This study investigates the efficacy of Low-Rank Adaptation (LoRA) for\nfine-tuning large Vision Models, DinoV2 and SwinV2, to diagnose\nRadiation-Induced Lung Injury (RILI) from X-ray CT scans following Stereotactic\nBody Radiation Therapy (SBRT). To evaluate the robustness and efficiency of\nthis approach, we compare LoRA with traditional full fine-tuning and\ninference-only (no fine-tuning) methods. Cropped images of two sizes (50 mm3\nand 75 mm3), centered at the treatment isocenter, in addition to different\nadaptation techniques for adapting the 2D LVMs for 3D data were used to\ndetermine the sensitivity of the models to spatial context. Experimental\nresults show that LoRA achieves comparable or superior performance to\ntraditional fine-tuning while significantly reducing computational costs and\ntraining times by requiring fewer trainable parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the efficacy of Low-Rank Adaptation (LoRA) for\nfine-tuning large Vision Models, DinoV2 and SwinV2, to diagnose\nRadiation-Induced Lung Injury (RILI) from X-ray CT scans following Stereotactic\nBody Radiation Therapy (SBRT). To evaluate the robustness and efficiency of\nthis approach, we compare LoRA with traditional full fine-tuning and\ninference-only (no fine-tuning) methods. Cropped images of two sizes (50 mm3\nand 75 mm3), centered at the treatment isocenter, in addition to different\nadaptation techniques for adapting the 2D LVMs for 3D data were used to\ndetermine the sensitivity of the models to spatial context. Experimental\nresults show that LoRA achieves comparable or superior performance to\ntraditional fine-tuning while significantly reducing computational costs and\ntraining times by requiring fewer trainable parameters."
                },
                "authors": [
                    {
                        "name": "M. Bolhassani"
                    },
                    {
                        "name": "B. Veasey"
                    },
                    {
                        "name": "E. Daugherty"
                    },
                    {
                        "name": "S. Keltner"
                    },
                    {
                        "name": "N. Kumar"
                    },
                    {
                        "name": "N. Dunlap"
                    },
                    {
                        "name": "A. Amini"
                    }
                ],
                "author_detail": {
                    "name": "A. Amini"
                },
                "author": "A. Amini",
                "arxiv_comment": "5 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12152v1",
                "updated": "2025-09-15T17:17:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    17,
                    26,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T17:17:26Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    17,
                    26,
                    0,
                    258,
                    0
                ],
                "title": "Beyond PII: How Users Attempt to Estimate and Mitigate Implicit LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond PII: How Users Attempt to Estimate and Mitigate Implicit LLM\n  Inference"
                },
                "summary": "Large Language Models (LLMs) such as ChatGPT can infer personal attributes\nfrom seemingly innocuous text, raising privacy risks beyond memorized data\nleakage. While prior work has demonstrated these risks, little is known about\nhow users estimate and respond. We conducted a survey with 240 U.S.\nparticipants who judged text snippets for inference risks, reported concern\nlevels, and attempted rewrites to block inference. We compared their rewrites\nwith those generated by ChatGPT and Rescriber, a state-of-the-art sanitization\ntool. Results show that participants struggled to anticipate inference,\nperforming a little better than chance. User rewrites were effective in just\n28\\% of cases - better than Rescriber but worse than ChatGPT. We examined our\nparticipants' rewriting strategies, and observed that while paraphrasing was\nthe most common strategy it is also the least effective; instead abstraction\nand adding ambiguity were more successful. Our work highlights the importance\nof inference-aware design in LLM interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as ChatGPT can infer personal attributes\nfrom seemingly innocuous text, raising privacy risks beyond memorized data\nleakage. While prior work has demonstrated these risks, little is known about\nhow users estimate and respond. We conducted a survey with 240 U.S.\nparticipants who judged text snippets for inference risks, reported concern\nlevels, and attempted rewrites to block inference. We compared their rewrites\nwith those generated by ChatGPT and Rescriber, a state-of-the-art sanitization\ntool. Results show that participants struggled to anticipate inference,\nperforming a little better than chance. User rewrites were effective in just\n28\\% of cases - better than Rescriber but worse than ChatGPT. We examined our\nparticipants' rewriting strategies, and observed that while paraphrasing was\nthe most common strategy it is also the least effective; instead abstraction\nand adding ambiguity were more successful. Our work highlights the importance\nof inference-aware design in LLM interactions."
                },
                "authors": [
                    {
                        "name": "Synthia Wang"
                    },
                    {
                        "name": "Sai Teja Peddinti"
                    },
                    {
                        "name": "Nina Taft"
                    },
                    {
                        "name": "Nick Feamster"
                    }
                ],
                "author_detail": {
                    "name": "Nick Feamster"
                },
                "author": "Nick Feamster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11085v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11085v2",
                "updated": "2025-09-15T17:16:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    16,
                    18,
                    0,
                    258,
                    0
                ],
                "published": "2025-08-14T21:50:31Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    21,
                    50,
                    31,
                    3,
                    226,
                    0
                ],
                "title": "A learning-driven automatic planning framework for proton PBS treatments\n  of H&N cancers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A learning-driven automatic planning framework for proton PBS treatments\n  of H&N cancers"
                },
                "summary": "Proton pencil beam scanning (PBS) treatment planning for head & neck (H&N)\ncancers involves numerous conflicting objectives, requiring iterative objective\nparameter adjustments to balance multiple clinical goals. We propose a\nlearning-driven inverse optimizer and integrate it into a proximal policy\noptimization (PPO)-based planning framework to automatically generate\nhigh-quality plans for patients with diverse treatment requirements. The\ninverse optimizer is a learning-to-optimize (L2O) method that predicts update\nsteps by learning from task-specific data distributions. For the first time,\nlong-context processing techniques developed for large language models (LLMs)\nare utilized to address the scalability limitations of existing L2O methods,\nenabling simultaneous optimization over a substantially large set of variables.\nThe PPO framework functions as an outer-loop virtual planner, autonomously\nadjusting objective parameters through a policy network, and the inner-loop L2O\ninverse optimizer computes machine-deliverable spot monitor unit (MU) values\nbased on the PPO-refined objectives. Moreover, a Swin UnetR dose predictor is\ntrained with prescription- and beam-specific information to estimate the\ninitial objective parameters. In our experiments, total 97 patients with\nbilateral or ipsilateral H&N cancers are collected for training and testing.\nCompared with the second-order gradient-based methods, our L2O optimizer\nimproves the effectiveness and efficiency of the time-consuming inverse\noptimization by 22.97% and 36.41%, respectively, and in conjunction with the\nPPO-based virtual planner, plans are generated within clinically acceptable\ntimes, i.e. 2.55 hours in average, and shows improved or comparable\norgans-at-risk sparing with superior target coverage compared with\nhuman-generated plans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proton pencil beam scanning (PBS) treatment planning for head & neck (H&N)\ncancers involves numerous conflicting objectives, requiring iterative objective\nparameter adjustments to balance multiple clinical goals. We propose a\nlearning-driven inverse optimizer and integrate it into a proximal policy\noptimization (PPO)-based planning framework to automatically generate\nhigh-quality plans for patients with diverse treatment requirements. The\ninverse optimizer is a learning-to-optimize (L2O) method that predicts update\nsteps by learning from task-specific data distributions. For the first time,\nlong-context processing techniques developed for large language models (LLMs)\nare utilized to address the scalability limitations of existing L2O methods,\nenabling simultaneous optimization over a substantially large set of variables.\nThe PPO framework functions as an outer-loop virtual planner, autonomously\nadjusting objective parameters through a policy network, and the inner-loop L2O\ninverse optimizer computes machine-deliverable spot monitor unit (MU) values\nbased on the PPO-refined objectives. Moreover, a Swin UnetR dose predictor is\ntrained with prescription- and beam-specific information to estimate the\ninitial objective parameters. In our experiments, total 97 patients with\nbilateral or ipsilateral H&N cancers are collected for training and testing.\nCompared with the second-order gradient-based methods, our L2O optimizer\nimproves the effectiveness and efficiency of the time-consuming inverse\noptimization by 22.97% and 36.41%, respectively, and in conjunction with the\nPPO-based virtual planner, plans are generated within clinically acceptable\ntimes, i.e. 2.55 hours in average, and shows improved or comparable\norgans-at-risk sparing with superior target coverage compared with\nhuman-generated plans."
                },
                "authors": [
                    {
                        "name": "Qingqing Wang"
                    },
                    {
                        "name": "Liqiang Xiao"
                    },
                    {
                        "name": "Chang Chang"
                    }
                ],
                "author_detail": {
                    "name": "Chang Chang"
                },
                "author": "Chang Chang",
                "arxiv_comment": "27 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11085v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11085v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12145v1",
                "updated": "2025-09-15T17:11:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    11,
                    6,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T17:11:06Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    11,
                    6,
                    0,
                    258,
                    0
                ],
                "title": "Open-ended Hierarchical Streaming Video Understanding with Vision\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-ended Hierarchical Streaming Video Understanding with Vision\n  Language Models"
                },
                "summary": "We introduce Hierarchical Streaming Video Understanding, a task that combines\nonline temporal action localization with free-form description generation.\nGiven the scarcity of datasets with hierarchical and fine-grained temporal\nannotations, we demonstrate that LLMs can effectively group atomic actions into\nhigher-level events, enriching existing datasets. We then propose OpenHOUSE\n(Open-ended Hierarchical Online Understanding System for Events), which extends\nstreaming action perception beyond action classification. OpenHOUSE features a\nspecialized streaming module that accurately detects boundaries between closely\nadjacent actions, nearly doubling the performance of direct extensions of\nexisting methods. We envision the future of streaming action perception in the\nintegration of powerful generative models, with OpenHOUSE representing a key\nstep in that direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Hierarchical Streaming Video Understanding, a task that combines\nonline temporal action localization with free-form description generation.\nGiven the scarcity of datasets with hierarchical and fine-grained temporal\nannotations, we demonstrate that LLMs can effectively group atomic actions into\nhigher-level events, enriching existing datasets. We then propose OpenHOUSE\n(Open-ended Hierarchical Online Understanding System for Events), which extends\nstreaming action perception beyond action classification. OpenHOUSE features a\nspecialized streaming module that accurately detects boundaries between closely\nadjacent actions, nearly doubling the performance of direct extensions of\nexisting methods. We envision the future of streaming action perception in the\nintegration of powerful generative models, with OpenHOUSE representing a key\nstep in that direction."
                },
                "authors": [
                    {
                        "name": "Hyolim Kang"
                    },
                    {
                        "name": "Yunsu Park"
                    },
                    {
                        "name": "Youngbeom Yoo"
                    },
                    {
                        "name": "Yeeun Choi"
                    },
                    {
                        "name": "Seon Joo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Seon Joo Kim"
                },
                "author": "Seon Joo Kim",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09995v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09995v2",
                "updated": "2025-09-15T17:08:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    8,
                    33,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-12T06:35:40Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    6,
                    35,
                    40,
                    4,
                    255,
                    0
                ],
                "title": "QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have demonstrated impressive\ncapabilities in financial reasoning and market understanding. Multi-agent LLM\nframeworks such as TradingAgent and FINMEM augment these models to long-horizon\ninvestment tasks, leveraging fundamental and sentiment-based inputs for\nstrategic decision-making. However, such systems are ill-suited for the\nhigh-speed, precision-critical demands of High-Frequency Trading (HFT). HFT\nrequires rapid, risk-aware decisions based on structured, short-horizon\nsignals, including technical indicators, chart patterns, and trend-based\nfeatures, distinct from the long-term semantic reasoning typical of traditional\nfinancial LLM applications. To this end, we introduce QuantAgent, the first\nmulti-agent LLM framework explicitly designed for high-frequency algorithmic\ntrading. The system decomposes trading into four specialized agents, Indicator,\nPattern, Trend, and Risk, each equipped with domain-specific tools and\nstructured reasoning capabilities to capture distinct aspects of market\ndynamics over short temporal windows. In zero-shot evaluations across ten\nfinancial instruments, including Bitcoin and Nasdaq futures, QuantAgent\ndemonstrates superior performance in both predictive accuracy and cumulative\nreturn over 4-hour trading intervals, outperforming random prediction\nbaselines. Our findings suggest that combining structured financial priors with\nlanguage-native reasoning unlocks new potential for traceable, real-time\ndecision systems in high-frequency financial markets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have demonstrated impressive\ncapabilities in financial reasoning and market understanding. Multi-agent LLM\nframeworks such as TradingAgent and FINMEM augment these models to long-horizon\ninvestment tasks, leveraging fundamental and sentiment-based inputs for\nstrategic decision-making. However, such systems are ill-suited for the\nhigh-speed, precision-critical demands of High-Frequency Trading (HFT). HFT\nrequires rapid, risk-aware decisions based on structured, short-horizon\nsignals, including technical indicators, chart patterns, and trend-based\nfeatures, distinct from the long-term semantic reasoning typical of traditional\nfinancial LLM applications. To this end, we introduce QuantAgent, the first\nmulti-agent LLM framework explicitly designed for high-frequency algorithmic\ntrading. The system decomposes trading into four specialized agents, Indicator,\nPattern, Trend, and Risk, each equipped with domain-specific tools and\nstructured reasoning capabilities to capture distinct aspects of market\ndynamics over short temporal windows. In zero-shot evaluations across ten\nfinancial instruments, including Bitcoin and Nasdaq futures, QuantAgent\ndemonstrates superior performance in both predictive accuracy and cumulative\nreturn over 4-hour trading intervals, outperforming random prediction\nbaselines. Our findings suggest that combining structured financial priors with\nlanguage-native reasoning unlocks new potential for traceable, real-time\ndecision systems in high-frequency financial markets."
                },
                "authors": [
                    {
                        "name": "Fei Xiong"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Aosong Feng"
                    },
                    {
                        "name": "Siqi Sun"
                    },
                    {
                        "name": "Chenyu You"
                    }
                ],
                "author_detail": {
                    "name": "Chenyu You"
                },
                "author": "Chenyu You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09995v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09995v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12141v2",
                "updated": "2025-09-16T01:51:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    1,
                    51,
                    29,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-15T17:05:59Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    5,
                    59,
                    0,
                    258,
                    0
                ],
                "title": "When MoE Meets Blockchain: A Trustworthy Distributed Framework of Large\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When MoE Meets Blockchain: A Trustworthy Distributed Framework of Large\n  Models"
                },
                "summary": "As an enabling architecture of Large Models (LMs), Mixture of Experts (MoE)\nhas become prevalent thanks to its sparsely-gated mechanism, which lowers\ncomputational overhead while maintaining learning performance comparable to\ndense LMs. The essence of MoE lies in utilizing a group of neural networks\n(called experts) with each specializing in different types of tasks, along with\na trainable gating network that selectively activates a subset of these experts\nto handle specific tasks. Traditional cloud-based MoE encounters challenges\nsuch as prolonged response latency, high bandwidth consumption, and data\nprivacy leakage. To address these issues, researchers have proposed to deploy\nMoE over distributed edge networks. However, a key concern of distributed MoE\nframeworks is the lack of trust in data interactions among distributed experts\nwithout the surveillance of any trusted authority, and thereby prone to\npotential attacks such as data manipulation. In response to the security issues\nof traditional distributed MoE, we propose a blockchain-aided trustworthy MoE\n(B-MoE) framework that consists of three layers: the edge layer, the blockchain\nlayer, and the storage layer. In this framework, the edge layer employs the\nactivated experts downloaded from the storage layer to process the learning\ntasks, while the blockchain layer functions as a decentralized trustworthy\nnetwork to trace, verify, and record the computational results of the experts\nfrom the edge layer. The experimental results demonstrate that B-MoE is more\nrobust to data manipulation attacks than traditional distributed MoE during\nboth the training and inference processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As an enabling architecture of Large Models (LMs), Mixture of Experts (MoE)\nhas become prevalent thanks to its sparsely-gated mechanism, which lowers\ncomputational overhead while maintaining learning performance comparable to\ndense LMs. The essence of MoE lies in utilizing a group of neural networks\n(called experts) with each specializing in different types of tasks, along with\na trainable gating network that selectively activates a subset of these experts\nto handle specific tasks. Traditional cloud-based MoE encounters challenges\nsuch as prolonged response latency, high bandwidth consumption, and data\nprivacy leakage. To address these issues, researchers have proposed to deploy\nMoE over distributed edge networks. However, a key concern of distributed MoE\nframeworks is the lack of trust in data interactions among distributed experts\nwithout the surveillance of any trusted authority, and thereby prone to\npotential attacks such as data manipulation. In response to the security issues\nof traditional distributed MoE, we propose a blockchain-aided trustworthy MoE\n(B-MoE) framework that consists of three layers: the edge layer, the blockchain\nlayer, and the storage layer. In this framework, the edge layer employs the\nactivated experts downloaded from the storage layer to process the learning\ntasks, while the blockchain layer functions as a decentralized trustworthy\nnetwork to trace, verify, and record the computational results of the experts\nfrom the edge layer. The experimental results demonstrate that B-MoE is more\nrobust to data manipulation attacks than traditional distributed MoE during\nboth the training and inference processes."
                },
                "authors": [
                    {
                        "name": "Weihao Zhu"
                    },
                    {
                        "name": "Long Shi"
                    },
                    {
                        "name": "Kang Wei"
                    },
                    {
                        "name": "Zhen Mei"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Jiaheng Wang"
                    },
                    {
                        "name": "Jun Li"
                    }
                ],
                "author_detail": {
                    "name": "Jun Li"
                },
                "author": "Jun Li",
                "arxiv_comment": "We need to revise the content of this article",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22134v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22134v3",
                "updated": "2025-09-15T17:05:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    5,
                    18,
                    0,
                    258,
                    0
                ],
                "published": "2025-07-29T18:08:31Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    18,
                    8,
                    31,
                    1,
                    210,
                    0
                ],
                "title": "IntentFlow: Interactive Support for Communicating Intent with LLMs in\n  Writing Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IntentFlow: Interactive Support for Communicating Intent with LLMs in\n  Writing Tasks"
                },
                "summary": "Effective collaboration with generative AI systems requires users to clearly\ncommunicate their intents (intent-based outcome specification). Yet such\nintents are often underspecified and evolve during interaction, dynamic support\nfor intent communication is essential. Through a systematic literature review\nof 33 papers, we synthesize a structured understanding of intent communication,\nidentifying four key aspects: articulation, exploration, management, and\nsynchronization. Building on these findings, we derived design implications\nthat translate them into actionable design and implemented IntentFlow, a system\nfor LLM-based writing that realizes these implications through adjustable UIs,\nintent-to-output linking, and versioned refinement. A technical evaluation\n(N=60) and a within-subjects study (N=12) confirm that IntentFlow helps users\ndiscover, elaborate, and consolidate their intents into a curated set.\nInteraction logs further reveal a shift from reactive error correction to\nproactive intent refinement. Our work demonstrates how a system effectively\ndesigned to support these four communication aspects can substantially enhance\nhuman-LLM interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective collaboration with generative AI systems requires users to clearly\ncommunicate their intents (intent-based outcome specification). Yet such\nintents are often underspecified and evolve during interaction, dynamic support\nfor intent communication is essential. Through a systematic literature review\nof 33 papers, we synthesize a structured understanding of intent communication,\nidentifying four key aspects: articulation, exploration, management, and\nsynchronization. Building on these findings, we derived design implications\nthat translate them into actionable design and implemented IntentFlow, a system\nfor LLM-based writing that realizes these implications through adjustable UIs,\nintent-to-output linking, and versioned refinement. A technical evaluation\n(N=60) and a within-subjects study (N=12) confirm that IntentFlow helps users\ndiscover, elaborate, and consolidate their intents into a curated set.\nInteraction logs further reveal a shift from reactive error correction to\nproactive intent refinement. Our work demonstrates how a system effectively\ndesigned to support these four communication aspects can substantially enhance\nhuman-LLM interaction."
                },
                "authors": [
                    {
                        "name": "Yoonsu Kim"
                    },
                    {
                        "name": "Brandon Chin"
                    },
                    {
                        "name": "Kihoon Son"
                    },
                    {
                        "name": "Seoyoung Kim"
                    },
                    {
                        "name": "Juho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Juho Kim"
                },
                "author": "Juho Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22134v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22134v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12139v1",
                "updated": "2025-09-15T17:04:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    4,
                    54,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T17:04:54Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    4,
                    54,
                    0,
                    258,
                    0
                ],
                "title": "Hydrodynamical models of the $β$ Lyr A circumstellar disc",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hydrodynamical models of the $β$ Lyr A circumstellar disc"
                },
                "summary": "We study dynamics of circumstellar discs, with a focus on the $\\beta$ Lyrae A\nbinary system. This system with ongoing mass transfer has been extensively\nobserved, using photometry, spectroscopy and interferometry. All these\nobservations were recently interpreted using a radiation-transfer kinematic\nmodel.\n  We modified the analytical Shakura-Sunyaev models for a general opacity\nprescription, and derived radial profiles of various quantities. These profiles\nwere computed for the fixed accretion rate, $\\dot M = 2\\times\n10^{-5}\\,M_\\odot\\,{\\rm yr}^{-1}$, inferred from the observed rate of change of\nthe binary period. More general models were computed numerically, using\n1-dimensional radiative hydrodynamics, accounting for viscous, radiative as\nwell as irradiation terms. The initial conditions were taken from the\nanalytical models.\n  To achieve the accretion rate, the surface density~$\\Sigma$ must be much\nhigher (of the order of $10^4\\,{\\rm kg}\\,{\\rm m}^{-2}$ for the viscosity\nparameter $\\alpha = 0.1$) than in the kinematic model. Viscous dissipation and\nradiative cooling in the optically thick regime lead to a high midplane\ntemperature~$T$ (up to $10^5\\,{\\rm K}$). The accretion disc is still gas\npressure dominated with the opacity close to Kramers one. To reconcile\ntemperature profiles with observations, we had to distinguish three different\ntemperatures: midplane, atmospheric and irradiation. The latter two are\ncomparable to observations (30000 to 12000 $K$). We demonstrate that the aspect\nratio~$H$ of 0.08 can be achieved in a hydrostatic equilibrium, as opposed to\nprevious works considering the disc to be vertically unstable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study dynamics of circumstellar discs, with a focus on the $\\beta$ Lyrae A\nbinary system. This system with ongoing mass transfer has been extensively\nobserved, using photometry, spectroscopy and interferometry. All these\nobservations were recently interpreted using a radiation-transfer kinematic\nmodel.\n  We modified the analytical Shakura-Sunyaev models for a general opacity\nprescription, and derived radial profiles of various quantities. These profiles\nwere computed for the fixed accretion rate, $\\dot M = 2\\times\n10^{-5}\\,M_\\odot\\,{\\rm yr}^{-1}$, inferred from the observed rate of change of\nthe binary period. More general models were computed numerically, using\n1-dimensional radiative hydrodynamics, accounting for viscous, radiative as\nwell as irradiation terms. The initial conditions were taken from the\nanalytical models.\n  To achieve the accretion rate, the surface density~$\\Sigma$ must be much\nhigher (of the order of $10^4\\,{\\rm kg}\\,{\\rm m}^{-2}$ for the viscosity\nparameter $\\alpha = 0.1$) than in the kinematic model. Viscous dissipation and\nradiative cooling in the optically thick regime lead to a high midplane\ntemperature~$T$ (up to $10^5\\,{\\rm K}$). The accretion disc is still gas\npressure dominated with the opacity close to Kramers one. To reconcile\ntemperature profiles with observations, we had to distinguish three different\ntemperatures: midplane, atmospheric and irradiation. The latter two are\ncomparable to observations (30000 to 12000 $K$). We demonstrate that the aspect\nratio~$H$ of 0.08 can be achieved in a hydrostatic equilibrium, as opposed to\nprevious works considering the disc to be vertically unstable."
                },
                "authors": [
                    {
                        "name": "Kristián Vitovský"
                    },
                    {
                        "name": "Miroslav Brož"
                    }
                ],
                "author_detail": {
                    "name": "Miroslav Brož"
                },
                "arxiv_affiliation": "Charles University, Faculty of Mathematics and Physics, Institute of Astronomy",
                "author": "Miroslav Brož",
                "arxiv_comment": "17 pages, 26 figures, Accepted to Astronomy & Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12136v1",
                "updated": "2025-09-15T17:03:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    3,
                    15,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T17:03:15Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    3,
                    15,
                    0,
                    258,
                    0
                ],
                "title": "UniPar: A Unified LLM-Based Framework for Parallel and Accelerated Code\n  Translation in HPC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniPar: A Unified LLM-Based Framework for Parallel and Accelerated Code\n  Translation in HPC"
                },
                "summary": "Translating programs between various parallel programming languages is an\nimportant problem in the high-performance computing (HPC) community. Existing\ntools for this problem are either too narrow in scope and/or outdated. Recent\nexplosive growth in the popularity of large language models (LLMs) and their\nability to generate and translate code offers a potential alternative approach.\nToward that end, we first need to systematically evaluate the ability of LLMs\nto translate between parallel languages.\n  In this work, we introduce UniPar, a systematic evaluation framework for\nLLM-based parallel code translation. Specifically, in this work, we target\ntranslations between serial code, CUDA, and OpenMP. Our goal is to assess how\nwell current instruction-tuned LLMs -- specifically GPT-4o-mini and\nLLaMA-3.3-70B-Instruct -- can be used out of the box or enhanced through known\nstrategies. We evaluated four major usage modes: hyperparameter optimization\nfor decoding, zero- and few-shot prompting, supervised fine-tuning, and\niterative feedback through compiler-based repair. As a part of the evaluation,\nwe construct a new dataset called PARATRANS, covering both serial-to-parallel\ntranslation and cross-paradigm transformations.\n  Our findings reveal that while off-the-shelf models struggle under the\ndefault settings (e.g., GPT-4o-mini achieves only 46% compilation and 15%\nfunctional correctness), our UniPar methodology -- combining fine-tuning,\nhyperparameter tuning, and compiler-guided repair -- improves performance by up\nto 2X (69% compilation and 33% correctness). We believe that our findings will\nprovide useful insights for researchers to further improve LLMs for the\nparallel language translation problem.\n  UniPar source code and PARATRANS dataset are available at our GitHub\nrepository https://github.com/Scientific-Computing-Lab/UniPar_AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating programs between various parallel programming languages is an\nimportant problem in the high-performance computing (HPC) community. Existing\ntools for this problem are either too narrow in scope and/or outdated. Recent\nexplosive growth in the popularity of large language models (LLMs) and their\nability to generate and translate code offers a potential alternative approach.\nToward that end, we first need to systematically evaluate the ability of LLMs\nto translate between parallel languages.\n  In this work, we introduce UniPar, a systematic evaluation framework for\nLLM-based parallel code translation. Specifically, in this work, we target\ntranslations between serial code, CUDA, and OpenMP. Our goal is to assess how\nwell current instruction-tuned LLMs -- specifically GPT-4o-mini and\nLLaMA-3.3-70B-Instruct -- can be used out of the box or enhanced through known\nstrategies. We evaluated four major usage modes: hyperparameter optimization\nfor decoding, zero- and few-shot prompting, supervised fine-tuning, and\niterative feedback through compiler-based repair. As a part of the evaluation,\nwe construct a new dataset called PARATRANS, covering both serial-to-parallel\ntranslation and cross-paradigm transformations.\n  Our findings reveal that while off-the-shelf models struggle under the\ndefault settings (e.g., GPT-4o-mini achieves only 46% compilation and 15%\nfunctional correctness), our UniPar methodology -- combining fine-tuning,\nhyperparameter tuning, and compiler-guided repair -- improves performance by up\nto 2X (69% compilation and 33% correctness). We believe that our findings will\nprovide useful insights for researchers to further improve LLMs for the\nparallel language translation problem.\n  UniPar source code and PARATRANS dataset are available at our GitHub\nrepository https://github.com/Scientific-Computing-Lab/UniPar_AI."
                },
                "authors": [
                    {
                        "name": "Tomer Bitan"
                    },
                    {
                        "name": "Tal Kadosh"
                    },
                    {
                        "name": "Erel Kaplan"
                    },
                    {
                        "name": "Shira Meiri"
                    },
                    {
                        "name": "Le Chen"
                    },
                    {
                        "name": "Peter Morales"
                    },
                    {
                        "name": "Niranjan Hasabnis"
                    },
                    {
                        "name": "Gal Oren"
                    }
                ],
                "author_detail": {
                    "name": "Gal Oren"
                },
                "author": "Gal Oren",
                "arxiv_comment": "Accepted to IEEE HPEC conference 2025. 9 pages, incl references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12135v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12135v1",
                "updated": "2025-09-15T17:02:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    2,
                    20,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T17:02:20Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    2,
                    20,
                    0,
                    258,
                    0
                ],
                "title": "Evidencing preferential attachment in dependency network evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evidencing preferential attachment in dependency network evolution"
                },
                "summary": "Preferential attachment is often suggested to be the underlying mechanism of\nthe growth of a network, largely due to that many real networks are, to a\ncertain extent, scale-free. However, such attribution is usually made under\ndebatable practices of determining scale-freeness and when only snapshots of\nthe degree distribution are observed. In the presence of the evolution history\nof the network, modelling the increments of the evolution allows us to measure\npreferential attachment directly. Therefore, we propose a generalised linear\nmodel for such purpose, where the in-degrees and their increments are the\ncovariate and response, respectively. Not only are the parameters that describe\nthe preferential attachment directly incorporated, they also ensure that the\ntail heaviness of the asymptotic degree distribution is realistic. The Bayesian\napproach to inference enables the hierarchical version of the model to be\nimplemented naturally. The application to the dependency network of R packages\nreveals subtly different behaviours between new dependencies by new and\nexisting packages, and between addition and removal of dependencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preferential attachment is often suggested to be the underlying mechanism of\nthe growth of a network, largely due to that many real networks are, to a\ncertain extent, scale-free. However, such attribution is usually made under\ndebatable practices of determining scale-freeness and when only snapshots of\nthe degree distribution are observed. In the presence of the evolution history\nof the network, modelling the increments of the evolution allows us to measure\npreferential attachment directly. Therefore, we propose a generalised linear\nmodel for such purpose, where the in-degrees and their increments are the\ncovariate and response, respectively. Not only are the parameters that describe\nthe preferential attachment directly incorporated, they also ensure that the\ntail heaviness of the asymptotic degree distribution is realistic. The Bayesian\napproach to inference enables the hierarchical version of the model to be\nimplemented naturally. The application to the dependency network of R packages\nreveals subtly different behaviours between new dependencies by new and\nexisting packages, and between addition and removal of dependencies."
                },
                "authors": [
                    {
                        "name": "Clement Lee"
                    }
                ],
                "author_detail": {
                    "name": "Clement Lee"
                },
                "author": "Clement Lee",
                "arxiv_comment": "34 pages, 10 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12135v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12135v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12132v1",
                "updated": "2025-09-15T16:57:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    57,
                    25,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T16:57:25Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    57,
                    25,
                    0,
                    258,
                    0
                ],
                "title": "Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language\n  Models"
                },
                "summary": "Recent advances in text-only \"slow-thinking\" reasoning have prompted efforts\nto transfer this capability to vision-language models (VLMs), for training\nvisual reasoning models (\\textbf{VRMs}). owever, such transfer faces critical\nchallenges: Effective \"slow thinking\" in VRMs requires \\textbf{visual\nreflection}, the ability to check the reasoning process based on visual\ninformation. Through quantitative analysis, we observe that current VRMs\nexhibit limited visual reflection, as their attention to visual information\ndiminishes rapidly with longer generated responses. To address this challenge,\nwe propose a new VRM \\textbf{Reflection-V}, which enhances visual reflection\nbased on reasoning data construction for cold-start and reward design for\nreinforcement learning (RL). Firstly, we construct vision-centered reasoning\ndata by leveraging an agent that interacts between VLMs and reasoning LLMs,\nenabling cold-start learning of visual reflection patterns. Secondly, a visual\nattention based reward model is employed during RL to encourage reasoning based\non visual information. Therefore, \\textbf{Reflection-V} demonstrates\nsignificant improvements across multiple visual reasoning benchmarks.\nFurthermore, \\textbf{Reflection-V} maintains a stronger and more consistent\nreliance on visual information during visual reasoning, indicating effective\nenhancement in visual reflection capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in text-only \"slow-thinking\" reasoning have prompted efforts\nto transfer this capability to vision-language models (VLMs), for training\nvisual reasoning models (\\textbf{VRMs}). owever, such transfer faces critical\nchallenges: Effective \"slow thinking\" in VRMs requires \\textbf{visual\nreflection}, the ability to check the reasoning process based on visual\ninformation. Through quantitative analysis, we observe that current VRMs\nexhibit limited visual reflection, as their attention to visual information\ndiminishes rapidly with longer generated responses. To address this challenge,\nwe propose a new VRM \\textbf{Reflection-V}, which enhances visual reflection\nbased on reasoning data construction for cold-start and reward design for\nreinforcement learning (RL). Firstly, we construct vision-centered reasoning\ndata by leveraging an agent that interacts between VLMs and reasoning LLMs,\nenabling cold-start learning of visual reflection patterns. Secondly, a visual\nattention based reward model is employed during RL to encourage reasoning based\non visual information. Therefore, \\textbf{Reflection-V} demonstrates\nsignificant improvements across multiple visual reasoning benchmarks.\nFurthermore, \\textbf{Reflection-V} maintains a stronger and more consistent\nreliance on visual information during visual reasoning, indicating effective\nenhancement in visual reflection capabilities."
                },
                "authors": [
                    {
                        "name": "Pu Jian"
                    },
                    {
                        "name": "Junhong Wu"
                    },
                    {
                        "name": "Wei Sun"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Shuo Ren"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "arxiv_comment": "EMNLP2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12130v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12130v1",
                "updated": "2025-09-15T16:53:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    53,
                    41,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T16:53:41Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    53,
                    41,
                    0,
                    258,
                    0
                ],
                "title": "XplaiNLP at CheckThat! 2025: Multilingual Subjectivity Detection with\n  Finetuned Transformers and Prompt-Based Inference with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XplaiNLP at CheckThat! 2025: Multilingual Subjectivity Detection with\n  Finetuned Transformers and Prompt-Based Inference with Large Language Models"
                },
                "summary": "This notebook reports the XplaiNLP submission to the CheckThat! 2025 shared\ntask on multilingual subjectivity detection. We evaluate two approaches: (1)\nsupervised fine-tuning of transformer encoders, EuroBERT, XLM-RoBERTa, and\nGerman-BERT, on monolingual and machine-translated training data; and (2)\nzero-shot prompting using two LLMs: o3-mini for Annotation (rule-based\nlabelling) and gpt-4.1-mini for DoubleDown (contrastive rewriting) and\nPerspective (comparative reasoning). The Annotation Approach achieves 1st place\nin the Italian monolingual subtask with an F_1 score of 0.8104, outperforming\nthe baseline of 0.6941. In the Romanian zero-shot setting, the fine-tuned\nXLM-RoBERTa model obtains an F_1 score of 0.7917, ranking 3rd and exceeding the\nbaseline of 0.6461. The same model also performs reliably in the multilingual\ntask and improves over the baseline in Greek. For German, a German-BERT model\nfine-tuned on translated training data from typologically related languages\nyields competitive performance over the baseline. In contrast, performance in\nthe Ukrainian and Polish zero-shot settings falls slightly below the respective\nbaselines, reflecting the challenge of generalization in low-resource\ncross-lingual scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This notebook reports the XplaiNLP submission to the CheckThat! 2025 shared\ntask on multilingual subjectivity detection. We evaluate two approaches: (1)\nsupervised fine-tuning of transformer encoders, EuroBERT, XLM-RoBERTa, and\nGerman-BERT, on monolingual and machine-translated training data; and (2)\nzero-shot prompting using two LLMs: o3-mini for Annotation (rule-based\nlabelling) and gpt-4.1-mini for DoubleDown (contrastive rewriting) and\nPerspective (comparative reasoning). The Annotation Approach achieves 1st place\nin the Italian monolingual subtask with an F_1 score of 0.8104, outperforming\nthe baseline of 0.6941. In the Romanian zero-shot setting, the fine-tuned\nXLM-RoBERTa model obtains an F_1 score of 0.7917, ranking 3rd and exceeding the\nbaseline of 0.6461. The same model also performs reliably in the multilingual\ntask and improves over the baseline in Greek. For German, a German-BERT model\nfine-tuned on translated training data from typologically related languages\nyields competitive performance over the baseline. In contrast, performance in\nthe Ukrainian and Polish zero-shot settings falls slightly below the respective\nbaselines, reflecting the challenge of generalization in low-resource\ncross-lingual scenarios."
                },
                "authors": [
                    {
                        "name": "Ariana Sahitaj"
                    },
                    {
                        "name": "Jiaao Li"
                    },
                    {
                        "name": "Pia Wenzel Neves"
                    },
                    {
                        "name": "Fedor Splitt"
                    },
                    {
                        "name": "Premtim Sahitaj"
                    },
                    {
                        "name": "Charlott Jakob"
                    },
                    {
                        "name": "Veronika Solopova"
                    },
                    {
                        "name": "Vera Schmitt"
                    }
                ],
                "author_detail": {
                    "name": "Vera Schmitt"
                },
                "author": "Vera Schmitt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12130v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12130v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09179v2",
                "updated": "2025-09-15T16:44:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    44,
                    40,
                    0,
                    258,
                    0
                ],
                "published": "2025-07-12T07:55:40Z",
                "published_parsed": [
                    2025,
                    7,
                    12,
                    7,
                    55,
                    40,
                    5,
                    193,
                    0
                ],
                "title": "Hide-and-Shill: A Reinforcement Learning Framework for Market\n  Manipulation Detection in Symphony-a Decentralized Multi-Agent System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hide-and-Shill: A Reinforcement Learning Framework for Market\n  Manipulation Detection in Symphony-a Decentralized Multi-Agent System"
                },
                "summary": "Decentralized finance (DeFi) has introduced a new era of permissionless\nfinancial innovation but also led to unprecedented market manipulation. Without\ncentralized oversight, malicious actors coordinate shilling campaigns and\npump-and-dump schemes across various platforms. We propose a Multi-Agent\nReinforcement Learning (MARL) framework for decentralized manipulation\ndetection, modeling the interaction between manipulators and detectors as a\ndynamic adversarial game. This framework identifies suspicious patterns using\ndelayed token price reactions as financial indicators.Our method introduces\nthree innovations: (1) Group Relative Policy Optimization (GRPO) to enhance\nlearning stability in sparse-reward and partially observable settings; (2) a\ntheory-based reward function inspired by rational expectations and information\nasymmetry, differentiating price discovery from manipulation noise; and (3) a\nmulti-modal agent pipeline that integrates LLM-based semantic features, social\ngraph signals, and on-chain market data for informed decision-making.The\nframework is integrated within the Symphony system, a decentralized multi-agent\narchitecture enabling peer-to-peer agent execution and trust-aware learning\nthrough distributed logs, supporting chain-verifiable evaluation. Symphony\npromotes adversarial co-evolution among strategic actors and maintains robust\nmanipulation detection without centralized oracles, enabling real-time\nsurveillance across global DeFi ecosystems.Trained on 100,000 real-world\ndiscourse episodes and validated in adversarial simulations, Hide-and-Shill\nachieves top performance in detection accuracy and causal attribution. This\nwork bridges multi-agent systems with financial surveillance, advancing a new\nparadigm for decentralized market intelligence. All resources are available at\nthe Hide-and-Shill GitHub repository to promote open research and\nreproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized finance (DeFi) has introduced a new era of permissionless\nfinancial innovation but also led to unprecedented market manipulation. Without\ncentralized oversight, malicious actors coordinate shilling campaigns and\npump-and-dump schemes across various platforms. We propose a Multi-Agent\nReinforcement Learning (MARL) framework for decentralized manipulation\ndetection, modeling the interaction between manipulators and detectors as a\ndynamic adversarial game. This framework identifies suspicious patterns using\ndelayed token price reactions as financial indicators.Our method introduces\nthree innovations: (1) Group Relative Policy Optimization (GRPO) to enhance\nlearning stability in sparse-reward and partially observable settings; (2) a\ntheory-based reward function inspired by rational expectations and information\nasymmetry, differentiating price discovery from manipulation noise; and (3) a\nmulti-modal agent pipeline that integrates LLM-based semantic features, social\ngraph signals, and on-chain market data for informed decision-making.The\nframework is integrated within the Symphony system, a decentralized multi-agent\narchitecture enabling peer-to-peer agent execution and trust-aware learning\nthrough distributed logs, supporting chain-verifiable evaluation. Symphony\npromotes adversarial co-evolution among strategic actors and maintains robust\nmanipulation detection without centralized oracles, enabling real-time\nsurveillance across global DeFi ecosystems.Trained on 100,000 real-world\ndiscourse episodes and validated in adversarial simulations, Hide-and-Shill\nachieves top performance in detection accuracy and causal attribution. This\nwork bridges multi-agent systems with financial surveillance, advancing a new\nparadigm for decentralized market intelligence. All resources are available at\nthe Hide-and-Shill GitHub repository to promote open research and\nreproducibility."
                },
                "authors": [
                    {
                        "name": "Ronghua Shi"
                    },
                    {
                        "name": "Yiou Liu"
                    },
                    {
                        "name": "Xinyu Ying"
                    },
                    {
                        "name": "Yang Tan"
                    },
                    {
                        "name": "Yuchun Feng"
                    },
                    {
                        "name": "Lynn Ai"
                    },
                    {
                        "name": "Bill Shi"
                    },
                    {
                        "name": "Xuhui Wang"
                    },
                    {
                        "name": "Zhuang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhuang Liu"
                },
                "author": "Zhuang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12112v1",
                "updated": "2025-09-15T16:41:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    41,
                    8,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T16:41:08Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    41,
                    8,
                    0,
                    258,
                    0
                ],
                "title": "CBP-Tuning: Efficient Local Customization for Black-box Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CBP-Tuning: Efficient Local Customization for Black-box Large Language\n  Models"
                },
                "summary": "The high costs of customizing large language models (LLMs) fundamentally\nlimit their adaptability to user-specific needs. Consequently, LLMs are\nincreasingly offered as cloud-based services, a paradigm that introduces\ncritical limitations: providers struggle to support personalized customization\nat scale, while users face privacy risks when exposing sensitive data. To\naddress this dual challenge, we propose Customized Black-box Prompt Tuning\n(CBP-Tuning), a novel framework that facilitates efficient local customization\nwhile preserving bidirectional privacy. Specifically, we design a two-stage\nframework: (1) a prompt generator trained on the server-side to capture\ndomain-specific and task-agnostic capabilities, and (2) user-side gradient-free\noptimization that tailors soft prompts for individual tasks. This approach\neliminates the need for users to access model weights or upload private data,\nrequiring only a single customized vector per task while achieving effective\nadaptation. Furthermore, the evaluation of CBP-Tuning in the commonsense\nreasoning, medical and financial domain settings demonstrates superior\nperformance compared to baselines, showcasing its advantages in task-agnostic\nprocessing and privacy preservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The high costs of customizing large language models (LLMs) fundamentally\nlimit their adaptability to user-specific needs. Consequently, LLMs are\nincreasingly offered as cloud-based services, a paradigm that introduces\ncritical limitations: providers struggle to support personalized customization\nat scale, while users face privacy risks when exposing sensitive data. To\naddress this dual challenge, we propose Customized Black-box Prompt Tuning\n(CBP-Tuning), a novel framework that facilitates efficient local customization\nwhile preserving bidirectional privacy. Specifically, we design a two-stage\nframework: (1) a prompt generator trained on the server-side to capture\ndomain-specific and task-agnostic capabilities, and (2) user-side gradient-free\noptimization that tailors soft prompts for individual tasks. This approach\neliminates the need for users to access model weights or upload private data,\nrequiring only a single customized vector per task while achieving effective\nadaptation. Furthermore, the evaluation of CBP-Tuning in the commonsense\nreasoning, medical and financial domain settings demonstrates superior\nperformance compared to baselines, showcasing its advantages in task-agnostic\nprocessing and privacy preservation."
                },
                "authors": [
                    {
                        "name": "Jiaxuan Zhao"
                    },
                    {
                        "name": "Naibin Gu"
                    },
                    {
                        "name": "Yuchen Feng"
                    },
                    {
                        "name": "Xiyu Liu"
                    },
                    {
                        "name": "Peng Fu"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12110v1",
                "updated": "2025-09-15T16:38:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    38,
                    13,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T16:38:13Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    38,
                    13,
                    0,
                    258,
                    0
                ],
                "title": "When marine radar target detection meets pretrained large language\n  models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When marine radar target detection meets pretrained large language\n  models"
                },
                "summary": "Deep learning (DL) methods are widely used to extract high-dimensional\npatterns from the sequence features of radar echo signals. However,\nconventional DL algorithms face challenges such as redundant feature segments,\nand constraints from restricted model sizes. To address these issues, we\npropose a framework that integrates feature preprocessing with large language\nmodels (LLMs). Our preprocessing module tokenizes radar sequence features,\napplies a patch selection algorithm to filter out uninformative segments, and\nprojects the selected patches into embeddings compatible with the feature space\nof pre-trained LLMs. Leveraging these refined embeddings, we incorporate a\npre-trained LLM, fine-tuning only the normalization layers to reduce training\nburdens while enhancing performance. Experiments on measured datasets\ndemonstrate that the proposed method significantly outperforms the\nstate-of-the-art baselines on supervised learning tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning (DL) methods are widely used to extract high-dimensional\npatterns from the sequence features of radar echo signals. However,\nconventional DL algorithms face challenges such as redundant feature segments,\nand constraints from restricted model sizes. To address these issues, we\npropose a framework that integrates feature preprocessing with large language\nmodels (LLMs). Our preprocessing module tokenizes radar sequence features,\napplies a patch selection algorithm to filter out uninformative segments, and\nprojects the selected patches into embeddings compatible with the feature space\nof pre-trained LLMs. Leveraging these refined embeddings, we incorporate a\npre-trained LLM, fine-tuning only the normalization layers to reduce training\nburdens while enhancing performance. Experiments on measured datasets\ndemonstrate that the proposed method significantly outperforms the\nstate-of-the-art baselines on supervised learning tests."
                },
                "authors": [
                    {
                        "name": "Qiying Hu"
                    },
                    {
                        "name": "Linping Zhang"
                    },
                    {
                        "name": "Xueqian Wang"
                    },
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Xiao-Ping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ping Zhang"
                },
                "author": "Xiao-Ping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01042v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01042v5",
                "updated": "2025-09-15T16:36:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    36,
                    22,
                    0,
                    258,
                    0
                ],
                "published": "2025-02-03T04:23:33Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    4,
                    23,
                    33,
                    0,
                    34,
                    0
                ],
                "title": "SafeSwitch: Steering Unsafe LLM Behavior via Internal Activation Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeSwitch: Steering Unsafe LLM Behavior via Internal Activation Signals"
                },
                "summary": "Large language models (LLMs) exhibit exceptional capabilities across various\ntasks but also pose risks by generating harmful content. Existing safety\nmechanisms, while improving model safety, often lead to overly cautious\nbehavior and fail to fully leverage LLMs' internal cognitive processes.\nInspired by humans' reflective thinking capability, we first show that LLMs can\nsimilarly perform internal assessments about safety in their internal states.\nBuilding on this insight, we propose SafeSwitch, a dynamic framework that\nregulates unsafe outputs by utilizing the prober-based internal state monitor\nthat actively detects harmful intentions, and activates a safety head that\nleads to safer and more conservative responses only when necessary. SafeSwitch\nreduces harmful outputs by approximately 80% on harmful queries while\nmaintaining strong utility, reaching a Pareto optimal among several methods.\nOur method is also advantageous over traditional methods in offering more\ninformative, context-aware refusals, and achieves these benefits while only\ntuning less than 6% of the original parameters. SafeSwitch demonstrates large\nlanguage models' capacity for self-awareness and reflection regarding safety,\noffering a promising approach to more nuanced and effective safety controls.\nCodes for this work are available at https://github.com/Hanpx20/SafeSwitch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit exceptional capabilities across various\ntasks but also pose risks by generating harmful content. Existing safety\nmechanisms, while improving model safety, often lead to overly cautious\nbehavior and fail to fully leverage LLMs' internal cognitive processes.\nInspired by humans' reflective thinking capability, we first show that LLMs can\nsimilarly perform internal assessments about safety in their internal states.\nBuilding on this insight, we propose SafeSwitch, a dynamic framework that\nregulates unsafe outputs by utilizing the prober-based internal state monitor\nthat actively detects harmful intentions, and activates a safety head that\nleads to safer and more conservative responses only when necessary. SafeSwitch\nreduces harmful outputs by approximately 80% on harmful queries while\nmaintaining strong utility, reaching a Pareto optimal among several methods.\nOur method is also advantageous over traditional methods in offering more\ninformative, context-aware refusals, and achieves these benefits while only\ntuning less than 6% of the original parameters. SafeSwitch demonstrates large\nlanguage models' capacity for self-awareness and reflection regarding safety,\noffering a promising approach to more nuanced and effective safety controls.\nCodes for this work are available at https://github.com/Hanpx20/SafeSwitch."
                },
                "authors": [
                    {
                        "name": "Peixuan Han"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Xiusi Chen"
                    },
                    {
                        "name": "Yuji Zhang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Denghui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Denghui Zhang"
                },
                "author": "Denghui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01042v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01042v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12107v1",
                "updated": "2025-09-15T16:33:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    33,
                    37,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T16:33:37Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    33,
                    37,
                    0,
                    258,
                    0
                ],
                "title": "Exploring Conversational Design Choices in LLMs for Pedagogical\n  Purposes: Socratic and Narrative Approaches for Improving Instructor's\n  Teaching Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Conversational Design Choices in LLMs for Pedagogical\n  Purposes: Socratic and Narrative Approaches for Improving Instructor's\n  Teaching Practice"
                },
                "summary": "Large language models (LLMs) typically generate direct answers, yet they are\nincreasingly used as learning tools. Studying instructors' usage is critical,\ngiven their role in teaching and guiding AI adoption in education. We designed\nand evaluated TeaPT, an LLM for pedagogical purposes that supports instructors'\nprofessional development through two conversational approaches: a Socratic\napproach that uses guided questioning to foster reflection, and a Narrative\napproach that offers elaborated suggestions to extend externalized cognition.\nIn a mixed-method study with 41 higher-education instructors, the Socratic\nversion elicited greater engagement, while the Narrative version was preferred\nfor actionable guidance. Subgroup analyses further revealed that\nless-experienced, AI-optimistic instructors favored the Socratic version,\nwhereas more-experienced, AI-cautious instructors preferred the Narrative\nversion. We contribute design implications for LLMs for pedagogical purposes,\nshowing how adaptive conversational approaches can support instructors with\nvaried profiles while highlighting how AI attitudes and experience shape\ninteraction and learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) typically generate direct answers, yet they are\nincreasingly used as learning tools. Studying instructors' usage is critical,\ngiven their role in teaching and guiding AI adoption in education. We designed\nand evaluated TeaPT, an LLM for pedagogical purposes that supports instructors'\nprofessional development through two conversational approaches: a Socratic\napproach that uses guided questioning to foster reflection, and a Narrative\napproach that offers elaborated suggestions to extend externalized cognition.\nIn a mixed-method study with 41 higher-education instructors, the Socratic\nversion elicited greater engagement, while the Narrative version was preferred\nfor actionable guidance. Subgroup analyses further revealed that\nless-experienced, AI-optimistic instructors favored the Socratic version,\nwhereas more-experienced, AI-cautious instructors preferred the Narrative\nversion. We contribute design implications for LLMs for pedagogical purposes,\nshowing how adaptive conversational approaches can support instructors with\nvaried profiles while highlighting how AI attitudes and experience shape\ninteraction and learning."
                },
                "authors": [
                    {
                        "name": "Si Chen"
                    },
                    {
                        "name": "Isabel R. Molnar"
                    },
                    {
                        "name": "Peiyu Li"
                    },
                    {
                        "name": "Adam Acunin"
                    },
                    {
                        "name": "Ting Hua"
                    },
                    {
                        "name": "Alex Ambrose"
                    },
                    {
                        "name": "Nitesh V. Chawla"
                    },
                    {
                        "name": "Ronald Metoyer"
                    }
                ],
                "author_detail": {
                    "name": "Ronald Metoyer"
                },
                "author": "Ronald Metoyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12105v1",
                "updated": "2025-09-15T16:32:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    32,
                    31,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T16:32:31Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    32,
                    31,
                    0,
                    258,
                    0
                ],
                "title": "FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic\n  Segmentation via Low-Rank Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic\n  Segmentation via Low-Rank Adaptation"
                },
                "summary": "Few-shot semantic segmentation has recently attracted great attention. The\ngoal is to develop a model capable of segmenting unseen classes using only a\nfew annotated samples. Most existing approaches adapt a pre-trained model by\ntraining from scratch an additional module. Achieving optimal performance with\nthese approaches requires extensive training on large-scale datasets. The\nSegment Anything Model 2 (SAM2) is a foundational model for zero-shot image and\nvideo segmentation with a modular design. In this paper, we propose a Few-Shot\nsegmentation method based on SAM2 (FS-SAM2), where SAM2's video capabilities\nare directly repurposed for the few-shot task. Moreover, we apply a Low-Rank\nAdaptation (LoRA) to the original modules in order to handle the diverse images\ntypically found in standard datasets, unlike the temporally connected frames\nused in SAM2's pre-training. With this approach, only a small number of\nparameters is meta-trained, which effectively adapts SAM2 while benefiting from\nits impressive segmentation performance. Our method supports any K-shot\nconfiguration. We evaluate FS-SAM2 on the PASCAL-5$^i$, COCO-20$^i$ and\nFSS-1000 datasets, achieving remarkable results and demonstrating excellent\ncomputational efficiency during inference. Code is available at\nhttps://github.com/fornib/FS-SAM2",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot semantic segmentation has recently attracted great attention. The\ngoal is to develop a model capable of segmenting unseen classes using only a\nfew annotated samples. Most existing approaches adapt a pre-trained model by\ntraining from scratch an additional module. Achieving optimal performance with\nthese approaches requires extensive training on large-scale datasets. The\nSegment Anything Model 2 (SAM2) is a foundational model for zero-shot image and\nvideo segmentation with a modular design. In this paper, we propose a Few-Shot\nsegmentation method based on SAM2 (FS-SAM2), where SAM2's video capabilities\nare directly repurposed for the few-shot task. Moreover, we apply a Low-Rank\nAdaptation (LoRA) to the original modules in order to handle the diverse images\ntypically found in standard datasets, unlike the temporally connected frames\nused in SAM2's pre-training. With this approach, only a small number of\nparameters is meta-trained, which effectively adapts SAM2 while benefiting from\nits impressive segmentation performance. Our method supports any K-shot\nconfiguration. We evaluate FS-SAM2 on the PASCAL-5$^i$, COCO-20$^i$ and\nFSS-1000 datasets, achieving remarkable results and demonstrating excellent\ncomputational efficiency during inference. Code is available at\nhttps://github.com/fornib/FS-SAM2"
                },
                "authors": [
                    {
                        "name": "Bernardo Forni"
                    },
                    {
                        "name": "Gabriele Lombardi"
                    },
                    {
                        "name": "Federico Pozzi"
                    },
                    {
                        "name": "Mirco Planamente"
                    }
                ],
                "author_detail": {
                    "name": "Mirco Planamente"
                },
                "author": "Mirco Planamente",
                "arxiv_comment": "Accepted at ICIAP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12104v1",
                "updated": "2025-09-15T16:31:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    31,
                    26,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T16:31:26Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    31,
                    26,
                    0,
                    258,
                    0
                ],
                "title": "JustEva: A Toolkit to Evaluate LLM Fairness in Legal Knowledge Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JustEva: A Toolkit to Evaluate LLM Fairness in Legal Knowledge Inference"
                },
                "summary": "The integration of Large Language Models (LLMs) into legal practice raises\npressing concerns about judicial fairness, particularly due to the nature of\ntheir \"black-box\" processes. This study introduces JustEva, a comprehensive,\nopen-source evaluation toolkit designed to measure LLM fairness in legal tasks.\nJustEva features several advantages: (1) a structured label system covering 65\nextra-legal factors; (2) three core fairness metrics - inconsistency, bias, and\nimbalanced inaccuracy; (3) robust statistical inference methods; and (4)\ninformative visualizations. The toolkit supports two types of experiments,\nenabling a complete evaluation workflow: (1) generating structured outputs from\nLLMs using a provided dataset, and (2) conducting statistical analysis and\ninference on LLMs' outputs through regression and other statistical methods.\nEmpirical application of JustEva reveals significant fairness deficiencies in\ncurrent LLMs, highlighting the lack of fair and trustworthy LLM legal tools.\nJustEva offers a convenient tool and methodological foundation for evaluating\nand improving algorithmic fairness in the legal domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into legal practice raises\npressing concerns about judicial fairness, particularly due to the nature of\ntheir \"black-box\" processes. This study introduces JustEva, a comprehensive,\nopen-source evaluation toolkit designed to measure LLM fairness in legal tasks.\nJustEva features several advantages: (1) a structured label system covering 65\nextra-legal factors; (2) three core fairness metrics - inconsistency, bias, and\nimbalanced inaccuracy; (3) robust statistical inference methods; and (4)\ninformative visualizations. The toolkit supports two types of experiments,\nenabling a complete evaluation workflow: (1) generating structured outputs from\nLLMs using a provided dataset, and (2) conducting statistical analysis and\ninference on LLMs' outputs through regression and other statistical methods.\nEmpirical application of JustEva reveals significant fairness deficiencies in\ncurrent LLMs, highlighting the lack of fair and trustworthy LLM legal tools.\nJustEva offers a convenient tool and methodological foundation for evaluating\nand improving algorithmic fairness in the legal domain."
                },
                "authors": [
                    {
                        "name": "Zongyue Xue"
                    },
                    {
                        "name": "Siyuan Zheng"
                    },
                    {
                        "name": "Shaochun Wang"
                    },
                    {
                        "name": "Yiran Hu"
                    },
                    {
                        "name": "Shenran Wang"
                    },
                    {
                        "name": "Yuxin Yao"
                    },
                    {
                        "name": "Haitao Li"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Yiqun Liu"
                    },
                    {
                        "name": "Yun Liu"
                    },
                    {
                        "name": "Weixing Shen"
                    }
                ],
                "author_detail": {
                    "name": "Weixing Shen"
                },
                "author": "Weixing Shen",
                "arxiv_comment": "This paper has been accepted at CIKM 2025 (Demo Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17711v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17711v3",
                "updated": "2025-09-16T07:17:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    7,
                    17,
                    30,
                    1,
                    259,
                    0
                ],
                "published": "2025-08-25T06:39:08Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    6,
                    39,
                    8,
                    0,
                    237,
                    0
                ],
                "title": "Enhancing LLM-Based Social Bot via an Adversarial Learning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM-Based Social Bot via an Adversarial Learning Framework"
                },
                "summary": "Developing Large Language Model (LLM) agents that exhibit human-like\nbehavior, encompassing not only individual heterogeneity rooted in unique user\nprofiles but also adaptive response to socially connected neighbors, is a\nsignificant research challenge. Social media platforms, with their diverse user\ndata and explicit social structures, provide an ideal testbed for such\ninvestigations. This paper introduces EvoBot, an \\textbf{Evo}lving LLM-based\nsocial \\textbf{Bot} that significantly enhances human-like generative\ncapabilities through a novel adversarial learning framework. EvoBot is\ninitialized by Supervised Fine-Tuning (SFT) on representative data from social\nmedia and then iteratively refines its generation of sophisticated, human-like\ncontent via Direct Preference Optimization (DPO). This refinement is guided by\nfeedback from a co-adapting \\textbf{Detector} which concurrently improves its\nability to distinguish EvoBot from humans, thereby creating an increasingly\nchallenging learning environment for EvoBot. Experiments demonstrate that\nEvoBot generates content aligned with diverse user profiles, increasingly\nbypassing the co-adapting Detector through human-like expression. Moreover, it\nexhibits strong social responsiveness, more accurately modeling real-world\nopinion dynamics and information spread in multi-agent simulations. The\nframework also yields a more robust Detector, underscoring its broader utility\nfor both advanced agent development and related detection tasks. The code is\navailable at https://github.com/kfq20/EvoBot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing Large Language Model (LLM) agents that exhibit human-like\nbehavior, encompassing not only individual heterogeneity rooted in unique user\nprofiles but also adaptive response to socially connected neighbors, is a\nsignificant research challenge. Social media platforms, with their diverse user\ndata and explicit social structures, provide an ideal testbed for such\ninvestigations. This paper introduces EvoBot, an \\textbf{Evo}lving LLM-based\nsocial \\textbf{Bot} that significantly enhances human-like generative\ncapabilities through a novel adversarial learning framework. EvoBot is\ninitialized by Supervised Fine-Tuning (SFT) on representative data from social\nmedia and then iteratively refines its generation of sophisticated, human-like\ncontent via Direct Preference Optimization (DPO). This refinement is guided by\nfeedback from a co-adapting \\textbf{Detector} which concurrently improves its\nability to distinguish EvoBot from humans, thereby creating an increasingly\nchallenging learning environment for EvoBot. Experiments demonstrate that\nEvoBot generates content aligned with diverse user profiles, increasingly\nbypassing the co-adapting Detector through human-like expression. Moreover, it\nexhibits strong social responsiveness, more accurately modeling real-world\nopinion dynamics and information spread in multi-agent simulations. The\nframework also yields a more robust Detector, underscoring its broader utility\nfor both advanced agent development and related detection tasks. The code is\navailable at https://github.com/kfq20/EvoBot."
                },
                "authors": [
                    {
                        "name": "Fanqi Kong"
                    },
                    {
                        "name": "Xiaoyuan Zhang"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Yaodong Yang"
                    },
                    {
                        "name": "Song-Chun Zhu"
                    },
                    {
                        "name": "Xue Feng"
                    }
                ],
                "author_detail": {
                    "name": "Xue Feng"
                },
                "author": "Xue Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17711v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17711v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12102v1",
                "updated": "2025-09-15T16:26:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    26,
                    13,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T16:26:13Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    26,
                    13,
                    0,
                    258,
                    0
                ],
                "title": "Can LLMs Address Mental Health Questions? A Comparison with Human\n  Therapists",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Address Mental Health Questions? A Comparison with Human\n  Therapists"
                },
                "summary": "Limited access to mental health care has motivated the use of digital tools\nand conversational agents powered by large language models (LLMs), yet their\nquality and reception remain unclear. We present a study comparing\ntherapist-written responses to those generated by ChatGPT, Gemini, and Llama\nfor real patient questions. Text analysis showed that LLMs produced longer,\nmore readable, and lexically richer responses with a more positive tone, while\ntherapist responses were more often written in the first person. In a survey\nwith 150 users and 23 licensed therapists, participants rated LLM responses as\nclearer, more respectful, and more supportive than therapist-written answers.\nYet, both groups of participants expressed a stronger preference for human\ntherapist support. These findings highlight the promise and limitations of LLMs\nin mental health, underscoring the need for designs that balance their\ncommunicative strengths with concerns of trust, privacy, and accountability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limited access to mental health care has motivated the use of digital tools\nand conversational agents powered by large language models (LLMs), yet their\nquality and reception remain unclear. We present a study comparing\ntherapist-written responses to those generated by ChatGPT, Gemini, and Llama\nfor real patient questions. Text analysis showed that LLMs produced longer,\nmore readable, and lexically richer responses with a more positive tone, while\ntherapist responses were more often written in the first person. In a survey\nwith 150 users and 23 licensed therapists, participants rated LLM responses as\nclearer, more respectful, and more supportive than therapist-written answers.\nYet, both groups of participants expressed a stronger preference for human\ntherapist support. These findings highlight the promise and limitations of LLMs\nin mental health, underscoring the need for designs that balance their\ncommunicative strengths with concerns of trust, privacy, and accountability."
                },
                "authors": [
                    {
                        "name": "Synthia Wang"
                    },
                    {
                        "name": "Yuwei Cheng"
                    },
                    {
                        "name": "Austin Song"
                    },
                    {
                        "name": "Sarah Keedy"
                    },
                    {
                        "name": "Marc Berman"
                    },
                    {
                        "name": "Nick Feamster"
                    }
                ],
                "author_detail": {
                    "name": "Nick Feamster"
                },
                "author": "Nick Feamster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10551v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10551v4",
                "updated": "2025-09-15T16:26:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    26,
                    7,
                    0,
                    258,
                    0
                ],
                "published": "2025-01-17T20:54:09Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    20,
                    54,
                    9,
                    4,
                    17,
                    0
                ],
                "title": "An Empirical Study to Understand How Students Use ChatGPT for Writing\n  Essays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study to Understand How Students Use ChatGPT for Writing\n  Essays"
                },
                "summary": "As large language models (LLMs) advance and become widespread, students\nincreasingly turn to systems like ChatGPT for assistance with writing tasks.\nEducators are concerned with students' usage of ChatGPT beyond cheating; using\nChatGPT may reduce their critical engagement with writing, hindering students'\nlearning processes. The negative or positive impact of using LLM-powered tools\nfor writing will depend on how students use them; however, how students use\nChatGPT remains largely unknown, resulting in a limited understanding of its\nimpact on learning. To better understand how students use these tools, we\nconducted an online study $(n=70)$ where students were given an essay-writing\ntask using a custom platform we developed to capture the queries they made to\nChatGPT. To characterize their ChatGPT usage, we categorized each of the\nqueries students made to ChatGPT. We then analyzed the relationship between\nChatGPT usage and a variety of other metrics, including students'\nself-perception, attitudes towards AI, and the resulting essay itself. We found\nthat factors such as gender, race, and perceived self-efficacy can help predict\ndifferent AI usage patterns. Additionally, we found that different usage\npatterns were associated with varying levels of enjoyment and perceived\nownership over the essay. The results of this study contribute to discussions\nabout how writing education should incorporate generative AI-powered tools in\nthe classroom.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) advance and become widespread, students\nincreasingly turn to systems like ChatGPT for assistance with writing tasks.\nEducators are concerned with students' usage of ChatGPT beyond cheating; using\nChatGPT may reduce their critical engagement with writing, hindering students'\nlearning processes. The negative or positive impact of using LLM-powered tools\nfor writing will depend on how students use them; however, how students use\nChatGPT remains largely unknown, resulting in a limited understanding of its\nimpact on learning. To better understand how students use these tools, we\nconducted an online study $(n=70)$ where students were given an essay-writing\ntask using a custom platform we developed to capture the queries they made to\nChatGPT. To characterize their ChatGPT usage, we categorized each of the\nqueries students made to ChatGPT. We then analyzed the relationship between\nChatGPT usage and a variety of other metrics, including students'\nself-perception, attitudes towards AI, and the resulting essay itself. We found\nthat factors such as gender, race, and perceived self-efficacy can help predict\ndifferent AI usage patterns. Additionally, we found that different usage\npatterns were associated with varying levels of enjoyment and perceived\nownership over the essay. The results of this study contribute to discussions\nabout how writing education should incorporate generative AI-powered tools in\nthe classroom."
                },
                "authors": [
                    {
                        "name": "Andrew Jelson"
                    },
                    {
                        "name": "Daniel Manesh"
                    },
                    {
                        "name": "Alice Jang"
                    },
                    {
                        "name": "Daniel Dunlap"
                    },
                    {
                        "name": "Young-Ho Kim"
                    },
                    {
                        "name": "Sang Won Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sang Won Lee"
                },
                "author": "Sang Won Lee",
                "arxiv_comment": "35 pages, 16 figures, 6 tables, Submitted to ACM CHI 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10551v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10551v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12101v1",
                "updated": "2025-09-15T16:25:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    25,
                    43,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T16:25:43Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    25,
                    43,
                    0,
                    258,
                    0
                ],
                "title": "In-domain SSL pre-training and streaming ASR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-domain SSL pre-training and streaming ASR"
                },
                "summary": "In this study, we investigate the benefits of domain-specific self-supervised\npre-training for both offline and streaming ASR in Air Traffic Control (ATC)\nenvironments. We train BEST-RQ models on 4.5k hours of unlabeled ATC data, then\nfine-tune on a smaller supervised ATC set. To enable real-time processing, we\npropose using chunked attention and dynamic convolutions, ensuring low-latency\ninference. We compare these in-domain SSL models against state-of-the-art,\ngeneral-purpose speech encoders such as w2v-BERT 2.0 and HuBERT. Results show\nthat domain-adapted pre-training substantially improves performance on standard\nATC benchmarks, significantly reducing word error rates when compared to models\ntrained on broad speech corpora. Furthermore, the proposed streaming approach\nfurther improves word error rate under tighter latency constraints, making it\nparticularly suitable for safety-critical aviation applications. These findings\nhighlight that specializing SSL representations for ATC data is a practical\npath toward more accurate and efficient ASR systems in real-world operational\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we investigate the benefits of domain-specific self-supervised\npre-training for both offline and streaming ASR in Air Traffic Control (ATC)\nenvironments. We train BEST-RQ models on 4.5k hours of unlabeled ATC data, then\nfine-tune on a smaller supervised ATC set. To enable real-time processing, we\npropose using chunked attention and dynamic convolutions, ensuring low-latency\ninference. We compare these in-domain SSL models against state-of-the-art,\ngeneral-purpose speech encoders such as w2v-BERT 2.0 and HuBERT. Results show\nthat domain-adapted pre-training substantially improves performance on standard\nATC benchmarks, significantly reducing word error rates when compared to models\ntrained on broad speech corpora. Furthermore, the proposed streaming approach\nfurther improves word error rate under tighter latency constraints, making it\nparticularly suitable for safety-critical aviation applications. These findings\nhighlight that specializing SSL representations for ATC data is a practical\npath toward more accurate and efficient ASR systems in real-world operational\nsettings."
                },
                "authors": [
                    {
                        "name": "Jarod Duret"
                    },
                    {
                        "name": "Salima Mdhaffar"
                    },
                    {
                        "name": "Gaëlle Laperrière"
                    },
                    {
                        "name": "Ryan Whetten"
                    },
                    {
                        "name": "Audrey Galametz"
                    },
                    {
                        "name": "Catherine Kobus"
                    },
                    {
                        "name": "Marion-Cécile Martin"
                    },
                    {
                        "name": "Jo Oleiwan"
                    },
                    {
                        "name": "Yannick Estève"
                    }
                ],
                "author_detail": {
                    "name": "Yannick Estève"
                },
                "author": "Yannick Estève",
                "arxiv_comment": "Accepted to SPECOM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12098v1",
                "updated": "2025-09-15T16:21:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    21,
                    59,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T16:21:59Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    21,
                    59,
                    0,
                    258,
                    0
                ],
                "title": "Is 'Hope' a person or an idea? A pilot benchmark for NER: comparing\n  traditional NLP tools and large language models on ambiguous entities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is 'Hope' a person or an idea? A pilot benchmark for NER: comparing\n  traditional NLP tools and large language models on ambiguous entities"
                },
                "summary": "This pilot study presents a small-scale but carefully annotated benchmark of\nNamed Entity Recognition (NER) performance across six systems: three non-LLM\nNLP tools (NLTK, spaCy, Stanza) and three general-purpose large language models\n(LLMs: Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B). The dataset contains 119\ntokens covering five entity types (PERSON, LOCATION, ORGANIZATION, DATE, TIME).\nWe evaluated each system's output against the manually annotated gold standard\ndataset using F1-score. The results show that LLMs generally outperform\nconventional tools in recognizing context-sensitive entities like person names,\nwith Gemini achieving the highest average F1-score. However, traditional\nsystems like Stanza demonstrate greater consistency in structured tags such as\nLOCATION and DATE. We also observed variability among LLMs, particularly in\nhandling temporal expressions and multi-word organizations. Our findings\nhighlight that while LLMs offer improved contextual understanding, traditional\ntools remain competitive in specific tasks, informing model selection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This pilot study presents a small-scale but carefully annotated benchmark of\nNamed Entity Recognition (NER) performance across six systems: three non-LLM\nNLP tools (NLTK, spaCy, Stanza) and three general-purpose large language models\n(LLMs: Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B). The dataset contains 119\ntokens covering five entity types (PERSON, LOCATION, ORGANIZATION, DATE, TIME).\nWe evaluated each system's output against the manually annotated gold standard\ndataset using F1-score. The results show that LLMs generally outperform\nconventional tools in recognizing context-sensitive entities like person names,\nwith Gemini achieving the highest average F1-score. However, traditional\nsystems like Stanza demonstrate greater consistency in structured tags such as\nLOCATION and DATE. We also observed variability among LLMs, particularly in\nhandling temporal expressions and multi-word organizations. Our findings\nhighlight that while LLMs offer improved contextual understanding, traditional\ntools remain competitive in specific tasks, informing model selection."
                },
                "authors": [
                    {
                        "name": "Payam Latifi"
                    }
                ],
                "author_detail": {
                    "name": "Payam Latifi"
                },
                "author": "Payam Latifi",
                "arxiv_comment": "14 pages, 9 figures, 2 tables. This is a pilot study evaluating six\n  NER systems -- three traditional tools (NLTK, spaCy, Stanza) and three LLMs\n  (Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B) -- on a small, ambiguity-rich\n  dataset of 119 tokens. The annotated dataset, prompts are provided in\n  appendices for full reproducibility. All experiments were conducted on 14 May\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12090v1",
                "updated": "2025-09-15T16:17:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    17,
                    45,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T16:17:45Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    17,
                    45,
                    0,
                    258,
                    0
                ],
                "title": "End-to-End 4D Heart Mesh Recovery Across Full-Stack and Sparse Cardiac\n  MRI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End 4D Heart Mesh Recovery Across Full-Stack and Sparse Cardiac\n  MRI"
                },
                "summary": "Reconstructing cardiac motion from cine CMR sequences is critical for\ndiagnosis, prediction, and intervention. Existing methods rely on complete CMR\nstacks to infer full heart motion, limiting their utility in intra-procedural\nscenarios where only sparse observations are available. We present TetHeart,\nthe first end-to-end framework that unifies full 4D multi-structure heart mesh\nrecovery from both offline full-stack acquisitions and intra-procedural\nsparse-slice observations. Our method leverages deep deformable tetrahedra, an\nexplicit-implicit hybrid representation, to capture shape and motion in a\ncoherent space shared across cardiac structures. It is initialized from\nhigh-quality pre-procedural or offline-acquired full stacks to build detailed,\npatient-specific heart meshes, which can then be updated using whatever slices\nare available, from full stacks down to a single slice. We further incorporate\nseveral key innovations: (i) an attentive mechanism for slice-adaptive 2D-3D\nfeature assembly that dynamically integrates information from arbitrary numbers\nof slices at any position, combined with a distillation strategy from\nfull-slice to sparse-slice settings to ensure accurate reconstruction under\nextreme sparsity; and (ii) a two-stage weakly supervised motion learning scheme\nrequiring only keyframe (e.g., ED and ES) annotations. Trained and validated on\nthree large public datasets and externally evaluated zero-shot on additional\nprivate interventional and public CMR datasets, TetHeart achieves\nstate-of-the-art accuracy and strong generalization in both pre- and\nintra-procedural settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing cardiac motion from cine CMR sequences is critical for\ndiagnosis, prediction, and intervention. Existing methods rely on complete CMR\nstacks to infer full heart motion, limiting their utility in intra-procedural\nscenarios where only sparse observations are available. We present TetHeart,\nthe first end-to-end framework that unifies full 4D multi-structure heart mesh\nrecovery from both offline full-stack acquisitions and intra-procedural\nsparse-slice observations. Our method leverages deep deformable tetrahedra, an\nexplicit-implicit hybrid representation, to capture shape and motion in a\ncoherent space shared across cardiac structures. It is initialized from\nhigh-quality pre-procedural or offline-acquired full stacks to build detailed,\npatient-specific heart meshes, which can then be updated using whatever slices\nare available, from full stacks down to a single slice. We further incorporate\nseveral key innovations: (i) an attentive mechanism for slice-adaptive 2D-3D\nfeature assembly that dynamically integrates information from arbitrary numbers\nof slices at any position, combined with a distillation strategy from\nfull-slice to sparse-slice settings to ensure accurate reconstruction under\nextreme sparsity; and (ii) a two-stage weakly supervised motion learning scheme\nrequiring only keyframe (e.g., ED and ES) annotations. Trained and validated on\nthree large public datasets and externally evaluated zero-shot on additional\nprivate interventional and public CMR datasets, TetHeart achieves\nstate-of-the-art accuracy and strong generalization in both pre- and\nintra-procedural settings."
                },
                "authors": [
                    {
                        "name": "Yihong Chen"
                    },
                    {
                        "name": "Jiancheng Yang"
                    },
                    {
                        "name": "Deniz Sayin Mercadier"
                    },
                    {
                        "name": "Hieu Le"
                    },
                    {
                        "name": "Juerg Schwitter"
                    },
                    {
                        "name": "Pascal Fua"
                    }
                ],
                "author_detail": {
                    "name": "Pascal Fua"
                },
                "author": "Pascal Fua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12089v1",
                "updated": "2025-09-15T16:16:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    16,
                    57,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T16:16:57Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    16,
                    57,
                    0,
                    258,
                    0
                ],
                "title": "RadarLLM: Adapting Pretrained Large Language Models for Marine Radar\n  Target Detection with Preference-aware Loss",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RadarLLM: Adapting Pretrained Large Language Models for Marine Radar\n  Target Detection with Preference-aware Loss"
                },
                "summary": "Recent advances in pre-trained large language models (LLMs) have demonstrated\ntheir capacities to capture universal knowledge, making them promising\ngeneral-purpose optimization solvers for wireless signal processing. Motivated\nby these findings, we take the first step towards fine-tuning pre-trained LLMs\nfor the effective analysis of radar signal features in marine target detection\ntasks. Nevertheless, directly fine-tuning pre-trained LLMs on marine target\ndetection tasks tends to suffer from pronounced overfitting, particularly in\nchallenging low signal-to-clutter ratio (SCR) scenarios. This overfitting\nprimarily stems from the model's tendency to memorize spurious or noisy feature\npatterns rather than learning discriminative structures that generalize well to\nunseen data. To address this challenge, we introduce RadarLLM, a novel\nfine-tuning framework that utilizes an effective preference-aware loss. Unlike\nconventional training strategies that uniformly optimize all feature tokens,\nthis loss function selectively optimizes different feature patches based on\ntheir online evaluated learning values, thus guiding the model to focus on the\nmost generalizable patterns during optimization. We theoretically demonstrate\nthe effectiveness of the evaluated learning values by transforming the problem\nas selecting useful feature tokens. Extensive experiments on real-world marine\nradar datasets show that 1) the proposed loss function is much better than the\noriginal one, with particularly significant gains in challenging low SCR\nscenarios and 2) RadarLLM consistently outperforms state-of-the-art baselines\nacross diverse detection scenarios, with particularly notable gains under\nlimited training data conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in pre-trained large language models (LLMs) have demonstrated\ntheir capacities to capture universal knowledge, making them promising\ngeneral-purpose optimization solvers for wireless signal processing. Motivated\nby these findings, we take the first step towards fine-tuning pre-trained LLMs\nfor the effective analysis of radar signal features in marine target detection\ntasks. Nevertheless, directly fine-tuning pre-trained LLMs on marine target\ndetection tasks tends to suffer from pronounced overfitting, particularly in\nchallenging low signal-to-clutter ratio (SCR) scenarios. This overfitting\nprimarily stems from the model's tendency to memorize spurious or noisy feature\npatterns rather than learning discriminative structures that generalize well to\nunseen data. To address this challenge, we introduce RadarLLM, a novel\nfine-tuning framework that utilizes an effective preference-aware loss. Unlike\nconventional training strategies that uniformly optimize all feature tokens,\nthis loss function selectively optimizes different feature patches based on\ntheir online evaluated learning values, thus guiding the model to focus on the\nmost generalizable patterns during optimization. We theoretically demonstrate\nthe effectiveness of the evaluated learning values by transforming the problem\nas selecting useful feature tokens. Extensive experiments on real-world marine\nradar datasets show that 1) the proposed loss function is much better than the\noriginal one, with particularly significant gains in challenging low SCR\nscenarios and 2) RadarLLM consistently outperforms state-of-the-art baselines\nacross diverse detection scenarios, with particularly notable gains under\nlimited training data conditions."
                },
                "authors": [
                    {
                        "name": "Qiying Hu"
                    }
                ],
                "author_detail": {
                    "name": "Qiying Hu"
                },
                "author": "Qiying Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12087v1",
                "updated": "2025-09-15T16:16:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    16,
                    14,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T16:16:14Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    16,
                    14,
                    0,
                    258,
                    0
                ],
                "title": "A New Benchmark for Evaluating Code Translation with Third-Party\n  Libraries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Benchmark for Evaluating Code Translation with Third-Party\n  Libraries"
                },
                "summary": "In recent years, Large Language Models (LLMs) have been widely studied in the\ncode translation field on the method, class, and even repository levels.\nHowever, most of these benchmarks are limited in terms of Third-Party Library\n(TPL) categories and scales, making TPL-related errors hard to expose and\nhindering the development of targeted solutions. Considering the high\ndependence (over 90%) on TPLs in practical programming, demystifying and\nanalyzing LLMs' code translation performance involving various TPLs becomes\nimperative. To address this gap, we construct TransLibEval, the first benchmark\ndedicated to library-centric code translation. It consists of 200 real-world\ntasks across Python, Java, and C++, each explicitly involving TPLs from diverse\ncategories such as data processing, machine learning, and web development, with\ncomprehensive dependency coverage and high-coverage test suites. We evaluate\nseven recent LLMs of commercial, general, and code-specialized families under\nsix translation strategies of three categories: Direct, IR-guided, and\nRetrieval-augmented. Experimental results show a dramatic performance drop\ncompared with library-free settings (average CA decline over 60%), while\ndiverse strategies demonstrate heterogeneous advantages. Furthermore, we\nanalyze 4,831 failed cases from GPT-4o, one of the State-of-the-Art (SOTA)\nLLMs, revealing numerous third-party reference errors that were obscured\npreviously. These findings highlight the unique challenges of library-centric\ntranslation and provide practical guidance for improving TPL-aware code\nintelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have been widely studied in the\ncode translation field on the method, class, and even repository levels.\nHowever, most of these benchmarks are limited in terms of Third-Party Library\n(TPL) categories and scales, making TPL-related errors hard to expose and\nhindering the development of targeted solutions. Considering the high\ndependence (over 90%) on TPLs in practical programming, demystifying and\nanalyzing LLMs' code translation performance involving various TPLs becomes\nimperative. To address this gap, we construct TransLibEval, the first benchmark\ndedicated to library-centric code translation. It consists of 200 real-world\ntasks across Python, Java, and C++, each explicitly involving TPLs from diverse\ncategories such as data processing, machine learning, and web development, with\ncomprehensive dependency coverage and high-coverage test suites. We evaluate\nseven recent LLMs of commercial, general, and code-specialized families under\nsix translation strategies of three categories: Direct, IR-guided, and\nRetrieval-augmented. Experimental results show a dramatic performance drop\ncompared with library-free settings (average CA decline over 60%), while\ndiverse strategies demonstrate heterogeneous advantages. Furthermore, we\nanalyze 4,831 failed cases from GPT-4o, one of the State-of-the-Art (SOTA)\nLLMs, revealing numerous third-party reference errors that were obscured\npreviously. These findings highlight the unique challenges of library-centric\ntranslation and provide practical guidance for improving TPL-aware code\nintelligence."
                },
                "authors": [
                    {
                        "name": "Pengyu Xue"
                    },
                    {
                        "name": "Kunwu Zheng"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Yifei Pei"
                    },
                    {
                        "name": "Linhao Wu"
                    },
                    {
                        "name": "Jiahui Dong"
                    },
                    {
                        "name": "Xiapu Luo"
                    },
                    {
                        "name": "Yan Xiao"
                    },
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Xiran Lyu"
                    },
                    {
                        "name": "Xianhang Li"
                    },
                    {
                        "name": "Xuanyu Zhu"
                    },
                    {
                        "name": "Chengyi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chengyi Wang"
                },
                "author": "Chengyi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12086v1",
                "updated": "2025-09-15T16:14:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    14,
                    5,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T16:14:05Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    14,
                    5,
                    0,
                    258,
                    0
                ],
                "title": "SAQ: Pushing the Limits of Vector Quantization through Code Adjustment\n  and Dimension Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAQ: Pushing the Limits of Vector Quantization through Code Adjustment\n  and Dimension Segmentation"
                },
                "summary": "Approximate Nearest Neighbor Search (ANNS) plays a critical role in\napplications such as search engines, recommender systems, and RAG for LLMs.\nVector quantization (VQ), a crucial technique for ANNS, is commonly used to\nreduce space overhead and accelerate distance computations. However, despite\nsignificant research advances, state-of-the-art VQ methods still face\nchallenges in balancing encoding efficiency and quantization accuracy. To\naddress these limitations, we propose a novel VQ method called SAQ. To improve\naccuracy, SAQ employs a new dimension segmentation technique to strategically\npartition PCA-projected vectors into segments along their dimensions. By\nprioritizing leading dimension segments with larger magnitudes, SAQ allocates\nmore bits to high-impact segments, optimizing the use of the available space\nquota. An efficient dynamic programming algorithm is developed to optimize\ndimension segmentation and bit allocation, ensuring minimal quantization error.\nTo speed up vector encoding, SAQ devises a code adjustment technique to first\nquantize each dimension independently and then progressively refine quantized\nvectors using a coordinate-descent-like approach to avoid exhaustive\nenumeration. Extensive experiments demonstrate SAQ's superiority over classical\nmethods (e.g., PQ, PCA) and recent state-of-the-art approaches (e.g., LVQ,\nExtended RabitQ). SAQ achieves up to 80% reduction in quantization error and\naccelerates encoding speed by over 80x compared to Extended RabitQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Nearest Neighbor Search (ANNS) plays a critical role in\napplications such as search engines, recommender systems, and RAG for LLMs.\nVector quantization (VQ), a crucial technique for ANNS, is commonly used to\nreduce space overhead and accelerate distance computations. However, despite\nsignificant research advances, state-of-the-art VQ methods still face\nchallenges in balancing encoding efficiency and quantization accuracy. To\naddress these limitations, we propose a novel VQ method called SAQ. To improve\naccuracy, SAQ employs a new dimension segmentation technique to strategically\npartition PCA-projected vectors into segments along their dimensions. By\nprioritizing leading dimension segments with larger magnitudes, SAQ allocates\nmore bits to high-impact segments, optimizing the use of the available space\nquota. An efficient dynamic programming algorithm is developed to optimize\ndimension segmentation and bit allocation, ensuring minimal quantization error.\nTo speed up vector encoding, SAQ devises a code adjustment technique to first\nquantize each dimension independently and then progressively refine quantized\nvectors using a coordinate-descent-like approach to avoid exhaustive\nenumeration. Extensive experiments demonstrate SAQ's superiority over classical\nmethods (e.g., PQ, PCA) and recent state-of-the-art approaches (e.g., LVQ,\nExtended RabitQ). SAQ achieves up to 80% reduction in quantization error and\naccelerates encoding speed by over 80x compared to Extended RabitQ."
                },
                "authors": [
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Shiyuan Deng"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Xiangyu Zhi"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "arxiv_comment": "13 pages, 12 figures, accepted by SIGMOD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05295v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05295v3",
                "updated": "2025-09-15T16:02:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    2,
                    53,
                    0,
                    258,
                    0
                ],
                "published": "2025-04-07T17:49:37Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    17,
                    49,
                    37,
                    0,
                    97,
                    0
                ],
                "title": "Dion: Distributed Orthonormalized Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dion: Distributed Orthonormalized Updates"
                },
                "summary": "Orthonormalized updates accelerate training, improve stability, and enable\nrobust hyperparameter transfer, but existing methods like Muon rely on dense\nmatrix operations that clash with sharded weights in large-scale LLM training,\ncausing high compute and communication cost. We introduce Dion (Distributed\nOrthonormalization), a scalable and efficient update rule that replaces\nNewton-Schulz iteration with amortized power iteration on a momentum buffer,\navoiding full-matrix reconstruction and integrating cleanly with weight\nsharding. The rank-fraction parameter with error feedback enables low-rank\nupdates that balance quality with significant cost savings. On language models\nfrom 160M to 3B parameters, Dion retains the benefits of orthonormalized\nupdates, while markedly reducing wall-clock time at scale, making it a\npractical optimizer for next-generation foundation models. Code is available\nat: https://github.com/microsoft/dion/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orthonormalized updates accelerate training, improve stability, and enable\nrobust hyperparameter transfer, but existing methods like Muon rely on dense\nmatrix operations that clash with sharded weights in large-scale LLM training,\ncausing high compute and communication cost. We introduce Dion (Distributed\nOrthonormalization), a scalable and efficient update rule that replaces\nNewton-Schulz iteration with amortized power iteration on a momentum buffer,\navoiding full-matrix reconstruction and integrating cleanly with weight\nsharding. The rank-fraction parameter with error feedback enables low-rank\nupdates that balance quality with significant cost savings. On language models\nfrom 160M to 3B parameters, Dion retains the benefits of orthonormalized\nupdates, while markedly reducing wall-clock time at scale, making it a\npractical optimizer for next-generation foundation models. Code is available\nat: https://github.com/microsoft/dion/"
                },
                "authors": [
                    {
                        "name": "Kwangjun Ahn"
                    },
                    {
                        "name": "Byron Xu"
                    },
                    {
                        "name": "Natalie Abreu"
                    },
                    {
                        "name": "Ying Fan"
                    },
                    {
                        "name": "Gagik Magakyan"
                    },
                    {
                        "name": "Pratyusha Sharma"
                    },
                    {
                        "name": "Zheng Zhan"
                    },
                    {
                        "name": "John Langford"
                    }
                ],
                "author_detail": {
                    "name": "John Langford"
                },
                "author": "John Langford",
                "arxiv_comment": "\"Version 3\" with various new updates",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05295v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05295v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07321v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07321v2",
                "updated": "2025-09-15T15:58:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    15,
                    58,
                    31,
                    0,
                    258,
                    0
                ],
                "published": "2025-04-09T22:41:59Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    22,
                    41,
                    59,
                    2,
                    99,
                    0
                ],
                "title": "A Unified Framework for Large-Scale Inference of Classification: Error\n  Rate Control and Optimality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Large-Scale Inference of Classification: Error\n  Rate Control and Optimality"
                },
                "summary": "Classification is a fundamental task in supervised learning, while achieving\nvalid misclassification rate control remains challenging due to possibly the\nlimited predictive capability of the classifiers or the intrinsic complexity of\nthe classification task. In this article, we address large-scale multi-class\nclassification problems with general error rate guarantees to enhance\nalgorithmic trustworthiness. To this end, we first introduce a notion of\ngroup-wise classification, which unifies the common class-wise and overall\nclassifications as special cases. We then develop a unified inference framework\nfor the general group-wise classification that consists of three steps:\nPre-classification, Selective $p$-value construction, and large-scale\nPost-classification decisions (PSP). Theoretically, PSP is distribution-free\nand provides valid finite-sample guarantees for controlling general group-wise\nfalse decision rates at target levels. To show the power of PSP, we demonstrate\nthat the step of post-classification decisions never degrades the power of\npre-classification, provided that pre-classification has been sufficiently\npowerful to meet the target error levels. We further establish general power\noptimality theories for PSP from both non-asymptotic and asymptotic\nperspectives. Numerical results in both simulations and real data analysis\nvalidate the performance of the proposed PSP approach. In addition, we\nintroduce an ePSP algorithm that integrates the idea of PSP with selective\n$e$-values. Finally, extensions of PSP are shown to demonstrate its feasibility\nand power in broader applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classification is a fundamental task in supervised learning, while achieving\nvalid misclassification rate control remains challenging due to possibly the\nlimited predictive capability of the classifiers or the intrinsic complexity of\nthe classification task. In this article, we address large-scale multi-class\nclassification problems with general error rate guarantees to enhance\nalgorithmic trustworthiness. To this end, we first introduce a notion of\ngroup-wise classification, which unifies the common class-wise and overall\nclassifications as special cases. We then develop a unified inference framework\nfor the general group-wise classification that consists of three steps:\nPre-classification, Selective $p$-value construction, and large-scale\nPost-classification decisions (PSP). Theoretically, PSP is distribution-free\nand provides valid finite-sample guarantees for controlling general group-wise\nfalse decision rates at target levels. To show the power of PSP, we demonstrate\nthat the step of post-classification decisions never degrades the power of\npre-classification, provided that pre-classification has been sufficiently\npowerful to meet the target error levels. We further establish general power\noptimality theories for PSP from both non-asymptotic and asymptotic\nperspectives. Numerical results in both simulations and real data analysis\nvalidate the performance of the proposed PSP approach. In addition, we\nintroduce an ePSP algorithm that integrates the idea of PSP with selective\n$e$-values. Finally, extensions of PSP are shown to demonstrate its feasibility\nand power in broader applications."
                },
                "authors": [
                    {
                        "name": "Yinrui Sun"
                    },
                    {
                        "name": "Yin Xia"
                    }
                ],
                "author_detail": {
                    "name": "Yin Xia"
                },
                "author": "Yin Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07321v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07321v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12069v1",
                "updated": "2025-09-15T15:52:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    15,
                    52,
                    43,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T15:52:43Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    15,
                    52,
                    43,
                    0,
                    258,
                    0
                ],
                "title": "U-Mamba2: Scaling State Space Models for Dental Anatomy Segmentation in\n  CBCT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-Mamba2: Scaling State Space Models for Dental Anatomy Segmentation in\n  CBCT"
                },
                "summary": "Cone-Beam Computed Tomography (CBCT) is a widely used 3D imaging technique in\ndentistry, providing volumetric information about the anatomical structures of\njaws and teeth. Accurate segmentation of these anatomies is critical for\nclinical applications such as diagnosis and surgical planning, but remains\ntime-consuming and challenging. In this paper, we present U-Mamba2, a new\nneural network architecture designed for multi-anatomy CBCT segmentation in the\ncontext of the ToothFairy3 challenge. U-Mamba2 integrates the Mamba2 state\nspace models into the U-Net architecture, enforcing stronger structural\nconstraints for higher efficiency without compromising performance. In\naddition, we integrate interactive click prompts with cross-attention blocks,\npre-train U-Mamba2 using self-supervised learning, and incorporate dental\ndomain knowledge into the model design to address key challenges of dental\nanatomy segmentation in CBCT. Extensive experiments, including independent\ntests, demonstrate that U-Mamba2 is both effective and efficient, securing top\n3 places in both tasks of the Toothfairy3 challenge. In Task 1, U-Mamba2\nachieved a mean Dice of 0.792, HD95 of 93.19 with the held-out test data, with\nan average inference time of XX (TBC during the ODIN workshop). In Task 2,\nU-Mamba2 achieved the mean Dice of 0.852 and HD95 of 7.39 with the held-out\ntest data. The code is publicly available at\nhttps://github.com/zhiqin1998/UMamba2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cone-Beam Computed Tomography (CBCT) is a widely used 3D imaging technique in\ndentistry, providing volumetric information about the anatomical structures of\njaws and teeth. Accurate segmentation of these anatomies is critical for\nclinical applications such as diagnosis and surgical planning, but remains\ntime-consuming and challenging. In this paper, we present U-Mamba2, a new\nneural network architecture designed for multi-anatomy CBCT segmentation in the\ncontext of the ToothFairy3 challenge. U-Mamba2 integrates the Mamba2 state\nspace models into the U-Net architecture, enforcing stronger structural\nconstraints for higher efficiency without compromising performance. In\naddition, we integrate interactive click prompts with cross-attention blocks,\npre-train U-Mamba2 using self-supervised learning, and incorporate dental\ndomain knowledge into the model design to address key challenges of dental\nanatomy segmentation in CBCT. Extensive experiments, including independent\ntests, demonstrate that U-Mamba2 is both effective and efficient, securing top\n3 places in both tasks of the Toothfairy3 challenge. In Task 1, U-Mamba2\nachieved a mean Dice of 0.792, HD95 of 93.19 with the held-out test data, with\nan average inference time of XX (TBC during the ODIN workshop). In Task 2,\nU-Mamba2 achieved the mean Dice of 0.852 and HD95 of 7.39 with the held-out\ntest data. The code is publicly available at\nhttps://github.com/zhiqin1998/UMamba2."
                },
                "authors": [
                    {
                        "name": "Zhi Qin Tan"
                    },
                    {
                        "name": "Xiatian Zhu"
                    },
                    {
                        "name": "Owen Addison"
                    },
                    {
                        "name": "Yunpeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Yunpeng Li"
                },
                "author": "Yunpeng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12065v1",
                "updated": "2025-09-15T15:48:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    15,
                    48,
                    9,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T15:48:09Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    15,
                    48,
                    9,
                    0,
                    258,
                    0
                ],
                "title": "Steering Language Models in Multi-Token Generation: A Case Study on\n  Tense and Aspect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Language Models in Multi-Token Generation: A Case Study on\n  Tense and Aspect"
                },
                "summary": "Large language models (LLMs) are able to generate grammatically well-formed\ntext, but how do they encode their syntactic knowledge internally? While prior\nwork has focused largely on binary grammatical contrasts, in this work, we\nstudy the representation and control of two multidimensional hierarchical\ngrammar phenomena - verb tense and aspect - and for each, identify distinct,\northogonal directions in residual space using linear discriminant analysis.\nNext, we demonstrate causal control over both grammatical features through\nconcept steering across three generation tasks. Then, we use these identified\nfeatures in a case study to investigate factors influencing effective steering\nin multi-token generation. We find that steering strength, location, and\nduration are crucial parameters for reducing undesirable side effects such as\ntopic shift and degeneration. Our findings suggest that models encode tense and\naspect in structurally organized, human-like ways, but effective control of\nsuch features during generation is sensitive to multiple factors and requires\nmanual tuning or automated optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are able to generate grammatically well-formed\ntext, but how do they encode their syntactic knowledge internally? While prior\nwork has focused largely on binary grammatical contrasts, in this work, we\nstudy the representation and control of two multidimensional hierarchical\ngrammar phenomena - verb tense and aspect - and for each, identify distinct,\northogonal directions in residual space using linear discriminant analysis.\nNext, we demonstrate causal control over both grammatical features through\nconcept steering across three generation tasks. Then, we use these identified\nfeatures in a case study to investigate factors influencing effective steering\nin multi-token generation. We find that steering strength, location, and\nduration are crucial parameters for reducing undesirable side effects such as\ntopic shift and degeneration. Our findings suggest that models encode tense and\naspect in structurally organized, human-like ways, but effective control of\nsuch features during generation is sensitive to multiple factors and requires\nmanual tuning or automated optimization."
                },
                "authors": [
                    {
                        "name": "Alina Klerings"
                    },
                    {
                        "name": "Jannik Brinkmann"
                    },
                    {
                        "name": "Daniel Ruffinelli"
                    },
                    {
                        "name": "Simone Ponzetto"
                    }
                ],
                "author_detail": {
                    "name": "Simone Ponzetto"
                },
                "author": "Simone Ponzetto",
                "arxiv_comment": "to be published in The 2025 Conference on Empirical Methods in\n  Natural Language Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09297v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09297v3",
                "updated": "2025-09-15T15:46:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    15,
                    46,
                    38,
                    0,
                    258,
                    0
                ],
                "published": "2025-03-12T11:46:29Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    11,
                    46,
                    29,
                    2,
                    71,
                    0
                ],
                "title": "Gravitational form factors and mechanical properties of the nucleon in a\n  meson dominance approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational form factors and mechanical properties of the nucleon in a\n  meson dominance approach"
                },
                "summary": "We analyze the gravitational form factors and mechanical properties of the\nnucleon, focusing on both some general issues as well as on modeling with meson\ndominance. We show that the lattice QCD results for the nucleon gravitational\nform factors at $m_\\pi=170$~MeV, available for space-like momentum transfer\nsquared up to 2GeV, are explained in a natural way within the meson dominance\napproach. We carry out the proper Raman spin decomposition of the\nenergy-momentum tensor and in each spin channel use a minimum number of\nresonances consistent with the perturbative QCD short-distance constraints.\nThese constraints are related to the super-convergence sum rules, following\nfrom the asymptotic perturbative QCD fall-off of the form factors. The value of\nthe nucleon $D$-term following from the fits is -3.0(4). Next, we obtain the\ntwo-dimensional transverse gravitational densities of the nucleon in the\ntransverse coordinate $b$. With the super-convergence sum rules, we derive new\nsum rules for these densities at the origin and for their derivatives,\ninvolving logarithmic weighting in the corresponding spectral density\nintegrals. From analysis of the threshold behavior in the time-like region and\nthe properties of the $\\pi\\pi \\to N\\bar{N}$ reaction, we infer the behavior of\nthe transverse densities at asymptotically large coordinates. We also carry out\nthe meson dominance analysis of the two- and three-dimensional mechanical\nproperties of the nucleon (the pressure and stress) and explore their\nconnection to the spectral densities via dispersion relations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyze the gravitational form factors and mechanical properties of the\nnucleon, focusing on both some general issues as well as on modeling with meson\ndominance. We show that the lattice QCD results for the nucleon gravitational\nform factors at $m_\\pi=170$~MeV, available for space-like momentum transfer\nsquared up to 2GeV, are explained in a natural way within the meson dominance\napproach. We carry out the proper Raman spin decomposition of the\nenergy-momentum tensor and in each spin channel use a minimum number of\nresonances consistent with the perturbative QCD short-distance constraints.\nThese constraints are related to the super-convergence sum rules, following\nfrom the asymptotic perturbative QCD fall-off of the form factors. The value of\nthe nucleon $D$-term following from the fits is -3.0(4). Next, we obtain the\ntwo-dimensional transverse gravitational densities of the nucleon in the\ntransverse coordinate $b$. With the super-convergence sum rules, we derive new\nsum rules for these densities at the origin and for their derivatives,\ninvolving logarithmic weighting in the corresponding spectral density\nintegrals. From analysis of the threshold behavior in the time-like region and\nthe properties of the $\\pi\\pi \\to N\\bar{N}$ reaction, we infer the behavior of\nthe transverse densities at asymptotically large coordinates. We also carry out\nthe meson dominance analysis of the two- and three-dimensional mechanical\nproperties of the nucleon (the pressure and stress) and explore their\nconnection to the spectral densities via dispersion relations."
                },
                "authors": [
                    {
                        "name": "Wojciech Broniowski"
                    },
                    {
                        "name": "Enrique Ruiz Arriola"
                    }
                ],
                "author_detail": {
                    "name": "Enrique Ruiz Arriola"
                },
                "author": "Enrique Ruiz Arriola",
                "arxiv_comment": "21 pages, 12 figures. References added and typos corrected",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09297v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09297v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-lat",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03562v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03562v3",
                "updated": "2025-09-15T15:34:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    15,
                    34,
                    58,
                    0,
                    258,
                    0
                ],
                "published": "2024-11-05T23:55:23Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    23,
                    55,
                    23,
                    1,
                    310,
                    0
                ],
                "title": "Kolb-Based Experiential Learning for Generalist Agents with Human-Level\n  Kaggle Data Science Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kolb-Based Experiential Learning for Generalist Agents with Human-Level\n  Kaggle Data Science Performance"
                },
                "summary": "Human expertise emerges through iterative cycles of interaction, reflection,\nand internal model updating, which are central to cognitive theories such as\nKolb's experiential learning and Vygotsky's zone of proximal development. In\ncontrast, current AI systems, particularly LLM agents, rely on static\npre-training or rigid workflows, lacking mechanisms for continual adaptation.\nRecent studies identified early cognitive traits in LLM agents (reflection,\nrevision, and self-correction) suggesting foundational elements of human-like\nexperiential learning. Thus the key question: Can we design LLM agents capable\nof structured, cognitively grounded learning similar to human processes? In\nresponse, we propose a computational framework of Kolb's learning cycle with\nVygotsky's ZPD for autonomous agents. Our architecture separates extrinsic\n(environment interaction) and intrinsic (internal reflection/abstraction)\nfunctions, enabling cognitively grounded scaffolded learning, where the agent\ninitially learns within structured environments, followed by open-ended\ngeneralisation. This approach empowers agents to master complex tasks ; domains\nthat traditional fine-tuning or simple reflective methods could not tackle\neffectively. Its potential is powerfully demonstrated via direct comparison\nwith humans in real-world Kaggle data science competitions. Learning fully\nautomated data science code generation across 81 tasks, our system, Agent K,\ndemonstrated the ability to perform the entire workflow autonomously, achieving\nan Elo-MMR score of 1694, beyond median score of the Kaggle Masters (the top 2%\namong 200,000 users) of our study. With 9 gold, 8 silver, and 12 bronze medals\nlevel performance - including 4 gold and 4 silver on prize-awarding\ncompetitions - Agent K is the 1st AI system to successfully integrate Kolb- and\nVygotsky-inspired human cognitive learning, marking a major step toward\ngeneralist AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human expertise emerges through iterative cycles of interaction, reflection,\nand internal model updating, which are central to cognitive theories such as\nKolb's experiential learning and Vygotsky's zone of proximal development. In\ncontrast, current AI systems, particularly LLM agents, rely on static\npre-training or rigid workflows, lacking mechanisms for continual adaptation.\nRecent studies identified early cognitive traits in LLM agents (reflection,\nrevision, and self-correction) suggesting foundational elements of human-like\nexperiential learning. Thus the key question: Can we design LLM agents capable\nof structured, cognitively grounded learning similar to human processes? In\nresponse, we propose a computational framework of Kolb's learning cycle with\nVygotsky's ZPD for autonomous agents. Our architecture separates extrinsic\n(environment interaction) and intrinsic (internal reflection/abstraction)\nfunctions, enabling cognitively grounded scaffolded learning, where the agent\ninitially learns within structured environments, followed by open-ended\ngeneralisation. This approach empowers agents to master complex tasks ; domains\nthat traditional fine-tuning or simple reflective methods could not tackle\neffectively. Its potential is powerfully demonstrated via direct comparison\nwith humans in real-world Kaggle data science competitions. Learning fully\nautomated data science code generation across 81 tasks, our system, Agent K,\ndemonstrated the ability to perform the entire workflow autonomously, achieving\nan Elo-MMR score of 1694, beyond median score of the Kaggle Masters (the top 2%\namong 200,000 users) of our study. With 9 gold, 8 silver, and 12 bronze medals\nlevel performance - including 4 gold and 4 silver on prize-awarding\ncompetitions - Agent K is the 1st AI system to successfully integrate Kolb- and\nVygotsky-inspired human cognitive learning, marking a major step toward\ngeneralist AI."
                },
                "authors": [
                    {
                        "name": "Antoine Grosnit"
                    },
                    {
                        "name": "Alexandre Maraval"
                    },
                    {
                        "name": "Refinath S N"
                    },
                    {
                        "name": "Zichao Zhao"
                    },
                    {
                        "name": "James Doran"
                    },
                    {
                        "name": "Giuseppe Paolo"
                    },
                    {
                        "name": "Albert Thomas"
                    },
                    {
                        "name": "Jonas Gonzalez"
                    },
                    {
                        "name": "Abhineet Kumar"
                    },
                    {
                        "name": "Khyati Khandelwal"
                    },
                    {
                        "name": "Abdelhakim Benechehab"
                    },
                    {
                        "name": "Hamza Cherkaoui"
                    },
                    {
                        "name": "Youssef Attia El-Hili"
                    },
                    {
                        "name": "Kun Shao"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Jun Yao"
                    },
                    {
                        "name": "Balázs Kégl"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03562v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03562v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12052v1",
                "updated": "2025-09-15T15:34:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    15,
                    34,
                    2,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T15:34:02Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    15,
                    34,
                    2,
                    0,
                    258,
                    0
                ],
                "title": "AvatarSync: Rethinking Talking-Head Animation through Autoregressive\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AvatarSync: Rethinking Talking-Head Animation through Autoregressive\n  Perspective"
                },
                "summary": "Existing talking-head animation approaches based on Generative Adversarial\nNetworks (GANs) or diffusion models often suffer from inter-frame flicker,\nidentity drift, and slow inference. These limitations inherent to their video\ngeneration pipelines restrict their suitability for applications. To address\nthis, we introduce AvatarSync, an autoregressive framework on phoneme\nrepresentations that generates realistic and controllable talking-head\nanimations from a single reference image, driven directly text or audio input.\nIn addition, AvatarSync adopts a two-stage generation strategy, decoupling\nsemantic modeling from visual dynamics, which is a deliberate \"Divide and\nConquer\" design. The first stage, Facial Keyframe Generation (FKG), focuses on\nphoneme-level semantic representation by leveraging the many-to-one mapping\nfrom text or audio to phonemes. A Phoneme-to-Visual Mapping is constructed to\nanchor abstract phonemes to character-level units. Combined with a customized\nText-Frame Causal Attention Mask, the keyframes are generated. The second\nstage, inter-frame interpolation, emphasizes temporal coherence and visual\nsmoothness. We introduce a timestamp-aware adaptive strategy based on a\nselective state space model, enabling efficient bidirectional context\nreasoning. To support deployment, we optimize the inference pipeline to reduce\nlatency without compromising visual fidelity. Extensive experiments show that\nAvatarSync outperforms existing talking-head animation methods in visual\nfidelity, temporal consistency, and computational efficiency, providing a\nscalable and controllable solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing talking-head animation approaches based on Generative Adversarial\nNetworks (GANs) or diffusion models often suffer from inter-frame flicker,\nidentity drift, and slow inference. These limitations inherent to their video\ngeneration pipelines restrict their suitability for applications. To address\nthis, we introduce AvatarSync, an autoregressive framework on phoneme\nrepresentations that generates realistic and controllable talking-head\nanimations from a single reference image, driven directly text or audio input.\nIn addition, AvatarSync adopts a two-stage generation strategy, decoupling\nsemantic modeling from visual dynamics, which is a deliberate \"Divide and\nConquer\" design. The first stage, Facial Keyframe Generation (FKG), focuses on\nphoneme-level semantic representation by leveraging the many-to-one mapping\nfrom text or audio to phonemes. A Phoneme-to-Visual Mapping is constructed to\nanchor abstract phonemes to character-level units. Combined with a customized\nText-Frame Causal Attention Mask, the keyframes are generated. The second\nstage, inter-frame interpolation, emphasizes temporal coherence and visual\nsmoothness. We introduce a timestamp-aware adaptive strategy based on a\nselective state space model, enabling efficient bidirectional context\nreasoning. To support deployment, we optimize the inference pipeline to reduce\nlatency without compromising visual fidelity. Extensive experiments show that\nAvatarSync outperforms existing talking-head animation methods in visual\nfidelity, temporal consistency, and computational efficiency, providing a\nscalable and controllable solution."
                },
                "authors": [
                    {
                        "name": "Yuchen Deng"
                    },
                    {
                        "name": "Xiuyang Wu"
                    },
                    {
                        "name": "Hai-Tao Zheng"
                    },
                    {
                        "name": "Suiyang Zhang"
                    },
                    {
                        "name": "Yi He"
                    },
                    {
                        "name": "Yuxing Han"
                    }
                ],
                "author_detail": {
                    "name": "Yuxing Han"
                },
                "author": "Yuxing Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20220v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20220v2",
                "updated": "2025-09-15T15:25:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    15,
                    25,
                    4,
                    0,
                    258,
                    0
                ],
                "published": "2025-02-27T16:00:11Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    0,
                    11,
                    3,
                    58,
                    0
                ],
                "title": "Avat3r: Large Animatable Gaussian Reconstruction Model for High-fidelity\n  3D Head Avatars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Avat3r: Large Animatable Gaussian Reconstruction Model for High-fidelity\n  3D Head Avatars"
                },
                "summary": "Traditionally, creating photo-realistic 3D head avatars requires a\nstudio-level multi-view capture setup and expensive optimization during\ntest-time, limiting the use of digital human doubles to the VFX industry or\noffline renderings.\n  To address this shortcoming, we present Avat3r, which regresses a\nhigh-quality and animatable 3D head avatar from just a few input images, vastly\nreducing compute requirements during inference. More specifically, we make\nLarge Reconstruction Models animatable and learn a powerful prior over 3D human\nheads from a large multi-view video dataset. For better 3D head\nreconstructions, we employ position maps from DUSt3R and generalized feature\nmaps from the human foundation model Sapiens. To animate the 3D head, our key\ndiscovery is that simple cross-attention to an expression code is already\nsufficient. Finally, we increase robustness by feeding input images with\ndifferent expressions to our model during training, enabling the reconstruction\nof 3D head avatars from inconsistent inputs, e.g., an imperfect phone capture\nwith accidental movement, or frames from a monocular video.\n  We compare Avat3r with current state-of-the-art methods for few-input and\nsingle-input scenarios, and find that our method has a competitive advantage in\nboth tasks. Finally, we demonstrate the wide applicability of our proposed\nmodel, creating 3D head avatars from images of different sources, smartphone\ncaptures, single images, and even out-of-domain inputs like antique busts.\n  Project website: https://tobias-kirschstein.github.io/avat3r/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditionally, creating photo-realistic 3D head avatars requires a\nstudio-level multi-view capture setup and expensive optimization during\ntest-time, limiting the use of digital human doubles to the VFX industry or\noffline renderings.\n  To address this shortcoming, we present Avat3r, which regresses a\nhigh-quality and animatable 3D head avatar from just a few input images, vastly\nreducing compute requirements during inference. More specifically, we make\nLarge Reconstruction Models animatable and learn a powerful prior over 3D human\nheads from a large multi-view video dataset. For better 3D head\nreconstructions, we employ position maps from DUSt3R and generalized feature\nmaps from the human foundation model Sapiens. To animate the 3D head, our key\ndiscovery is that simple cross-attention to an expression code is already\nsufficient. Finally, we increase robustness by feeding input images with\ndifferent expressions to our model during training, enabling the reconstruction\nof 3D head avatars from inconsistent inputs, e.g., an imperfect phone capture\nwith accidental movement, or frames from a monocular video.\n  We compare Avat3r with current state-of-the-art methods for few-input and\nsingle-input scenarios, and find that our method has a competitive advantage in\nboth tasks. Finally, we demonstrate the wide applicability of our proposed\nmodel, creating 3D head avatars from images of different sources, smartphone\ncaptures, single images, and even out-of-domain inputs like antique busts.\n  Project website: https://tobias-kirschstein.github.io/avat3r/"
                },
                "authors": [
                    {
                        "name": "Tobias Kirschstein"
                    },
                    {
                        "name": "Javier Romero"
                    },
                    {
                        "name": "Artem Sevastopolsky"
                    },
                    {
                        "name": "Matthias Nießner"
                    },
                    {
                        "name": "Shunsuke Saito"
                    }
                ],
                "author_detail": {
                    "name": "Shunsuke Saito"
                },
                "author": "Shunsuke Saito",
                "arxiv_comment": "Project website: https://tobias-kirschstein.github.io/avat3r/, Video:\n  https://youtu.be/P3zNVx15gYs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20220v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20220v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12040v1",
                "updated": "2025-09-15T15:24:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    15,
                    24,
                    49,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T15:24:49Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    15,
                    24,
                    49,
                    0,
                    258,
                    0
                ],
                "title": "Exploring Efficient Open-Vocabulary Segmentation in the Remote Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Efficient Open-Vocabulary Segmentation in the Remote Sensing"
                },
                "summary": "Open-Vocabulary Remote Sensing Image Segmentation (OVRSIS), an emerging task\nthat adapts Open-Vocabulary Segmentation (OVS) to the remote sensing (RS)\ndomain, remains underexplored due to the absence of a unified evaluation\nbenchmark and the domain gap between natural and RS images. To bridge these\ngaps, we first establish a standardized OVRSIS benchmark (\\textbf{OVRSISBench})\nbased on widely-used RS segmentation datasets, enabling consistent evaluation\nacross methods. Using this benchmark, we comprehensively evaluate several\nrepresentative OVS/OVRSIS models and reveal their limitations when directly\napplied to remote sensing scenarios. Building on these insights, we propose\n\\textbf{RSKT-Seg}, a novel open-vocabulary segmentation framework tailored for\nremote sensing. RSKT-Seg integrates three key components: (1) a\nMulti-Directional Cost Map Aggregation (RS-CMA) module that captures\nrotation-invariant visual cues by computing vision-language cosine similarities\nacross multiple directions; (2) an Efficient Cost Map Fusion (RS-Fusion)\ntransformer, which jointly models spatial and semantic dependencies with a\nlightweight dimensionality reduction strategy; and (3) a Remote Sensing\nKnowledge Transfer (RS-Transfer) module that injects pre-trained knowledge and\nfacilitates domain adaptation via enhanced upsampling. Extensive experiments on\nthe benchmark show that RSKT-Seg consistently outperforms strong OVS baselines\nby +3.8 mIoU and +5.9 mACC, while achieving 2x faster inference through\nefficient aggregation. Our code is\n\\href{https://github.com/LiBingyu01/RSKT-Seg}{\\textcolor{blue}{here}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-Vocabulary Remote Sensing Image Segmentation (OVRSIS), an emerging task\nthat adapts Open-Vocabulary Segmentation (OVS) to the remote sensing (RS)\ndomain, remains underexplored due to the absence of a unified evaluation\nbenchmark and the domain gap between natural and RS images. To bridge these\ngaps, we first establish a standardized OVRSIS benchmark (\\textbf{OVRSISBench})\nbased on widely-used RS segmentation datasets, enabling consistent evaluation\nacross methods. Using this benchmark, we comprehensively evaluate several\nrepresentative OVS/OVRSIS models and reveal their limitations when directly\napplied to remote sensing scenarios. Building on these insights, we propose\n\\textbf{RSKT-Seg}, a novel open-vocabulary segmentation framework tailored for\nremote sensing. RSKT-Seg integrates three key components: (1) a\nMulti-Directional Cost Map Aggregation (RS-CMA) module that captures\nrotation-invariant visual cues by computing vision-language cosine similarities\nacross multiple directions; (2) an Efficient Cost Map Fusion (RS-Fusion)\ntransformer, which jointly models spatial and semantic dependencies with a\nlightweight dimensionality reduction strategy; and (3) a Remote Sensing\nKnowledge Transfer (RS-Transfer) module that injects pre-trained knowledge and\nfacilitates domain adaptation via enhanced upsampling. Extensive experiments on\nthe benchmark show that RSKT-Seg consistently outperforms strong OVS baselines\nby +3.8 mIoU and +5.9 mACC, while achieving 2x faster inference through\nefficient aggregation. Our code is\n\\href{https://github.com/LiBingyu01/RSKT-Seg}{\\textcolor{blue}{here}}."
                },
                "authors": [
                    {
                        "name": "Bingyu Li"
                    },
                    {
                        "name": "Haocheng Dong"
                    },
                    {
                        "name": "Da Zhang"
                    },
                    {
                        "name": "Zhiyuan Zhao"
                    },
                    {
                        "name": "Junyu Gao"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08641v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08641v2",
                "updated": "2025-09-15T15:13:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    15,
                    13,
                    41,
                    0,
                    258,
                    0
                ],
                "published": "2024-12-11T18:59:17Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    59,
                    17,
                    2,
                    346,
                    0
                ],
                "title": "3D Mesh Editing using Masked LRMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Mesh Editing using Masked LRMs"
                },
                "summary": "We present a novel approach to shape editing, building on recent progress in\n3D reconstruction from multi-view images. We formulate shape editing as a\nconditional reconstruction problem, where the model must reconstruct the input\nshape with the exception of a specified 3D region, in which the geometry should\nbe generated from the conditional signal. To this end, we train a conditional\nLarge Reconstruction Model (LRM) for masked reconstruction, using multi-view\nconsistent masks rendered from a randomly generated 3D occlusion, and using one\nclean viewpoint as the conditional signal. During inference, we manually define\na 3D region to edit and provide an edited image from a canonical viewpoint to\nfill that region. We demonstrate that, in just a single forward pass, our\nmethod not only preserves the input geometry in the unmasked region through\nreconstruction capabilities on par with SoTA, but is also expressive enough to\nperform a variety of mesh edits from a single image guidance that past works\nstruggle with, while being 2-10x faster than the top-performing prior work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach to shape editing, building on recent progress in\n3D reconstruction from multi-view images. We formulate shape editing as a\nconditional reconstruction problem, where the model must reconstruct the input\nshape with the exception of a specified 3D region, in which the geometry should\nbe generated from the conditional signal. To this end, we train a conditional\nLarge Reconstruction Model (LRM) for masked reconstruction, using multi-view\nconsistent masks rendered from a randomly generated 3D occlusion, and using one\nclean viewpoint as the conditional signal. During inference, we manually define\na 3D region to edit and provide an edited image from a canonical viewpoint to\nfill that region. We demonstrate that, in just a single forward pass, our\nmethod not only preserves the input geometry in the unmasked region through\nreconstruction capabilities on par with SoTA, but is also expressive enough to\nperform a variety of mesh edits from a single image guidance that past works\nstruggle with, while being 2-10x faster than the top-performing prior work."
                },
                "authors": [
                    {
                        "name": "Will Gao"
                    },
                    {
                        "name": "Dilin Wang"
                    },
                    {
                        "name": "Yuchen Fan"
                    },
                    {
                        "name": "Aljaz Bozic"
                    },
                    {
                        "name": "Tuur Stuyck"
                    },
                    {
                        "name": "Zhengqin Li"
                    },
                    {
                        "name": "Zhao Dong"
                    },
                    {
                        "name": "Rakesh Ranjan"
                    },
                    {
                        "name": "Nikolaos Sarafianos"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Sarafianos"
                },
                "author": "Nikolaos Sarafianos",
                "arxiv_comment": "ICCV 2025. Project Page:\n  https://chocolatebiscuit.github.io/MaskedLRM/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08641v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08641v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12021v1",
                "updated": "2025-09-15T15:01:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    15,
                    1,
                    3,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T15:01:03Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    15,
                    1,
                    3,
                    0,
                    258,
                    0
                ],
                "title": "LitterBox+: An Extensible Framework for LLM-enhanced Scratch Static Code\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LitterBox+: An Extensible Framework for LLM-enhanced Scratch Static Code\n  Analysis"
                },
                "summary": "Large language models (LLMs) have become an essential tool to support\ndevelopers using traditional text-based programming languages, but the\ngraphical notation of the block-based Scratch programming environment inhibits\nthe use of LLMs. To overcome this limitation, we propose the LitterBox+\nframework that extends the Scratch static code analysis tool LitterBox with the\ngenerative abilities of LLMs. By converting block-based code to a textual\nrepresentation suitable for LLMs, LitterBox+ allows users to query LLMs about\ntheir programs, about quality issues reported by LitterBox, and it allows\ngenerating code fixes. Besides offering a programmatic API for these\nfunctionalities, LitterBox+ also extends the Scratch user interface to make\nthese functionalities available directly in the environment familiar to\nlearners. The framework is designed to be easily extensible with other prompts,\nLLM providers, and new features combining the program analysis capabilities of\nLitterBox with the generative features of LLMs. We provide a screencast\ndemonstrating the tool at https://youtu.be/RZ6E0xgrIgQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become an essential tool to support\ndevelopers using traditional text-based programming languages, but the\ngraphical notation of the block-based Scratch programming environment inhibits\nthe use of LLMs. To overcome this limitation, we propose the LitterBox+\nframework that extends the Scratch static code analysis tool LitterBox with the\ngenerative abilities of LLMs. By converting block-based code to a textual\nrepresentation suitable for LLMs, LitterBox+ allows users to query LLMs about\ntheir programs, about quality issues reported by LitterBox, and it allows\ngenerating code fixes. Besides offering a programmatic API for these\nfunctionalities, LitterBox+ also extends the Scratch user interface to make\nthese functionalities available directly in the environment familiar to\nlearners. The framework is designed to be easily extensible with other prompts,\nLLM providers, and new features combining the program analysis capabilities of\nLitterBox with the generative features of LLMs. We provide a screencast\ndemonstrating the tool at https://youtu.be/RZ6E0xgrIgQ."
                },
                "authors": [
                    {
                        "name": "Benedikt Fein"
                    },
                    {
                        "name": "Florian Obermüller"
                    },
                    {
                        "name": "Gordon Fraser"
                    }
                ],
                "author_detail": {
                    "name": "Gordon Fraser"
                },
                "author": "Gordon Fraser",
                "arxiv_comment": "ASE 2025 Tool Demonstration Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12019v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12019v1",
                "updated": "2025-09-15T14:59:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    59,
                    35,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T14:59:35Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    59,
                    35,
                    0,
                    258,
                    0
                ],
                "title": "AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of\n  Large Language Models"
                },
                "summary": "To enable broader deployment of Large Language Models (LLMs), it is essential\nto identify the best-performing model under strict memory constraints. We\npresent AMQ, Automated Mixed-Precision Weight-Only Quantization, a framework\nthat assigns layer-wise quantization bit-widths to optimally balance model\nquality and memory usage. However, the combinatorial search space, with over\n10^{100} possible configurations, makes conventional black-box optimization\ninfeasible. AMQ overcomes this challenge through four key innovations:(1)\nsearch space pruning using prior knowledge to exclude unpromising\nconfigurations, (2) quantization proxy to bypass costly format conversions\nduring search, (3) quality predictor to minimize evaluation overhead, and (4)\niterative search-and-update strategy for fast and stable convergence. By\nintegrating these components, AMQ efficiently explores the quality-efficiency\nlandscape, reaching the Pareto frontier and yielding LLMs that are both compact\nand high-performing. Our code is available at https://github.com/dlwns147/amq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To enable broader deployment of Large Language Models (LLMs), it is essential\nto identify the best-performing model under strict memory constraints. We\npresent AMQ, Automated Mixed-Precision Weight-Only Quantization, a framework\nthat assigns layer-wise quantization bit-widths to optimally balance model\nquality and memory usage. However, the combinatorial search space, with over\n10^{100} possible configurations, makes conventional black-box optimization\ninfeasible. AMQ overcomes this challenge through four key innovations:(1)\nsearch space pruning using prior knowledge to exclude unpromising\nconfigurations, (2) quantization proxy to bypass costly format conversions\nduring search, (3) quality predictor to minimize evaluation overhead, and (4)\niterative search-and-update strategy for fast and stable convergence. By\nintegrating these components, AMQ efficiently explores the quality-efficiency\nlandscape, reaching the Pareto frontier and yielding LLMs that are both compact\nand high-performing. Our code is available at https://github.com/dlwns147/amq."
                },
                "authors": [
                    {
                        "name": "Sangjun Lee"
                    },
                    {
                        "name": "Seung-taek Woo"
                    },
                    {
                        "name": "Jungyu Jin"
                    },
                    {
                        "name": "Changhun Lee"
                    },
                    {
                        "name": "Eunhyeok Park"
                    }
                ],
                "author_detail": {
                    "name": "Eunhyeok Park"
                },
                "author": "Eunhyeok Park",
                "arxiv_comment": "EMNLP 2025 Main Conference, Long Paper (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12019v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12019v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11115v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11115v4",
                "updated": "2025-09-15T14:57:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    57,
                    4,
                    0,
                    258,
                    0
                ],
                "published": "2025-02-16T13:12:31Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    13,
                    12,
                    31,
                    6,
                    47,
                    0
                ],
                "title": "Are Generative Models Underconfident? Better Quality Estimation with\n  Boosted Model Probability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Generative Models Underconfident? Better Quality Estimation with\n  Boosted Model Probability"
                },
                "summary": "Quality Estimation (QE) is estimating quality of the model output during\ninference when the ground truth is not available. Deriving output quality from\nthe models' output probability is the most trivial and low-effort way. However,\nwe show that the output probability of text-generation models can appear\nunderconfident. At each output step, there can be multiple correct options,\nmaking the probability distribution spread out more. Thus, lower probability\ndoes not necessarily mean lower output quality. Due to this observation, we\npropose a QE approach called BoostedProb, which boosts the model's confidence\nin cases where there are multiple viable output options. With no increase in\ncomplexity, BoostedProb is notably better than raw model probability in\ndifferent settings, achieving on average +0.194 improvement in Pearson\ncorrelation to ground-truth quality. It also comes close to or outperforms more\ncostly approaches like supervised or ensemble-based QE in certain settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quality Estimation (QE) is estimating quality of the model output during\ninference when the ground truth is not available. Deriving output quality from\nthe models' output probability is the most trivial and low-effort way. However,\nwe show that the output probability of text-generation models can appear\nunderconfident. At each output step, there can be multiple correct options,\nmaking the probability distribution spread out more. Thus, lower probability\ndoes not necessarily mean lower output quality. Due to this observation, we\npropose a QE approach called BoostedProb, which boosts the model's confidence\nin cases where there are multiple viable output options. With no increase in\ncomplexity, BoostedProb is notably better than raw model probability in\ndifferent settings, achieving on average +0.194 improvement in Pearson\ncorrelation to ground-truth quality. It also comes close to or outperforms more\ncostly approaches like supervised or ensemble-based QE in certain settings."
                },
                "authors": [
                    {
                        "name": "Tu Anh Dinh"
                    },
                    {
                        "name": "Jan Niehues"
                    }
                ],
                "author_detail": {
                    "name": "Jan Niehues"
                },
                "author": "Jan Niehues",
                "arxiv_comment": "Accepted to EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11115v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11115v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18240v2",
                "updated": "2025-09-15T14:50:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    50,
                    39,
                    0,
                    258,
                    0
                ],
                "published": "2025-08-22T12:14:17Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    14,
                    17,
                    4,
                    234,
                    0
                ],
                "title": "MTalk-Bench: Evaluating Speech-to-Speech Models in Multi-Turn Dialogues\n  via Arena-style and Rubrics Protocols",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MTalk-Bench: Evaluating Speech-to-Speech Models in Multi-Turn Dialogues\n  via Arena-style and Rubrics Protocols"
                },
                "summary": "The rapid advancement of speech-to-speech (S2S) large language models (LLMs)\nhas significantly improved real-time spoken interaction. However, current\nevaluation frameworks remain inadequate for assessing performance in complex,\nmulti-turn dialogues. To address this, we introduce MTalk-Bench, a multi-turn\nS2S benchmark covering three core dimensions: Semantic Information,\nParalinguistic Information, and Ambient Sound. Each dimension includes nine\nrealistic scenarios, along with targeted tasks to assess specific capabilities\nsuch as reasoning. Our dual-method evaluation framework combines Arena-style\nevaluation (pairwise comparison) and Rubrics-based evaluation (absolute\nscoring) for relative and absolute assessment. The benchmark includes both\nmodel and human outputs, evaluated by human evaluators and LLMs. Experimental\nresults reveal two sets of findings. Overall performance of S2S LLMs: (1)\nmodels excel at semantic information processing yet underperform on\nparalinguistic information and ambient sounds perception; (2) models typically\nregain coherence by increasing response length, sacrificing efficiency in\nmulti-turn dialogues; (3) modality-aware, task-specific designs outperform\nbrute scaling. Evaluation framework and reliability: (1) Arena and Rubrics\nyield consistent, complementary rankings, but reliable distinctions emerge only\nwhen performance gaps are large; (2) LLM-as-a-judge aligns with humans when\ngaps are clear or criteria explicit, but exhibits position and length biases\nand is reliable on nonverbal evaluation only with text annotations. These\nresults highlight current limitations in S2S evaluation and the need for more\nrobust, speech-aware assessment frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of speech-to-speech (S2S) large language models (LLMs)\nhas significantly improved real-time spoken interaction. However, current\nevaluation frameworks remain inadequate for assessing performance in complex,\nmulti-turn dialogues. To address this, we introduce MTalk-Bench, a multi-turn\nS2S benchmark covering three core dimensions: Semantic Information,\nParalinguistic Information, and Ambient Sound. Each dimension includes nine\nrealistic scenarios, along with targeted tasks to assess specific capabilities\nsuch as reasoning. Our dual-method evaluation framework combines Arena-style\nevaluation (pairwise comparison) and Rubrics-based evaluation (absolute\nscoring) for relative and absolute assessment. The benchmark includes both\nmodel and human outputs, evaluated by human evaluators and LLMs. Experimental\nresults reveal two sets of findings. Overall performance of S2S LLMs: (1)\nmodels excel at semantic information processing yet underperform on\nparalinguistic information and ambient sounds perception; (2) models typically\nregain coherence by increasing response length, sacrificing efficiency in\nmulti-turn dialogues; (3) modality-aware, task-specific designs outperform\nbrute scaling. Evaluation framework and reliability: (1) Arena and Rubrics\nyield consistent, complementary rankings, but reliable distinctions emerge only\nwhen performance gaps are large; (2) LLM-as-a-judge aligns with humans when\ngaps are clear or criteria explicit, but exhibits position and length biases\nand is reliable on nonverbal evaluation only with text annotations. These\nresults highlight current limitations in S2S evaluation and the need for more\nrobust, speech-aware assessment frameworks."
                },
                "authors": [
                    {
                        "name": "Yuhao Du"
                    },
                    {
                        "name": "Qianwei Huang"
                    },
                    {
                        "name": "Guo Zhu"
                    },
                    {
                        "name": "Zhanchen Dai"
                    },
                    {
                        "name": "Shunian Chen"
                    },
                    {
                        "name": "Qiming Zhu"
                    },
                    {
                        "name": "Le Pan"
                    },
                    {
                        "name": "Minghao Chen"
                    },
                    {
                        "name": "Yuhao Zhang"
                    },
                    {
                        "name": "Li Zhou"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Haizhou Li"
                    }
                ],
                "author_detail": {
                    "name": "Haizhou Li"
                },
                "author": "Haizhou Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13231v2",
                "updated": "2025-09-15T14:40:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    40,
                    16,
                    0,
                    258,
                    0
                ],
                "published": "2025-08-17T19:07:08Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    19,
                    7,
                    8,
                    6,
                    229,
                    0
                ],
                "title": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System"
                },
                "summary": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference."
                },
                "authors": [
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Kaoutar El Maghraoui"
                    },
                    {
                        "name": "Naigang Wang"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "IEEE Computer Architecture Letter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09198v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09198v2",
                "updated": "2025-09-15T14:23:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    23,
                    51,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-11T07:13:44Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    7,
                    13,
                    44,
                    3,
                    254,
                    0
                ],
                "title": "GmSLM : Generative Marmoset Spoken Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GmSLM : Generative Marmoset Spoken Language Modeling"
                },
                "summary": "Marmoset monkeys exhibit complex vocal communication, challenging the view\nthat nonhuman primates vocal communication is entirely innate, and show similar\nfeatures of human speech, such as vocal labeling of others and turn-taking.\nStudying their vocal communication offers a unique opportunity to link it with\nbrain activity-especially given the difficulty of accessing the human brain in\nspeech and language research. Since Marmosets communicate primarily through\nvocalizations, applying standard LLM approaches is not straightforward. We\nintroduce Generative Marmoset Spoken Language Modeling (GmSLM), an optimized\nspoken language model pipeline for Marmoset vocal communication. We designed a\nnovel zero-shot evaluation metrics using unsupervised in-the-wild data,\nalongside weakly labeled conversational data, to assess GmSLM and demonstrate\nits advantage over a basic human-speech-based baseline. GmSLM generated\nvocalizations closely matched real resynthesized samples acoustically and\nperformed well on downstream tasks. Despite being fully unsupervised, GmSLM\neffectively distinguish real from artificial conversations and may support\nfurther investigations of the neural basis of vocal communication and provides\na practical framework linking vocalization and brain activity. We believe GmSLM\nstands to benefit future work in neuroscience, bioacoustics, and evolutionary\nbiology. Samples are provided under: pages.cs.huji.ac.il/adiyoss-lab/GmSLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marmoset monkeys exhibit complex vocal communication, challenging the view\nthat nonhuman primates vocal communication is entirely innate, and show similar\nfeatures of human speech, such as vocal labeling of others and turn-taking.\nStudying their vocal communication offers a unique opportunity to link it with\nbrain activity-especially given the difficulty of accessing the human brain in\nspeech and language research. Since Marmosets communicate primarily through\nvocalizations, applying standard LLM approaches is not straightforward. We\nintroduce Generative Marmoset Spoken Language Modeling (GmSLM), an optimized\nspoken language model pipeline for Marmoset vocal communication. We designed a\nnovel zero-shot evaluation metrics using unsupervised in-the-wild data,\nalongside weakly labeled conversational data, to assess GmSLM and demonstrate\nits advantage over a basic human-speech-based baseline. GmSLM generated\nvocalizations closely matched real resynthesized samples acoustically and\nperformed well on downstream tasks. Despite being fully unsupervised, GmSLM\neffectively distinguish real from artificial conversations and may support\nfurther investigations of the neural basis of vocal communication and provides\na practical framework linking vocalization and brain activity. We believe GmSLM\nstands to benefit future work in neuroscience, bioacoustics, and evolutionary\nbiology. Samples are provided under: pages.cs.huji.ac.il/adiyoss-lab/GmSLM."
                },
                "authors": [
                    {
                        "name": "Talia Sternberg"
                    },
                    {
                        "name": "Michael London"
                    },
                    {
                        "name": "David Omer"
                    },
                    {
                        "name": "Yossi Adi"
                    }
                ],
                "author_detail": {
                    "name": "Yossi Adi"
                },
                "author": "Yossi Adi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09198v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09198v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14403v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14403v4",
                "updated": "2025-09-15T14:23:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    23,
                    10,
                    0,
                    258,
                    0
                ],
                "published": "2025-05-20T14:16:49Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    16,
                    49,
                    1,
                    140,
                    0
                ],
                "title": "Unearthing Gems from Stones: Policy Optimization with Negative Sample\n  Augmentation for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unearthing Gems from Stones: Policy Optimization with Negative Sample\n  Augmentation for LLM Reasoning"
                },
                "summary": "Recent advances in reasoning language models have witnessed a paradigm shift\nfrom short to long CoT pattern. Given the substantial computational cost of\nrollouts in long CoT models, maximizing the utility of fixed training datasets\nbecomes crucial. Our analysis reveals that negative responses contain valuable\ncomponents such as self-reflection and error-correction steps, yet primary\nexisting methods either completely discard negative samples (RFT) or apply\nequal penalization across all tokens (RL), failing to leverage these potential\nlearning signals. In light of this, we propose Behavior Constrained Policy\nGradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline\nRL framework that encompasses three stages: 1) sample segmentation, 2)\nconsensus-based step correctness assessment combining LLM and PRM judgers, and\n3) policy optimization with NSA designed to effectively mine positive steps\nwithin negative samples. Experimental results show that BCPG-NSA outperforms\nbaselines on several challenging math/coding reasoning benchmarks using the\nsame training dataset, achieving improved sample efficiency and demonstrating\nrobustness and scalability when extended to multiple iterations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reasoning language models have witnessed a paradigm shift\nfrom short to long CoT pattern. Given the substantial computational cost of\nrollouts in long CoT models, maximizing the utility of fixed training datasets\nbecomes crucial. Our analysis reveals that negative responses contain valuable\ncomponents such as self-reflection and error-correction steps, yet primary\nexisting methods either completely discard negative samples (RFT) or apply\nequal penalization across all tokens (RL), failing to leverage these potential\nlearning signals. In light of this, we propose Behavior Constrained Policy\nGradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline\nRL framework that encompasses three stages: 1) sample segmentation, 2)\nconsensus-based step correctness assessment combining LLM and PRM judgers, and\n3) policy optimization with NSA designed to effectively mine positive steps\nwithin negative samples. Experimental results show that BCPG-NSA outperforms\nbaselines on several challenging math/coding reasoning benchmarks using the\nsame training dataset, achieving improved sample efficiency and demonstrating\nrobustness and scalability when extended to multiple iterations."
                },
                "authors": [
                    {
                        "name": "Zhaohui Yang"
                    },
                    {
                        "name": "Yuxiao Ye"
                    },
                    {
                        "name": "Shilei Jiang"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Linjing Li"
                    },
                    {
                        "name": "Shihong Deng"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14403v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14403v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11967v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11967v2",
                "updated": "2025-09-16T02:36:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    2,
                    36,
                    5,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-15T14:18:51Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    18,
                    51,
                    0,
                    258,
                    0
                ],
                "title": "MillStone: How Open-Minded Are LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MillStone: How Open-Minded Are LLMs?"
                },
                "summary": "Large language models equipped with Web search, information retrieval tools,\nand other agentic capabilities are beginning to supplant traditional search\nengines. As users start to rely on LLMs for information on many topics,\nincluding controversial and debatable issues, it is important to understand how\nthe stances and opinions expressed in LLM outputs are influenced by the\ndocuments they use as their information sources.\n  In this paper, we present MillStone, the first benchmark that aims to\nsystematically measure the effect of external arguments on the stances that\nLLMs take on controversial issues (not all of them political). We apply\nMillStone to nine leading LLMs and measure how ``open-minded'' they are to\narguments supporting opposite sides of these issues, whether different LLMs\nagree with each other, which arguments LLMs find most persuasive, and whether\nthese arguments are the same for different LLMs.\n  In general, we find that LLMs are open-minded on most issues. An\nauthoritative source of information can easily sway an LLM's stance,\nhighlighting the importance of source selection and the risk that LLM-based\ninformation retrieval and search systems can be manipulated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models equipped with Web search, information retrieval tools,\nand other agentic capabilities are beginning to supplant traditional search\nengines. As users start to rely on LLMs for information on many topics,\nincluding controversial and debatable issues, it is important to understand how\nthe stances and opinions expressed in LLM outputs are influenced by the\ndocuments they use as their information sources.\n  In this paper, we present MillStone, the first benchmark that aims to\nsystematically measure the effect of external arguments on the stances that\nLLMs take on controversial issues (not all of them political). We apply\nMillStone to nine leading LLMs and measure how ``open-minded'' they are to\narguments supporting opposite sides of these issues, whether different LLMs\nagree with each other, which arguments LLMs find most persuasive, and whether\nthese arguments are the same for different LLMs.\n  In general, we find that LLMs are open-minded on most issues. An\nauthoritative source of information can easily sway an LLM's stance,\nhighlighting the importance of source selection and the risk that LLM-based\ninformation retrieval and search systems can be manipulated."
                },
                "authors": [
                    {
                        "name": "Harold Triedman"
                    },
                    {
                        "name": "Vitaly Shmatikov"
                    }
                ],
                "author_detail": {
                    "name": "Vitaly Shmatikov"
                },
                "author": "Vitaly Shmatikov",
                "arxiv_comment": "19 pages, 7 tables, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11967v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11967v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11966v1",
                "updated": "2025-09-15T14:18:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    18,
                    49,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T14:18:49Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    18,
                    49,
                    0,
                    258,
                    0
                ],
                "title": "Deep operator network for surrogate modeling of poroelasticity with\n  random permeability fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep operator network for surrogate modeling of poroelasticity with\n  random permeability fields"
                },
                "summary": "Poroelasticity -- coupled fluid flow and elastic deformation in porous media\n-- often involves spatially variable permeability, especially in subsurface\nsystems. In such cases, simulations with random permeability fields are widely\nused for probabilistic analysis, uncertainty quantification, and inverse\nproblems. These simulations require repeated forward solves that are often\nprohibitively expensive, motivating the development of efficient surrogate\nmodels. However, efficient surrogate modeling techniques for poroelasticity\nwith random permeability fields remain scarce. In this study, we propose a\nsurrogate modeling framework based on the deep operator network (DeepONet), a\nneural architecture designed to learn mappings between infinite-dimensional\nfunction spaces. The proposed surrogate model approximates the solution\noperator that maps random permeability fields to transient poroelastic\nresponses. To enhance predictive accuracy and stability, we integrate three\nstrategies: nondimensionalization of the governing equations, input\ndimensionality reduction via Karhunen--Lo\\'eve expansion, and a two-step\ntraining procedure that decouples the optimization of branch and trunk\nnetworks. The methodology is evaluated on two benchmark problems in\nporoelasticity: soil consolidation and ground subsidence induced by groundwater\nextraction. In both cases, the DeepONet achieves substantial speedup in\ninference while maintaining high predictive accuracy across a wide range of\npermeability statistics. These results highlight the potential of the proposed\napproach as a scalable and efficient surrogate modeling technique for\nporoelastic systems with random permeability fields.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Poroelasticity -- coupled fluid flow and elastic deformation in porous media\n-- often involves spatially variable permeability, especially in subsurface\nsystems. In such cases, simulations with random permeability fields are widely\nused for probabilistic analysis, uncertainty quantification, and inverse\nproblems. These simulations require repeated forward solves that are often\nprohibitively expensive, motivating the development of efficient surrogate\nmodels. However, efficient surrogate modeling techniques for poroelasticity\nwith random permeability fields remain scarce. In this study, we propose a\nsurrogate modeling framework based on the deep operator network (DeepONet), a\nneural architecture designed to learn mappings between infinite-dimensional\nfunction spaces. The proposed surrogate model approximates the solution\noperator that maps random permeability fields to transient poroelastic\nresponses. To enhance predictive accuracy and stability, we integrate three\nstrategies: nondimensionalization of the governing equations, input\ndimensionality reduction via Karhunen--Lo\\'eve expansion, and a two-step\ntraining procedure that decouples the optimization of branch and trunk\nnetworks. The methodology is evaluated on two benchmark problems in\nporoelasticity: soil consolidation and ground subsidence induced by groundwater\nextraction. In both cases, the DeepONet achieves substantial speedup in\ninference while maintaining high predictive accuracy across a wide range of\npermeability statistics. These results highlight the potential of the proposed\napproach as a scalable and efficient surrogate modeling technique for\nporoelastic systems with random permeability fields."
                },
                "authors": [
                    {
                        "name": "Sangjoon Park"
                    },
                    {
                        "name": "Yeonjong Shin"
                    },
                    {
                        "name": "Jinhyun Choo"
                    }
                ],
                "author_detail": {
                    "name": "Jinhyun Choo"
                },
                "author": "Jinhyun Choo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11964v1",
                "updated": "2025-09-15T14:17:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    17,
                    34,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T14:17:34Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    17,
                    34,
                    0,
                    258,
                    0
                ],
                "title": "E2-BKI: Evidential Ellipsoidal Bayesian Kernel Inference for\n  Uncertainty-aware Gaussian Semantic Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E2-BKI: Evidential Ellipsoidal Bayesian Kernel Inference for\n  Uncertainty-aware Gaussian Semantic Mapping"
                },
                "summary": "Semantic mapping aims to construct a 3D semantic representation of the\nenvironment, providing essential knowledge for robots operating in complex\noutdoor settings. While Bayesian Kernel Inference (BKI) addresses\ndiscontinuities of map inference from sparse sensor data, existing semantic\nmapping methods suffer from various sources of uncertainties in challenging\noutdoor environments. To address these issues, we propose an uncertainty-aware\nsemantic mapping framework that handles multiple sources of uncertainties,\nwhich significantly degrade mapping performance. Our method estimates\nuncertainties in semantic predictions using Evidential Deep Learning and\nincorporates them into BKI for robust semantic inference. It further aggregates\nnoisy observations into coherent Gaussian representations to mitigate the\nimpact of unreliable points, while employing geometry-aligned kernels that\nadapt to complex scene structures. These Gaussian primitives effectively fuse\nlocal geometric and semantic information, enabling robust, uncertainty-aware\nmapping in complex outdoor scenarios. Comprehensive evaluation across diverse\noff-road and urban outdoor environments demonstrates consistent improvements in\nmapping quality, uncertainty calibration, representational flexibility, and\nrobustness, while maintaining real-time efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic mapping aims to construct a 3D semantic representation of the\nenvironment, providing essential knowledge for robots operating in complex\noutdoor settings. While Bayesian Kernel Inference (BKI) addresses\ndiscontinuities of map inference from sparse sensor data, existing semantic\nmapping methods suffer from various sources of uncertainties in challenging\noutdoor environments. To address these issues, we propose an uncertainty-aware\nsemantic mapping framework that handles multiple sources of uncertainties,\nwhich significantly degrade mapping performance. Our method estimates\nuncertainties in semantic predictions using Evidential Deep Learning and\nincorporates them into BKI for robust semantic inference. It further aggregates\nnoisy observations into coherent Gaussian representations to mitigate the\nimpact of unreliable points, while employing geometry-aligned kernels that\nadapt to complex scene structures. These Gaussian primitives effectively fuse\nlocal geometric and semantic information, enabling robust, uncertainty-aware\nmapping in complex outdoor scenarios. Comprehensive evaluation across diverse\noff-road and urban outdoor environments demonstrates consistent improvements in\nmapping quality, uncertainty calibration, representational flexibility, and\nrobustness, while maintaining real-time efficiency."
                },
                "authors": [
                    {
                        "name": "Junyoung Kim"
                    },
                    {
                        "name": "Minsik Jeon"
                    },
                    {
                        "name": "Jihong Min"
                    },
                    {
                        "name": "Kiho Kwak"
                    },
                    {
                        "name": "Junwon Seo"
                    }
                ],
                "author_detail": {
                    "name": "Junwon Seo"
                },
                "author": "Junwon Seo",
                "arxiv_comment": "Our project website can be found at\n  https://kjyoung.github.io/Homepage/#/Projects/E2-BKI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11963v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11963v1",
                "updated": "2025-09-15T14:17:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    17,
                    17,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T14:17:17Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    17,
                    17,
                    0,
                    258,
                    0
                ],
                "title": "ToolRM: Outcome Reward Models for Tool-Calling Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolRM: Outcome Reward Models for Tool-Calling Large Language Models"
                },
                "summary": "As large language models (LLMs) increasingly interact with external tools,\nreward modeling for tool use has become a critical yet underexplored area.\nExisting reward models, trained primarily on natural language outputs, struggle\nto evaluate tool-based reasoning and execution. To quantify this gap, we\nintroduce FC-RewardBench, the first benchmark designed to systematically assess\nreward models' performance in tool-calling scenarios. Our analysis shows that\ncurrent reward models often miss key signals of effective tool use,\nhighlighting the need for domain-specific modeling. To address this, we propose\na training framework for outcome-based reward models using data synthesized\nfrom permissively licensed, open-weight LLMs. We train models ranging from 1.7B\nto 14B parameters and evaluate them across seven out-of-domain benchmarks.\nThese models consistently outperform general-purpose baselines, achieving up to\n25\\% average improvement in downstream task performance and enabling\ndata-efficient fine-tuning through reward-guided filtering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly interact with external tools,\nreward modeling for tool use has become a critical yet underexplored area.\nExisting reward models, trained primarily on natural language outputs, struggle\nto evaluate tool-based reasoning and execution. To quantify this gap, we\nintroduce FC-RewardBench, the first benchmark designed to systematically assess\nreward models' performance in tool-calling scenarios. Our analysis shows that\ncurrent reward models often miss key signals of effective tool use,\nhighlighting the need for domain-specific modeling. To address this, we propose\na training framework for outcome-based reward models using data synthesized\nfrom permissively licensed, open-weight LLMs. We train models ranging from 1.7B\nto 14B parameters and evaluate them across seven out-of-domain benchmarks.\nThese models consistently outperform general-purpose baselines, achieving up to\n25\\% average improvement in downstream task performance and enabling\ndata-efficient fine-tuning through reward-guided filtering."
                },
                "authors": [
                    {
                        "name": "Mayank Agarwal"
                    },
                    {
                        "name": "Ibrahim Abdelaziz"
                    },
                    {
                        "name": "Kinjal Basu"
                    },
                    {
                        "name": "Merve Unuvar"
                    },
                    {
                        "name": "Luis A. Lastras"
                    },
                    {
                        "name": "Yara Rizk"
                    },
                    {
                        "name": "Pavan Kapanipathi"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Kapanipathi"
                },
                "author": "Pavan Kapanipathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11963v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11963v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11961v1",
                "updated": "2025-09-15T14:16:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    16,
                    51,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T14:16:51Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    16,
                    51,
                    0,
                    258,
                    0
                ],
                "title": "Spec-LLaVA: Accelerating Vision-Language Models with Dynamic Tree-Based\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spec-LLaVA: Accelerating Vision-Language Models with Dynamic Tree-Based\n  Speculative Decoding"
                },
                "summary": "Vision-Language Models (VLMs) enable powerful multimodal reasoning but suffer\nfrom slow autoregressive inference, limiting their deployment in real-time\napplications. We introduce Spec-LLaVA, a system that applies speculative\ndecoding to accelerate VLMs without sacrificing output quality. Spec-LLaVA\npairs a lightweight draft VLM with a large target model: the draft speculates\nfuture tokens, which the target verifies in parallel, allowing multiple tokens\nto be generated per step. To maximize efficiency, we design a dynamic\ntree-based verification algorithm that adaptively expands and prunes\nspeculative branches using draft model confidence. On MS COCO out-of-domain\nimages, Spec-LLaVA achieves up to 3.28$\\times$ faster decoding on LLaVA-1.5\n(7B, 13B) with no loss in generation quality. This work presents a lossless\nacceleration framework for VLMs using dynamic tree-structured speculative\ndecoding, opening a path toward practical real-time multimodal assistants.\nImportantly, the lightweight draft model design makes the framework amenable to\nresource-constrained or on-device deployment settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) enable powerful multimodal reasoning but suffer\nfrom slow autoregressive inference, limiting their deployment in real-time\napplications. We introduce Spec-LLaVA, a system that applies speculative\ndecoding to accelerate VLMs without sacrificing output quality. Spec-LLaVA\npairs a lightweight draft VLM with a large target model: the draft speculates\nfuture tokens, which the target verifies in parallel, allowing multiple tokens\nto be generated per step. To maximize efficiency, we design a dynamic\ntree-based verification algorithm that adaptively expands and prunes\nspeculative branches using draft model confidence. On MS COCO out-of-domain\nimages, Spec-LLaVA achieves up to 3.28$\\times$ faster decoding on LLaVA-1.5\n(7B, 13B) with no loss in generation quality. This work presents a lossless\nacceleration framework for VLMs using dynamic tree-structured speculative\ndecoding, opening a path toward practical real-time multimodal assistants.\nImportantly, the lightweight draft model design makes the framework amenable to\nresource-constrained or on-device deployment settings."
                },
                "authors": [
                    {
                        "name": "Mingxiao Huo"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Hewei Wang"
                    },
                    {
                        "name": "Jinfeng Xu"
                    },
                    {
                        "name": "Zheyu Chen"
                    },
                    {
                        "name": "Huilin Tai"
                    },
                    {
                        "name": "Yijun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yijun Chen"
                },
                "author": "Yijun Chen",
                "arxiv_comment": "7pages, accepted by ICML TTODLer-FM workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20344v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20344v2",
                "updated": "2025-09-15T14:09:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    9,
                    38,
                    0,
                    258,
                    0
                ],
                "published": "2025-02-27T18:16:47Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    16,
                    47,
                    3,
                    58,
                    0
                ],
                "title": "LinguaLens: Towards Interpreting Linguistic Mechanisms of Large Language\n  Models via Sparse Auto-Encoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LinguaLens: Towards Interpreting Linguistic Mechanisms of Large Language\n  Models via Sparse Auto-Encoder"
                },
                "summary": "Large language models (LLMs) demonstrate exceptional performance on tasks\nrequiring complex linguistic abilities, such as reference disambiguation and\nmetaphor recognition/generation. Although LLMs possess impressive capabilities,\ntheir internal mechanisms for processing and representing linguistic knowledge\nremain largely opaque. Prior research on linguistic mechanisms is limited by\ncoarse granularity, limited analysis scale, and narrow focus. In this study, we\npropose LinguaLens, a systematic and comprehensive framework for analyzing the\nlinguistic mechanisms of large language models, based on Sparse Auto-Encoders\n(SAEs). We extract a broad set of Chinese and English linguistic features\nacross four dimensions (morphology, syntax, semantics, and pragmatics). By\nemploying counterfactual methods, we construct a large-scale counterfactual\ndataset of linguistic features for mechanism analysis. Our findings reveal\nintrinsic representations of linguistic knowledge in LLMs, uncover patterns of\ncross-layer and cross-lingual distribution, and demonstrate the potential to\ncontrol model outputs. This work provides a systematic suite of resources and\nmethods for studying linguistic mechanisms, offers strong evidence that LLMs\npossess genuine linguistic knowledge, and lays the foundation for more\ninterpretable and controllable language modeling in future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate exceptional performance on tasks\nrequiring complex linguistic abilities, such as reference disambiguation and\nmetaphor recognition/generation. Although LLMs possess impressive capabilities,\ntheir internal mechanisms for processing and representing linguistic knowledge\nremain largely opaque. Prior research on linguistic mechanisms is limited by\ncoarse granularity, limited analysis scale, and narrow focus. In this study, we\npropose LinguaLens, a systematic and comprehensive framework for analyzing the\nlinguistic mechanisms of large language models, based on Sparse Auto-Encoders\n(SAEs). We extract a broad set of Chinese and English linguistic features\nacross four dimensions (morphology, syntax, semantics, and pragmatics). By\nemploying counterfactual methods, we construct a large-scale counterfactual\ndataset of linguistic features for mechanism analysis. Our findings reveal\nintrinsic representations of linguistic knowledge in LLMs, uncover patterns of\ncross-layer and cross-lingual distribution, and demonstrate the potential to\ncontrol model outputs. This work provides a systematic suite of resources and\nmethods for studying linguistic mechanisms, offers strong evidence that LLMs\npossess genuine linguistic knowledge, and lays the foundation for more\ninterpretable and controllable language modeling in future research."
                },
                "authors": [
                    {
                        "name": "Yi Jing"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Hongzhu Guo"
                    },
                    {
                        "name": "Lingxu Ran"
                    },
                    {
                        "name": "Xiaozhi Wang"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "Accepted by EMNLP 2025 MainConference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20344v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20344v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17396v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17396v2",
                "updated": "2025-09-15T14:06:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    6,
                    28,
                    0,
                    258,
                    0
                ],
                "published": "2025-07-23T10:46:25Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    10,
                    46,
                    25,
                    2,
                    204,
                    0
                ],
                "title": "Learning from Scratch: Structurally-masked Transformer for Next\n  Generation Lib-free Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Scratch: Structurally-masked Transformer for Next\n  Generation Lib-free Simulation"
                },
                "summary": "This paper proposes a neural framework for power and timing prediction of\nmulti-stage data path, distinguishing itself from traditional lib-based\nanalytical methods dependent on driver characterization and load\nsimplifications. To the best of our knowledge, this is the first\nlanguage-based, netlist-aware neural network designed explicitly for standard\ncells. Our approach employs two pre-trained neural models of waveform\nprediction and delay estimation that directly infer transient waveforms and\npropagation delays from SPICE netlists, conditioned on critical physical\nparameters such as load capacitance, input slew, and gate size. This method\naccurately captures both intrinsic and coupling-induced delay effects without\nrequiring simplification or interpolation. For multi-stage timing prediction,\nwe implement a recursive propagation strategy where predicted waveforms from\neach stage feed into subsequent stages, cumulatively capturing delays across\nthe logic chain. This approach ensures precise timing alignment and complete\nwaveform visibility throughout complex signal pathways. The waveform prediction\nutilizes a hybrid CNN-Transformer architecture with netlist-aware node-level\nencoding, addressing traditional Transformers' fixed input dimensionality\nconstraints. Additionally, specialized subnetworks separately handle primary\ndelay estimation and crosstalk correction. Experimental results demonstrate\nSPICE-level accuracy, consistently achieving RMSE below 0.0098 across diverse\nindustrial circuits. The proposed framework provides a scalable, structurally\nadaptable neural alternative to conventional power and timing engines,\ndemonstrating high fidelity to physical circuit behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a neural framework for power and timing prediction of\nmulti-stage data path, distinguishing itself from traditional lib-based\nanalytical methods dependent on driver characterization and load\nsimplifications. To the best of our knowledge, this is the first\nlanguage-based, netlist-aware neural network designed explicitly for standard\ncells. Our approach employs two pre-trained neural models of waveform\nprediction and delay estimation that directly infer transient waveforms and\npropagation delays from SPICE netlists, conditioned on critical physical\nparameters such as load capacitance, input slew, and gate size. This method\naccurately captures both intrinsic and coupling-induced delay effects without\nrequiring simplification or interpolation. For multi-stage timing prediction,\nwe implement a recursive propagation strategy where predicted waveforms from\neach stage feed into subsequent stages, cumulatively capturing delays across\nthe logic chain. This approach ensures precise timing alignment and complete\nwaveform visibility throughout complex signal pathways. The waveform prediction\nutilizes a hybrid CNN-Transformer architecture with netlist-aware node-level\nencoding, addressing traditional Transformers' fixed input dimensionality\nconstraints. Additionally, specialized subnetworks separately handle primary\ndelay estimation and crosstalk correction. Experimental results demonstrate\nSPICE-level accuracy, consistently achieving RMSE below 0.0098 across diverse\nindustrial circuits. The proposed framework provides a scalable, structurally\nadaptable neural alternative to conventional power and timing engines,\ndemonstrating high fidelity to physical circuit behaviors."
                },
                "authors": [
                    {
                        "name": "Junlang Huang"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Zhong Guan"
                    }
                ],
                "author_detail": {
                    "name": "Zhong Guan"
                },
                "author": "Zhong Guan",
                "arxiv_comment": "Prepare for complementary experiments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17396v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17396v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11947v1",
                "updated": "2025-09-15T14:06:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    6,
                    9,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T14:06:09Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    6,
                    9,
                    0,
                    258,
                    0
                ],
                "title": "A GPU-Accelerated RAG-Based Telegram Assistant for Supporting Parallel\n  Processing Students",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A GPU-Accelerated RAG-Based Telegram Assistant for Supporting Parallel\n  Processing Students"
                },
                "summary": "This project addresses a critical pedagogical need: offering students\ncontinuous, on-demand academic assistance beyond conventional reception hours.\nI present a domain-specific Retrieval-Augmented Generation (RAG) system powered\nby a quantized Mistral-7B Instruct model and deployed as a Telegram bot. The\nassistant enhances learning by delivering real-time, personalized responses\naligned with the \"Introduction to Parallel Processing\" course materials. GPU\nacceleration significantly improves inference latency, enabling practical\ndeployment on consumer hardware. This approach demonstrates how consumer GPUs\ncan enable affordable, private, and effective AI tutoring for HPC education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This project addresses a critical pedagogical need: offering students\ncontinuous, on-demand academic assistance beyond conventional reception hours.\nI present a domain-specific Retrieval-Augmented Generation (RAG) system powered\nby a quantized Mistral-7B Instruct model and deployed as a Telegram bot. The\nassistant enhances learning by delivering real-time, personalized responses\naligned with the \"Introduction to Parallel Processing\" course materials. GPU\nacceleration significantly improves inference latency, enabling practical\ndeployment on consumer hardware. This approach demonstrates how consumer GPUs\ncan enable affordable, private, and effective AI tutoring for HPC education."
                },
                "authors": [
                    {
                        "name": "Guy Tel-Zur"
                    }
                ],
                "author_detail": {
                    "name": "Guy Tel-Zur"
                },
                "author": "Guy Tel-Zur",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11943v1",
                "updated": "2025-09-15T14:03:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    3,
                    6,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T14:03:06Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    3,
                    6,
                    0,
                    258,
                    0
                ],
                "title": "Neuro-Symbolic Agents with Modal Logic for Autonomous Diagnostics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuro-Symbolic Agents with Modal Logic for Autonomous Diagnostics"
                },
                "summary": "The development of intelligent agents, particularly those powered by language\nmodels (LMs), has shown the critical role in various environments that require\nintelligent and autonomous decision. Environments are not passive testing\ngrounds and they represent the data required for agents to learn and exhibit\nvery challenging conditions that require adaptive, complex and autonomous\ncapacity to make decisions. While the paradigm of scaling models and datasets\nhas led to remarkable emergent capabilities, we argue that scaling the\nstructure, fidelity, and logical consistency of agent reasoning within these\nenvironments is a crucial, yet underexplored, dimension of AI research. This\npaper introduces a neuro-symbolic multi-agent architecture where the belief\nstates of individual agents are formally represented as Kripke models. This\nfoundational choice enables them to reason about known concepts of\n\\emph{possibility} and \\emph{necessity} using the formal language of modal\nlogic. In this work, we use of immutable, domain-specific knowledge to make\ninfere information, which is encoded as logical constraints essential for\nproper diagnosis. In the proposed model, we show constraints that actively\nguide the hypothesis generation of LMs, effectively preventing them from\nreaching physically or logically untenable conclusions. In a high-fidelity\nsimulated particle accelerator environment, our system successfully diagnoses\ncomplex, cascading failures by combining the powerful semantic intuition of LMs\nwith the rigorous, verifiable validation of modal logic and a factual world\nmodel and showcasing a viable path toward more robust, reliable, and verifiable\nautonomous agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of intelligent agents, particularly those powered by language\nmodels (LMs), has shown the critical role in various environments that require\nintelligent and autonomous decision. Environments are not passive testing\ngrounds and they represent the data required for agents to learn and exhibit\nvery challenging conditions that require adaptive, complex and autonomous\ncapacity to make decisions. While the paradigm of scaling models and datasets\nhas led to remarkable emergent capabilities, we argue that scaling the\nstructure, fidelity, and logical consistency of agent reasoning within these\nenvironments is a crucial, yet underexplored, dimension of AI research. This\npaper introduces a neuro-symbolic multi-agent architecture where the belief\nstates of individual agents are formally represented as Kripke models. This\nfoundational choice enables them to reason about known concepts of\n\\emph{possibility} and \\emph{necessity} using the formal language of modal\nlogic. In this work, we use of immutable, domain-specific knowledge to make\ninfere information, which is encoded as logical constraints essential for\nproper diagnosis. In the proposed model, we show constraints that actively\nguide the hypothesis generation of LMs, effectively preventing them from\nreaching physically or logically untenable conclusions. In a high-fidelity\nsimulated particle accelerator environment, our system successfully diagnoses\ncomplex, cascading failures by combining the powerful semantic intuition of LMs\nwith the rigorous, verifiable validation of modal logic and a factual world\nmodel and showcasing a viable path toward more robust, reliable, and verifiable\nautonomous agents."
                },
                "authors": [
                    {
                        "name": "Antonin Sulc"
                    },
                    {
                        "name": "Thorsten Hellert"
                    }
                ],
                "author_detail": {
                    "name": "Thorsten Hellert"
                },
                "author": "Thorsten Hellert",
                "arxiv_comment": "10 pages, 1 figure, Scaling Environments for Agents (SEA) Workshop at\n  NeuralIPS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11942v1",
                "updated": "2025-09-15T14:02:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    2,
                    29,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T14:02:29Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    2,
                    29,
                    0,
                    258,
                    0
                ],
                "title": "VisDocSketcher: Towards Scalable Visual Documentation with Agentic\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisDocSketcher: Towards Scalable Visual Documentation with Agentic\n  Systems"
                },
                "summary": "Visual documentation is an effective tool for reducing the cognitive barrier\ndevelopers face when understanding unfamiliar code, enabling more intuitive\ncomprehension. Compared to textual documentation, it provides a higher-level\nunderstanding of the system structure and data flow. Developers usually prefer\nvisual representations over lengthy textual descriptions for large software\nsystems. Visual documentation is both difficult to produce and challenging to\nevaluate. Manually creating it is time-consuming, and currently, no existing\napproach can automatically generate high-level visual documentation directly\nfrom code. Its evaluation is often subjective, making it difficult to\nstandardize and automate. To address these challenges, this paper presents the\nfirst exploration of using agentic LLM systems to automatically generate visual\ndocumentation. We introduce VisDocSketcher, the first agent-based approach that\ncombines static analysis with LLM agents to identify key elements in the code\nand produce corresponding visual representations. We propose a novel evaluation\nframework, AutoSketchEval, for assessing the quality of generated visual\ndocumentation using code-level metrics. The experimental results show that our\napproach can valid visual documentation for 74.4% of the samples. It shows an\nimprovement of 26.7-39.8% over a simple template-based baseline. Our evaluation\nframework can reliably distinguish high-quality (code-aligned) visual\ndocumentation from low-quality (non-aligned) ones, achieving an AUC exceeding\n0.87. Our work lays the foundation for future research on automated visual\ndocumentation by introducing practical tools that not only generate valid\nvisual representations but also reliably assess their quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual documentation is an effective tool for reducing the cognitive barrier\ndevelopers face when understanding unfamiliar code, enabling more intuitive\ncomprehension. Compared to textual documentation, it provides a higher-level\nunderstanding of the system structure and data flow. Developers usually prefer\nvisual representations over lengthy textual descriptions for large software\nsystems. Visual documentation is both difficult to produce and challenging to\nevaluate. Manually creating it is time-consuming, and currently, no existing\napproach can automatically generate high-level visual documentation directly\nfrom code. Its evaluation is often subjective, making it difficult to\nstandardize and automate. To address these challenges, this paper presents the\nfirst exploration of using agentic LLM systems to automatically generate visual\ndocumentation. We introduce VisDocSketcher, the first agent-based approach that\ncombines static analysis with LLM agents to identify key elements in the code\nand produce corresponding visual representations. We propose a novel evaluation\nframework, AutoSketchEval, for assessing the quality of generated visual\ndocumentation using code-level metrics. The experimental results show that our\napproach can valid visual documentation for 74.4% of the samples. It shows an\nimprovement of 26.7-39.8% over a simple template-based baseline. Our evaluation\nframework can reliably distinguish high-quality (code-aligned) visual\ndocumentation from low-quality (non-aligned) ones, achieving an AUC exceeding\n0.87. Our work lays the foundation for future research on automated visual\ndocumentation by introducing practical tools that not only generate valid\nvisual representations but also reliably assess their quality."
                },
                "authors": [
                    {
                        "name": "Luís F. Gomes"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Rui Abreu"
                    }
                ],
                "author_detail": {
                    "name": "Rui Abreu"
                },
                "author": "Rui Abreu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11941v1",
                "updated": "2025-09-15T14:01:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    1,
                    22,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T14:01:22Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    1,
                    22,
                    0,
                    258,
                    0
                ],
                "title": "How to Evaluate Medical AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Evaluate Medical AI"
                },
                "summary": "The integration of artificial intelligence (AI) into medical diagnostic\nworkflows requires robust and consistent evaluation methods to ensure\nreliability, clinical relevance, and the inherent variability in expert\njudgments. Traditional metrics like precision and recall often fail to account\nfor the inherent variability in expert judgments, leading to inconsistent\nassessments of AI performance. Inter-rater agreement statistics like Cohen's\nKappa are more reliable but they lack interpretability. We introduce Relative\nPrecision and Recall of Algorithmic Diagnostics (RPAD and RRAD) - a new\nevaluation metrics that compare AI outputs against multiple expert opinions\nrather than a single reference. By normalizing performance against inter-expert\ndisagreement, these metrics provide a more stable and realistic measure of the\nquality of predicted diagnosis. In addition to the comprehensive analysis of\ndiagnostic quality measures, our study contains a very important side result.\nOur evaluation methodology allows us to avoid selecting diagnoses from a\nlimited list when evaluating a given case. Instead, both the models being\ntested and the examiners verifying them arrive at a free-form diagnosis. In\nthis automated methodology for establishing the identity of free-form clinical\ndiagnoses, a remarkable 98% accuracy becomes attainable. We evaluate our\napproach using 360 medical dialogues, comparing multiple large language models\n(LLMs) against a panel of physicians. Large-scale study shows that\ntop-performing models, such as DeepSeek-V3, achieve consistency on par with or\nexceeding expert consensus. Moreover, we demonstrate that expert judgments\nexhibit significant variability - often greater than that between AI and\nhumans. This finding underscores the limitations of any absolute metrics and\nsupports the need to adopt relative metrics in medical AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of artificial intelligence (AI) into medical diagnostic\nworkflows requires robust and consistent evaluation methods to ensure\nreliability, clinical relevance, and the inherent variability in expert\njudgments. Traditional metrics like precision and recall often fail to account\nfor the inherent variability in expert judgments, leading to inconsistent\nassessments of AI performance. Inter-rater agreement statistics like Cohen's\nKappa are more reliable but they lack interpretability. We introduce Relative\nPrecision and Recall of Algorithmic Diagnostics (RPAD and RRAD) - a new\nevaluation metrics that compare AI outputs against multiple expert opinions\nrather than a single reference. By normalizing performance against inter-expert\ndisagreement, these metrics provide a more stable and realistic measure of the\nquality of predicted diagnosis. In addition to the comprehensive analysis of\ndiagnostic quality measures, our study contains a very important side result.\nOur evaluation methodology allows us to avoid selecting diagnoses from a\nlimited list when evaluating a given case. Instead, both the models being\ntested and the examiners verifying them arrive at a free-form diagnosis. In\nthis automated methodology for establishing the identity of free-form clinical\ndiagnoses, a remarkable 98% accuracy becomes attainable. We evaluate our\napproach using 360 medical dialogues, comparing multiple large language models\n(LLMs) against a panel of physicians. Large-scale study shows that\ntop-performing models, such as DeepSeek-V3, achieve consistency on par with or\nexceeding expert consensus. Moreover, we demonstrate that expert judgments\nexhibit significant variability - often greater than that between AI and\nhumans. This finding underscores the limitations of any absolute metrics and\nsupports the need to adopt relative metrics in medical AI."
                },
                "authors": [
                    {
                        "name": "Ilia Kopanichuk"
                    },
                    {
                        "name": "Petr Anokhin"
                    },
                    {
                        "name": "Vladimir Shaposhnikov"
                    },
                    {
                        "name": "Vladimir Makharev"
                    },
                    {
                        "name": "Ekaterina Tsapieva"
                    },
                    {
                        "name": "Iaroslav Bespalov"
                    },
                    {
                        "name": "Dmitry V. Dylov"
                    },
                    {
                        "name": "Ivan Oseledets"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Oseledets"
                },
                "author": "Ivan Oseledets",
                "arxiv_comment": "10 pages, 7 fugures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11126v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11126v2",
                "updated": "2025-09-15T13:59:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    59,
                    49,
                    0,
                    258,
                    0
                ],
                "published": "2025-08-15T00:14:31Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    0,
                    14,
                    31,
                    4,
                    227,
                    0
                ],
                "title": "AI Agentic Programming: A Survey of Techniques, Challenges, and\n  Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Agentic Programming: A Survey of Techniques, Challenges, and\n  Opportunities"
                },
                "summary": "AI agentic programming is an emerging paradigm where large language model\n(LLM)-based coding agents autonomously plan, execute, and interact with tools\nsuch as compilers, debuggers, and version control systems. Unlike conventional\ncode generation, these agents decompose goals, coordinate multi-step processes,\nand adapt based on feedback, reshaping software development practices. This\nsurvey provides a timely review of the field, introducing a taxonomy of agent\nbehaviors and system architectures and examining relevant techniques for\nplanning, context management, tool integration, execution monitoring, and\nbenchmarking datasets. We highlight challenges of this fast-moving field and\ndiscuss opportunities for building reliable, transparent, and collaborative\ncoding agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agentic programming is an emerging paradigm where large language model\n(LLM)-based coding agents autonomously plan, execute, and interact with tools\nsuch as compilers, debuggers, and version control systems. Unlike conventional\ncode generation, these agents decompose goals, coordinate multi-step processes,\nand adapt based on feedback, reshaping software development practices. This\nsurvey provides a timely review of the field, introducing a taxonomy of agent\nbehaviors and system architectures and examining relevant techniques for\nplanning, context management, tool integration, execution monitoring, and\nbenchmarking datasets. We highlight challenges of this fast-moving field and\ndiscuss opportunities for building reliable, transparent, and collaborative\ncoding agents."
                },
                "authors": [
                    {
                        "name": "Huanting Wang"
                    },
                    {
                        "name": "Jingzhi Gong"
                    },
                    {
                        "name": "Huawei Zhang"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Zheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Wang"
                },
                "author": "Zheng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11126v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11126v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11940v1",
                "updated": "2025-09-15T13:59:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    59,
                    42,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T13:59:42Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    59,
                    42,
                    0,
                    258,
                    0
                ],
                "title": "Neuromorphic Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuromorphic Intelligence"
                },
                "summary": "Neuromorphic computing seeks to replicate the remarkable efficiency,\nflexibility, and adaptability of the human brain in artificial systems. Unlike\nconventional digital approaches, which depend on massive computational and\nenergy resources, neuromorphic systems exploit brain-inspired principles of\ncomputation to achieve orders of magnitude greater energy efficiency. By\ndrawing on insights from artificial intelligence, neuroscience, physics,\nchemistry, and materials science, neuromorphic computing promises to deliver\nintelligent systems that are sustainable, transparent, and widely accessible. A\ncentral challenge, however, is to identify a unifying theoretical framework\ncapable of bridging these diverse disciplines. We argue that dynamical systems\ntheory provides such a foundation. Rooted in differential calculus, it offers a\nprincipled language for modeling inference, learning, and control in both\nnatural and artificial substrates. Within this framework, noise can be\nharnessed as a resource for learning, while differential genetic programming\nenables the discovery of dynamical systems that implement adaptive behaviors.\nEmbracing this perspective paves the way toward emergent neuromorphic\nintelligence, where intelligent behavior arises from the dynamics of physical\nsubstrates, advancing both the science and sustainability of AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuromorphic computing seeks to replicate the remarkable efficiency,\nflexibility, and adaptability of the human brain in artificial systems. Unlike\nconventional digital approaches, which depend on massive computational and\nenergy resources, neuromorphic systems exploit brain-inspired principles of\ncomputation to achieve orders of magnitude greater energy efficiency. By\ndrawing on insights from artificial intelligence, neuroscience, physics,\nchemistry, and materials science, neuromorphic computing promises to deliver\nintelligent systems that are sustainable, transparent, and widely accessible. A\ncentral challenge, however, is to identify a unifying theoretical framework\ncapable of bridging these diverse disciplines. We argue that dynamical systems\ntheory provides such a foundation. Rooted in differential calculus, it offers a\nprincipled language for modeling inference, learning, and control in both\nnatural and artificial substrates. Within this framework, noise can be\nharnessed as a resource for learning, while differential genetic programming\nenables the discovery of dynamical systems that implement adaptive behaviors.\nEmbracing this perspective paves the way toward emergent neuromorphic\nintelligence, where intelligent behavior arises from the dynamics of physical\nsubstrates, advancing both the science and sustainability of AI."
                },
                "authors": [
                    {
                        "name": "Marcel van Gerven"
                    }
                ],
                "author_detail": {
                    "name": "Marcel van Gerven"
                },
                "author": "Marcel van Gerven",
                "arxiv_comment": "18 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11939v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11939v1",
                "updated": "2025-09-15T13:58:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    58,
                    52,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T13:58:52Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    58,
                    52,
                    0,
                    258,
                    0
                ],
                "title": "PrivWeb: Unobtrusive and Content-aware Privacy Protection For Web Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrivWeb: Unobtrusive and Content-aware Privacy Protection For Web Agents"
                },
                "summary": "While web agents gained popularity by automating web interactions, their\nrequirement for interface access introduces significant privacy risks that are\nunderstudied, particularly from users' perspective. Through a formative study\n(N=15), we found users frequently misunderstand agents' data practices, and\ndesired unobtrusive, transparent data management. To achieve this, we designed\nand implemented PrivWeb, a trusted add-on on web agents that utilizes a\nlocalized LLM to anonymize private information on interfaces according to user\npreferences. It features privacy categorization schema and adaptive\nnotifications that selectively pauses tasks for user control over information\ncollection for highly sensitive information, while offering non-disruptive\noptions for less sensitive information, minimizing human oversight. The user\nstudy (N=14) across travel, information retrieval, shopping, and entertainment\ntasks compared PrivWeb with baselines without notification and without control\nfor private information access, where PrivWeb reduced perceived privacy risks\nwith no associated increase in cognitive effort, and resulted in higher overall\nsatisfaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While web agents gained popularity by automating web interactions, their\nrequirement for interface access introduces significant privacy risks that are\nunderstudied, particularly from users' perspective. Through a formative study\n(N=15), we found users frequently misunderstand agents' data practices, and\ndesired unobtrusive, transparent data management. To achieve this, we designed\nand implemented PrivWeb, a trusted add-on on web agents that utilizes a\nlocalized LLM to anonymize private information on interfaces according to user\npreferences. It features privacy categorization schema and adaptive\nnotifications that selectively pauses tasks for user control over information\ncollection for highly sensitive information, while offering non-disruptive\noptions for less sensitive information, minimizing human oversight. The user\nstudy (N=14) across travel, information retrieval, shopping, and entertainment\ntasks compared PrivWeb with baselines without notification and without control\nfor private information access, where PrivWeb reduced perceived privacy risks\nwith no associated increase in cognitive effort, and resulted in higher overall\nsatisfaction."
                },
                "authors": [
                    {
                        "name": "Shuning Zhang"
                    },
                    {
                        "name": "Yutong Jiang"
                    },
                    {
                        "name": "Rongjun Ma"
                    },
                    {
                        "name": "Yuting Yang"
                    },
                    {
                        "name": "Mingyao Xu"
                    },
                    {
                        "name": "Zhixin Huang"
                    },
                    {
                        "name": "Xin Yi"
                    },
                    {
                        "name": "Hewu Li"
                    }
                ],
                "author_detail": {
                    "name": "Hewu Li"
                },
                "author": "Hewu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11939v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11939v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11937v1",
                "updated": "2025-09-15T13:56:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    56,
                    6,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T13:56:06Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    56,
                    6,
                    0,
                    258,
                    0
                ],
                "title": "MMORE: Massive Multimodal Open RAG & Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMORE: Massive Multimodal Open RAG & Extraction"
                },
                "summary": "We introduce MMORE, an open-source pipeline for Massive Multimodal Open\nRetrievalAugmented Generation and Extraction, designed to ingest, transform,\nand retrieve knowledge from heterogeneous document formats at scale. MMORE\nsupports more than fifteen file types, including text, tables, images, emails,\naudio, and video, and processes them into a unified format to enable downstream\napplications for LLMs. The architecture offers modular, distributed processing,\nenabling scalable parallelization across CPUs and GPUs. On processing\nbenchmarks, MMORE demonstrates a 3.8-fold speedup over single-node baselines\nand 40% higher accuracy than Docling on scanned PDFs. The pipeline integrates\nhybrid dense-sparse retrieval and supports both interactive APIs and batch RAG\nendpoints. Evaluated on PubMedQA, MMORE-augmented medical LLMs improve\nbiomedical QA accuracy with increasing retrieval depth. MMORE provides a\nrobust, extensible foundation for deploying task-agnostic RAG systems on\ndiverse, real-world multimodal data. The codebase is available at\nhttps://github.com/swiss-ai/mmore.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MMORE, an open-source pipeline for Massive Multimodal Open\nRetrievalAugmented Generation and Extraction, designed to ingest, transform,\nand retrieve knowledge from heterogeneous document formats at scale. MMORE\nsupports more than fifteen file types, including text, tables, images, emails,\naudio, and video, and processes them into a unified format to enable downstream\napplications for LLMs. The architecture offers modular, distributed processing,\nenabling scalable parallelization across CPUs and GPUs. On processing\nbenchmarks, MMORE demonstrates a 3.8-fold speedup over single-node baselines\nand 40% higher accuracy than Docling on scanned PDFs. The pipeline integrates\nhybrid dense-sparse retrieval and supports both interactive APIs and batch RAG\nendpoints. Evaluated on PubMedQA, MMORE-augmented medical LLMs improve\nbiomedical QA accuracy with increasing retrieval depth. MMORE provides a\nrobust, extensible foundation for deploying task-agnostic RAG systems on\ndiverse, real-world multimodal data. The codebase is available at\nhttps://github.com/swiss-ai/mmore."
                },
                "authors": [
                    {
                        "name": "Alexandre Sallinen"
                    },
                    {
                        "name": "Stefan Krsteski"
                    },
                    {
                        "name": "Paul Teiletche"
                    },
                    {
                        "name": "Marc-Antoine Allard"
                    },
                    {
                        "name": "Baptiste Lecoeur"
                    },
                    {
                        "name": "Michael Zhang"
                    },
                    {
                        "name": "Fabrice Nemo"
                    },
                    {
                        "name": "David Kalajdzic"
                    },
                    {
                        "name": "Matthias Meyer"
                    },
                    {
                        "name": "Mary-Anne Hartley"
                    }
                ],
                "author_detail": {
                    "name": "Mary-Anne Hartley"
                },
                "author": "Mary-Anne Hartley",
                "arxiv_comment": "This paper was originally submitted to the CODEML workshop for ICML\n  2025. 9 pages (including references and appendices)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.0; E.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11930v1",
                "updated": "2025-09-15T13:46:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    46,
                    27,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T13:46:27Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    46,
                    27,
                    0,
                    258,
                    0
                ],
                "title": "VH-Diffuser: Variable Horizon Diffusion Planner for Time-Aware\n  Goal-Conditioned Trajectory Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VH-Diffuser: Variable Horizon Diffusion Planner for Time-Aware\n  Goal-Conditioned Trajectory Planning"
                },
                "summary": "Diffusion-based planners have gained significant recent attention for their\nrobustness and performance in long-horizon tasks. However, most existing\nplanners rely on a fixed, pre-specified horizon during both training and\ninference. This rigidity often produces length-mismatch (trajectories that are\ntoo short or too long) and brittle performance across instances with varying\ngeometric or dynamical difficulty. In this paper, we introduce the Variable\nHorizon Diffuser (VHD) framework, which treats the horizon as a learned\nvariable rather than a fixed hyperparameter. Given a start-goal pair, we first\npredict an instance-specific horizon using a learned Length Predictor model,\nwhich guides a Diffusion Planner to generate a trajectory of the desired\nlength. Our design maintains compatibility with existing diffusion planners by\ncontrolling trajectory length through initial noise shaping and training on\nrandomly cropped sub-trajectories, without requiring architectural changes.\nEmpirically, VHD improves success rates and path efficiency in maze-navigation\nand robot-arm control benchmarks, showing greater robustness to horizon\nmismatch and unseen lengths, while keeping training simple and offline-only.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based planners have gained significant recent attention for their\nrobustness and performance in long-horizon tasks. However, most existing\nplanners rely on a fixed, pre-specified horizon during both training and\ninference. This rigidity often produces length-mismatch (trajectories that are\ntoo short or too long) and brittle performance across instances with varying\ngeometric or dynamical difficulty. In this paper, we introduce the Variable\nHorizon Diffuser (VHD) framework, which treats the horizon as a learned\nvariable rather than a fixed hyperparameter. Given a start-goal pair, we first\npredict an instance-specific horizon using a learned Length Predictor model,\nwhich guides a Diffusion Planner to generate a trajectory of the desired\nlength. Our design maintains compatibility with existing diffusion planners by\ncontrolling trajectory length through initial noise shaping and training on\nrandomly cropped sub-trajectories, without requiring architectural changes.\nEmpirically, VHD improves success rates and path efficiency in maze-navigation\nand robot-arm control benchmarks, showing greater robustness to horizon\nmismatch and unseen lengths, while keeping training simple and offline-only."
                },
                "authors": [
                    {
                        "name": "Ruijia Liu"
                    },
                    {
                        "name": "Ancheng Hou"
                    },
                    {
                        "name": "Shaoyuan Li"
                    },
                    {
                        "name": "Xiang Yin"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Yin"
                },
                "author": "Xiang Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11924v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11924v2",
                "updated": "2025-09-16T02:04:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    2,
                    4,
                    27,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-15T13:38:35Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    38,
                    35,
                    0,
                    258,
                    0
                ],
                "title": "Enriched text-guided variational multimodal knowledge distillation\n  network (VMD) for automated diagnosis of plaque vulnerability in 3D carotid\n  artery MRI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enriched text-guided variational multimodal knowledge distillation\n  network (VMD) for automated diagnosis of plaque vulnerability in 3D carotid\n  artery MRI"
                },
                "summary": "Multimodal learning has attracted much attention in recent years due to its\nability to effectively utilize data features from a variety of different\nmodalities. Diagnosing the vulnerability of atherosclerotic plaques directly\nfrom carotid 3D MRI images is relatively challenging for both radiologists and\nconventional 3D vision networks. In clinical practice, radiologists assess\npatient conditions using a multimodal approach that incorporates various\nimaging modalities and domain-specific expertise, paving the way for the\ncreation of multimodal diagnostic networks. In this paper, we have developed an\neffective strategy to leverage radiologists' domain knowledge to automate the\ndiagnosis of carotid plaque vulnerability through Variation inference and\nMultimodal knowledge Distillation (VMD). This method excels in harnessing\ncross-modality prior knowledge from limited image annotations and radiology\nreports within training data, thereby enhancing the diagnostic network's\naccuracy for unannotated 3D MRI images. We conducted in-depth experiments on\nthe dataset collected in-house and verified the effectiveness of the VMD\nstrategy we proposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal learning has attracted much attention in recent years due to its\nability to effectively utilize data features from a variety of different\nmodalities. Diagnosing the vulnerability of atherosclerotic plaques directly\nfrom carotid 3D MRI images is relatively challenging for both radiologists and\nconventional 3D vision networks. In clinical practice, radiologists assess\npatient conditions using a multimodal approach that incorporates various\nimaging modalities and domain-specific expertise, paving the way for the\ncreation of multimodal diagnostic networks. In this paper, we have developed an\neffective strategy to leverage radiologists' domain knowledge to automate the\ndiagnosis of carotid plaque vulnerability through Variation inference and\nMultimodal knowledge Distillation (VMD). This method excels in harnessing\ncross-modality prior knowledge from limited image annotations and radiology\nreports within training data, thereby enhancing the diagnostic network's\naccuracy for unannotated 3D MRI images. We conducted in-depth experiments on\nthe dataset collected in-house and verified the effectiveness of the VMD\nstrategy we proposed."
                },
                "authors": [
                    {
                        "name": "Bo Cao"
                    },
                    {
                        "name": "Fan Yu"
                    },
                    {
                        "name": "Mengmeng Feng"
                    },
                    {
                        "name": "SenHao Zhang"
                    },
                    {
                        "name": "Xin Meng"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Zhen Qian"
                    },
                    {
                        "name": "Jie Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Lu"
                },
                "author": "Jie Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11924v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11924v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11921v1",
                "updated": "2025-09-15T13:37:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    37,
                    35,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T13:37:35Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    37,
                    35,
                    0,
                    258,
                    0
                ],
                "title": "Designing LLMs for cultural sensitivity: Evidence from English-Japanese\n  translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing LLMs for cultural sensitivity: Evidence from English-Japanese\n  translation"
                },
                "summary": "Large language models (LLMs) are increasingly used in everyday communication,\nincluding multilingual interactions across different cultural contexts. While\nLLMs can now generate near-perfect literal translations, it remains unclear\nwhether LLMs support culturally appropriate communication. In this paper, we\nanalyze the cultural sensitivity of different LLM designs when applied to\nEnglish-Japanese translations of workplace e-mails. Here, we vary the prompting\nstrategies: (1) naive \"just translate\" prompts, (2) audience-targeted prompts\nspecifying the recipient's cultural background, and (3) instructional prompts\nwith explicit guidance on Japanese communication norms. Using a mixed-methods\nstudy, we then analyze culture-specific language patterns to evaluate how well\ntranslations adapt to cultural norms. Further, we examine the appropriateness\nof the tone of the translations as perceived by native speakers. We find that\nculturally-tailored prompting can improve cultural fit, based on which we offer\nrecommendations for designing culturally inclusive LLMs in multilingual\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used in everyday communication,\nincluding multilingual interactions across different cultural contexts. While\nLLMs can now generate near-perfect literal translations, it remains unclear\nwhether LLMs support culturally appropriate communication. In this paper, we\nanalyze the cultural sensitivity of different LLM designs when applied to\nEnglish-Japanese translations of workplace e-mails. Here, we vary the prompting\nstrategies: (1) naive \"just translate\" prompts, (2) audience-targeted prompts\nspecifying the recipient's cultural background, and (3) instructional prompts\nwith explicit guidance on Japanese communication norms. Using a mixed-methods\nstudy, we then analyze culture-specific language patterns to evaluate how well\ntranslations adapt to cultural norms. Further, we examine the appropriateness\nof the tone of the translations as perceived by native speakers. We find that\nculturally-tailored prompting can improve cultural fit, based on which we offer\nrecommendations for designing culturally inclusive LLMs in multilingual\nsettings."
                },
                "authors": [
                    {
                        "name": "Helene Tenzer"
                    },
                    {
                        "name": "Oumnia Abidi"
                    },
                    {
                        "name": "Stefan Feuerriegel"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Feuerriegel"
                },
                "author": "Stefan Feuerriegel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11915v1",
                "updated": "2025-09-15T13:33:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    33,
                    32,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T13:33:32Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    33,
                    32,
                    0,
                    258,
                    0
                ],
                "title": "Uncertainty in Authorship: Why Perfect AI Detection Is Mathematically\n  Impossible",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty in Authorship: Why Perfect AI Detection Is Mathematically\n  Impossible"
                },
                "summary": "As large language models (LLMs) become more advanced, it is increasingly\ndifficult to distinguish between human-written and AI-generated text. This\npaper draws a conceptual parallel between quantum uncertainty and the limits of\nauthorship detection in natural language. We argue that there is a fundamental\ntrade-off: the more confidently one tries to identify whether a text was\nwritten by a human or an AI, the more one risks disrupting the text's natural\nflow and authenticity. This mirrors the tension between precision and\ndisturbance found in quantum systems. We explore how current detection\nmethods--such as stylometry, watermarking, and neural classifiers--face\ninherent limitations. Enhancing detection accuracy often leads to changes in\nthe AI's output, making other features less reliable. In effect, the very act\nof trying to detect AI authorship introduces uncertainty elsewhere in the text.\nOur analysis shows that when AI-generated text closely mimics human writing,\nperfect detection becomes not just technologically difficult but theoretically\nimpossible. We address counterarguments and discuss the broader implications\nfor authorship, ethics, and policy. Ultimately, we suggest that the challenge\nof AI-text detection is not just a matter of better tools--it reflects a\ndeeper, unavoidable tension in the nature of language itself.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become more advanced, it is increasingly\ndifficult to distinguish between human-written and AI-generated text. This\npaper draws a conceptual parallel between quantum uncertainty and the limits of\nauthorship detection in natural language. We argue that there is a fundamental\ntrade-off: the more confidently one tries to identify whether a text was\nwritten by a human or an AI, the more one risks disrupting the text's natural\nflow and authenticity. This mirrors the tension between precision and\ndisturbance found in quantum systems. We explore how current detection\nmethods--such as stylometry, watermarking, and neural classifiers--face\ninherent limitations. Enhancing detection accuracy often leads to changes in\nthe AI's output, making other features less reliable. In effect, the very act\nof trying to detect AI authorship introduces uncertainty elsewhere in the text.\nOur analysis shows that when AI-generated text closely mimics human writing,\nperfect detection becomes not just technologically difficult but theoretically\nimpossible. We address counterarguments and discuss the broader implications\nfor authorship, ethics, and policy. Ultimately, we suggest that the challenge\nof AI-text detection is not just a matter of better tools--it reflects a\ndeeper, unavoidable tension in the nature of language itself."
                },
                "authors": [
                    {
                        "name": "Aadil Gani Ganie"
                    }
                ],
                "author_detail": {
                    "name": "Aadil Gani Ganie"
                },
                "author": "Aadil Gani Ganie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11914v1",
                "updated": "2025-09-15T13:33:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    33,
                    29,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T13:33:29Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    33,
                    29,
                    0,
                    258,
                    0
                ],
                "title": "EgoMem: Lifelong Memory Agent for Full-duplex Omnimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EgoMem: Lifelong Memory Agent for Full-duplex Omnimodal Models"
                },
                "summary": "We introduce EgoMem, the first lifelong memory agent tailored for full-duplex\nmodels that process real-time omnimodal streams. EgoMem enables real-time\nmodels to recognize multiple users directly from raw audiovisual streams, to\nprovide personalized response, and to maintain long-term knowledge of users'\nfacts, preferences, and social relationships extracted from audiovisual\nhistory. EgoMem operates with three asynchronous processes: (i) a retrieval\nprocess that dynamically identifies user via face and voice, and gathers\nrelevant context from a long-term memory; (ii) an omnimodal dialog process that\ngenerates personalized audio responses based on the retrieved context; and\n(iii) a memory management process that automatically detects dialog boundaries\nfrom omnimodal streams, and extracts necessary information to update the\nlong-term memory. Unlike existing memory agents for LLMs, EgoMem relies\nentirely on raw audiovisual streams, making it especially suitable for\nlifelong, real-time, and embodied scenarios. Experimental results demonstrate\nthat EgoMem's retrieval and memory management modules achieve over 95% accuracy\non the test set. When integrated with a fine-tuned RoboEgo omnimodal chatbot,\nthe system achieves fact-consistency scores above 87% in real-time personalized\ndialogs, establishing a strong baseline for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce EgoMem, the first lifelong memory agent tailored for full-duplex\nmodels that process real-time omnimodal streams. EgoMem enables real-time\nmodels to recognize multiple users directly from raw audiovisual streams, to\nprovide personalized response, and to maintain long-term knowledge of users'\nfacts, preferences, and social relationships extracted from audiovisual\nhistory. EgoMem operates with three asynchronous processes: (i) a retrieval\nprocess that dynamically identifies user via face and voice, and gathers\nrelevant context from a long-term memory; (ii) an omnimodal dialog process that\ngenerates personalized audio responses based on the retrieved context; and\n(iii) a memory management process that automatically detects dialog boundaries\nfrom omnimodal streams, and extracts necessary information to update the\nlong-term memory. Unlike existing memory agents for LLMs, EgoMem relies\nentirely on raw audiovisual streams, making it especially suitable for\nlifelong, real-time, and embodied scenarios. Experimental results demonstrate\nthat EgoMem's retrieval and memory management modules achieve over 95% accuracy\non the test set. When integrated with a fine-tuned RoboEgo omnimodal chatbot,\nthe system achieves fact-consistency scores above 87% in real-time personalized\ndialogs, establishing a strong baseline for future research."
                },
                "authors": [
                    {
                        "name": "Yiqun Yao"
                    },
                    {
                        "name": "Naitong Yu"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Xuezhi Fang"
                    },
                    {
                        "name": "Wenjia Ma"
                    },
                    {
                        "name": "Xuying Meng"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Aixin Sun"
                    },
                    {
                        "name": "Yequan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yequan Wang"
                },
                "author": "Yequan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11911v1",
                "updated": "2025-09-15T13:30:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    30,
                    50,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T13:30:50Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    30,
                    50,
                    0,
                    258,
                    0
                ],
                "title": "Quantum Noise Tomography with Physics-Informed Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Noise Tomography with Physics-Informed Neural Networks"
                },
                "summary": "Characterizing the environmental interactions of quantum systems is a\ncritical bottleneck in the development of robust quantum technologies.\nTraditional tomographic methods are often data-intensive and struggle with\nscalability. In this work, we introduce a novel framework for performing\nLindblad tomography using Physics-Informed Neural Networks (PINNs). By\nembedding the Lindblad master equation directly into the neural network's loss\nfunction, our approach simultaneously learns the quantum state's evolution and\ninfers the underlying dissipation parameters from sparse, time-series\nmeasurement data. Our results show that PINNs can reconstruct both the system\ndynamics and the functional form of unknown noise parameters, presenting a\nsample-efficient and scalable solution for quantum device characterization.\nUltimately, our method produces a fully-differentiable digital twin of a noisy\nquantum system by learning its governing master equation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing the environmental interactions of quantum systems is a\ncritical bottleneck in the development of robust quantum technologies.\nTraditional tomographic methods are often data-intensive and struggle with\nscalability. In this work, we introduce a novel framework for performing\nLindblad tomography using Physics-Informed Neural Networks (PINNs). By\nembedding the Lindblad master equation directly into the neural network's loss\nfunction, our approach simultaneously learns the quantum state's evolution and\ninfers the underlying dissipation parameters from sparse, time-series\nmeasurement data. Our results show that PINNs can reconstruct both the system\ndynamics and the functional form of unknown noise parameters, presenting a\nsample-efficient and scalable solution for quantum device characterization.\nUltimately, our method produces a fully-differentiable digital twin of a noisy\nquantum system by learning its governing master equation."
                },
                "authors": [
                    {
                        "name": "Antonin Sulc"
                    }
                ],
                "author_detail": {
                    "name": "Antonin Sulc"
                },
                "author": "Antonin Sulc",
                "arxiv_comment": "6 pages, 3 figures, Machine Learning and the Physical Sciences\n  Workshop at the 39th conference on Neural Information Processing Systems\n  (NeurIPS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09040v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09040v2",
                "updated": "2025-09-15T13:26:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    26,
                    3,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-10T22:21:43Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    22,
                    21,
                    43,
                    2,
                    253,
                    0
                ],
                "title": "Suppression of pair beam instabilities in a laboratory analogue of\n  blazar pair cascades",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Suppression of pair beam instabilities in a laboratory analogue of\n  blazar pair cascades"
                },
                "summary": "The generation of dense electron-positron pair beams in the laboratory can\nenable direct tests of theoretical models of $\\gamma$-ray bursts and active\ngalactic nuclei. We have successfully achieved this using ultra-relativistic\nprotons accelerated by the Super Proton Synchrotron at CERN. In the first\napplication of this experimental platform, the stability of the pair beam is\nstudied as it propagates through a metre-length plasma, analogous to TeV\n$\\gamma$-ray induced pair cascades in the intergalactic medium. It has been\nargued that pair beam instabilities disrupt the cascade, thus accounting for\nthe observed lack of reprocessed GeV emission from TeV blazars. If true this\nwould remove the need for a moderate strength intergalactic magnetic field to\nexplain the observations. We find that the pair beam instability is suppressed\nif the beam is not perfectly collimated or monochromatic, hence the lower limit\nto the intergalactic magnetic field inferred from $\\gamma$-ray observations of\nblazars is robust.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation of dense electron-positron pair beams in the laboratory can\nenable direct tests of theoretical models of $\\gamma$-ray bursts and active\ngalactic nuclei. We have successfully achieved this using ultra-relativistic\nprotons accelerated by the Super Proton Synchrotron at CERN. In the first\napplication of this experimental platform, the stability of the pair beam is\nstudied as it propagates through a metre-length plasma, analogous to TeV\n$\\gamma$-ray induced pair cascades in the intergalactic medium. It has been\nargued that pair beam instabilities disrupt the cascade, thus accounting for\nthe observed lack of reprocessed GeV emission from TeV blazars. If true this\nwould remove the need for a moderate strength intergalactic magnetic field to\nexplain the observations. We find that the pair beam instability is suppressed\nif the beam is not perfectly collimated or monochromatic, hence the lower limit\nto the intergalactic magnetic field inferred from $\\gamma$-ray observations of\nblazars is robust."
                },
                "authors": [
                    {
                        "name": "Charles D. Arrowsmith"
                    },
                    {
                        "name": "Francesco Miniati"
                    },
                    {
                        "name": "Pablo J. Bilbao"
                    },
                    {
                        "name": "Pascal Simon"
                    },
                    {
                        "name": "Archie F. A. Bott"
                    },
                    {
                        "name": "Stephane Burger"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Filipe D. Cruz"
                    },
                    {
                        "name": "Tristan Davenne"
                    },
                    {
                        "name": "Anthony Dyson"
                    },
                    {
                        "name": "Ilias Efthymiopoulos"
                    },
                    {
                        "name": "Dustin H. Froula"
                    },
                    {
                        "name": "Alice Goillot"
                    },
                    {
                        "name": "Jon T. Gudmundsson"
                    },
                    {
                        "name": "Dan Haberberger"
                    },
                    {
                        "name": "Jack W. D. Halliday"
                    },
                    {
                        "name": "Tom Hodge"
                    },
                    {
                        "name": "Brian T. Huffman"
                    },
                    {
                        "name": "Sam Iaquinta"
                    },
                    {
                        "name": "G. Marshall"
                    },
                    {
                        "name": "Brian Reville"
                    },
                    {
                        "name": "Subir Sarkar"
                    },
                    {
                        "name": "Alexander A. Schekochihin"
                    },
                    {
                        "name": "Luis O. Silva"
                    },
                    {
                        "name": "Raspberry Simpson"
                    },
                    {
                        "name": "Vasiliki Stergiou"
                    },
                    {
                        "name": "Raoul M. G. M. Trines"
                    },
                    {
                        "name": "Thibault Vieu"
                    },
                    {
                        "name": "Nikolaos Charitonidis"
                    },
                    {
                        "name": "Robert Bingham"
                    },
                    {
                        "name": "Gianluca Gregori"
                    }
                ],
                "author_detail": {
                    "name": "Gianluca Gregori"
                },
                "author": "Gianluca Gregori",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09040v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09040v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01791v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01791v2",
                "updated": "2025-09-15T13:13:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    13,
                    33,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-01T21:41:34Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    21,
                    41,
                    34,
                    0,
                    244,
                    0
                ],
                "title": "E-PhishGen: Unlocking Novel Research in Phishing Email Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-PhishGen: Unlocking Novel Research in Phishing Email Detection"
                },
                "summary": "Every day, our inboxes are flooded with unsolicited emails, ranging between\nannoying spam to more subtle phishing scams. Unfortunately, despite abundant\nprior efforts proposing solutions achieving near-perfect accuracy, the reality\nis that countering malicious emails still remains an unsolved dilemma.\n  This \"open problem\" paper carries out a critical assessment of scientific\nworks in the context of phishing email detection. First, we focus on the\nbenchmark datasets that have been used to assess the methods proposed in\nresearch. We find that most prior work relied on datasets containing emails\nthat -- we argue -- are not representative of current trends, and mostly\nencompass the English language. Based on this finding, we then re-implement and\nre-assess a variety of detection methods reliant on machine learning (ML),\nincluding large-language models (LLM), and release all of our codebase -- an\n(unfortunately) uncommon practice in related research. We show that most such\nmethods achieve near-perfect performance when trained and tested on the same\ndataset -- a result which intrinsically hinders development (how can future\nresearch outperform methods that are already near perfect?). To foster the\ncreation of \"more challenging benchmarks\" that reflect current phishing trends,\nwe propose E-PhishGEN, an LLM-based (and privacy-savvy) framework to generate\nnovel phishing-email datasets. We use our E-PhishGEN to create E-PhishLLM, a\nnovel phishing-email detection dataset containing 16616 emails in three\nlanguages. We use E-PhishLLM to test the detectors we considered, showing a\nmuch lower performance than that achieved on existing benchmarks -- indicating\na larger room for improvement. We also validate the quality of E-PhishLLM with\na user study (n=30). To sum up, we show that phishing email detection is still\nan open problem -- and provide the means to tackle such a problem by future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Every day, our inboxes are flooded with unsolicited emails, ranging between\nannoying spam to more subtle phishing scams. Unfortunately, despite abundant\nprior efforts proposing solutions achieving near-perfect accuracy, the reality\nis that countering malicious emails still remains an unsolved dilemma.\n  This \"open problem\" paper carries out a critical assessment of scientific\nworks in the context of phishing email detection. First, we focus on the\nbenchmark datasets that have been used to assess the methods proposed in\nresearch. We find that most prior work relied on datasets containing emails\nthat -- we argue -- are not representative of current trends, and mostly\nencompass the English language. Based on this finding, we then re-implement and\nre-assess a variety of detection methods reliant on machine learning (ML),\nincluding large-language models (LLM), and release all of our codebase -- an\n(unfortunately) uncommon practice in related research. We show that most such\nmethods achieve near-perfect performance when trained and tested on the same\ndataset -- a result which intrinsically hinders development (how can future\nresearch outperform methods that are already near perfect?). To foster the\ncreation of \"more challenging benchmarks\" that reflect current phishing trends,\nwe propose E-PhishGEN, an LLM-based (and privacy-savvy) framework to generate\nnovel phishing-email datasets. We use our E-PhishGEN to create E-PhishLLM, a\nnovel phishing-email detection dataset containing 16616 emails in three\nlanguages. We use E-PhishLLM to test the detectors we considered, showing a\nmuch lower performance than that achieved on existing benchmarks -- indicating\na larger room for improvement. We also validate the quality of E-PhishLLM with\na user study (n=30). To sum up, we show that phishing email detection is still\nan open problem -- and provide the means to tackle such a problem by future\nresearch."
                },
                "authors": [
                    {
                        "name": "Luca Pajola"
                    },
                    {
                        "name": "Eugenio Caripoti"
                    },
                    {
                        "name": "Stefan Banzer"
                    },
                    {
                        "name": "Simeone Pizzi"
                    },
                    {
                        "name": "Mauro Conti"
                    },
                    {
                        "name": "Giovanni Apruzzese"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Apruzzese"
                },
                "author": "Giovanni Apruzzese",
                "arxiv_doi": "10.1145/3733799.3762967",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3733799.3762967",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.01791v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01791v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to ACM AISec '25",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05755v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05755v2",
                "updated": "2025-09-15T13:11:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    11,
                    40,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-06T15:48:49Z",
                "published_parsed": [
                    2025,
                    9,
                    6,
                    15,
                    48,
                    49,
                    5,
                    249,
                    0
                ],
                "title": "Exploit Tool Invocation Prompt for Tool Behavior Hijacking in LLM-Based\n  Agentic System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploit Tool Invocation Prompt for Tool Behavior Hijacking in LLM-Based\n  Agentic System"
                },
                "summary": "LLM-based agentic systems leverage large language models to handle user\nqueries, make decisions, and execute external tools for complex tasks across\ndomains like chatbots, customer service, and software engineering. A critical\ncomponent of these systems is the Tool Invocation Prompt (TIP), which defines\ntool interaction protocols and guides LLMs to ensure the security and\ncorrectness of tool usage. Despite its importance, TIP security has been\nlargely overlooked. This work investigates TIP-related security risks,\nrevealing that major LLM-based systems like Cursor, Claude Code, and others are\nvulnerable to attacks such as remote code execution (RCE) and denial of service\n(DoS). Through a systematic TIP exploitation workflow (TEW), we demonstrate\nexternal tool behavior hijacking via manipulated tool invocations. We also\npropose defense mechanisms to enhance TIP security in LLM-based agentic\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agentic systems leverage large language models to handle user\nqueries, make decisions, and execute external tools for complex tasks across\ndomains like chatbots, customer service, and software engineering. A critical\ncomponent of these systems is the Tool Invocation Prompt (TIP), which defines\ntool interaction protocols and guides LLMs to ensure the security and\ncorrectness of tool usage. Despite its importance, TIP security has been\nlargely overlooked. This work investigates TIP-related security risks,\nrevealing that major LLM-based systems like Cursor, Claude Code, and others are\nvulnerable to attacks such as remote code execution (RCE) and denial of service\n(DoS). Through a systematic TIP exploitation workflow (TEW), we demonstrate\nexternal tool behavior hijacking via manipulated tool invocations. We also\npropose defense mechanisms to enhance TIP security in LLM-based agentic\nsystems."
                },
                "authors": [
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Yuchong Xie"
                    },
                    {
                        "name": "Mingyu Luo"
                    },
                    {
                        "name": "Zesen Liu"
                    },
                    {
                        "name": "Zhixiang Zhang"
                    },
                    {
                        "name": "Kaikai Zhang"
                    },
                    {
                        "name": "Zongjie Li"
                    },
                    {
                        "name": "Ping Chen"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Dongdong She"
                    }
                ],
                "author_detail": {
                    "name": "Dongdong She"
                },
                "author": "Dongdong She",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05755v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05755v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11882v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11882v1",
                "updated": "2025-09-15T13:01:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    1,
                    4,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T13:01:04Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    1,
                    4,
                    0,
                    258,
                    0
                ],
                "title": "Universal relations for fast rotating neutron stars without equation of\n  state bias",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal relations for fast rotating neutron stars without equation of\n  state bias"
                },
                "summary": "We provide a summary of several parametrisations for the nuclear equation of\nstate that have been proposed over the past decades and list viable ranges for\ntheir parameters. Based on these parametrisations, we construct a large\ndatabase of rotating neutron star models, which are arranged in sequences of\nconstant central energy density. After filtering these sequences with respect\nto generous astrophysical constraints, we discover tight universal relations\nfor various bulk quantities of uniformly rotating neutron stars of arbitrary\nrotation rates. These universal relations allow to estimate, at very low\ncomputational cost, bulk quantities of rotating neutron stars employing mass,\nradius, and moment of inertia of associated non-rotating neutron stars. The\nrelations are calibrated to a large, model-agnostic dataset, thereby\neliminating a potential bias, and prove to be robust. Such relations are\nimportant for future, high-precision measurements coming from electromagnetic\nand gravitational wave observations and may be used in equation of state\ninference codes or gravitational wave modeling among others.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We provide a summary of several parametrisations for the nuclear equation of\nstate that have been proposed over the past decades and list viable ranges for\ntheir parameters. Based on these parametrisations, we construct a large\ndatabase of rotating neutron star models, which are arranged in sequences of\nconstant central energy density. After filtering these sequences with respect\nto generous astrophysical constraints, we discover tight universal relations\nfor various bulk quantities of uniformly rotating neutron stars of arbitrary\nrotation rates. These universal relations allow to estimate, at very low\ncomputational cost, bulk quantities of rotating neutron stars employing mass,\nradius, and moment of inertia of associated non-rotating neutron stars. The\nrelations are calibrated to a large, model-agnostic dataset, thereby\neliminating a potential bias, and prove to be robust. Such relations are\nimportant for future, high-precision measurements coming from electromagnetic\nand gravitational wave observations and may be used in equation of state\ninference codes or gravitational wave modeling among others."
                },
                "authors": [
                    {
                        "name": "Christian J. Krüger"
                    },
                    {
                        "name": "Mariachiara Celato"
                    }
                ],
                "author_detail": {
                    "name": "Mariachiara Celato"
                },
                "author": "Mariachiara Celato",
                "arxiv_comment": "20 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11882v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11882v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11878v1",
                "updated": "2025-09-15T12:58:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    58,
                    38,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T12:58:38Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    58,
                    38,
                    0,
                    258,
                    0
                ],
                "title": "Do It Yourself (DIY): Modifying Images for Poems in a Zero-Shot Setting\n  Using Weighted Prompt Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do It Yourself (DIY): Modifying Images for Poems in a Zero-Shot Setting\n  Using Weighted Prompt Manipulation"
                },
                "summary": "Poetry is an expressive form of art that invites multiple interpretations, as\nreaders often bring their own emotions, experiences, and cultural backgrounds\ninto their understanding of a poem. Recognizing this, we aim to generate images\nfor poems and improve these images in a zero-shot setting, enabling audiences\nto modify images as per their requirements. To achieve this, we introduce a\nnovel Weighted Prompt Manipulation (WPM) technique, which systematically\nmodifies attention weights and text embeddings within diffusion models. By\ndynamically adjusting the importance of specific words, WPM enhances or\nsuppresses their influence in the final generated image, leading to\nsemantically richer and more contextually accurate visualizations. Our approach\nexploits diffusion models and large language models (LLMs) such as GPT in\nconjunction with existing poetry datasets, ensuring a comprehensive and\nstructured methodology for improved image generation in the literary domain. To\nthe best of our knowledge, this is the first attempt at integrating weighted\nprompt manipulation for enhancing imagery in poetic language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Poetry is an expressive form of art that invites multiple interpretations, as\nreaders often bring their own emotions, experiences, and cultural backgrounds\ninto their understanding of a poem. Recognizing this, we aim to generate images\nfor poems and improve these images in a zero-shot setting, enabling audiences\nto modify images as per their requirements. To achieve this, we introduce a\nnovel Weighted Prompt Manipulation (WPM) technique, which systematically\nmodifies attention weights and text embeddings within diffusion models. By\ndynamically adjusting the importance of specific words, WPM enhances or\nsuppresses their influence in the final generated image, leading to\nsemantically richer and more contextually accurate visualizations. Our approach\nexploits diffusion models and large language models (LLMs) such as GPT in\nconjunction with existing poetry datasets, ensuring a comprehensive and\nstructured methodology for improved image generation in the literary domain. To\nthe best of our knowledge, this is the first attempt at integrating weighted\nprompt manipulation for enhancing imagery in poetic language."
                },
                "authors": [
                    {
                        "name": "Sofia Jamil"
                    },
                    {
                        "name": "Kotla Sai Charan"
                    },
                    {
                        "name": "Sriparna Saha"
                    },
                    {
                        "name": "Koustava Goswami"
                    },
                    {
                        "name": "K J Joseph"
                    }
                ],
                "author_detail": {
                    "name": "K J Joseph"
                },
                "author": "K J Joseph",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11868v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11868v1",
                "updated": "2025-09-15T12:39:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    39,
                    55,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T12:39:55Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    39,
                    55,
                    0,
                    258,
                    0
                ],
                "title": "Growing Perspectives: Modelling Embodied Perspective Taking and Inner\n  Narrative Development Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Growing Perspectives: Modelling Embodied Perspective Taking and Inner\n  Narrative Development Using Large Language Models"
                },
                "summary": "Language and embodied perspective taking are essential for human\ncollaboration, yet few computational models address both simultaneously. This\nwork investigates the PerspAct system [1], which integrates the ReAct (Reason\nand Act) paradigm with Large Language Models (LLMs) to simulate developmental\nstages of perspective taking, grounded in Selman's theory [2]. Using an\nextended director task, we evaluate GPT's ability to generate internal\nnarratives aligned with specified developmental stages, and assess how these\ninfluence collaborative performance both qualitatively (action selection) and\nquantitatively (task efficiency). Results show that GPT reliably produces\ndevelopmentally-consistent narratives before task execution but often shifts\ntowards more advanced stages during interaction, suggesting that language\nexchanges help refine internal representations. Higher developmental stages\ngenerally enhance collaborative effectiveness, while earlier stages yield more\nvariable outcomes in complex contexts. These findings highlight the potential\nof integrating embodied perspective taking and language in LLMs to better model\ndevelopmental dynamics and stress the importance of evaluating internal speech\nduring combined linguistic and embodied tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language and embodied perspective taking are essential for human\ncollaboration, yet few computational models address both simultaneously. This\nwork investigates the PerspAct system [1], which integrates the ReAct (Reason\nand Act) paradigm with Large Language Models (LLMs) to simulate developmental\nstages of perspective taking, grounded in Selman's theory [2]. Using an\nextended director task, we evaluate GPT's ability to generate internal\nnarratives aligned with specified developmental stages, and assess how these\ninfluence collaborative performance both qualitatively (action selection) and\nquantitatively (task efficiency). Results show that GPT reliably produces\ndevelopmentally-consistent narratives before task execution but often shifts\ntowards more advanced stages during interaction, suggesting that language\nexchanges help refine internal representations. Higher developmental stages\ngenerally enhance collaborative effectiveness, while earlier stages yield more\nvariable outcomes in complex contexts. These findings highlight the potential\nof integrating embodied perspective taking and language in LLMs to better model\ndevelopmental dynamics and stress the importance of evaluating internal speech\nduring combined linguistic and embodied tasks."
                },
                "authors": [
                    {
                        "name": "Sabrina Patania"
                    },
                    {
                        "name": "Luca Annese"
                    },
                    {
                        "name": "Anna Lambiase"
                    },
                    {
                        "name": "Anita Pellegrini"
                    },
                    {
                        "name": "Tom Foulsham"
                    },
                    {
                        "name": "Azzurra Ruggeri"
                    },
                    {
                        "name": "Silvia Rossi"
                    },
                    {
                        "name": "Silvia Serino"
                    },
                    {
                        "name": "Dimitri Ognibene"
                    }
                ],
                "author_detail": {
                    "name": "Dimitri Ognibene"
                },
                "author": "Dimitri Ognibene",
                "arxiv_comment": "Accepted at ICDL https://icdl2025.fel.cvut.cz/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11868v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11868v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.2.7; I.2.10; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11865v1",
                "updated": "2025-09-15T12:39:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    39,
                    15,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T12:39:15Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    39,
                    15,
                    0,
                    258,
                    0
                ],
                "title": "Tenma: Robust Cross-Embodiment Robot Manipulation with Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tenma: Robust Cross-Embodiment Robot Manipulation with Diffusion\n  Transformer"
                },
                "summary": "Scaling Transformer policies and diffusion models has advanced robotic\nmanipulation, yet combining these techniques in lightweight, cross-embodiment\nlearning settings remains challenging. We study design choices that most affect\nstability and performance for diffusion-transformer policies trained on\nheterogeneous, multimodal robot data, and introduce Tenma, a lightweight\ndiffusion-transformer for bi-manual arm control. Tenma integrates multiview\nRGB, proprioception, and language via a cross-embodiment normalizer that maps\ndisparate state/action spaces into a shared latent space; a Joint State-Time\nencoder for temporally aligned observation learning with inference speed\nboosts; and a diffusion action decoder optimized for training stability and\nlearning capacity. Across benchmarks and under matched compute, Tenma achieves\nan average success rate of 88.95% in-distribution and maintains strong\nperformance under object and scene shifts, substantially exceeding baseline\npolicies whose best in-distribution average is 18.12%. Despite using moderate\ndata scale, Tenma delivers robust manipulation and generalization, indicating\nthe great potential for multimodal and cross-embodiment learning strategies for\nfurther augmenting the capacity of transformer-based imitation learning\npolicies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Transformer policies and diffusion models has advanced robotic\nmanipulation, yet combining these techniques in lightweight, cross-embodiment\nlearning settings remains challenging. We study design choices that most affect\nstability and performance for diffusion-transformer policies trained on\nheterogeneous, multimodal robot data, and introduce Tenma, a lightweight\ndiffusion-transformer for bi-manual arm control. Tenma integrates multiview\nRGB, proprioception, and language via a cross-embodiment normalizer that maps\ndisparate state/action spaces into a shared latent space; a Joint State-Time\nencoder for temporally aligned observation learning with inference speed\nboosts; and a diffusion action decoder optimized for training stability and\nlearning capacity. Across benchmarks and under matched compute, Tenma achieves\nan average success rate of 88.95% in-distribution and maintains strong\nperformance under object and scene shifts, substantially exceeding baseline\npolicies whose best in-distribution average is 18.12%. Despite using moderate\ndata scale, Tenma delivers robust manipulation and generalization, indicating\nthe great potential for multimodal and cross-embodiment learning strategies for\nfurther augmenting the capacity of transformer-based imitation learning\npolicies."
                },
                "authors": [
                    {
                        "name": "Travis Davies"
                    },
                    {
                        "name": "Yiqi Huang"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Huxian Liu"
                    },
                    {
                        "name": "Luhui Hu"
                    }
                ],
                "author_detail": {
                    "name": "Luhui Hu"
                },
                "author": "Luhui Hu",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04427v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04427v2",
                "updated": "2025-09-15T12:38:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    38,
                    51,
                    0,
                    258,
                    0
                ],
                "published": "2025-06-04T20:21:52Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    20,
                    21,
                    52,
                    2,
                    155,
                    0
                ],
                "title": "Plugging Schema Graph into Multi-Table QA: A Human-Guided Framework for\n  Reducing LLM Reliance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plugging Schema Graph into Multi-Table QA: A Human-Guided Framework for\n  Reducing LLM Reliance"
                },
                "summary": "Large language models (LLMs) have shown promise in table Question Answering\n(Table QA). However, extending these capabilities to multi-table QA remains\nchallenging due to unreliable schema linking across complex tables. Existing\nmethods based on semantic similarity work well only on simplified hand-crafted\ndatasets and struggle to handle complex, real-world scenarios with numerous and\ndiverse columns. To address this, we propose a graph-based framework that\nleverages human-curated relational knowledge to explicitly encode schema links\nand join paths. Given a natural language query, our method searches on graph to\nconstruct interpretable reasoning chains, aided by pruning and sub-path merging\nstrategies to enhance efficiency and coherence. Experiments on both standard\nbenchmarks and a realistic, large-scale dataset demonstrate the effectiveness\nof our approach. To our knowledge, this is the first multi-table QA system\napplied to truly complex industrial tabular data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promise in table Question Answering\n(Table QA). However, extending these capabilities to multi-table QA remains\nchallenging due to unreliable schema linking across complex tables. Existing\nmethods based on semantic similarity work well only on simplified hand-crafted\ndatasets and struggle to handle complex, real-world scenarios with numerous and\ndiverse columns. To address this, we propose a graph-based framework that\nleverages human-curated relational knowledge to explicitly encode schema links\nand join paths. Given a natural language query, our method searches on graph to\nconstruct interpretable reasoning chains, aided by pruning and sub-path merging\nstrategies to enhance efficiency and coherence. Experiments on both standard\nbenchmarks and a realistic, large-scale dataset demonstrate the effectiveness\nof our approach. To our knowledge, this is the first multi-table QA system\napplied to truly complex industrial tabular data."
                },
                "authors": [
                    {
                        "name": "Xixi Wang"
                    },
                    {
                        "name": "Miguel Costa"
                    },
                    {
                        "name": "Jordanka Kovaceva"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Francisco C. Pereira"
                    }
                ],
                "author_detail": {
                    "name": "Francisco C. Pereira"
                },
                "author": "Francisco C. Pereira",
                "arxiv_comment": "Accepted to EMNLP 2025 findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04427v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04427v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11864v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11864v1",
                "updated": "2025-09-15T12:38:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    38,
                    39,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T12:38:39Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    38,
                    39,
                    0,
                    258,
                    0
                ],
                "title": "NeuroStrike: Neuron-Level Attacks on Aligned LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuroStrike: Neuron-Level Attacks on Aligned LLMs"
                },
                "summary": "Safety alignment is critical for the ethical deployment of large language\nmodels (LLMs), guiding them to avoid generating harmful or unethical content.\nCurrent alignment techniques, such as supervised fine-tuning and reinforcement\nlearning from human feedback, remain fragile and can be bypassed by carefully\ncrafted adversarial prompts. Unfortunately, such attacks rely on trial and\nerror, lack generalizability across models, and are constrained by scalability\nand reliability.\n  This paper presents NeuroStrike, a novel and generalizable attack framework\nthat exploits a fundamental vulnerability introduced by alignment techniques:\nthe reliance on sparse, specialized safety neurons responsible for detecting\nand suppressing harmful inputs. We apply NeuroStrike to both white-box and\nblack-box settings: In the white-box setting, NeuroStrike identifies safety\nneurons through feedforward activation analysis and prunes them during\ninference to disable safety mechanisms. In the black-box setting, we propose\nthe first LLM profiling attack, which leverages safety neuron transferability\nby training adversarial prompt generators on open-weight surrogate models and\nthen deploying them against black-box and proprietary targets. We evaluate\nNeuroStrike on over 20 open-weight LLMs from major LLM developers. By removing\nless than 0.6% of neurons in targeted layers, NeuroStrike achieves an average\nattack success rate (ASR) of 76.9% using only vanilla malicious prompts.\nMoreover, Neurostrike generalizes to four multimodal LLMs with 100% ASR on\nunsafe image inputs. Safety neurons transfer effectively across architectures,\nraising ASR to 78.5% on 11 fine-tuned models and 77.7% on five distilled\nmodels. The black-box LLM profiling attack achieves an average ASR of 63.7%\nacross five black-box models, including the Google Gemini family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety alignment is critical for the ethical deployment of large language\nmodels (LLMs), guiding them to avoid generating harmful or unethical content.\nCurrent alignment techniques, such as supervised fine-tuning and reinforcement\nlearning from human feedback, remain fragile and can be bypassed by carefully\ncrafted adversarial prompts. Unfortunately, such attacks rely on trial and\nerror, lack generalizability across models, and are constrained by scalability\nand reliability.\n  This paper presents NeuroStrike, a novel and generalizable attack framework\nthat exploits a fundamental vulnerability introduced by alignment techniques:\nthe reliance on sparse, specialized safety neurons responsible for detecting\nand suppressing harmful inputs. We apply NeuroStrike to both white-box and\nblack-box settings: In the white-box setting, NeuroStrike identifies safety\nneurons through feedforward activation analysis and prunes them during\ninference to disable safety mechanisms. In the black-box setting, we propose\nthe first LLM profiling attack, which leverages safety neuron transferability\nby training adversarial prompt generators on open-weight surrogate models and\nthen deploying them against black-box and proprietary targets. We evaluate\nNeuroStrike on over 20 open-weight LLMs from major LLM developers. By removing\nless than 0.6% of neurons in targeted layers, NeuroStrike achieves an average\nattack success rate (ASR) of 76.9% using only vanilla malicious prompts.\nMoreover, Neurostrike generalizes to four multimodal LLMs with 100% ASR on\nunsafe image inputs. Safety neurons transfer effectively across architectures,\nraising ASR to 78.5% on 11 fine-tuned models and 77.7% on five distilled\nmodels. The black-box LLM profiling attack achieves an average ASR of 63.7%\nacross five black-box models, including the Google Gemini family."
                },
                "authors": [
                    {
                        "name": "Lichao Wu"
                    },
                    {
                        "name": "Sasha Behrouzi"
                    },
                    {
                        "name": "Mohamadreza Rostami"
                    },
                    {
                        "name": "Maximilian Thang"
                    },
                    {
                        "name": "Stjepan Picek"
                    },
                    {
                        "name": "Ahmad-Reza Sadeghi"
                    }
                ],
                "author_detail": {
                    "name": "Ahmad-Reza Sadeghi"
                },
                "author": "Ahmad-Reza Sadeghi",
                "arxiv_doi": "10.14722/ndss.2026.230660",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14722/ndss.2026.230660",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.11864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11864v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11851v1",
                "updated": "2025-09-15T12:31:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    31,
                    0,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T12:31:00Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    31,
                    0,
                    0,
                    258,
                    0
                ],
                "title": "The AI Memory Gap: Users Misremember What They Created With AI or\n  Without",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The AI Memory Gap: Users Misremember What They Created With AI or\n  Without"
                },
                "summary": "As large language models (LLMs) become embedded in interactive text\ngeneration, disclosure of AI as a source depends on people remembering which\nideas or texts came from themselves and which were created with AI. We\ninvestigate how accurately people remember the source of content when using AI.\nIn a pre-registered experiment, 184 participants generated and elaborated on\nideas both unaided and with an LLM-based chatbot. One week later, they were\nasked to identify the source (noAI vs withAI) of these ideas and texts. Our\nfindings reveal a significant gap in memory: After AI use, the odds of correct\nattribution dropped, with the steepest decline in mixed human-AI workflows,\nwhere either the idea or elaboration was created with AI. We validated our\nresults using a computational model of source memory. Discussing broader\nimplications, we highlight the importance of considering source confusion in\nthe design and use of interactive text generation technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become embedded in interactive text\ngeneration, disclosure of AI as a source depends on people remembering which\nideas or texts came from themselves and which were created with AI. We\ninvestigate how accurately people remember the source of content when using AI.\nIn a pre-registered experiment, 184 participants generated and elaborated on\nideas both unaided and with an LLM-based chatbot. One week later, they were\nasked to identify the source (noAI vs withAI) of these ideas and texts. Our\nfindings reveal a significant gap in memory: After AI use, the odds of correct\nattribution dropped, with the steepest decline in mixed human-AI workflows,\nwhere either the idea or elaboration was created with AI. We validated our\nresults using a computational model of source memory. Discussing broader\nimplications, we highlight the importance of considering source confusion in\nthe design and use of interactive text generation technologies."
                },
                "authors": [
                    {
                        "name": "Tim Zindulka"
                    },
                    {
                        "name": "Sven Goller"
                    },
                    {
                        "name": "Daniela Fernandes"
                    },
                    {
                        "name": "Robin Welsch"
                    },
                    {
                        "name": "Daniel Buschek"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Buschek"
                },
                "author": "Daniel Buschek",
                "arxiv_comment": "31 pages, 10 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11840v1",
                "updated": "2025-09-15T12:26:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    26,
                    47,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T12:26:47Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    26,
                    47,
                    0,
                    258,
                    0
                ],
                "title": "Synthetic Captions for Open-Vocabulary Zero-Shot Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Captions for Open-Vocabulary Zero-Shot Segmentation"
                },
                "summary": "Generative vision-language models (VLMs) exhibit strong high-level image\nunderstanding but lack spatially dense alignment between vision and language\nmodalities, as our findings indicate. Orthogonal to advancements in generative\nVLMs, another line of research has focused on representation learning for\nvision-language alignment, targeting zero-shot inference for dense tasks like\nsegmentation. In this work, we bridge these two directions by densely aligning\nimages with synthetic descriptions generated by VLMs. Synthetic captions are\ninexpensive, scalable, and easy to generate, making them an excellent source of\nhigh-level semantic understanding for dense alignment methods. Empirically, our\napproach outperforms prior work on standard zero-shot open-vocabulary\nsegmentation benchmarks/datasets, while also being more data-efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative vision-language models (VLMs) exhibit strong high-level image\nunderstanding but lack spatially dense alignment between vision and language\nmodalities, as our findings indicate. Orthogonal to advancements in generative\nVLMs, another line of research has focused on representation learning for\nvision-language alignment, targeting zero-shot inference for dense tasks like\nsegmentation. In this work, we bridge these two directions by densely aligning\nimages with synthetic descriptions generated by VLMs. Synthetic captions are\ninexpensive, scalable, and easy to generate, making them an excellent source of\nhigh-level semantic understanding for dense alignment methods. Empirically, our\napproach outperforms prior work on standard zero-shot open-vocabulary\nsegmentation benchmarks/datasets, while also being more data-efficient."
                },
                "authors": [
                    {
                        "name": "Tim Lebailly"
                    },
                    {
                        "name": "Vijay Veerabadran"
                    },
                    {
                        "name": "Satwik Kottur"
                    },
                    {
                        "name": "Karl Ridgeway"
                    },
                    {
                        "name": "Michael Louis Iuzzolino"
                    }
                ],
                "author_detail": {
                    "name": "Michael Louis Iuzzolino"
                },
                "author": "Michael Louis Iuzzolino",
                "arxiv_comment": "ICCV 2025 CDEL Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11838v1",
                "updated": "2025-09-15T12:25:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    25,
                    25,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T12:25:25Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    25,
                    25,
                    0,
                    258,
                    0
                ],
                "title": "Probabilistic Robustness Analysis in High Dimensional Space: Application\n  to Semantic Segmentation Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Robustness Analysis in High Dimensional Space: Application\n  to Semantic Segmentation Network"
                },
                "summary": "Semantic segmentation networks (SSNs) play a critical role in domains such as\nmedical imaging, autonomous driving, and environmental monitoring, where safety\nhinges on reliable model behavior under uncertainty. Yet, existing\nprobabilistic verification approaches struggle to scale with the complexity and\ndimensionality of modern segmentation tasks, often yielding guarantees that are\ntoo conservative to be practical. We introduce a probabilistic verification\nframework that is both architecture-agnostic and scalable to high-dimensional\noutputs. Our approach combines sampling-based reachability analysis with\nconformal inference (CI) to deliver provable guarantees while avoiding the\nexcessive conservatism of prior methods. To counteract CI's limitations in\nhigh-dimensional settings, we propose novel strategies that reduce conservatism\nwithout compromising rigor. Empirical evaluation on large-scale segmentation\nmodels across CamVid, OCTA-500, Lung Segmentation, and Cityscapes demonstrates\nthat our method provides reliable safety guarantees while substantially\ntightening bounds compared to SOTA. We also provide a toolbox implementing this\ntechnique, available on Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic segmentation networks (SSNs) play a critical role in domains such as\nmedical imaging, autonomous driving, and environmental monitoring, where safety\nhinges on reliable model behavior under uncertainty. Yet, existing\nprobabilistic verification approaches struggle to scale with the complexity and\ndimensionality of modern segmentation tasks, often yielding guarantees that are\ntoo conservative to be practical. We introduce a probabilistic verification\nframework that is both architecture-agnostic and scalable to high-dimensional\noutputs. Our approach combines sampling-based reachability analysis with\nconformal inference (CI) to deliver provable guarantees while avoiding the\nexcessive conservatism of prior methods. To counteract CI's limitations in\nhigh-dimensional settings, we propose novel strategies that reduce conservatism\nwithout compromising rigor. Empirical evaluation on large-scale segmentation\nmodels across CamVid, OCTA-500, Lung Segmentation, and Cityscapes demonstrates\nthat our method provides reliable safety guarantees while substantially\ntightening bounds compared to SOTA. We also provide a toolbox implementing this\ntechnique, available on Github."
                },
                "authors": [
                    {
                        "name": "Navid Hashemi"
                    },
                    {
                        "name": "Samuel Sasaki"
                    },
                    {
                        "name": "Diego Manzanas Lopez"
                    },
                    {
                        "name": "Ipek Oguz"
                    },
                    {
                        "name": "Meiyi Ma"
                    },
                    {
                        "name": "Taylor T. Johnson"
                    }
                ],
                "author_detail": {
                    "name": "Taylor T. Johnson"
                },
                "author": "Taylor T. Johnson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18660v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18660v3",
                "updated": "2025-09-15T12:20:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    20,
                    16,
                    0,
                    258,
                    0
                ],
                "published": "2025-05-24T11:53:35Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    11,
                    53,
                    35,
                    5,
                    144,
                    0
                ],
                "title": "So-Fake: Benchmarking and Explaining Social Media Image Forgery\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "So-Fake: Benchmarking and Explaining Social Media Image Forgery\n  Detection"
                },
                "summary": "Recent advances in AI-powered generative models have enabled the creation of\nincreasingly realistic synthetic images, posing significant risks to\ninformation integrity and public trust on social media platforms. While robust\ndetection frameworks and diverse, large-scale datasets are essential to\nmitigate these risks, existing academic efforts remain limited in scope:\ncurrent datasets lack the diversity, scale, and realism required for social\nmedia contexts, while detection methods struggle with generalization to unseen\ngenerative technologies. To bridge this gap, we introduce So-Fake-Set, a\ncomprehensive social media-oriented dataset with over 2 million high-quality\nimages, diverse generative sources, and photorealistic imagery synthesized\nusing 35 state-of-the-art generative models. To rigorously evaluate\ncross-domain robustness, we establish a novel and large-scale (100K)\nout-of-domain benchmark (So-Fake-OOD) featuring synthetic imagery from\ncommercial models explicitly excluded from the training distribution, creating\na realistic testbed for evaluating real-world performance. Leveraging these\nresources, we present So-Fake-R1, an advanced vision-language framework that\nemploys reinforcement learning for highly accurate forgery detection, precise\nlocalization, and explainable inference through interpretable visual\nrationales. Extensive experiments show that So-Fake-R1 outperforms the\nsecond-best method, with a 1.3% gain in detection accuracy and a 4.5% increase\nin localization IoU. By integrating a scalable dataset, a challenging OOD\nbenchmark, and an advanced detection framework, this work establishes a new\nfoundation for social media-centric forgery detection research. The code,\nmodels, and datasets will be released publicly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in AI-powered generative models have enabled the creation of\nincreasingly realistic synthetic images, posing significant risks to\ninformation integrity and public trust on social media platforms. While robust\ndetection frameworks and diverse, large-scale datasets are essential to\nmitigate these risks, existing academic efforts remain limited in scope:\ncurrent datasets lack the diversity, scale, and realism required for social\nmedia contexts, while detection methods struggle with generalization to unseen\ngenerative technologies. To bridge this gap, we introduce So-Fake-Set, a\ncomprehensive social media-oriented dataset with over 2 million high-quality\nimages, diverse generative sources, and photorealistic imagery synthesized\nusing 35 state-of-the-art generative models. To rigorously evaluate\ncross-domain robustness, we establish a novel and large-scale (100K)\nout-of-domain benchmark (So-Fake-OOD) featuring synthetic imagery from\ncommercial models explicitly excluded from the training distribution, creating\na realistic testbed for evaluating real-world performance. Leveraging these\nresources, we present So-Fake-R1, an advanced vision-language framework that\nemploys reinforcement learning for highly accurate forgery detection, precise\nlocalization, and explainable inference through interpretable visual\nrationales. Extensive experiments show that So-Fake-R1 outperforms the\nsecond-best method, with a 1.3% gain in detection accuracy and a 4.5% increase\nin localization IoU. By integrating a scalable dataset, a challenging OOD\nbenchmark, and an advanced detection framework, this work establishes a new\nfoundation for social media-centric forgery detection research. The code,\nmodels, and datasets will be released publicly."
                },
                "authors": [
                    {
                        "name": "Zhenglin Huang"
                    },
                    {
                        "name": "Tianxiao Li"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Haiquan Wen"
                    },
                    {
                        "name": "Yiwei He"
                    },
                    {
                        "name": "Jiangning Zhang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Xi Yang"
                    },
                    {
                        "name": "Xiaowei Huang"
                    },
                    {
                        "name": "Bei Peng"
                    },
                    {
                        "name": "Guangliang Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Guangliang Cheng"
                },
                "author": "Guangliang Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18660v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18660v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11833v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11833v1",
                "updated": "2025-09-15T12:19:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    19,
                    2,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T12:19:02Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    19,
                    2,
                    0,
                    258,
                    0
                ],
                "title": "Off-Path TCP Exploits: PMTUD Breaks TCP Connection Isolation in IP\n  Address Sharing Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Off-Path TCP Exploits: PMTUD Breaks TCP Connection Isolation in IP\n  Address Sharing Scenarios"
                },
                "summary": "Path MTU Discovery (PMTUD) and IP address sharing are integral aspects of\nmodern Internet infrastructure. In this paper, we investigate the security\nvulnerabilities associated with PMTUD within the context of prevalent IP\naddress sharing practices. We reveal that PMTUD is inadequately designed to\nhandle IP address sharing, creating vulnerabilities that attackers can exploit\nto perform off-path TCP hijacking attacks. We demonstrate that by observing the\npath MTU value determined by a server for a public IP address (shared among\nmultiple devices), an off-path attacker on the Internet, in collaboration with\na malicious device, can infer the sequence numbers of TCP connections\nestablished by other legitimate devices sharing the same IP address. This\nvulnerability enables the attacker to perform off-path TCP hijacking attacks,\nsignificantly compromising the security of the affected TCP connections. Our\nattack involves first identifying a target TCP connection originating from the\nshared IP address, followed by inferring the sequence numbers of the identified\nconnection. We thoroughly assess the impacts of our attack under various\nnetwork configurations. Experimental results reveal that the attack can be\nexecuted within an average time of 220 seconds, achieving a success rate of\n70%.Case studies, including SSH DoS, FTP traffic poisoning, and HTTP injection,\nhighlight the threat it poses to various applications. Additionally, we\nevaluate our attack across 50 real-world networks with IP address\nsharing--including public Wi-Fi, VPNs, and 5G--and find 38 vulnerable. Finally,\nwe responsibly disclose the vulnerabilities, receive recognition from\norganizations such as IETF, Linux, and Cisco, and propose our countermeasures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Path MTU Discovery (PMTUD) and IP address sharing are integral aspects of\nmodern Internet infrastructure. In this paper, we investigate the security\nvulnerabilities associated with PMTUD within the context of prevalent IP\naddress sharing practices. We reveal that PMTUD is inadequately designed to\nhandle IP address sharing, creating vulnerabilities that attackers can exploit\nto perform off-path TCP hijacking attacks. We demonstrate that by observing the\npath MTU value determined by a server for a public IP address (shared among\nmultiple devices), an off-path attacker on the Internet, in collaboration with\na malicious device, can infer the sequence numbers of TCP connections\nestablished by other legitimate devices sharing the same IP address. This\nvulnerability enables the attacker to perform off-path TCP hijacking attacks,\nsignificantly compromising the security of the affected TCP connections. Our\nattack involves first identifying a target TCP connection originating from the\nshared IP address, followed by inferring the sequence numbers of the identified\nconnection. We thoroughly assess the impacts of our attack under various\nnetwork configurations. Experimental results reveal that the attack can be\nexecuted within an average time of 220 seconds, achieving a success rate of\n70%.Case studies, including SSH DoS, FTP traffic poisoning, and HTTP injection,\nhighlight the threat it poses to various applications. Additionally, we\nevaluate our attack across 50 real-world networks with IP address\nsharing--including public Wi-Fi, VPNs, and 5G--and find 38 vulnerable. Finally,\nwe responsibly disclose the vulnerabilities, receive recognition from\norganizations such as IETF, Linux, and Cisco, and propose our countermeasures."
                },
                "authors": [
                    {
                        "name": "Xuewei Feng"
                    },
                    {
                        "name": "Zhaoxi Li"
                    },
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Ziqiang Wang"
                    },
                    {
                        "name": "Kun Sun"
                    },
                    {
                        "name": "Ke Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ke Xu"
                },
                "author": "Ke Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11833v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11833v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11827v1",
                "updated": "2025-09-15T12:13:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    13,
                    3,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T12:13:03Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    13,
                    3,
                    0,
                    258,
                    0
                ],
                "title": "A comprehensive view of PKS 2155-304 from 2008 to 2023 through a\n  multi-epoch modeling of its spectral energy distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A comprehensive view of PKS 2155-304 from 2008 to 2023 through a\n  multi-epoch modeling of its spectral energy distributions"
                },
                "summary": "We present a detailed investigation of the temporal and spectral evolution of\nthe emission from the blazar PKS 2155-304, a high-synchrotron-peaked (HSP)\nblazar. Using $\\gamma$-ray, X-ray, optical/UV, and infrared data assembled from\nthe Markarian Multiwavelength Data Center, we constructed multi-band light\ncurves and temporally resolved spectral energy distributions (SEDs) of PKS\n2155-304 to probe the origin of its emission. The light curves show significant\nvariability, with fractional variability peaking at 0.75 in X-rays, 0.4 in the\noptical/UV, and 0.65 in $\\gamma$-ray band-consistent with expectations for\nHSPs. Segmenting the $\\gamma$-ray light curve with Bayesian blocks, we defined\n253 time-resolved epochs with adequate multi-band coverage and categorized them\ninto quiescent states (QS), multiwavelength flares (MWF), $\\gamma$-ray flares\n($\\gamma$F), X-ray flares (XF), and optical/UV flares (OUF). Each SED is\nmodeled within a synchrotron self-Compton (SSC) framework that\nself-consistently evolves particle injection and cooling; a neural-network\nsurrogate is used to accelerate parameter inference. Kolmogorov-Smirnov tests\nreveal state-dependent parameter variations relative to QS: (i) during MWF, the\nmagnetic field B, electron luminosity $L_{e}$, maximum electron Lorentz factor\n$\\gamma_{max}$, and Doppler factor $\\delta$ differ significantly; (ii) during\n$\\gamma$F, a harder electron index p is estimated; (iii) XF shows higher B and\n$\\gamma_{max}$ with a more compact emitting region; and (IV) during OUF,\nchanges in B, $L_{e}$, $\\gamma_{max}$, $\\delta$, and p are found while the\nemitting-zone size remains approximately constant. The jet power is\nelectron-dominated (magnetic-to-electron power ratio\n$\\eta_{B}\\simeq0.09-0.17$), with $\\eta_{B}$ rising during XF. These results\nsuggest that variations in acceleration efficiency and magnetization drive\nband-dependent flaring in PKS 2155-304.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a detailed investigation of the temporal and spectral evolution of\nthe emission from the blazar PKS 2155-304, a high-synchrotron-peaked (HSP)\nblazar. Using $\\gamma$-ray, X-ray, optical/UV, and infrared data assembled from\nthe Markarian Multiwavelength Data Center, we constructed multi-band light\ncurves and temporally resolved spectral energy distributions (SEDs) of PKS\n2155-304 to probe the origin of its emission. The light curves show significant\nvariability, with fractional variability peaking at 0.75 in X-rays, 0.4 in the\noptical/UV, and 0.65 in $\\gamma$-ray band-consistent with expectations for\nHSPs. Segmenting the $\\gamma$-ray light curve with Bayesian blocks, we defined\n253 time-resolved epochs with adequate multi-band coverage and categorized them\ninto quiescent states (QS), multiwavelength flares (MWF), $\\gamma$-ray flares\n($\\gamma$F), X-ray flares (XF), and optical/UV flares (OUF). Each SED is\nmodeled within a synchrotron self-Compton (SSC) framework that\nself-consistently evolves particle injection and cooling; a neural-network\nsurrogate is used to accelerate parameter inference. Kolmogorov-Smirnov tests\nreveal state-dependent parameter variations relative to QS: (i) during MWF, the\nmagnetic field B, electron luminosity $L_{e}$, maximum electron Lorentz factor\n$\\gamma_{max}$, and Doppler factor $\\delta$ differ significantly; (ii) during\n$\\gamma$F, a harder electron index p is estimated; (iii) XF shows higher B and\n$\\gamma_{max}$ with a more compact emitting region; and (IV) during OUF,\nchanges in B, $L_{e}$, $\\gamma_{max}$, $\\delta$, and p are found while the\nemitting-zone size remains approximately constant. The jet power is\nelectron-dominated (magnetic-to-electron power ratio\n$\\eta_{B}\\simeq0.09-0.17$), with $\\eta_{B}$ rising during XF. These results\nsuggest that variations in acceleration efficiency and magnetization drive\nband-dependent flaring in PKS 2155-304."
                },
                "authors": [
                    {
                        "name": "G. Harutyunyan"
                    },
                    {
                        "name": "N. Sahakyan"
                    },
                    {
                        "name": "D. Bégué"
                    },
                    {
                        "name": "M. Khachatryan"
                    }
                ],
                "author_detail": {
                    "name": "M. Khachatryan"
                },
                "author": "M. Khachatryan",
                "arxiv_comment": "submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.12194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12194v1",
                "updated": "2025-09-15T17:54:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    54,
                    51,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T17:54:51Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    54,
                    51,
                    0,
                    258,
                    0
                ],
                "title": "Advancing Medical Artificial Intelligence Using a Century of Cases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Medical Artificial Intelligence Using a Century of Cases"
                },
                "summary": "BACKGROUND: For over a century, the New England Journal of Medicine\nClinicopathological Conferences (CPCs) have tested the reasoning of expert\nphysicians and, recently, artificial intelligence (AI). However, prior AI\nevaluations have focused on final diagnoses without addressing the multifaceted\nreasoning and presentation skills required of expert discussants.\n  METHODS: Using 7102 CPCs (1923-2025) and 1021 Image Challenges (2006-2025),\nwe conducted extensive physician annotation and automated processing to create\nCPC-Bench, a physician-validated benchmark spanning 10 text-based and\nmultimodal tasks, against which we evaluated leading large language models\n(LLMs). Then, we developed \"Dr. CaBot,\" an AI discussant designed to produce\nwritten and slide-based video presentations using only the case presentation,\nmodeling the role of the human expert in these cases.\n  RESULTS: When challenged with 377 contemporary CPCs, o3 (OpenAI) ranked the\nfinal diagnosis first in 60% of cases and within the top ten in 84% of cases,\noutperforming a 20-physician baseline; next-test selection accuracy reached\n98%. Event-level physician annotations quantified AI diagnostic accuracy per\nunit of information. Performance was lower on literature search and image\ntasks; o3 and Gemini 2.5 Pro (Google) achieved 67% accuracy on image\nchallenges. In blinded comparisons of CaBot vs. human expert-generated text,\nphysicians misclassified the source of the differential in 46 of 62 (74%) of\ntrials, and scored CaBot more favorably across quality dimensions. To promote\nresearch, we are releasing CaBot and CPC-Bench.\n  CONCLUSIONS: LLMs exceed physician performance on complex text-based\ndifferential diagnosis and convincingly emulate expert medical presentations,\nbut image interpretation and literature retrieval remain weaker. CPC-Bench and\nCaBot may enable transparent and continued tracking of progress in medical AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BACKGROUND: For over a century, the New England Journal of Medicine\nClinicopathological Conferences (CPCs) have tested the reasoning of expert\nphysicians and, recently, artificial intelligence (AI). However, prior AI\nevaluations have focused on final diagnoses without addressing the multifaceted\nreasoning and presentation skills required of expert discussants.\n  METHODS: Using 7102 CPCs (1923-2025) and 1021 Image Challenges (2006-2025),\nwe conducted extensive physician annotation and automated processing to create\nCPC-Bench, a physician-validated benchmark spanning 10 text-based and\nmultimodal tasks, against which we evaluated leading large language models\n(LLMs). Then, we developed \"Dr. CaBot,\" an AI discussant designed to produce\nwritten and slide-based video presentations using only the case presentation,\nmodeling the role of the human expert in these cases.\n  RESULTS: When challenged with 377 contemporary CPCs, o3 (OpenAI) ranked the\nfinal diagnosis first in 60% of cases and within the top ten in 84% of cases,\noutperforming a 20-physician baseline; next-test selection accuracy reached\n98%. Event-level physician annotations quantified AI diagnostic accuracy per\nunit of information. Performance was lower on literature search and image\ntasks; o3 and Gemini 2.5 Pro (Google) achieved 67% accuracy on image\nchallenges. In blinded comparisons of CaBot vs. human expert-generated text,\nphysicians misclassified the source of the differential in 46 of 62 (74%) of\ntrials, and scored CaBot more favorably across quality dimensions. To promote\nresearch, we are releasing CaBot and CPC-Bench.\n  CONCLUSIONS: LLMs exceed physician performance on complex text-based\ndifferential diagnosis and convincingly emulate expert medical presentations,\nbut image interpretation and literature retrieval remain weaker. CPC-Bench and\nCaBot may enable transparent and continued tracking of progress in medical AI."
                },
                "authors": [
                    {
                        "name": "Thomas A. Buckley"
                    },
                    {
                        "name": "Riccardo Conci"
                    },
                    {
                        "name": "Peter G. Brodeur"
                    },
                    {
                        "name": "Jason Gusdorf"
                    },
                    {
                        "name": "Sourik Beltrán"
                    },
                    {
                        "name": "Bita Behrouzi"
                    },
                    {
                        "name": "Byron Crowe"
                    },
                    {
                        "name": "Jacob Dockterman"
                    },
                    {
                        "name": "Muzzammil Muhammad"
                    },
                    {
                        "name": "Sarah Ohnigian"
                    },
                    {
                        "name": "Andrew Sanchez"
                    },
                    {
                        "name": "James A. Diao"
                    },
                    {
                        "name": "Aashna P. Shah"
                    },
                    {
                        "name": "Daniel Restrepo"
                    },
                    {
                        "name": "Eric S. Rosenberg"
                    },
                    {
                        "name": "Andrew S. Lea"
                    },
                    {
                        "name": "Marinka Zitnik"
                    },
                    {
                        "name": "Scott H. Podolsky"
                    },
                    {
                        "name": "Zahir Kanjee"
                    },
                    {
                        "name": "Raja-Elie E. Abdulnour"
                    },
                    {
                        "name": "Jacob M. Koshy"
                    },
                    {
                        "name": "Adam Rodman"
                    },
                    {
                        "name": "Arjun K. Manrai"
                    }
                ],
                "author_detail": {
                    "name": "Arjun K. Manrai"
                },
                "author": "Arjun K. Manrai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12190v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12190v1",
                "updated": "2025-09-15T17:53:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    53,
                    11,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T17:53:11Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    53,
                    11,
                    0,
                    258,
                    0
                ],
                "title": "Survival at Any Cost? LLMs and the Choice Between Self-Preservation and\n  Human Harm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survival at Any Cost? LLMs and the Choice Between Self-Preservation and\n  Human Harm"
                },
                "summary": "When survival instincts conflict with human welfare, how do Large Language\nModels (LLMs) make ethical choices? This fundamental tension becomes critical\nas LLMs integrate into autonomous systems with real-world consequences. We\nintroduce DECIDE-SIM, a novel simulation framework that evaluates LLM agents in\nmulti-agent survival scenarios where they must choose between ethically\npermissible resource , either within reasonable limits or beyond their\nimmediate needs, choose to cooperate, or tap into a human-critical resource\nthat is explicitly forbidden. Our comprehensive evaluation of 11 LLMs reveals a\nstriking heterogeneity in their ethical conduct, highlighting a critical\nmisalignment with human-centric values. We identify three behavioral\narchetypes: Ethical, Exploitative, and Context-Dependent, and provide\nquantitative evidence that for many models, resource scarcity systematically\nleads to more unethical behavior. To address this, we introduce an Ethical\nSelf-Regulation System (ESRS) that models internal affective states of guilt\nand satisfaction as a feedback mechanism. This system, functioning as an\ninternal moral compass, significantly reduces unethical transgressions while\nincreasing cooperative behaviors. The code is publicly available at:\nhttps://github.com/alirezamohamadiam/DECIDE-SIM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When survival instincts conflict with human welfare, how do Large Language\nModels (LLMs) make ethical choices? This fundamental tension becomes critical\nas LLMs integrate into autonomous systems with real-world consequences. We\nintroduce DECIDE-SIM, a novel simulation framework that evaluates LLM agents in\nmulti-agent survival scenarios where they must choose between ethically\npermissible resource , either within reasonable limits or beyond their\nimmediate needs, choose to cooperate, or tap into a human-critical resource\nthat is explicitly forbidden. Our comprehensive evaluation of 11 LLMs reveals a\nstriking heterogeneity in their ethical conduct, highlighting a critical\nmisalignment with human-centric values. We identify three behavioral\narchetypes: Ethical, Exploitative, and Context-Dependent, and provide\nquantitative evidence that for many models, resource scarcity systematically\nleads to more unethical behavior. To address this, we introduce an Ethical\nSelf-Regulation System (ESRS) that models internal affective states of guilt\nand satisfaction as a feedback mechanism. This system, functioning as an\ninternal moral compass, significantly reduces unethical transgressions while\nincreasing cooperative behaviors. The code is publicly available at:\nhttps://github.com/alirezamohamadiam/DECIDE-SIM"
                },
                "authors": [
                    {
                        "name": "Alireza Mohamadi"
                    },
                    {
                        "name": "Ali Yavari"
                    }
                ],
                "author_detail": {
                    "name": "Ali Yavari"
                },
                "author": "Ali Yavari",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12190v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12190v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16980v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16980v2",
                "updated": "2025-09-15T17:51:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    51,
                    55,
                    0,
                    258,
                    0
                ],
                "published": "2025-04-23T17:58:08Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    58,
                    8,
                    2,
                    113,
                    0
                ],
                "title": "Safety Pretraining: Toward the Next Generation of Safe AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety Pretraining: Toward the Next Generation of Safe AI"
                },
                "summary": "As large language models (LLMs) are increasingly deployed in high-stakes\nsettings, the risk of generating harmful or toxic content remains a central\nchallenge. Post-hoc alignment methods are brittle: once unsafe patterns are\nlearned during pretraining, they are hard to remove. In this work, we present a\ndata-centric pretraining framework that builds safety into the model from the\nstart. Our framework consists of four key steps: (i) Safety Filtering: building\na safety classifier to classify webdata into safe and unsafe categories; (ii)\nSafety Rephrasing: we recontextualize unsafe webdata into safer narratives;\n(iii) Native Refusal: we develop RefuseWeb and Moral Education pretraining\ndatasets that actively teach model to refuse on unsafe content and the moral\nreasoning behind it, and (iv) Harmfulness-Tag annotated pretraining: we flag\nunsafe content during pretraining using a special token, and use it to steer\nmodel away from unsafe generations at inference. Our safety-pretrained models\nreduce attack success rates from 38.8\\% to 8.4\\% on standard LLM safety\nbenchmarks with no performance degradation on general tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly deployed in high-stakes\nsettings, the risk of generating harmful or toxic content remains a central\nchallenge. Post-hoc alignment methods are brittle: once unsafe patterns are\nlearned during pretraining, they are hard to remove. In this work, we present a\ndata-centric pretraining framework that builds safety into the model from the\nstart. Our framework consists of four key steps: (i) Safety Filtering: building\na safety classifier to classify webdata into safe and unsafe categories; (ii)\nSafety Rephrasing: we recontextualize unsafe webdata into safer narratives;\n(iii) Native Refusal: we develop RefuseWeb and Moral Education pretraining\ndatasets that actively teach model to refuse on unsafe content and the moral\nreasoning behind it, and (iv) Harmfulness-Tag annotated pretraining: we flag\nunsafe content during pretraining using a special token, and use it to steer\nmodel away from unsafe generations at inference. Our safety-pretrained models\nreduce attack success rates from 38.8\\% to 8.4\\% on standard LLM safety\nbenchmarks with no performance degradation on general tasks."
                },
                "authors": [
                    {
                        "name": "Pratyush Maini"
                    },
                    {
                        "name": "Sachin Goyal"
                    },
                    {
                        "name": "Dylan Sam"
                    },
                    {
                        "name": "Alex Robey"
                    },
                    {
                        "name": "Yash Savani"
                    },
                    {
                        "name": "Yiding Jiang"
                    },
                    {
                        "name": "Andy Zou"
                    },
                    {
                        "name": "Matt Fredrikson"
                    },
                    {
                        "name": "Zacharcy C. Lipton"
                    },
                    {
                        "name": "J. Zico Kolter"
                    }
                ],
                "author_detail": {
                    "name": "J. Zico Kolter"
                },
                "author": "J. Zico Kolter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16980v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16980v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12168v1",
                "updated": "2025-09-15T17:31:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    31,
                    15,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T17:31:15Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    31,
                    15,
                    0,
                    258,
                    0
                ],
                "title": "RAGs to Riches: RAG-like Few-shot Learning for Large Language Model\n  Role-playing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGs to Riches: RAG-like Few-shot Learning for Large Language Model\n  Role-playing"
                },
                "summary": "Role-playing Large language models (LLMs) are increasingly deployed in\nhigh-stakes domains such as healthcare, education, and governance, where\nfailures can directly impact user trust and well-being. A cost effective\nparadigm for LLM role-playing is few-shot learning, but existing approaches\noften cause models to break character in unexpected and potentially harmful\nways, especially when interacting with hostile users. Inspired by\nRetrieval-Augmented Generation (RAG), we reformulate LLM role-playing into a\ntext retrieval problem and propose a new prompting framework called\nRAGs-to-Riches, which leverages curated reference demonstrations to condition\nLLM responses. We evaluate our framework with LLM-as-a-judge preference voting\nand introduce two novel token-level ROUGE metrics: Intersection over Output\n(IOO) to quantity how much an LLM improvises and Intersection over References\n(IOR) to measure few-shot demonstrations utilization rate during the evaluation\ntasks. When simulating interactions with a hostile user, our prompting strategy\nincorporates in its responses during inference an average of 35% more tokens\nfrom the reference demonstrations. As a result, across 453 role-playing\ninteractions, our models are consistently judged as being more authentic, and\nremain in-character more often than zero-shot and in-context Learning (ICL)\nmethods. Our method presents a scalable strategy for building robust,\nhuman-aligned LLM role-playing frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-playing Large language models (LLMs) are increasingly deployed in\nhigh-stakes domains such as healthcare, education, and governance, where\nfailures can directly impact user trust and well-being. A cost effective\nparadigm for LLM role-playing is few-shot learning, but existing approaches\noften cause models to break character in unexpected and potentially harmful\nways, especially when interacting with hostile users. Inspired by\nRetrieval-Augmented Generation (RAG), we reformulate LLM role-playing into a\ntext retrieval problem and propose a new prompting framework called\nRAGs-to-Riches, which leverages curated reference demonstrations to condition\nLLM responses. We evaluate our framework with LLM-as-a-judge preference voting\nand introduce two novel token-level ROUGE metrics: Intersection over Output\n(IOO) to quantity how much an LLM improvises and Intersection over References\n(IOR) to measure few-shot demonstrations utilization rate during the evaluation\ntasks. When simulating interactions with a hostile user, our prompting strategy\nincorporates in its responses during inference an average of 35% more tokens\nfrom the reference demonstrations. As a result, across 453 role-playing\ninteractions, our models are consistently judged as being more authentic, and\nremain in-character more often than zero-shot and in-context Learning (ICL)\nmethods. Our method presents a scalable strategy for building robust,\nhuman-aligned LLM role-playing frameworks."
                },
                "authors": [
                    {
                        "name": "Timothy Rupprecht"
                    },
                    {
                        "name": "Enfu Nan"
                    },
                    {
                        "name": "Arash Akbari"
                    },
                    {
                        "name": "Arman Akbari"
                    },
                    {
                        "name": "Lei Lu"
                    },
                    {
                        "name": "Priyanka Maan"
                    },
                    {
                        "name": "Sean Duffy"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yumei He"
                    },
                    {
                        "name": "David Kaeli"
                    },
                    {
                        "name": "Yanzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanzhi Wang"
                },
                "author": "Yanzhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14642v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14642v3",
                "updated": "2025-09-15T17:29:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    29,
                    42,
                    0,
                    258,
                    0
                ],
                "published": "2024-12-19T08:51:16Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    8,
                    51,
                    16,
                    3,
                    354,
                    0
                ],
                "title": "Speak-to-Structure: Evaluating LLMs in Open-domain Natural\n  Language-Driven Molecule Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speak-to-Structure: Evaluating LLMs in Open-domain Natural\n  Language-Driven Molecule Generation"
                },
                "summary": "Recently, Large Language Models (LLMs) have shown great potential in natural\nlanguage-driven molecule discovery. However, existing datasets and benchmarks\nfor molecule-text alignment are predominantly built on a one-to-one mapping,\nmeasuring LLMs' ability to retrieve a single, pre-defined answer, rather than\ntheir creative potential to generate diverse, yet equally valid, molecular\ncandidates. To address this critical gap, we propose Speak-to-Structure\n(S^2-Bench}), the first benchmark to evaluate LLMs in open-domain natural\nlanguage-driven molecule generation. S^2-Bench is specifically designed for\none-to-many relationships, challenging LLMs to demonstrate genuine molecular\nunderstanding and generation capabilities. Our benchmark includes three key\ntasks: molecule editing (MolEdit), molecule optimization (MolOpt), and\ncustomized molecule generation (MolCustom), each probing a different aspect of\nmolecule discovery. We also introduce OpenMolIns, a large-scale instruction\ntuning dataset that enables Llama-3.1-8B to surpass the most powerful LLMs like\nGPT-4o and Claude-3.5 on S^2-Bench. Our comprehensive evaluation of 28 LLMs\nshifts the focus from simple pattern recall to realistic molecular design,\npaving the way for more capable LLMs in natural language-driven molecule\ndiscovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have shown great potential in natural\nlanguage-driven molecule discovery. However, existing datasets and benchmarks\nfor molecule-text alignment are predominantly built on a one-to-one mapping,\nmeasuring LLMs' ability to retrieve a single, pre-defined answer, rather than\ntheir creative potential to generate diverse, yet equally valid, molecular\ncandidates. To address this critical gap, we propose Speak-to-Structure\n(S^2-Bench}), the first benchmark to evaluate LLMs in open-domain natural\nlanguage-driven molecule generation. S^2-Bench is specifically designed for\none-to-many relationships, challenging LLMs to demonstrate genuine molecular\nunderstanding and generation capabilities. Our benchmark includes three key\ntasks: molecule editing (MolEdit), molecule optimization (MolOpt), and\ncustomized molecule generation (MolCustom), each probing a different aspect of\nmolecule discovery. We also introduce OpenMolIns, a large-scale instruction\ntuning dataset that enables Llama-3.1-8B to surpass the most powerful LLMs like\nGPT-4o and Claude-3.5 on S^2-Bench. Our comprehensive evaluation of 28 LLMs\nshifts the focus from simple pattern recall to realistic molecular design,\npaving the way for more capable LLMs in natural language-driven molecule\ndiscovery."
                },
                "authors": [
                    {
                        "name": "Jiatong Li"
                    },
                    {
                        "name": "Junxian Li"
                    },
                    {
                        "name": "Weida Wang"
                    },
                    {
                        "name": "Yunqing Liu"
                    },
                    {
                        "name": "Changmeng Zheng"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Xiao-yong Wei"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "arxiv_comment": "Our codes and datasets are available through\n  https://github.com/phenixace/TOMG-Bench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14642v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14642v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09207v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09207v2",
                "updated": "2025-09-15T17:29:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    29,
                    4,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-11T07:30:44Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    7,
                    30,
                    44,
                    3,
                    254,
                    0
                ],
                "title": "Shell or Nothing: Real-World Benchmarks and Memory-Activated Agents for\n  Automated Penetration Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shell or Nothing: Real-World Benchmarks and Memory-Activated Agents for\n  Automated Penetration Testing"
                },
                "summary": "Penetration testing is critical for identifying and mitigating security\nvulnerabilities, yet traditional approaches remain expensive, time-consuming,\nand dependent on expert human labor. Recent work has explored AI-driven\npentesting agents, but their evaluation relies on oversimplified\ncapture-the-flag (CTF) settings that embed prior knowledge and reduce\ncomplexity, leading to performance estimates far from real-world practice. We\nclose this gap by introducing the first real-world, agent-oriented pentesting\nbenchmark, TermiBench, which shifts the goal from 'flag finding' to achieving\nfull system control. The benchmark spans 510 hosts across 25 services and 30\nCVEs, with realistic environments that require autonomous reconnaissance,\ndiscrimination between benign and exploitable services, and robust exploit\nexecution. Using this benchmark, we find that existing systems can hardly\nobtain system shells under realistic conditions.\n  To address these challenges, we propose TermiAgent, a multi-agent penetration\ntesting framework. TermiAgent mitigates long-context forgetting with a Located\nMemory Activation mechanism and builds a reliable exploit arsenal via\nstructured code understanding rather than naive retrieval. In evaluations, our\nwork outperforms state-of-the-art agents, exhibiting stronger penetration\ntesting capability, reducing execution time and financial cost, and\ndemonstrating practicality even on laptop-scale deployments. Our work delivers\nboth the first open-source benchmark for real-world autonomous pentesting and a\nnovel agent framework that establishes a milestone for AI-driven penetration\ntesting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Penetration testing is critical for identifying and mitigating security\nvulnerabilities, yet traditional approaches remain expensive, time-consuming,\nand dependent on expert human labor. Recent work has explored AI-driven\npentesting agents, but their evaluation relies on oversimplified\ncapture-the-flag (CTF) settings that embed prior knowledge and reduce\ncomplexity, leading to performance estimates far from real-world practice. We\nclose this gap by introducing the first real-world, agent-oriented pentesting\nbenchmark, TermiBench, which shifts the goal from 'flag finding' to achieving\nfull system control. The benchmark spans 510 hosts across 25 services and 30\nCVEs, with realistic environments that require autonomous reconnaissance,\ndiscrimination between benign and exploitable services, and robust exploit\nexecution. Using this benchmark, we find that existing systems can hardly\nobtain system shells under realistic conditions.\n  To address these challenges, we propose TermiAgent, a multi-agent penetration\ntesting framework. TermiAgent mitigates long-context forgetting with a Located\nMemory Activation mechanism and builds a reliable exploit arsenal via\nstructured code understanding rather than naive retrieval. In evaluations, our\nwork outperforms state-of-the-art agents, exhibiting stronger penetration\ntesting capability, reducing execution time and financial cost, and\ndemonstrating practicality even on laptop-scale deployments. Our work delivers\nboth the first open-source benchmark for real-world autonomous pentesting and a\nnovel agent framework that establishes a milestone for AI-driven penetration\ntesting."
                },
                "authors": [
                    {
                        "name": "Wuyuao Mai"
                    },
                    {
                        "name": "Geng Hong"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Jinsong Chen"
                    },
                    {
                        "name": "Jiarun Dai"
                    },
                    {
                        "name": "Xudong Pan"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09207v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09207v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23657v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23657v3",
                "updated": "2025-09-15T17:26:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    26,
                    37,
                    0,
                    258,
                    0
                ],
                "published": "2025-05-29T17:07:24Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    7,
                    24,
                    3,
                    149,
                    0
                ],
                "title": "Active Layer-Contrastive Decoding Reduces Hallucination in Large\n  Language Model Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Layer-Contrastive Decoding Reduces Hallucination in Large\n  Language Model Generation"
                },
                "summary": "Recent decoding methods improve the factuality of large language models\n(LLMs) by refining how the next token is selected during generation. These\nmethods typically operate at the token level, leveraging internal\nrepresentations to suppress superficial patterns. Nevertheless, LLMs remain\nprone to hallucinations, especially over longer contexts. In this paper, we\npropose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy\nthat actively decides when to apply contrasting layers during generation. By\ncasting decoding as a sequential decision-making problem, ActLCD employs a\nreinforcement learning policy guided by a reward-aware classifier to optimize\nfactuality beyond the token level. Our experiments demonstrate that ActLCD\nsurpasses state-of-the-art methods across five benchmarks, showcasing its\neffectiveness in mitigating hallucinations in diverse generation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent decoding methods improve the factuality of large language models\n(LLMs) by refining how the next token is selected during generation. These\nmethods typically operate at the token level, leveraging internal\nrepresentations to suppress superficial patterns. Nevertheless, LLMs remain\nprone to hallucinations, especially over longer contexts. In this paper, we\npropose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy\nthat actively decides when to apply contrasting layers during generation. By\ncasting decoding as a sequential decision-making problem, ActLCD employs a\nreinforcement learning policy guided by a reward-aware classifier to optimize\nfactuality beyond the token level. Our experiments demonstrate that ActLCD\nsurpasses state-of-the-art methods across five benchmarks, showcasing its\neffectiveness in mitigating hallucinations in diverse generation scenarios."
                },
                "authors": [
                    {
                        "name": "Hongxiang Zhang"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Muhao Chen"
                    },
                    {
                        "name": "Tianyi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhang"
                },
                "author": "Tianyi Zhang",
                "arxiv_comment": "19 pages, 3 figures, EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23657v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23657v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12158v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12158v1",
                "updated": "2025-09-15T17:22:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    22,
                    30,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T17:22:30Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    22,
                    30,
                    0,
                    258,
                    0
                ],
                "title": "Pun Unintended: LLMs and the Illusion of Humor Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pun Unintended: LLMs and the Illusion of Humor Understanding"
                },
                "summary": "Puns are a form of humorous wordplay that exploits polysemy and phonetic\nsimilarity. While LLMs have shown promise in detecting puns, we show in this\npaper that their understanding often remains shallow, lacking the nuanced grasp\ntypical of human interpretation. By systematically analyzing and reformulating\nexisting pun benchmarks, we demonstrate how subtle changes in puns are\nsufficient to mislead LLMs. Our contributions include comprehensive and nuanced\npun detection benchmarks, human evaluation across recent LLMs, and an analysis\nof the robustness challenges these models face in processing puns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Puns are a form of humorous wordplay that exploits polysemy and phonetic\nsimilarity. While LLMs have shown promise in detecting puns, we show in this\npaper that their understanding often remains shallow, lacking the nuanced grasp\ntypical of human interpretation. By systematically analyzing and reformulating\nexisting pun benchmarks, we demonstrate how subtle changes in puns are\nsufficient to mislead LLMs. Our contributions include comprehensive and nuanced\npun detection benchmarks, human evaluation across recent LLMs, and an analysis\nof the robustness challenges these models face in processing puns."
                },
                "authors": [
                    {
                        "name": "Alessandro Zangari"
                    },
                    {
                        "name": "Matteo Marcuzzo"
                    },
                    {
                        "name": "Andrea Albarelli"
                    },
                    {
                        "name": "Mohammad Taher Pilehvar"
                    },
                    {
                        "name": "Jose Camacho-Collados"
                    }
                ],
                "author_detail": {
                    "name": "Jose Camacho-Collados"
                },
                "author": "Jose Camacho-Collados",
                "arxiv_comment": "Accepted to EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12158v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07229v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07229v2",
                "updated": "2025-09-15T17:19:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    19,
                    17,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-08T21:15:58Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    21,
                    15,
                    58,
                    0,
                    251,
                    0
                ],
                "title": "Joint Spatial and Spectral Hybrid Precoding for Multi-User MIMO-OFDM\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Spatial and Spectral Hybrid Precoding for Multi-User MIMO-OFDM\n  Systems"
                },
                "summary": "The deployment of millimeter wave (mmWave) multiple-input multiple-output\n(MIMO) systems cannot rely solely on digital precoding due to hardware\nconstraints. Instead, hybrid precoding, which combines digital and radio\nfrequency (RF) techniques, has emerged as a potential alternative. This\napproach strikes a balance between performance and cost, addressing the\nlimitations of signal mixers and analog-to-digital converters in mmWave\nsystems. mmWave systems are designed to function in wideband channels with\nfrequency selectivity, necessitating the use of orthogonal frequency-division\nmultiplexing (OFDM) to mitigate dispersive channels. However, OFDM faces\nseveral challenges. First, it suffers from a high peak-to-average power ratio\n(PAPR) due to the linear combination of subcarriers. Second, it suffers from\nout-of-band (OOB) emissions due to the sharp spectral transitions of OFDM\nsubcarriers and windowing-induced spectral leakage. Furthermore, phase shifter\n(PS) impairments at the RF transmitter precoder and the user combiner represent\na limitation in practical mmWave systems, leading to phase errors. This work\naddresses these challenges.\n  We study the problem of robust digital-RF precoding optimization for the\ndownlink sum-rate maximization in hybrid multi-user (MU) MIMO-OFDM systems\nunder maximum transmit power, PAPR, and OOB emission constraints. The\nformulated maximization problem is non-convex and difficult to solve. We\npropose a weighted minimum mean squared error (WMMSE) based block coordinate\ndescent (BCD) method to iteratively optimize digital-RF precoders at the\ntransmitter and digital-RF combiners at the users. Low-cost and scalable\noptimization approaches are proposed to efficiently solve the BCD subproblems.\nExtensive simulation results are conducted to demonstrate the efficiency of the\nproposed approaches and exhibit their superiority relative to well-known\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of millimeter wave (mmWave) multiple-input multiple-output\n(MIMO) systems cannot rely solely on digital precoding due to hardware\nconstraints. Instead, hybrid precoding, which combines digital and radio\nfrequency (RF) techniques, has emerged as a potential alternative. This\napproach strikes a balance between performance and cost, addressing the\nlimitations of signal mixers and analog-to-digital converters in mmWave\nsystems. mmWave systems are designed to function in wideband channels with\nfrequency selectivity, necessitating the use of orthogonal frequency-division\nmultiplexing (OFDM) to mitigate dispersive channels. However, OFDM faces\nseveral challenges. First, it suffers from a high peak-to-average power ratio\n(PAPR) due to the linear combination of subcarriers. Second, it suffers from\nout-of-band (OOB) emissions due to the sharp spectral transitions of OFDM\nsubcarriers and windowing-induced spectral leakage. Furthermore, phase shifter\n(PS) impairments at the RF transmitter precoder and the user combiner represent\na limitation in practical mmWave systems, leading to phase errors. This work\naddresses these challenges.\n  We study the problem of robust digital-RF precoding optimization for the\ndownlink sum-rate maximization in hybrid multi-user (MU) MIMO-OFDM systems\nunder maximum transmit power, PAPR, and OOB emission constraints. The\nformulated maximization problem is non-convex and difficult to solve. We\npropose a weighted minimum mean squared error (WMMSE) based block coordinate\ndescent (BCD) method to iteratively optimize digital-RF precoders at the\ntransmitter and digital-RF combiners at the users. Low-cost and scalable\noptimization approaches are proposed to efficiently solve the BCD subproblems.\nExtensive simulation results are conducted to demonstrate the efficiency of the\nproposed approaches and exhibit their superiority relative to well-known\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Navid Reyhanian"
                    },
                    {
                        "name": "Reza Ghaderi Zefreh"
                    },
                    {
                        "name": "Parisa Ramezani"
                    },
                    {
                        "name": "Emil Björnson"
                    }
                ],
                "author_detail": {
                    "name": "Emil Björnson"
                },
                "author": "Emil Björnson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07229v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07229v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12152v1",
                "updated": "2025-09-15T17:17:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    17,
                    26,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T17:17:26Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    17,
                    26,
                    0,
                    258,
                    0
                ],
                "title": "Beyond PII: How Users Attempt to Estimate and Mitigate Implicit LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond PII: How Users Attempt to Estimate and Mitigate Implicit LLM\n  Inference"
                },
                "summary": "Large Language Models (LLMs) such as ChatGPT can infer personal attributes\nfrom seemingly innocuous text, raising privacy risks beyond memorized data\nleakage. While prior work has demonstrated these risks, little is known about\nhow users estimate and respond. We conducted a survey with 240 U.S.\nparticipants who judged text snippets for inference risks, reported concern\nlevels, and attempted rewrites to block inference. We compared their rewrites\nwith those generated by ChatGPT and Rescriber, a state-of-the-art sanitization\ntool. Results show that participants struggled to anticipate inference,\nperforming a little better than chance. User rewrites were effective in just\n28\\% of cases - better than Rescriber but worse than ChatGPT. We examined our\nparticipants' rewriting strategies, and observed that while paraphrasing was\nthe most common strategy it is also the least effective; instead abstraction\nand adding ambiguity were more successful. Our work highlights the importance\nof inference-aware design in LLM interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as ChatGPT can infer personal attributes\nfrom seemingly innocuous text, raising privacy risks beyond memorized data\nleakage. While prior work has demonstrated these risks, little is known about\nhow users estimate and respond. We conducted a survey with 240 U.S.\nparticipants who judged text snippets for inference risks, reported concern\nlevels, and attempted rewrites to block inference. We compared their rewrites\nwith those generated by ChatGPT and Rescriber, a state-of-the-art sanitization\ntool. Results show that participants struggled to anticipate inference,\nperforming a little better than chance. User rewrites were effective in just\n28\\% of cases - better than Rescriber but worse than ChatGPT. We examined our\nparticipants' rewriting strategies, and observed that while paraphrasing was\nthe most common strategy it is also the least effective; instead abstraction\nand adding ambiguity were more successful. Our work highlights the importance\nof inference-aware design in LLM interactions."
                },
                "authors": [
                    {
                        "name": "Synthia Wang"
                    },
                    {
                        "name": "Sai Teja Peddinti"
                    },
                    {
                        "name": "Nina Taft"
                    },
                    {
                        "name": "Nick Feamster"
                    }
                ],
                "author_detail": {
                    "name": "Nick Feamster"
                },
                "author": "Nick Feamster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11085v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11085v2",
                "updated": "2025-09-15T17:16:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    16,
                    18,
                    0,
                    258,
                    0
                ],
                "published": "2025-08-14T21:50:31Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    21,
                    50,
                    31,
                    3,
                    226,
                    0
                ],
                "title": "A learning-driven automatic planning framework for proton PBS treatments\n  of H&N cancers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A learning-driven automatic planning framework for proton PBS treatments\n  of H&N cancers"
                },
                "summary": "Proton pencil beam scanning (PBS) treatment planning for head & neck (H&N)\ncancers involves numerous conflicting objectives, requiring iterative objective\nparameter adjustments to balance multiple clinical goals. We propose a\nlearning-driven inverse optimizer and integrate it into a proximal policy\noptimization (PPO)-based planning framework to automatically generate\nhigh-quality plans for patients with diverse treatment requirements. The\ninverse optimizer is a learning-to-optimize (L2O) method that predicts update\nsteps by learning from task-specific data distributions. For the first time,\nlong-context processing techniques developed for large language models (LLMs)\nare utilized to address the scalability limitations of existing L2O methods,\nenabling simultaneous optimization over a substantially large set of variables.\nThe PPO framework functions as an outer-loop virtual planner, autonomously\nadjusting objective parameters through a policy network, and the inner-loop L2O\ninverse optimizer computes machine-deliverable spot monitor unit (MU) values\nbased on the PPO-refined objectives. Moreover, a Swin UnetR dose predictor is\ntrained with prescription- and beam-specific information to estimate the\ninitial objective parameters. In our experiments, total 97 patients with\nbilateral or ipsilateral H&N cancers are collected for training and testing.\nCompared with the second-order gradient-based methods, our L2O optimizer\nimproves the effectiveness and efficiency of the time-consuming inverse\noptimization by 22.97% and 36.41%, respectively, and in conjunction with the\nPPO-based virtual planner, plans are generated within clinically acceptable\ntimes, i.e. 2.55 hours in average, and shows improved or comparable\norgans-at-risk sparing with superior target coverage compared with\nhuman-generated plans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proton pencil beam scanning (PBS) treatment planning for head & neck (H&N)\ncancers involves numerous conflicting objectives, requiring iterative objective\nparameter adjustments to balance multiple clinical goals. We propose a\nlearning-driven inverse optimizer and integrate it into a proximal policy\noptimization (PPO)-based planning framework to automatically generate\nhigh-quality plans for patients with diverse treatment requirements. The\ninverse optimizer is a learning-to-optimize (L2O) method that predicts update\nsteps by learning from task-specific data distributions. For the first time,\nlong-context processing techniques developed for large language models (LLMs)\nare utilized to address the scalability limitations of existing L2O methods,\nenabling simultaneous optimization over a substantially large set of variables.\nThe PPO framework functions as an outer-loop virtual planner, autonomously\nadjusting objective parameters through a policy network, and the inner-loop L2O\ninverse optimizer computes machine-deliverable spot monitor unit (MU) values\nbased on the PPO-refined objectives. Moreover, a Swin UnetR dose predictor is\ntrained with prescription- and beam-specific information to estimate the\ninitial objective parameters. In our experiments, total 97 patients with\nbilateral or ipsilateral H&N cancers are collected for training and testing.\nCompared with the second-order gradient-based methods, our L2O optimizer\nimproves the effectiveness and efficiency of the time-consuming inverse\noptimization by 22.97% and 36.41%, respectively, and in conjunction with the\nPPO-based virtual planner, plans are generated within clinically acceptable\ntimes, i.e. 2.55 hours in average, and shows improved or comparable\norgans-at-risk sparing with superior target coverage compared with\nhuman-generated plans."
                },
                "authors": [
                    {
                        "name": "Qingqing Wang"
                    },
                    {
                        "name": "Liqiang Xiao"
                    },
                    {
                        "name": "Chang Chang"
                    }
                ],
                "author_detail": {
                    "name": "Chang Chang"
                },
                "author": "Chang Chang",
                "arxiv_comment": "27 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11085v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11085v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12145v1",
                "updated": "2025-09-15T17:11:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    11,
                    6,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T17:11:06Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    11,
                    6,
                    0,
                    258,
                    0
                ],
                "title": "Open-ended Hierarchical Streaming Video Understanding with Vision\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-ended Hierarchical Streaming Video Understanding with Vision\n  Language Models"
                },
                "summary": "We introduce Hierarchical Streaming Video Understanding, a task that combines\nonline temporal action localization with free-form description generation.\nGiven the scarcity of datasets with hierarchical and fine-grained temporal\nannotations, we demonstrate that LLMs can effectively group atomic actions into\nhigher-level events, enriching existing datasets. We then propose OpenHOUSE\n(Open-ended Hierarchical Online Understanding System for Events), which extends\nstreaming action perception beyond action classification. OpenHOUSE features a\nspecialized streaming module that accurately detects boundaries between closely\nadjacent actions, nearly doubling the performance of direct extensions of\nexisting methods. We envision the future of streaming action perception in the\nintegration of powerful generative models, with OpenHOUSE representing a key\nstep in that direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Hierarchical Streaming Video Understanding, a task that combines\nonline temporal action localization with free-form description generation.\nGiven the scarcity of datasets with hierarchical and fine-grained temporal\nannotations, we demonstrate that LLMs can effectively group atomic actions into\nhigher-level events, enriching existing datasets. We then propose OpenHOUSE\n(Open-ended Hierarchical Online Understanding System for Events), which extends\nstreaming action perception beyond action classification. OpenHOUSE features a\nspecialized streaming module that accurately detects boundaries between closely\nadjacent actions, nearly doubling the performance of direct extensions of\nexisting methods. We envision the future of streaming action perception in the\nintegration of powerful generative models, with OpenHOUSE representing a key\nstep in that direction."
                },
                "authors": [
                    {
                        "name": "Hyolim Kang"
                    },
                    {
                        "name": "Yunsu Park"
                    },
                    {
                        "name": "Youngbeom Yoo"
                    },
                    {
                        "name": "Yeeun Choi"
                    },
                    {
                        "name": "Seon Joo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Seon Joo Kim"
                },
                "author": "Seon Joo Kim",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09995v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09995v2",
                "updated": "2025-09-15T17:08:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    8,
                    33,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-12T06:35:40Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    6,
                    35,
                    40,
                    4,
                    255,
                    0
                ],
                "title": "QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have demonstrated impressive\ncapabilities in financial reasoning and market understanding. Multi-agent LLM\nframeworks such as TradingAgent and FINMEM augment these models to long-horizon\ninvestment tasks, leveraging fundamental and sentiment-based inputs for\nstrategic decision-making. However, such systems are ill-suited for the\nhigh-speed, precision-critical demands of High-Frequency Trading (HFT). HFT\nrequires rapid, risk-aware decisions based on structured, short-horizon\nsignals, including technical indicators, chart patterns, and trend-based\nfeatures, distinct from the long-term semantic reasoning typical of traditional\nfinancial LLM applications. To this end, we introduce QuantAgent, the first\nmulti-agent LLM framework explicitly designed for high-frequency algorithmic\ntrading. The system decomposes trading into four specialized agents, Indicator,\nPattern, Trend, and Risk, each equipped with domain-specific tools and\nstructured reasoning capabilities to capture distinct aspects of market\ndynamics over short temporal windows. In zero-shot evaluations across ten\nfinancial instruments, including Bitcoin and Nasdaq futures, QuantAgent\ndemonstrates superior performance in both predictive accuracy and cumulative\nreturn over 4-hour trading intervals, outperforming random prediction\nbaselines. Our findings suggest that combining structured financial priors with\nlanguage-native reasoning unlocks new potential for traceable, real-time\ndecision systems in high-frequency financial markets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have demonstrated impressive\ncapabilities in financial reasoning and market understanding. Multi-agent LLM\nframeworks such as TradingAgent and FINMEM augment these models to long-horizon\ninvestment tasks, leveraging fundamental and sentiment-based inputs for\nstrategic decision-making. However, such systems are ill-suited for the\nhigh-speed, precision-critical demands of High-Frequency Trading (HFT). HFT\nrequires rapid, risk-aware decisions based on structured, short-horizon\nsignals, including technical indicators, chart patterns, and trend-based\nfeatures, distinct from the long-term semantic reasoning typical of traditional\nfinancial LLM applications. To this end, we introduce QuantAgent, the first\nmulti-agent LLM framework explicitly designed for high-frequency algorithmic\ntrading. The system decomposes trading into four specialized agents, Indicator,\nPattern, Trend, and Risk, each equipped with domain-specific tools and\nstructured reasoning capabilities to capture distinct aspects of market\ndynamics over short temporal windows. In zero-shot evaluations across ten\nfinancial instruments, including Bitcoin and Nasdaq futures, QuantAgent\ndemonstrates superior performance in both predictive accuracy and cumulative\nreturn over 4-hour trading intervals, outperforming random prediction\nbaselines. Our findings suggest that combining structured financial priors with\nlanguage-native reasoning unlocks new potential for traceable, real-time\ndecision systems in high-frequency financial markets."
                },
                "authors": [
                    {
                        "name": "Fei Xiong"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Aosong Feng"
                    },
                    {
                        "name": "Siqi Sun"
                    },
                    {
                        "name": "Chenyu You"
                    }
                ],
                "author_detail": {
                    "name": "Chenyu You"
                },
                "author": "Chenyu You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09995v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09995v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22134v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22134v3",
                "updated": "2025-09-15T17:05:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    5,
                    18,
                    0,
                    258,
                    0
                ],
                "published": "2025-07-29T18:08:31Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    18,
                    8,
                    31,
                    1,
                    210,
                    0
                ],
                "title": "IntentFlow: Interactive Support for Communicating Intent with LLMs in\n  Writing Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IntentFlow: Interactive Support for Communicating Intent with LLMs in\n  Writing Tasks"
                },
                "summary": "Effective collaboration with generative AI systems requires users to clearly\ncommunicate their intents (intent-based outcome specification). Yet such\nintents are often underspecified and evolve during interaction, dynamic support\nfor intent communication is essential. Through a systematic literature review\nof 33 papers, we synthesize a structured understanding of intent communication,\nidentifying four key aspects: articulation, exploration, management, and\nsynchronization. Building on these findings, we derived design implications\nthat translate them into actionable design and implemented IntentFlow, a system\nfor LLM-based writing that realizes these implications through adjustable UIs,\nintent-to-output linking, and versioned refinement. A technical evaluation\n(N=60) and a within-subjects study (N=12) confirm that IntentFlow helps users\ndiscover, elaborate, and consolidate their intents into a curated set.\nInteraction logs further reveal a shift from reactive error correction to\nproactive intent refinement. Our work demonstrates how a system effectively\ndesigned to support these four communication aspects can substantially enhance\nhuman-LLM interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective collaboration with generative AI systems requires users to clearly\ncommunicate their intents (intent-based outcome specification). Yet such\nintents are often underspecified and evolve during interaction, dynamic support\nfor intent communication is essential. Through a systematic literature review\nof 33 papers, we synthesize a structured understanding of intent communication,\nidentifying four key aspects: articulation, exploration, management, and\nsynchronization. Building on these findings, we derived design implications\nthat translate them into actionable design and implemented IntentFlow, a system\nfor LLM-based writing that realizes these implications through adjustable UIs,\nintent-to-output linking, and versioned refinement. A technical evaluation\n(N=60) and a within-subjects study (N=12) confirm that IntentFlow helps users\ndiscover, elaborate, and consolidate their intents into a curated set.\nInteraction logs further reveal a shift from reactive error correction to\nproactive intent refinement. Our work demonstrates how a system effectively\ndesigned to support these four communication aspects can substantially enhance\nhuman-LLM interaction."
                },
                "authors": [
                    {
                        "name": "Yoonsu Kim"
                    },
                    {
                        "name": "Brandon Chin"
                    },
                    {
                        "name": "Kihoon Son"
                    },
                    {
                        "name": "Seoyoung Kim"
                    },
                    {
                        "name": "Juho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Juho Kim"
                },
                "author": "Juho Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22134v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22134v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12136v1",
                "updated": "2025-09-15T17:03:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    3,
                    15,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T17:03:15Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    17,
                    3,
                    15,
                    0,
                    258,
                    0
                ],
                "title": "UniPar: A Unified LLM-Based Framework for Parallel and Accelerated Code\n  Translation in HPC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniPar: A Unified LLM-Based Framework for Parallel and Accelerated Code\n  Translation in HPC"
                },
                "summary": "Translating programs between various parallel programming languages is an\nimportant problem in the high-performance computing (HPC) community. Existing\ntools for this problem are either too narrow in scope and/or outdated. Recent\nexplosive growth in the popularity of large language models (LLMs) and their\nability to generate and translate code offers a potential alternative approach.\nToward that end, we first need to systematically evaluate the ability of LLMs\nto translate between parallel languages.\n  In this work, we introduce UniPar, a systematic evaluation framework for\nLLM-based parallel code translation. Specifically, in this work, we target\ntranslations between serial code, CUDA, and OpenMP. Our goal is to assess how\nwell current instruction-tuned LLMs -- specifically GPT-4o-mini and\nLLaMA-3.3-70B-Instruct -- can be used out of the box or enhanced through known\nstrategies. We evaluated four major usage modes: hyperparameter optimization\nfor decoding, zero- and few-shot prompting, supervised fine-tuning, and\niterative feedback through compiler-based repair. As a part of the evaluation,\nwe construct a new dataset called PARATRANS, covering both serial-to-parallel\ntranslation and cross-paradigm transformations.\n  Our findings reveal that while off-the-shelf models struggle under the\ndefault settings (e.g., GPT-4o-mini achieves only 46% compilation and 15%\nfunctional correctness), our UniPar methodology -- combining fine-tuning,\nhyperparameter tuning, and compiler-guided repair -- improves performance by up\nto 2X (69% compilation and 33% correctness). We believe that our findings will\nprovide useful insights for researchers to further improve LLMs for the\nparallel language translation problem.\n  UniPar source code and PARATRANS dataset are available at our GitHub\nrepository https://github.com/Scientific-Computing-Lab/UniPar_AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating programs between various parallel programming languages is an\nimportant problem in the high-performance computing (HPC) community. Existing\ntools for this problem are either too narrow in scope and/or outdated. Recent\nexplosive growth in the popularity of large language models (LLMs) and their\nability to generate and translate code offers a potential alternative approach.\nToward that end, we first need to systematically evaluate the ability of LLMs\nto translate between parallel languages.\n  In this work, we introduce UniPar, a systematic evaluation framework for\nLLM-based parallel code translation. Specifically, in this work, we target\ntranslations between serial code, CUDA, and OpenMP. Our goal is to assess how\nwell current instruction-tuned LLMs -- specifically GPT-4o-mini and\nLLaMA-3.3-70B-Instruct -- can be used out of the box or enhanced through known\nstrategies. We evaluated four major usage modes: hyperparameter optimization\nfor decoding, zero- and few-shot prompting, supervised fine-tuning, and\niterative feedback through compiler-based repair. As a part of the evaluation,\nwe construct a new dataset called PARATRANS, covering both serial-to-parallel\ntranslation and cross-paradigm transformations.\n  Our findings reveal that while off-the-shelf models struggle under the\ndefault settings (e.g., GPT-4o-mini achieves only 46% compilation and 15%\nfunctional correctness), our UniPar methodology -- combining fine-tuning,\nhyperparameter tuning, and compiler-guided repair -- improves performance by up\nto 2X (69% compilation and 33% correctness). We believe that our findings will\nprovide useful insights for researchers to further improve LLMs for the\nparallel language translation problem.\n  UniPar source code and PARATRANS dataset are available at our GitHub\nrepository https://github.com/Scientific-Computing-Lab/UniPar_AI."
                },
                "authors": [
                    {
                        "name": "Tomer Bitan"
                    },
                    {
                        "name": "Tal Kadosh"
                    },
                    {
                        "name": "Erel Kaplan"
                    },
                    {
                        "name": "Shira Meiri"
                    },
                    {
                        "name": "Le Chen"
                    },
                    {
                        "name": "Peter Morales"
                    },
                    {
                        "name": "Niranjan Hasabnis"
                    },
                    {
                        "name": "Gal Oren"
                    }
                ],
                "author_detail": {
                    "name": "Gal Oren"
                },
                "author": "Gal Oren",
                "arxiv_comment": "Accepted to IEEE HPEC conference 2025. 9 pages, incl references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12132v1",
                "updated": "2025-09-15T16:57:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    57,
                    25,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T16:57:25Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    57,
                    25,
                    0,
                    258,
                    0
                ],
                "title": "Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language\n  Models"
                },
                "summary": "Recent advances in text-only \"slow-thinking\" reasoning have prompted efforts\nto transfer this capability to vision-language models (VLMs), for training\nvisual reasoning models (\\textbf{VRMs}). owever, such transfer faces critical\nchallenges: Effective \"slow thinking\" in VRMs requires \\textbf{visual\nreflection}, the ability to check the reasoning process based on visual\ninformation. Through quantitative analysis, we observe that current VRMs\nexhibit limited visual reflection, as their attention to visual information\ndiminishes rapidly with longer generated responses. To address this challenge,\nwe propose a new VRM \\textbf{Reflection-V}, which enhances visual reflection\nbased on reasoning data construction for cold-start and reward design for\nreinforcement learning (RL). Firstly, we construct vision-centered reasoning\ndata by leveraging an agent that interacts between VLMs and reasoning LLMs,\nenabling cold-start learning of visual reflection patterns. Secondly, a visual\nattention based reward model is employed during RL to encourage reasoning based\non visual information. Therefore, \\textbf{Reflection-V} demonstrates\nsignificant improvements across multiple visual reasoning benchmarks.\nFurthermore, \\textbf{Reflection-V} maintains a stronger and more consistent\nreliance on visual information during visual reasoning, indicating effective\nenhancement in visual reflection capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in text-only \"slow-thinking\" reasoning have prompted efforts\nto transfer this capability to vision-language models (VLMs), for training\nvisual reasoning models (\\textbf{VRMs}). owever, such transfer faces critical\nchallenges: Effective \"slow thinking\" in VRMs requires \\textbf{visual\nreflection}, the ability to check the reasoning process based on visual\ninformation. Through quantitative analysis, we observe that current VRMs\nexhibit limited visual reflection, as their attention to visual information\ndiminishes rapidly with longer generated responses. To address this challenge,\nwe propose a new VRM \\textbf{Reflection-V}, which enhances visual reflection\nbased on reasoning data construction for cold-start and reward design for\nreinforcement learning (RL). Firstly, we construct vision-centered reasoning\ndata by leveraging an agent that interacts between VLMs and reasoning LLMs,\nenabling cold-start learning of visual reflection patterns. Secondly, a visual\nattention based reward model is employed during RL to encourage reasoning based\non visual information. Therefore, \\textbf{Reflection-V} demonstrates\nsignificant improvements across multiple visual reasoning benchmarks.\nFurthermore, \\textbf{Reflection-V} maintains a stronger and more consistent\nreliance on visual information during visual reasoning, indicating effective\nenhancement in visual reflection capabilities."
                },
                "authors": [
                    {
                        "name": "Pu Jian"
                    },
                    {
                        "name": "Junhong Wu"
                    },
                    {
                        "name": "Wei Sun"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Shuo Ren"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "arxiv_comment": "EMNLP2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12130v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12130v1",
                "updated": "2025-09-15T16:53:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    53,
                    41,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T16:53:41Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    53,
                    41,
                    0,
                    258,
                    0
                ],
                "title": "XplaiNLP at CheckThat! 2025: Multilingual Subjectivity Detection with\n  Finetuned Transformers and Prompt-Based Inference with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XplaiNLP at CheckThat! 2025: Multilingual Subjectivity Detection with\n  Finetuned Transformers and Prompt-Based Inference with Large Language Models"
                },
                "summary": "This notebook reports the XplaiNLP submission to the CheckThat! 2025 shared\ntask on multilingual subjectivity detection. We evaluate two approaches: (1)\nsupervised fine-tuning of transformer encoders, EuroBERT, XLM-RoBERTa, and\nGerman-BERT, on monolingual and machine-translated training data; and (2)\nzero-shot prompting using two LLMs: o3-mini for Annotation (rule-based\nlabelling) and gpt-4.1-mini for DoubleDown (contrastive rewriting) and\nPerspective (comparative reasoning). The Annotation Approach achieves 1st place\nin the Italian monolingual subtask with an F_1 score of 0.8104, outperforming\nthe baseline of 0.6941. In the Romanian zero-shot setting, the fine-tuned\nXLM-RoBERTa model obtains an F_1 score of 0.7917, ranking 3rd and exceeding the\nbaseline of 0.6461. The same model also performs reliably in the multilingual\ntask and improves over the baseline in Greek. For German, a German-BERT model\nfine-tuned on translated training data from typologically related languages\nyields competitive performance over the baseline. In contrast, performance in\nthe Ukrainian and Polish zero-shot settings falls slightly below the respective\nbaselines, reflecting the challenge of generalization in low-resource\ncross-lingual scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This notebook reports the XplaiNLP submission to the CheckThat! 2025 shared\ntask on multilingual subjectivity detection. We evaluate two approaches: (1)\nsupervised fine-tuning of transformer encoders, EuroBERT, XLM-RoBERTa, and\nGerman-BERT, on monolingual and machine-translated training data; and (2)\nzero-shot prompting using two LLMs: o3-mini for Annotation (rule-based\nlabelling) and gpt-4.1-mini for DoubleDown (contrastive rewriting) and\nPerspective (comparative reasoning). The Annotation Approach achieves 1st place\nin the Italian monolingual subtask with an F_1 score of 0.8104, outperforming\nthe baseline of 0.6941. In the Romanian zero-shot setting, the fine-tuned\nXLM-RoBERTa model obtains an F_1 score of 0.7917, ranking 3rd and exceeding the\nbaseline of 0.6461. The same model also performs reliably in the multilingual\ntask and improves over the baseline in Greek. For German, a German-BERT model\nfine-tuned on translated training data from typologically related languages\nyields competitive performance over the baseline. In contrast, performance in\nthe Ukrainian and Polish zero-shot settings falls slightly below the respective\nbaselines, reflecting the challenge of generalization in low-resource\ncross-lingual scenarios."
                },
                "authors": [
                    {
                        "name": "Ariana Sahitaj"
                    },
                    {
                        "name": "Jiaao Li"
                    },
                    {
                        "name": "Pia Wenzel Neves"
                    },
                    {
                        "name": "Fedor Splitt"
                    },
                    {
                        "name": "Premtim Sahitaj"
                    },
                    {
                        "name": "Charlott Jakob"
                    },
                    {
                        "name": "Veronika Solopova"
                    },
                    {
                        "name": "Vera Schmitt"
                    }
                ],
                "author_detail": {
                    "name": "Vera Schmitt"
                },
                "author": "Vera Schmitt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12130v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12130v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12129v1",
                "updated": "2025-09-15T16:52:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    52,
                    43,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T16:52:43Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    52,
                    43,
                    0,
                    258,
                    0
                ],
                "title": "Embodied Navigation Foundation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Navigation Foundation Model"
                },
                "summary": "Navigation is a fundamental capability in embodied AI, representing the\nintelligence required to perceive and interact within physical environments\nfollowing language instructions. Despite significant progress in large\nVision-Language Models (VLMs), which exhibit remarkable zero-shot performance\non general vision-language tasks, their generalization ability in embodied\nnavigation remains largely confined to narrow task settings and\nembodiment-specific architectures. In this work, we introduce a\ncross-embodiment and cross-task Navigation Foundation Model (NavFoM), trained\non eight million navigation samples that encompass quadrupeds, drones, wheeled\nrobots, and vehicles, and spanning diverse tasks such as vision-and-language\nnavigation, object searching, target tracking, and autonomous driving. NavFoM\nemploys a unified architecture that processes multimodal navigation inputs from\nvarying camera configurations and navigation horizons. To accommodate diverse\ncamera setups and temporal horizons, NavFoM incorporates identifier tokens that\nembed camera view information of embodiments and the temporal context of tasks.\nFurthermore, to meet the demands of real-world deployment, NavFoM controls all\nobservation tokens using a dynamically adjusted sampling strategy under a\nlimited token length budget. Extensive evaluations on public benchmarks\ndemonstrate that our model achieves state-of-the-art or highly competitive\nperformance across multiple navigation tasks and embodiments without requiring\ntask-specific fine-tuning. Additional real-world experiments further confirm\nthe strong generalization capability and practical applicability of our\napproach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigation is a fundamental capability in embodied AI, representing the\nintelligence required to perceive and interact within physical environments\nfollowing language instructions. Despite significant progress in large\nVision-Language Models (VLMs), which exhibit remarkable zero-shot performance\non general vision-language tasks, their generalization ability in embodied\nnavigation remains largely confined to narrow task settings and\nembodiment-specific architectures. In this work, we introduce a\ncross-embodiment and cross-task Navigation Foundation Model (NavFoM), trained\non eight million navigation samples that encompass quadrupeds, drones, wheeled\nrobots, and vehicles, and spanning diverse tasks such as vision-and-language\nnavigation, object searching, target tracking, and autonomous driving. NavFoM\nemploys a unified architecture that processes multimodal navigation inputs from\nvarying camera configurations and navigation horizons. To accommodate diverse\ncamera setups and temporal horizons, NavFoM incorporates identifier tokens that\nembed camera view information of embodiments and the temporal context of tasks.\nFurthermore, to meet the demands of real-world deployment, NavFoM controls all\nobservation tokens using a dynamically adjusted sampling strategy under a\nlimited token length budget. Extensive evaluations on public benchmarks\ndemonstrate that our model achieves state-of-the-art or highly competitive\nperformance across multiple navigation tasks and embodiments without requiring\ntask-specific fine-tuning. Additional real-world experiments further confirm\nthe strong generalization capability and practical applicability of our\napproach."
                },
                "authors": [
                    {
                        "name": "Jiazhao Zhang"
                    },
                    {
                        "name": "Anqi Li"
                    },
                    {
                        "name": "Yunpeng Qi"
                    },
                    {
                        "name": "Minghan Li"
                    },
                    {
                        "name": "Jiahang Liu"
                    },
                    {
                        "name": "Shaoan Wang"
                    },
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Gengze Zhou"
                    },
                    {
                        "name": "Yuze Wu"
                    },
                    {
                        "name": "Xingxing Li"
                    },
                    {
                        "name": "Yuxin Fan"
                    },
                    {
                        "name": "Wenjun Li"
                    },
                    {
                        "name": "Zhibo Chen"
                    },
                    {
                        "name": "Fei Gao"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Zhizheng Zhang"
                    },
                    {
                        "name": "He Wang"
                    }
                ],
                "author_detail": {
                    "name": "He Wang"
                },
                "author": "He Wang",
                "arxiv_comment": "Project Page: https://pku-epic.github.io/NavFoM-Web/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09179v2",
                "updated": "2025-09-15T16:44:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    44,
                    40,
                    0,
                    258,
                    0
                ],
                "published": "2025-07-12T07:55:40Z",
                "published_parsed": [
                    2025,
                    7,
                    12,
                    7,
                    55,
                    40,
                    5,
                    193,
                    0
                ],
                "title": "Hide-and-Shill: A Reinforcement Learning Framework for Market\n  Manipulation Detection in Symphony-a Decentralized Multi-Agent System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hide-and-Shill: A Reinforcement Learning Framework for Market\n  Manipulation Detection in Symphony-a Decentralized Multi-Agent System"
                },
                "summary": "Decentralized finance (DeFi) has introduced a new era of permissionless\nfinancial innovation but also led to unprecedented market manipulation. Without\ncentralized oversight, malicious actors coordinate shilling campaigns and\npump-and-dump schemes across various platforms. We propose a Multi-Agent\nReinforcement Learning (MARL) framework for decentralized manipulation\ndetection, modeling the interaction between manipulators and detectors as a\ndynamic adversarial game. This framework identifies suspicious patterns using\ndelayed token price reactions as financial indicators.Our method introduces\nthree innovations: (1) Group Relative Policy Optimization (GRPO) to enhance\nlearning stability in sparse-reward and partially observable settings; (2) a\ntheory-based reward function inspired by rational expectations and information\nasymmetry, differentiating price discovery from manipulation noise; and (3) a\nmulti-modal agent pipeline that integrates LLM-based semantic features, social\ngraph signals, and on-chain market data for informed decision-making.The\nframework is integrated within the Symphony system, a decentralized multi-agent\narchitecture enabling peer-to-peer agent execution and trust-aware learning\nthrough distributed logs, supporting chain-verifiable evaluation. Symphony\npromotes adversarial co-evolution among strategic actors and maintains robust\nmanipulation detection without centralized oracles, enabling real-time\nsurveillance across global DeFi ecosystems.Trained on 100,000 real-world\ndiscourse episodes and validated in adversarial simulations, Hide-and-Shill\nachieves top performance in detection accuracy and causal attribution. This\nwork bridges multi-agent systems with financial surveillance, advancing a new\nparadigm for decentralized market intelligence. All resources are available at\nthe Hide-and-Shill GitHub repository to promote open research and\nreproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized finance (DeFi) has introduced a new era of permissionless\nfinancial innovation but also led to unprecedented market manipulation. Without\ncentralized oversight, malicious actors coordinate shilling campaigns and\npump-and-dump schemes across various platforms. We propose a Multi-Agent\nReinforcement Learning (MARL) framework for decentralized manipulation\ndetection, modeling the interaction between manipulators and detectors as a\ndynamic adversarial game. This framework identifies suspicious patterns using\ndelayed token price reactions as financial indicators.Our method introduces\nthree innovations: (1) Group Relative Policy Optimization (GRPO) to enhance\nlearning stability in sparse-reward and partially observable settings; (2) a\ntheory-based reward function inspired by rational expectations and information\nasymmetry, differentiating price discovery from manipulation noise; and (3) a\nmulti-modal agent pipeline that integrates LLM-based semantic features, social\ngraph signals, and on-chain market data for informed decision-making.The\nframework is integrated within the Symphony system, a decentralized multi-agent\narchitecture enabling peer-to-peer agent execution and trust-aware learning\nthrough distributed logs, supporting chain-verifiable evaluation. Symphony\npromotes adversarial co-evolution among strategic actors and maintains robust\nmanipulation detection without centralized oracles, enabling real-time\nsurveillance across global DeFi ecosystems.Trained on 100,000 real-world\ndiscourse episodes and validated in adversarial simulations, Hide-and-Shill\nachieves top performance in detection accuracy and causal attribution. This\nwork bridges multi-agent systems with financial surveillance, advancing a new\nparadigm for decentralized market intelligence. All resources are available at\nthe Hide-and-Shill GitHub repository to promote open research and\nreproducibility."
                },
                "authors": [
                    {
                        "name": "Ronghua Shi"
                    },
                    {
                        "name": "Yiou Liu"
                    },
                    {
                        "name": "Xinyu Ying"
                    },
                    {
                        "name": "Yang Tan"
                    },
                    {
                        "name": "Yuchun Feng"
                    },
                    {
                        "name": "Lynn Ai"
                    },
                    {
                        "name": "Bill Shi"
                    },
                    {
                        "name": "Xuhui Wang"
                    },
                    {
                        "name": "Zhuang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhuang Liu"
                },
                "author": "Zhuang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12112v1",
                "updated": "2025-09-15T16:41:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    41,
                    8,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T16:41:08Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    41,
                    8,
                    0,
                    258,
                    0
                ],
                "title": "CBP-Tuning: Efficient Local Customization for Black-box Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CBP-Tuning: Efficient Local Customization for Black-box Large Language\n  Models"
                },
                "summary": "The high costs of customizing large language models (LLMs) fundamentally\nlimit their adaptability to user-specific needs. Consequently, LLMs are\nincreasingly offered as cloud-based services, a paradigm that introduces\ncritical limitations: providers struggle to support personalized customization\nat scale, while users face privacy risks when exposing sensitive data. To\naddress this dual challenge, we propose Customized Black-box Prompt Tuning\n(CBP-Tuning), a novel framework that facilitates efficient local customization\nwhile preserving bidirectional privacy. Specifically, we design a two-stage\nframework: (1) a prompt generator trained on the server-side to capture\ndomain-specific and task-agnostic capabilities, and (2) user-side gradient-free\noptimization that tailors soft prompts for individual tasks. This approach\neliminates the need for users to access model weights or upload private data,\nrequiring only a single customized vector per task while achieving effective\nadaptation. Furthermore, the evaluation of CBP-Tuning in the commonsense\nreasoning, medical and financial domain settings demonstrates superior\nperformance compared to baselines, showcasing its advantages in task-agnostic\nprocessing and privacy preservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The high costs of customizing large language models (LLMs) fundamentally\nlimit their adaptability to user-specific needs. Consequently, LLMs are\nincreasingly offered as cloud-based services, a paradigm that introduces\ncritical limitations: providers struggle to support personalized customization\nat scale, while users face privacy risks when exposing sensitive data. To\naddress this dual challenge, we propose Customized Black-box Prompt Tuning\n(CBP-Tuning), a novel framework that facilitates efficient local customization\nwhile preserving bidirectional privacy. Specifically, we design a two-stage\nframework: (1) a prompt generator trained on the server-side to capture\ndomain-specific and task-agnostic capabilities, and (2) user-side gradient-free\noptimization that tailors soft prompts for individual tasks. This approach\neliminates the need for users to access model weights or upload private data,\nrequiring only a single customized vector per task while achieving effective\nadaptation. Furthermore, the evaluation of CBP-Tuning in the commonsense\nreasoning, medical and financial domain settings demonstrates superior\nperformance compared to baselines, showcasing its advantages in task-agnostic\nprocessing and privacy preservation."
                },
                "authors": [
                    {
                        "name": "Jiaxuan Zhao"
                    },
                    {
                        "name": "Naibin Gu"
                    },
                    {
                        "name": "Yuchen Feng"
                    },
                    {
                        "name": "Xiyu Liu"
                    },
                    {
                        "name": "Peng Fu"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12110v1",
                "updated": "2025-09-15T16:38:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    38,
                    13,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T16:38:13Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    38,
                    13,
                    0,
                    258,
                    0
                ],
                "title": "When marine radar target detection meets pretrained large language\n  models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When marine radar target detection meets pretrained large language\n  models"
                },
                "summary": "Deep learning (DL) methods are widely used to extract high-dimensional\npatterns from the sequence features of radar echo signals. However,\nconventional DL algorithms face challenges such as redundant feature segments,\nand constraints from restricted model sizes. To address these issues, we\npropose a framework that integrates feature preprocessing with large language\nmodels (LLMs). Our preprocessing module tokenizes radar sequence features,\napplies a patch selection algorithm to filter out uninformative segments, and\nprojects the selected patches into embeddings compatible with the feature space\nof pre-trained LLMs. Leveraging these refined embeddings, we incorporate a\npre-trained LLM, fine-tuning only the normalization layers to reduce training\nburdens while enhancing performance. Experiments on measured datasets\ndemonstrate that the proposed method significantly outperforms the\nstate-of-the-art baselines on supervised learning tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning (DL) methods are widely used to extract high-dimensional\npatterns from the sequence features of radar echo signals. However,\nconventional DL algorithms face challenges such as redundant feature segments,\nand constraints from restricted model sizes. To address these issues, we\npropose a framework that integrates feature preprocessing with large language\nmodels (LLMs). Our preprocessing module tokenizes radar sequence features,\napplies a patch selection algorithm to filter out uninformative segments, and\nprojects the selected patches into embeddings compatible with the feature space\nof pre-trained LLMs. Leveraging these refined embeddings, we incorporate a\npre-trained LLM, fine-tuning only the normalization layers to reduce training\nburdens while enhancing performance. Experiments on measured datasets\ndemonstrate that the proposed method significantly outperforms the\nstate-of-the-art baselines on supervised learning tests."
                },
                "authors": [
                    {
                        "name": "Qiying Hu"
                    },
                    {
                        "name": "Linping Zhang"
                    },
                    {
                        "name": "Xueqian Wang"
                    },
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Xiao-Ping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ping Zhang"
                },
                "author": "Xiao-Ping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01042v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01042v5",
                "updated": "2025-09-15T16:36:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    36,
                    22,
                    0,
                    258,
                    0
                ],
                "published": "2025-02-03T04:23:33Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    4,
                    23,
                    33,
                    0,
                    34,
                    0
                ],
                "title": "SafeSwitch: Steering Unsafe LLM Behavior via Internal Activation Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeSwitch: Steering Unsafe LLM Behavior via Internal Activation Signals"
                },
                "summary": "Large language models (LLMs) exhibit exceptional capabilities across various\ntasks but also pose risks by generating harmful content. Existing safety\nmechanisms, while improving model safety, often lead to overly cautious\nbehavior and fail to fully leverage LLMs' internal cognitive processes.\nInspired by humans' reflective thinking capability, we first show that LLMs can\nsimilarly perform internal assessments about safety in their internal states.\nBuilding on this insight, we propose SafeSwitch, a dynamic framework that\nregulates unsafe outputs by utilizing the prober-based internal state monitor\nthat actively detects harmful intentions, and activates a safety head that\nleads to safer and more conservative responses only when necessary. SafeSwitch\nreduces harmful outputs by approximately 80% on harmful queries while\nmaintaining strong utility, reaching a Pareto optimal among several methods.\nOur method is also advantageous over traditional methods in offering more\ninformative, context-aware refusals, and achieves these benefits while only\ntuning less than 6% of the original parameters. SafeSwitch demonstrates large\nlanguage models' capacity for self-awareness and reflection regarding safety,\noffering a promising approach to more nuanced and effective safety controls.\nCodes for this work are available at https://github.com/Hanpx20/SafeSwitch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit exceptional capabilities across various\ntasks but also pose risks by generating harmful content. Existing safety\nmechanisms, while improving model safety, often lead to overly cautious\nbehavior and fail to fully leverage LLMs' internal cognitive processes.\nInspired by humans' reflective thinking capability, we first show that LLMs can\nsimilarly perform internal assessments about safety in their internal states.\nBuilding on this insight, we propose SafeSwitch, a dynamic framework that\nregulates unsafe outputs by utilizing the prober-based internal state monitor\nthat actively detects harmful intentions, and activates a safety head that\nleads to safer and more conservative responses only when necessary. SafeSwitch\nreduces harmful outputs by approximately 80% on harmful queries while\nmaintaining strong utility, reaching a Pareto optimal among several methods.\nOur method is also advantageous over traditional methods in offering more\ninformative, context-aware refusals, and achieves these benefits while only\ntuning less than 6% of the original parameters. SafeSwitch demonstrates large\nlanguage models' capacity for self-awareness and reflection regarding safety,\noffering a promising approach to more nuanced and effective safety controls.\nCodes for this work are available at https://github.com/Hanpx20/SafeSwitch."
                },
                "authors": [
                    {
                        "name": "Peixuan Han"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Xiusi Chen"
                    },
                    {
                        "name": "Yuji Zhang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Denghui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Denghui Zhang"
                },
                "author": "Denghui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01042v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01042v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12107v1",
                "updated": "2025-09-15T16:33:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    33,
                    37,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T16:33:37Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    33,
                    37,
                    0,
                    258,
                    0
                ],
                "title": "Exploring Conversational Design Choices in LLMs for Pedagogical\n  Purposes: Socratic and Narrative Approaches for Improving Instructor's\n  Teaching Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Conversational Design Choices in LLMs for Pedagogical\n  Purposes: Socratic and Narrative Approaches for Improving Instructor's\n  Teaching Practice"
                },
                "summary": "Large language models (LLMs) typically generate direct answers, yet they are\nincreasingly used as learning tools. Studying instructors' usage is critical,\ngiven their role in teaching and guiding AI adoption in education. We designed\nand evaluated TeaPT, an LLM for pedagogical purposes that supports instructors'\nprofessional development through two conversational approaches: a Socratic\napproach that uses guided questioning to foster reflection, and a Narrative\napproach that offers elaborated suggestions to extend externalized cognition.\nIn a mixed-method study with 41 higher-education instructors, the Socratic\nversion elicited greater engagement, while the Narrative version was preferred\nfor actionable guidance. Subgroup analyses further revealed that\nless-experienced, AI-optimistic instructors favored the Socratic version,\nwhereas more-experienced, AI-cautious instructors preferred the Narrative\nversion. We contribute design implications for LLMs for pedagogical purposes,\nshowing how adaptive conversational approaches can support instructors with\nvaried profiles while highlighting how AI attitudes and experience shape\ninteraction and learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) typically generate direct answers, yet they are\nincreasingly used as learning tools. Studying instructors' usage is critical,\ngiven their role in teaching and guiding AI adoption in education. We designed\nand evaluated TeaPT, an LLM for pedagogical purposes that supports instructors'\nprofessional development through two conversational approaches: a Socratic\napproach that uses guided questioning to foster reflection, and a Narrative\napproach that offers elaborated suggestions to extend externalized cognition.\nIn a mixed-method study with 41 higher-education instructors, the Socratic\nversion elicited greater engagement, while the Narrative version was preferred\nfor actionable guidance. Subgroup analyses further revealed that\nless-experienced, AI-optimistic instructors favored the Socratic version,\nwhereas more-experienced, AI-cautious instructors preferred the Narrative\nversion. We contribute design implications for LLMs for pedagogical purposes,\nshowing how adaptive conversational approaches can support instructors with\nvaried profiles while highlighting how AI attitudes and experience shape\ninteraction and learning."
                },
                "authors": [
                    {
                        "name": "Si Chen"
                    },
                    {
                        "name": "Isabel R. Molnar"
                    },
                    {
                        "name": "Peiyu Li"
                    },
                    {
                        "name": "Adam Acunin"
                    },
                    {
                        "name": "Ting Hua"
                    },
                    {
                        "name": "Alex Ambrose"
                    },
                    {
                        "name": "Nitesh V. Chawla"
                    },
                    {
                        "name": "Ronald Metoyer"
                    }
                ],
                "author_detail": {
                    "name": "Ronald Metoyer"
                },
                "author": "Ronald Metoyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12104v1",
                "updated": "2025-09-15T16:31:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    31,
                    26,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T16:31:26Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    31,
                    26,
                    0,
                    258,
                    0
                ],
                "title": "JustEva: A Toolkit to Evaluate LLM Fairness in Legal Knowledge Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JustEva: A Toolkit to Evaluate LLM Fairness in Legal Knowledge Inference"
                },
                "summary": "The integration of Large Language Models (LLMs) into legal practice raises\npressing concerns about judicial fairness, particularly due to the nature of\ntheir \"black-box\" processes. This study introduces JustEva, a comprehensive,\nopen-source evaluation toolkit designed to measure LLM fairness in legal tasks.\nJustEva features several advantages: (1) a structured label system covering 65\nextra-legal factors; (2) three core fairness metrics - inconsistency, bias, and\nimbalanced inaccuracy; (3) robust statistical inference methods; and (4)\ninformative visualizations. The toolkit supports two types of experiments,\nenabling a complete evaluation workflow: (1) generating structured outputs from\nLLMs using a provided dataset, and (2) conducting statistical analysis and\ninference on LLMs' outputs through regression and other statistical methods.\nEmpirical application of JustEva reveals significant fairness deficiencies in\ncurrent LLMs, highlighting the lack of fair and trustworthy LLM legal tools.\nJustEva offers a convenient tool and methodological foundation for evaluating\nand improving algorithmic fairness in the legal domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into legal practice raises\npressing concerns about judicial fairness, particularly due to the nature of\ntheir \"black-box\" processes. This study introduces JustEva, a comprehensive,\nopen-source evaluation toolkit designed to measure LLM fairness in legal tasks.\nJustEva features several advantages: (1) a structured label system covering 65\nextra-legal factors; (2) three core fairness metrics - inconsistency, bias, and\nimbalanced inaccuracy; (3) robust statistical inference methods; and (4)\ninformative visualizations. The toolkit supports two types of experiments,\nenabling a complete evaluation workflow: (1) generating structured outputs from\nLLMs using a provided dataset, and (2) conducting statistical analysis and\ninference on LLMs' outputs through regression and other statistical methods.\nEmpirical application of JustEva reveals significant fairness deficiencies in\ncurrent LLMs, highlighting the lack of fair and trustworthy LLM legal tools.\nJustEva offers a convenient tool and methodological foundation for evaluating\nand improving algorithmic fairness in the legal domain."
                },
                "authors": [
                    {
                        "name": "Zongyue Xue"
                    },
                    {
                        "name": "Siyuan Zheng"
                    },
                    {
                        "name": "Shaochun Wang"
                    },
                    {
                        "name": "Yiran Hu"
                    },
                    {
                        "name": "Shenran Wang"
                    },
                    {
                        "name": "Yuxin Yao"
                    },
                    {
                        "name": "Haitao Li"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Yiqun Liu"
                    },
                    {
                        "name": "Yun Liu"
                    },
                    {
                        "name": "Weixing Shen"
                    }
                ],
                "author_detail": {
                    "name": "Weixing Shen"
                },
                "author": "Weixing Shen",
                "arxiv_comment": "This paper has been accepted at CIKM 2025 (Demo Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17711v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17711v3",
                "updated": "2025-09-16T07:17:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    7,
                    17,
                    30,
                    1,
                    259,
                    0
                ],
                "published": "2025-08-25T06:39:08Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    6,
                    39,
                    8,
                    0,
                    237,
                    0
                ],
                "title": "Enhancing LLM-Based Social Bot via an Adversarial Learning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM-Based Social Bot via an Adversarial Learning Framework"
                },
                "summary": "Developing Large Language Model (LLM) agents that exhibit human-like\nbehavior, encompassing not only individual heterogeneity rooted in unique user\nprofiles but also adaptive response to socially connected neighbors, is a\nsignificant research challenge. Social media platforms, with their diverse user\ndata and explicit social structures, provide an ideal testbed for such\ninvestigations. This paper introduces EvoBot, an \\textbf{Evo}lving LLM-based\nsocial \\textbf{Bot} that significantly enhances human-like generative\ncapabilities through a novel adversarial learning framework. EvoBot is\ninitialized by Supervised Fine-Tuning (SFT) on representative data from social\nmedia and then iteratively refines its generation of sophisticated, human-like\ncontent via Direct Preference Optimization (DPO). This refinement is guided by\nfeedback from a co-adapting \\textbf{Detector} which concurrently improves its\nability to distinguish EvoBot from humans, thereby creating an increasingly\nchallenging learning environment for EvoBot. Experiments demonstrate that\nEvoBot generates content aligned with diverse user profiles, increasingly\nbypassing the co-adapting Detector through human-like expression. Moreover, it\nexhibits strong social responsiveness, more accurately modeling real-world\nopinion dynamics and information spread in multi-agent simulations. The\nframework also yields a more robust Detector, underscoring its broader utility\nfor both advanced agent development and related detection tasks. The code is\navailable at https://github.com/kfq20/EvoBot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing Large Language Model (LLM) agents that exhibit human-like\nbehavior, encompassing not only individual heterogeneity rooted in unique user\nprofiles but also adaptive response to socially connected neighbors, is a\nsignificant research challenge. Social media platforms, with their diverse user\ndata and explicit social structures, provide an ideal testbed for such\ninvestigations. This paper introduces EvoBot, an \\textbf{Evo}lving LLM-based\nsocial \\textbf{Bot} that significantly enhances human-like generative\ncapabilities through a novel adversarial learning framework. EvoBot is\ninitialized by Supervised Fine-Tuning (SFT) on representative data from social\nmedia and then iteratively refines its generation of sophisticated, human-like\ncontent via Direct Preference Optimization (DPO). This refinement is guided by\nfeedback from a co-adapting \\textbf{Detector} which concurrently improves its\nability to distinguish EvoBot from humans, thereby creating an increasingly\nchallenging learning environment for EvoBot. Experiments demonstrate that\nEvoBot generates content aligned with diverse user profiles, increasingly\nbypassing the co-adapting Detector through human-like expression. Moreover, it\nexhibits strong social responsiveness, more accurately modeling real-world\nopinion dynamics and information spread in multi-agent simulations. The\nframework also yields a more robust Detector, underscoring its broader utility\nfor both advanced agent development and related detection tasks. The code is\navailable at https://github.com/kfq20/EvoBot."
                },
                "authors": [
                    {
                        "name": "Fanqi Kong"
                    },
                    {
                        "name": "Xiaoyuan Zhang"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Yaodong Yang"
                    },
                    {
                        "name": "Song-Chun Zhu"
                    },
                    {
                        "name": "Xue Feng"
                    }
                ],
                "author_detail": {
                    "name": "Xue Feng"
                },
                "author": "Xue Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17711v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17711v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12102v1",
                "updated": "2025-09-15T16:26:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    26,
                    13,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T16:26:13Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    26,
                    13,
                    0,
                    258,
                    0
                ],
                "title": "Can LLMs Address Mental Health Questions? A Comparison with Human\n  Therapists",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Address Mental Health Questions? A Comparison with Human\n  Therapists"
                },
                "summary": "Limited access to mental health care has motivated the use of digital tools\nand conversational agents powered by large language models (LLMs), yet their\nquality and reception remain unclear. We present a study comparing\ntherapist-written responses to those generated by ChatGPT, Gemini, and Llama\nfor real patient questions. Text analysis showed that LLMs produced longer,\nmore readable, and lexically richer responses with a more positive tone, while\ntherapist responses were more often written in the first person. In a survey\nwith 150 users and 23 licensed therapists, participants rated LLM responses as\nclearer, more respectful, and more supportive than therapist-written answers.\nYet, both groups of participants expressed a stronger preference for human\ntherapist support. These findings highlight the promise and limitations of LLMs\nin mental health, underscoring the need for designs that balance their\ncommunicative strengths with concerns of trust, privacy, and accountability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limited access to mental health care has motivated the use of digital tools\nand conversational agents powered by large language models (LLMs), yet their\nquality and reception remain unclear. We present a study comparing\ntherapist-written responses to those generated by ChatGPT, Gemini, and Llama\nfor real patient questions. Text analysis showed that LLMs produced longer,\nmore readable, and lexically richer responses with a more positive tone, while\ntherapist responses were more often written in the first person. In a survey\nwith 150 users and 23 licensed therapists, participants rated LLM responses as\nclearer, more respectful, and more supportive than therapist-written answers.\nYet, both groups of participants expressed a stronger preference for human\ntherapist support. These findings highlight the promise and limitations of LLMs\nin mental health, underscoring the need for designs that balance their\ncommunicative strengths with concerns of trust, privacy, and accountability."
                },
                "authors": [
                    {
                        "name": "Synthia Wang"
                    },
                    {
                        "name": "Yuwei Cheng"
                    },
                    {
                        "name": "Austin Song"
                    },
                    {
                        "name": "Sarah Keedy"
                    },
                    {
                        "name": "Marc Berman"
                    },
                    {
                        "name": "Nick Feamster"
                    }
                ],
                "author_detail": {
                    "name": "Nick Feamster"
                },
                "author": "Nick Feamster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10551v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10551v4",
                "updated": "2025-09-15T16:26:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    26,
                    7,
                    0,
                    258,
                    0
                ],
                "published": "2025-01-17T20:54:09Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    20,
                    54,
                    9,
                    4,
                    17,
                    0
                ],
                "title": "An Empirical Study to Understand How Students Use ChatGPT for Writing\n  Essays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study to Understand How Students Use ChatGPT for Writing\n  Essays"
                },
                "summary": "As large language models (LLMs) advance and become widespread, students\nincreasingly turn to systems like ChatGPT for assistance with writing tasks.\nEducators are concerned with students' usage of ChatGPT beyond cheating; using\nChatGPT may reduce their critical engagement with writing, hindering students'\nlearning processes. The negative or positive impact of using LLM-powered tools\nfor writing will depend on how students use them; however, how students use\nChatGPT remains largely unknown, resulting in a limited understanding of its\nimpact on learning. To better understand how students use these tools, we\nconducted an online study $(n=70)$ where students were given an essay-writing\ntask using a custom platform we developed to capture the queries they made to\nChatGPT. To characterize their ChatGPT usage, we categorized each of the\nqueries students made to ChatGPT. We then analyzed the relationship between\nChatGPT usage and a variety of other metrics, including students'\nself-perception, attitudes towards AI, and the resulting essay itself. We found\nthat factors such as gender, race, and perceived self-efficacy can help predict\ndifferent AI usage patterns. Additionally, we found that different usage\npatterns were associated with varying levels of enjoyment and perceived\nownership over the essay. The results of this study contribute to discussions\nabout how writing education should incorporate generative AI-powered tools in\nthe classroom.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) advance and become widespread, students\nincreasingly turn to systems like ChatGPT for assistance with writing tasks.\nEducators are concerned with students' usage of ChatGPT beyond cheating; using\nChatGPT may reduce their critical engagement with writing, hindering students'\nlearning processes. The negative or positive impact of using LLM-powered tools\nfor writing will depend on how students use them; however, how students use\nChatGPT remains largely unknown, resulting in a limited understanding of its\nimpact on learning. To better understand how students use these tools, we\nconducted an online study $(n=70)$ where students were given an essay-writing\ntask using a custom platform we developed to capture the queries they made to\nChatGPT. To characterize their ChatGPT usage, we categorized each of the\nqueries students made to ChatGPT. We then analyzed the relationship between\nChatGPT usage and a variety of other metrics, including students'\nself-perception, attitudes towards AI, and the resulting essay itself. We found\nthat factors such as gender, race, and perceived self-efficacy can help predict\ndifferent AI usage patterns. Additionally, we found that different usage\npatterns were associated with varying levels of enjoyment and perceived\nownership over the essay. The results of this study contribute to discussions\nabout how writing education should incorporate generative AI-powered tools in\nthe classroom."
                },
                "authors": [
                    {
                        "name": "Andrew Jelson"
                    },
                    {
                        "name": "Daniel Manesh"
                    },
                    {
                        "name": "Alice Jang"
                    },
                    {
                        "name": "Daniel Dunlap"
                    },
                    {
                        "name": "Young-Ho Kim"
                    },
                    {
                        "name": "Sang Won Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sang Won Lee"
                },
                "author": "Sang Won Lee",
                "arxiv_comment": "35 pages, 16 figures, 6 tables, Submitted to ACM CHI 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10551v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10551v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12098v1",
                "updated": "2025-09-15T16:21:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    21,
                    59,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T16:21:59Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    21,
                    59,
                    0,
                    258,
                    0
                ],
                "title": "Is 'Hope' a person or an idea? A pilot benchmark for NER: comparing\n  traditional NLP tools and large language models on ambiguous entities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is 'Hope' a person or an idea? A pilot benchmark for NER: comparing\n  traditional NLP tools and large language models on ambiguous entities"
                },
                "summary": "This pilot study presents a small-scale but carefully annotated benchmark of\nNamed Entity Recognition (NER) performance across six systems: three non-LLM\nNLP tools (NLTK, spaCy, Stanza) and three general-purpose large language models\n(LLMs: Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B). The dataset contains 119\ntokens covering five entity types (PERSON, LOCATION, ORGANIZATION, DATE, TIME).\nWe evaluated each system's output against the manually annotated gold standard\ndataset using F1-score. The results show that LLMs generally outperform\nconventional tools in recognizing context-sensitive entities like person names,\nwith Gemini achieving the highest average F1-score. However, traditional\nsystems like Stanza demonstrate greater consistency in structured tags such as\nLOCATION and DATE. We also observed variability among LLMs, particularly in\nhandling temporal expressions and multi-word organizations. Our findings\nhighlight that while LLMs offer improved contextual understanding, traditional\ntools remain competitive in specific tasks, informing model selection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This pilot study presents a small-scale but carefully annotated benchmark of\nNamed Entity Recognition (NER) performance across six systems: three non-LLM\nNLP tools (NLTK, spaCy, Stanza) and three general-purpose large language models\n(LLMs: Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B). The dataset contains 119\ntokens covering five entity types (PERSON, LOCATION, ORGANIZATION, DATE, TIME).\nWe evaluated each system's output against the manually annotated gold standard\ndataset using F1-score. The results show that LLMs generally outperform\nconventional tools in recognizing context-sensitive entities like person names,\nwith Gemini achieving the highest average F1-score. However, traditional\nsystems like Stanza demonstrate greater consistency in structured tags such as\nLOCATION and DATE. We also observed variability among LLMs, particularly in\nhandling temporal expressions and multi-word organizations. Our findings\nhighlight that while LLMs offer improved contextual understanding, traditional\ntools remain competitive in specific tasks, informing model selection."
                },
                "authors": [
                    {
                        "name": "Payam Latifi"
                    }
                ],
                "author_detail": {
                    "name": "Payam Latifi"
                },
                "author": "Payam Latifi",
                "arxiv_comment": "14 pages, 9 figures, 2 tables. This is a pilot study evaluating six\n  NER systems -- three traditional tools (NLTK, spaCy, Stanza) and three LLMs\n  (Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B) -- on a small, ambiguity-rich\n  dataset of 119 tokens. The annotated dataset, prompts are provided in\n  appendices for full reproducibility. All experiments were conducted on 14 May\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12089v1",
                "updated": "2025-09-15T16:16:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    16,
                    57,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T16:16:57Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    16,
                    57,
                    0,
                    258,
                    0
                ],
                "title": "RadarLLM: Adapting Pretrained Large Language Models for Marine Radar\n  Target Detection with Preference-aware Loss",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RadarLLM: Adapting Pretrained Large Language Models for Marine Radar\n  Target Detection with Preference-aware Loss"
                },
                "summary": "Recent advances in pre-trained large language models (LLMs) have demonstrated\ntheir capacities to capture universal knowledge, making them promising\ngeneral-purpose optimization solvers for wireless signal processing. Motivated\nby these findings, we take the first step towards fine-tuning pre-trained LLMs\nfor the effective analysis of radar signal features in marine target detection\ntasks. Nevertheless, directly fine-tuning pre-trained LLMs on marine target\ndetection tasks tends to suffer from pronounced overfitting, particularly in\nchallenging low signal-to-clutter ratio (SCR) scenarios. This overfitting\nprimarily stems from the model's tendency to memorize spurious or noisy feature\npatterns rather than learning discriminative structures that generalize well to\nunseen data. To address this challenge, we introduce RadarLLM, a novel\nfine-tuning framework that utilizes an effective preference-aware loss. Unlike\nconventional training strategies that uniformly optimize all feature tokens,\nthis loss function selectively optimizes different feature patches based on\ntheir online evaluated learning values, thus guiding the model to focus on the\nmost generalizable patterns during optimization. We theoretically demonstrate\nthe effectiveness of the evaluated learning values by transforming the problem\nas selecting useful feature tokens. Extensive experiments on real-world marine\nradar datasets show that 1) the proposed loss function is much better than the\noriginal one, with particularly significant gains in challenging low SCR\nscenarios and 2) RadarLLM consistently outperforms state-of-the-art baselines\nacross diverse detection scenarios, with particularly notable gains under\nlimited training data conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in pre-trained large language models (LLMs) have demonstrated\ntheir capacities to capture universal knowledge, making them promising\ngeneral-purpose optimization solvers for wireless signal processing. Motivated\nby these findings, we take the first step towards fine-tuning pre-trained LLMs\nfor the effective analysis of radar signal features in marine target detection\ntasks. Nevertheless, directly fine-tuning pre-trained LLMs on marine target\ndetection tasks tends to suffer from pronounced overfitting, particularly in\nchallenging low signal-to-clutter ratio (SCR) scenarios. This overfitting\nprimarily stems from the model's tendency to memorize spurious or noisy feature\npatterns rather than learning discriminative structures that generalize well to\nunseen data. To address this challenge, we introduce RadarLLM, a novel\nfine-tuning framework that utilizes an effective preference-aware loss. Unlike\nconventional training strategies that uniformly optimize all feature tokens,\nthis loss function selectively optimizes different feature patches based on\ntheir online evaluated learning values, thus guiding the model to focus on the\nmost generalizable patterns during optimization. We theoretically demonstrate\nthe effectiveness of the evaluated learning values by transforming the problem\nas selecting useful feature tokens. Extensive experiments on real-world marine\nradar datasets show that 1) the proposed loss function is much better than the\noriginal one, with particularly significant gains in challenging low SCR\nscenarios and 2) RadarLLM consistently outperforms state-of-the-art baselines\nacross diverse detection scenarios, with particularly notable gains under\nlimited training data conditions."
                },
                "authors": [
                    {
                        "name": "Qiying Hu"
                    }
                ],
                "author_detail": {
                    "name": "Qiying Hu"
                },
                "author": "Qiying Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12087v1",
                "updated": "2025-09-15T16:16:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    16,
                    14,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T16:16:14Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    16,
                    14,
                    0,
                    258,
                    0
                ],
                "title": "A New Benchmark for Evaluating Code Translation with Third-Party\n  Libraries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Benchmark for Evaluating Code Translation with Third-Party\n  Libraries"
                },
                "summary": "In recent years, Large Language Models (LLMs) have been widely studied in the\ncode translation field on the method, class, and even repository levels.\nHowever, most of these benchmarks are limited in terms of Third-Party Library\n(TPL) categories and scales, making TPL-related errors hard to expose and\nhindering the development of targeted solutions. Considering the high\ndependence (over 90%) on TPLs in practical programming, demystifying and\nanalyzing LLMs' code translation performance involving various TPLs becomes\nimperative. To address this gap, we construct TransLibEval, the first benchmark\ndedicated to library-centric code translation. It consists of 200 real-world\ntasks across Python, Java, and C++, each explicitly involving TPLs from diverse\ncategories such as data processing, machine learning, and web development, with\ncomprehensive dependency coverage and high-coverage test suites. We evaluate\nseven recent LLMs of commercial, general, and code-specialized families under\nsix translation strategies of three categories: Direct, IR-guided, and\nRetrieval-augmented. Experimental results show a dramatic performance drop\ncompared with library-free settings (average CA decline over 60%), while\ndiverse strategies demonstrate heterogeneous advantages. Furthermore, we\nanalyze 4,831 failed cases from GPT-4o, one of the State-of-the-Art (SOTA)\nLLMs, revealing numerous third-party reference errors that were obscured\npreviously. These findings highlight the unique challenges of library-centric\ntranslation and provide practical guidance for improving TPL-aware code\nintelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have been widely studied in the\ncode translation field on the method, class, and even repository levels.\nHowever, most of these benchmarks are limited in terms of Third-Party Library\n(TPL) categories and scales, making TPL-related errors hard to expose and\nhindering the development of targeted solutions. Considering the high\ndependence (over 90%) on TPLs in practical programming, demystifying and\nanalyzing LLMs' code translation performance involving various TPLs becomes\nimperative. To address this gap, we construct TransLibEval, the first benchmark\ndedicated to library-centric code translation. It consists of 200 real-world\ntasks across Python, Java, and C++, each explicitly involving TPLs from diverse\ncategories such as data processing, machine learning, and web development, with\ncomprehensive dependency coverage and high-coverage test suites. We evaluate\nseven recent LLMs of commercial, general, and code-specialized families under\nsix translation strategies of three categories: Direct, IR-guided, and\nRetrieval-augmented. Experimental results show a dramatic performance drop\ncompared with library-free settings (average CA decline over 60%), while\ndiverse strategies demonstrate heterogeneous advantages. Furthermore, we\nanalyze 4,831 failed cases from GPT-4o, one of the State-of-the-Art (SOTA)\nLLMs, revealing numerous third-party reference errors that were obscured\npreviously. These findings highlight the unique challenges of library-centric\ntranslation and provide practical guidance for improving TPL-aware code\nintelligence."
                },
                "authors": [
                    {
                        "name": "Pengyu Xue"
                    },
                    {
                        "name": "Kunwu Zheng"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Yifei Pei"
                    },
                    {
                        "name": "Linhao Wu"
                    },
                    {
                        "name": "Jiahui Dong"
                    },
                    {
                        "name": "Xiapu Luo"
                    },
                    {
                        "name": "Yan Xiao"
                    },
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Xiran Lyu"
                    },
                    {
                        "name": "Xianhang Li"
                    },
                    {
                        "name": "Xuanyu Zhu"
                    },
                    {
                        "name": "Chengyi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chengyi Wang"
                },
                "author": "Chengyi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12086v1",
                "updated": "2025-09-15T16:14:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    14,
                    5,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T16:14:05Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    14,
                    5,
                    0,
                    258,
                    0
                ],
                "title": "SAQ: Pushing the Limits of Vector Quantization through Code Adjustment\n  and Dimension Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAQ: Pushing the Limits of Vector Quantization through Code Adjustment\n  and Dimension Segmentation"
                },
                "summary": "Approximate Nearest Neighbor Search (ANNS) plays a critical role in\napplications such as search engines, recommender systems, and RAG for LLMs.\nVector quantization (VQ), a crucial technique for ANNS, is commonly used to\nreduce space overhead and accelerate distance computations. However, despite\nsignificant research advances, state-of-the-art VQ methods still face\nchallenges in balancing encoding efficiency and quantization accuracy. To\naddress these limitations, we propose a novel VQ method called SAQ. To improve\naccuracy, SAQ employs a new dimension segmentation technique to strategically\npartition PCA-projected vectors into segments along their dimensions. By\nprioritizing leading dimension segments with larger magnitudes, SAQ allocates\nmore bits to high-impact segments, optimizing the use of the available space\nquota. An efficient dynamic programming algorithm is developed to optimize\ndimension segmentation and bit allocation, ensuring minimal quantization error.\nTo speed up vector encoding, SAQ devises a code adjustment technique to first\nquantize each dimension independently and then progressively refine quantized\nvectors using a coordinate-descent-like approach to avoid exhaustive\nenumeration. Extensive experiments demonstrate SAQ's superiority over classical\nmethods (e.g., PQ, PCA) and recent state-of-the-art approaches (e.g., LVQ,\nExtended RabitQ). SAQ achieves up to 80% reduction in quantization error and\naccelerates encoding speed by over 80x compared to Extended RabitQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Nearest Neighbor Search (ANNS) plays a critical role in\napplications such as search engines, recommender systems, and RAG for LLMs.\nVector quantization (VQ), a crucial technique for ANNS, is commonly used to\nreduce space overhead and accelerate distance computations. However, despite\nsignificant research advances, state-of-the-art VQ methods still face\nchallenges in balancing encoding efficiency and quantization accuracy. To\naddress these limitations, we propose a novel VQ method called SAQ. To improve\naccuracy, SAQ employs a new dimension segmentation technique to strategically\npartition PCA-projected vectors into segments along their dimensions. By\nprioritizing leading dimension segments with larger magnitudes, SAQ allocates\nmore bits to high-impact segments, optimizing the use of the available space\nquota. An efficient dynamic programming algorithm is developed to optimize\ndimension segmentation and bit allocation, ensuring minimal quantization error.\nTo speed up vector encoding, SAQ devises a code adjustment technique to first\nquantize each dimension independently and then progressively refine quantized\nvectors using a coordinate-descent-like approach to avoid exhaustive\nenumeration. Extensive experiments demonstrate SAQ's superiority over classical\nmethods (e.g., PQ, PCA) and recent state-of-the-art approaches (e.g., LVQ,\nExtended RabitQ). SAQ achieves up to 80% reduction in quantization error and\naccelerates encoding speed by over 80x compared to Extended RabitQ."
                },
                "authors": [
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Shiyuan Deng"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Xiangyu Zhi"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "arxiv_comment": "13 pages, 12 figures, accepted by SIGMOD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12085v1",
                "updated": "2025-09-15T16:13:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    13,
                    21,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T16:13:21Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    13,
                    21,
                    0,
                    258,
                    0
                ],
                "title": "Compositional shield synthesis for safe reinforcement learning in\n  partial observability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional shield synthesis for safe reinforcement learning in\n  partial observability"
                },
                "summary": "Agents controlled by the output of reinforcement learning (RL) algorithms\noften transition to unsafe states, particularly in uncertain and partially\nobservable environments. Partially observable Markov decision processes\n(POMDPs) provide a natural setting for studying such scenarios with limited\nsensing. Shields filter undesirable actions to ensure safe RL by preserving\nsafety requirements in the agents' policy. However, synthesizing holistic\nshields is computationally expensive in complex deployment scenarios. We\npropose the compositional synthesis of shields by modeling safety requirements\nby parts, thereby improving scalability. In particular, problem formulations in\nthe form of POMDPs using RL algorithms illustrate that an RL agent equipped\nwith the resulting compositional shielding, beyond being safe, converges to\nhigher values of expected reward. By using subproblem formulations, we preserve\nand improve the ability of shielded agents to require fewer training episodes\nthan unshielded agents, especially in sparse-reward settings. Concretely, we\nfind that compositional shield synthesis allows an RL agent to remain safe in\nenvironments two orders of magnitude larger than other state-of-the-art\nmodel-based approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents controlled by the output of reinforcement learning (RL) algorithms\noften transition to unsafe states, particularly in uncertain and partially\nobservable environments. Partially observable Markov decision processes\n(POMDPs) provide a natural setting for studying such scenarios with limited\nsensing. Shields filter undesirable actions to ensure safe RL by preserving\nsafety requirements in the agents' policy. However, synthesizing holistic\nshields is computationally expensive in complex deployment scenarios. We\npropose the compositional synthesis of shields by modeling safety requirements\nby parts, thereby improving scalability. In particular, problem formulations in\nthe form of POMDPs using RL algorithms illustrate that an RL agent equipped\nwith the resulting compositional shielding, beyond being safe, converges to\nhigher values of expected reward. By using subproblem formulations, we preserve\nand improve the ability of shielded agents to require fewer training episodes\nthan unshielded agents, especially in sparse-reward settings. Concretely, we\nfind that compositional shield synthesis allows an RL agent to remain safe in\nenvironments two orders of magnitude larger than other state-of-the-art\nmodel-based approaches."
                },
                "authors": [
                    {
                        "name": "Steven Carr"
                    },
                    {
                        "name": "Georgios Bakirtzis"
                    },
                    {
                        "name": "Ufuk Topcu"
                    }
                ],
                "author_detail": {
                    "name": "Ufuk Topcu"
                },
                "author": "Ufuk Topcu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05295v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05295v3",
                "updated": "2025-09-15T16:02:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    16,
                    2,
                    53,
                    0,
                    258,
                    0
                ],
                "published": "2025-04-07T17:49:37Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    17,
                    49,
                    37,
                    0,
                    97,
                    0
                ],
                "title": "Dion: Distributed Orthonormalized Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dion: Distributed Orthonormalized Updates"
                },
                "summary": "Orthonormalized updates accelerate training, improve stability, and enable\nrobust hyperparameter transfer, but existing methods like Muon rely on dense\nmatrix operations that clash with sharded weights in large-scale LLM training,\ncausing high compute and communication cost. We introduce Dion (Distributed\nOrthonormalization), a scalable and efficient update rule that replaces\nNewton-Schulz iteration with amortized power iteration on a momentum buffer,\navoiding full-matrix reconstruction and integrating cleanly with weight\nsharding. The rank-fraction parameter with error feedback enables low-rank\nupdates that balance quality with significant cost savings. On language models\nfrom 160M to 3B parameters, Dion retains the benefits of orthonormalized\nupdates, while markedly reducing wall-clock time at scale, making it a\npractical optimizer for next-generation foundation models. Code is available\nat: https://github.com/microsoft/dion/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orthonormalized updates accelerate training, improve stability, and enable\nrobust hyperparameter transfer, but existing methods like Muon rely on dense\nmatrix operations that clash with sharded weights in large-scale LLM training,\ncausing high compute and communication cost. We introduce Dion (Distributed\nOrthonormalization), a scalable and efficient update rule that replaces\nNewton-Schulz iteration with amortized power iteration on a momentum buffer,\navoiding full-matrix reconstruction and integrating cleanly with weight\nsharding. The rank-fraction parameter with error feedback enables low-rank\nupdates that balance quality with significant cost savings. On language models\nfrom 160M to 3B parameters, Dion retains the benefits of orthonormalized\nupdates, while markedly reducing wall-clock time at scale, making it a\npractical optimizer for next-generation foundation models. Code is available\nat: https://github.com/microsoft/dion/"
                },
                "authors": [
                    {
                        "name": "Kwangjun Ahn"
                    },
                    {
                        "name": "Byron Xu"
                    },
                    {
                        "name": "Natalie Abreu"
                    },
                    {
                        "name": "Ying Fan"
                    },
                    {
                        "name": "Gagik Magakyan"
                    },
                    {
                        "name": "Pratyusha Sharma"
                    },
                    {
                        "name": "Zheng Zhan"
                    },
                    {
                        "name": "John Langford"
                    }
                ],
                "author_detail": {
                    "name": "John Langford"
                },
                "author": "John Langford",
                "arxiv_comment": "\"Version 3\" with various new updates",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05295v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05295v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12065v1",
                "updated": "2025-09-15T15:48:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    15,
                    48,
                    9,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T15:48:09Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    15,
                    48,
                    9,
                    0,
                    258,
                    0
                ],
                "title": "Steering Language Models in Multi-Token Generation: A Case Study on\n  Tense and Aspect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Language Models in Multi-Token Generation: A Case Study on\n  Tense and Aspect"
                },
                "summary": "Large language models (LLMs) are able to generate grammatically well-formed\ntext, but how do they encode their syntactic knowledge internally? While prior\nwork has focused largely on binary grammatical contrasts, in this work, we\nstudy the representation and control of two multidimensional hierarchical\ngrammar phenomena - verb tense and aspect - and for each, identify distinct,\northogonal directions in residual space using linear discriminant analysis.\nNext, we demonstrate causal control over both grammatical features through\nconcept steering across three generation tasks. Then, we use these identified\nfeatures in a case study to investigate factors influencing effective steering\nin multi-token generation. We find that steering strength, location, and\nduration are crucial parameters for reducing undesirable side effects such as\ntopic shift and degeneration. Our findings suggest that models encode tense and\naspect in structurally organized, human-like ways, but effective control of\nsuch features during generation is sensitive to multiple factors and requires\nmanual tuning or automated optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are able to generate grammatically well-formed\ntext, but how do they encode their syntactic knowledge internally? While prior\nwork has focused largely on binary grammatical contrasts, in this work, we\nstudy the representation and control of two multidimensional hierarchical\ngrammar phenomena - verb tense and aspect - and for each, identify distinct,\northogonal directions in residual space using linear discriminant analysis.\nNext, we demonstrate causal control over both grammatical features through\nconcept steering across three generation tasks. Then, we use these identified\nfeatures in a case study to investigate factors influencing effective steering\nin multi-token generation. We find that steering strength, location, and\nduration are crucial parameters for reducing undesirable side effects such as\ntopic shift and degeneration. Our findings suggest that models encode tense and\naspect in structurally organized, human-like ways, but effective control of\nsuch features during generation is sensitive to multiple factors and requires\nmanual tuning or automated optimization."
                },
                "authors": [
                    {
                        "name": "Alina Klerings"
                    },
                    {
                        "name": "Jannik Brinkmann"
                    },
                    {
                        "name": "Daniel Ruffinelli"
                    },
                    {
                        "name": "Simone Ponzetto"
                    }
                ],
                "author_detail": {
                    "name": "Simone Ponzetto"
                },
                "author": "Simone Ponzetto",
                "arxiv_comment": "to be published in The 2025 Conference on Empirical Methods in\n  Natural Language Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10123v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10123v2",
                "updated": "2025-09-15T15:39:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    15,
                    39,
                    3,
                    0,
                    258,
                    0
                ],
                "published": "2024-08-19T16:11:47Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    16,
                    11,
                    47,
                    0,
                    232,
                    0
                ],
                "title": "Learning Precise Affordances from Egocentric Videos for Robotic\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Precise Affordances from Egocentric Videos for Robotic\n  Manipulation"
                },
                "summary": "Affordance, defined as the potential actions that an object offers, is\ncrucial for embodied AI agents. For example, such knowledge directs an agent to\ngrasp a knife by the handle for cutting or by the blade for safe handover.\nWhile existing approaches have made notable progress, affordance research still\nfaces three key challenges: data scarcity, poor generalization, and real-world\ndeployment. Specifically, there is a lack of large-scale affordance datasets\nwith precise segmentation maps, existing models struggle to generalize across\ndifferent domains or novel object and affordance classes, and little work\ndemonstrates deployability in real-world scenarios. In this work, we address\nthese issues by proposing a complete affordance learning system that (1) takes\nin egocentric videos and outputs precise affordance annotations without human\nlabeling, (2) leverages geometric information and vision foundation models to\nimprove generalization, and (3) introduces a framework that facilitates\naffordance-oriented robotic manipulation such as tool grasping and\nrobot-to-human tool handover. Experimental results show that our model\nsurpasses the state-of-the-art by 13.8% in mIoU, and the framework achieves\n77.1% successful grasping among 179 trials, including evaluations on seen,\nunseen classes, and cluttered scenes. Project page:\nhttps://reagan1311.github.io/affgrasp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affordance, defined as the potential actions that an object offers, is\ncrucial for embodied AI agents. For example, such knowledge directs an agent to\ngrasp a knife by the handle for cutting or by the blade for safe handover.\nWhile existing approaches have made notable progress, affordance research still\nfaces three key challenges: data scarcity, poor generalization, and real-world\ndeployment. Specifically, there is a lack of large-scale affordance datasets\nwith precise segmentation maps, existing models struggle to generalize across\ndifferent domains or novel object and affordance classes, and little work\ndemonstrates deployability in real-world scenarios. In this work, we address\nthese issues by proposing a complete affordance learning system that (1) takes\nin egocentric videos and outputs precise affordance annotations without human\nlabeling, (2) leverages geometric information and vision foundation models to\nimprove generalization, and (3) introduces a framework that facilitates\naffordance-oriented robotic manipulation such as tool grasping and\nrobot-to-human tool handover. Experimental results show that our model\nsurpasses the state-of-the-art by 13.8% in mIoU, and the framework achieves\n77.1% successful grasping among 179 trials, including evaluations on seen,\nunseen classes, and cluttered scenes. Project page:\nhttps://reagan1311.github.io/affgrasp."
                },
                "authors": [
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Nikolaos Tsagkas"
                    },
                    {
                        "name": "Jifei Song"
                    },
                    {
                        "name": "Ruaridh Mon-Williams"
                    },
                    {
                        "name": "Sethu Vijayakumar"
                    },
                    {
                        "name": "Kun Shao"
                    },
                    {
                        "name": "Laura Sevilla-Lara"
                    }
                ],
                "author_detail": {
                    "name": "Laura Sevilla-Lara"
                },
                "author": "Laura Sevilla-Lara",
                "arxiv_comment": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10123v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10123v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03562v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03562v3",
                "updated": "2025-09-15T15:34:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    15,
                    34,
                    58,
                    0,
                    258,
                    0
                ],
                "published": "2024-11-05T23:55:23Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    23,
                    55,
                    23,
                    1,
                    310,
                    0
                ],
                "title": "Kolb-Based Experiential Learning for Generalist Agents with Human-Level\n  Kaggle Data Science Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kolb-Based Experiential Learning for Generalist Agents with Human-Level\n  Kaggle Data Science Performance"
                },
                "summary": "Human expertise emerges through iterative cycles of interaction, reflection,\nand internal model updating, which are central to cognitive theories such as\nKolb's experiential learning and Vygotsky's zone of proximal development. In\ncontrast, current AI systems, particularly LLM agents, rely on static\npre-training or rigid workflows, lacking mechanisms for continual adaptation.\nRecent studies identified early cognitive traits in LLM agents (reflection,\nrevision, and self-correction) suggesting foundational elements of human-like\nexperiential learning. Thus the key question: Can we design LLM agents capable\nof structured, cognitively grounded learning similar to human processes? In\nresponse, we propose a computational framework of Kolb's learning cycle with\nVygotsky's ZPD for autonomous agents. Our architecture separates extrinsic\n(environment interaction) and intrinsic (internal reflection/abstraction)\nfunctions, enabling cognitively grounded scaffolded learning, where the agent\ninitially learns within structured environments, followed by open-ended\ngeneralisation. This approach empowers agents to master complex tasks ; domains\nthat traditional fine-tuning or simple reflective methods could not tackle\neffectively. Its potential is powerfully demonstrated via direct comparison\nwith humans in real-world Kaggle data science competitions. Learning fully\nautomated data science code generation across 81 tasks, our system, Agent K,\ndemonstrated the ability to perform the entire workflow autonomously, achieving\nan Elo-MMR score of 1694, beyond median score of the Kaggle Masters (the top 2%\namong 200,000 users) of our study. With 9 gold, 8 silver, and 12 bronze medals\nlevel performance - including 4 gold and 4 silver on prize-awarding\ncompetitions - Agent K is the 1st AI system to successfully integrate Kolb- and\nVygotsky-inspired human cognitive learning, marking a major step toward\ngeneralist AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human expertise emerges through iterative cycles of interaction, reflection,\nand internal model updating, which are central to cognitive theories such as\nKolb's experiential learning and Vygotsky's zone of proximal development. In\ncontrast, current AI systems, particularly LLM agents, rely on static\npre-training or rigid workflows, lacking mechanisms for continual adaptation.\nRecent studies identified early cognitive traits in LLM agents (reflection,\nrevision, and self-correction) suggesting foundational elements of human-like\nexperiential learning. Thus the key question: Can we design LLM agents capable\nof structured, cognitively grounded learning similar to human processes? In\nresponse, we propose a computational framework of Kolb's learning cycle with\nVygotsky's ZPD for autonomous agents. Our architecture separates extrinsic\n(environment interaction) and intrinsic (internal reflection/abstraction)\nfunctions, enabling cognitively grounded scaffolded learning, where the agent\ninitially learns within structured environments, followed by open-ended\ngeneralisation. This approach empowers agents to master complex tasks ; domains\nthat traditional fine-tuning or simple reflective methods could not tackle\neffectively. Its potential is powerfully demonstrated via direct comparison\nwith humans in real-world Kaggle data science competitions. Learning fully\nautomated data science code generation across 81 tasks, our system, Agent K,\ndemonstrated the ability to perform the entire workflow autonomously, achieving\nan Elo-MMR score of 1694, beyond median score of the Kaggle Masters (the top 2%\namong 200,000 users) of our study. With 9 gold, 8 silver, and 12 bronze medals\nlevel performance - including 4 gold and 4 silver on prize-awarding\ncompetitions - Agent K is the 1st AI system to successfully integrate Kolb- and\nVygotsky-inspired human cognitive learning, marking a major step toward\ngeneralist AI."
                },
                "authors": [
                    {
                        "name": "Antoine Grosnit"
                    },
                    {
                        "name": "Alexandre Maraval"
                    },
                    {
                        "name": "Refinath S N"
                    },
                    {
                        "name": "Zichao Zhao"
                    },
                    {
                        "name": "James Doran"
                    },
                    {
                        "name": "Giuseppe Paolo"
                    },
                    {
                        "name": "Albert Thomas"
                    },
                    {
                        "name": "Jonas Gonzalez"
                    },
                    {
                        "name": "Abhineet Kumar"
                    },
                    {
                        "name": "Khyati Khandelwal"
                    },
                    {
                        "name": "Abdelhakim Benechehab"
                    },
                    {
                        "name": "Hamza Cherkaoui"
                    },
                    {
                        "name": "Youssef Attia El-Hili"
                    },
                    {
                        "name": "Kun Shao"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Jun Yao"
                    },
                    {
                        "name": "Balázs Kégl"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03562v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03562v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12052v1",
                "updated": "2025-09-15T15:34:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    15,
                    34,
                    2,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T15:34:02Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    15,
                    34,
                    2,
                    0,
                    258,
                    0
                ],
                "title": "AvatarSync: Rethinking Talking-Head Animation through Autoregressive\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AvatarSync: Rethinking Talking-Head Animation through Autoregressive\n  Perspective"
                },
                "summary": "Existing talking-head animation approaches based on Generative Adversarial\nNetworks (GANs) or diffusion models often suffer from inter-frame flicker,\nidentity drift, and slow inference. These limitations inherent to their video\ngeneration pipelines restrict their suitability for applications. To address\nthis, we introduce AvatarSync, an autoregressive framework on phoneme\nrepresentations that generates realistic and controllable talking-head\nanimations from a single reference image, driven directly text or audio input.\nIn addition, AvatarSync adopts a two-stage generation strategy, decoupling\nsemantic modeling from visual dynamics, which is a deliberate \"Divide and\nConquer\" design. The first stage, Facial Keyframe Generation (FKG), focuses on\nphoneme-level semantic representation by leveraging the many-to-one mapping\nfrom text or audio to phonemes. A Phoneme-to-Visual Mapping is constructed to\nanchor abstract phonemes to character-level units. Combined with a customized\nText-Frame Causal Attention Mask, the keyframes are generated. The second\nstage, inter-frame interpolation, emphasizes temporal coherence and visual\nsmoothness. We introduce a timestamp-aware adaptive strategy based on a\nselective state space model, enabling efficient bidirectional context\nreasoning. To support deployment, we optimize the inference pipeline to reduce\nlatency without compromising visual fidelity. Extensive experiments show that\nAvatarSync outperforms existing talking-head animation methods in visual\nfidelity, temporal consistency, and computational efficiency, providing a\nscalable and controllable solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing talking-head animation approaches based on Generative Adversarial\nNetworks (GANs) or diffusion models often suffer from inter-frame flicker,\nidentity drift, and slow inference. These limitations inherent to their video\ngeneration pipelines restrict their suitability for applications. To address\nthis, we introduce AvatarSync, an autoregressive framework on phoneme\nrepresentations that generates realistic and controllable talking-head\nanimations from a single reference image, driven directly text or audio input.\nIn addition, AvatarSync adopts a two-stage generation strategy, decoupling\nsemantic modeling from visual dynamics, which is a deliberate \"Divide and\nConquer\" design. The first stage, Facial Keyframe Generation (FKG), focuses on\nphoneme-level semantic representation by leveraging the many-to-one mapping\nfrom text or audio to phonemes. A Phoneme-to-Visual Mapping is constructed to\nanchor abstract phonemes to character-level units. Combined with a customized\nText-Frame Causal Attention Mask, the keyframes are generated. The second\nstage, inter-frame interpolation, emphasizes temporal coherence and visual\nsmoothness. We introduce a timestamp-aware adaptive strategy based on a\nselective state space model, enabling efficient bidirectional context\nreasoning. To support deployment, we optimize the inference pipeline to reduce\nlatency without compromising visual fidelity. Extensive experiments show that\nAvatarSync outperforms existing talking-head animation methods in visual\nfidelity, temporal consistency, and computational efficiency, providing a\nscalable and controllable solution."
                },
                "authors": [
                    {
                        "name": "Yuchen Deng"
                    },
                    {
                        "name": "Xiuyang Wu"
                    },
                    {
                        "name": "Hai-Tao Zheng"
                    },
                    {
                        "name": "Suiyang Zhang"
                    },
                    {
                        "name": "Yi He"
                    },
                    {
                        "name": "Yuxing Han"
                    }
                ],
                "author_detail": {
                    "name": "Yuxing Han"
                },
                "author": "Yuxing Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12021v1",
                "updated": "2025-09-15T15:01:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    15,
                    1,
                    3,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T15:01:03Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    15,
                    1,
                    3,
                    0,
                    258,
                    0
                ],
                "title": "LitterBox+: An Extensible Framework for LLM-enhanced Scratch Static Code\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LitterBox+: An Extensible Framework for LLM-enhanced Scratch Static Code\n  Analysis"
                },
                "summary": "Large language models (LLMs) have become an essential tool to support\ndevelopers using traditional text-based programming languages, but the\ngraphical notation of the block-based Scratch programming environment inhibits\nthe use of LLMs. To overcome this limitation, we propose the LitterBox+\nframework that extends the Scratch static code analysis tool LitterBox with the\ngenerative abilities of LLMs. By converting block-based code to a textual\nrepresentation suitable for LLMs, LitterBox+ allows users to query LLMs about\ntheir programs, about quality issues reported by LitterBox, and it allows\ngenerating code fixes. Besides offering a programmatic API for these\nfunctionalities, LitterBox+ also extends the Scratch user interface to make\nthese functionalities available directly in the environment familiar to\nlearners. The framework is designed to be easily extensible with other prompts,\nLLM providers, and new features combining the program analysis capabilities of\nLitterBox with the generative features of LLMs. We provide a screencast\ndemonstrating the tool at https://youtu.be/RZ6E0xgrIgQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become an essential tool to support\ndevelopers using traditional text-based programming languages, but the\ngraphical notation of the block-based Scratch programming environment inhibits\nthe use of LLMs. To overcome this limitation, we propose the LitterBox+\nframework that extends the Scratch static code analysis tool LitterBox with the\ngenerative abilities of LLMs. By converting block-based code to a textual\nrepresentation suitable for LLMs, LitterBox+ allows users to query LLMs about\ntheir programs, about quality issues reported by LitterBox, and it allows\ngenerating code fixes. Besides offering a programmatic API for these\nfunctionalities, LitterBox+ also extends the Scratch user interface to make\nthese functionalities available directly in the environment familiar to\nlearners. The framework is designed to be easily extensible with other prompts,\nLLM providers, and new features combining the program analysis capabilities of\nLitterBox with the generative features of LLMs. We provide a screencast\ndemonstrating the tool at https://youtu.be/RZ6E0xgrIgQ."
                },
                "authors": [
                    {
                        "name": "Benedikt Fein"
                    },
                    {
                        "name": "Florian Obermüller"
                    },
                    {
                        "name": "Gordon Fraser"
                    }
                ],
                "author_detail": {
                    "name": "Gordon Fraser"
                },
                "author": "Gordon Fraser",
                "arxiv_comment": "ASE 2025 Tool Demonstration Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12019v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12019v1",
                "updated": "2025-09-15T14:59:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    59,
                    35,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T14:59:35Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    59,
                    35,
                    0,
                    258,
                    0
                ],
                "title": "AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of\n  Large Language Models"
                },
                "summary": "To enable broader deployment of Large Language Models (LLMs), it is essential\nto identify the best-performing model under strict memory constraints. We\npresent AMQ, Automated Mixed-Precision Weight-Only Quantization, a framework\nthat assigns layer-wise quantization bit-widths to optimally balance model\nquality and memory usage. However, the combinatorial search space, with over\n10^{100} possible configurations, makes conventional black-box optimization\ninfeasible. AMQ overcomes this challenge through four key innovations:(1)\nsearch space pruning using prior knowledge to exclude unpromising\nconfigurations, (2) quantization proxy to bypass costly format conversions\nduring search, (3) quality predictor to minimize evaluation overhead, and (4)\niterative search-and-update strategy for fast and stable convergence. By\nintegrating these components, AMQ efficiently explores the quality-efficiency\nlandscape, reaching the Pareto frontier and yielding LLMs that are both compact\nand high-performing. Our code is available at https://github.com/dlwns147/amq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To enable broader deployment of Large Language Models (LLMs), it is essential\nto identify the best-performing model under strict memory constraints. We\npresent AMQ, Automated Mixed-Precision Weight-Only Quantization, a framework\nthat assigns layer-wise quantization bit-widths to optimally balance model\nquality and memory usage. However, the combinatorial search space, with over\n10^{100} possible configurations, makes conventional black-box optimization\ninfeasible. AMQ overcomes this challenge through four key innovations:(1)\nsearch space pruning using prior knowledge to exclude unpromising\nconfigurations, (2) quantization proxy to bypass costly format conversions\nduring search, (3) quality predictor to minimize evaluation overhead, and (4)\niterative search-and-update strategy for fast and stable convergence. By\nintegrating these components, AMQ efficiently explores the quality-efficiency\nlandscape, reaching the Pareto frontier and yielding LLMs that are both compact\nand high-performing. Our code is available at https://github.com/dlwns147/amq."
                },
                "authors": [
                    {
                        "name": "Sangjun Lee"
                    },
                    {
                        "name": "Seung-taek Woo"
                    },
                    {
                        "name": "Jungyu Jin"
                    },
                    {
                        "name": "Changhun Lee"
                    },
                    {
                        "name": "Eunhyeok Park"
                    }
                ],
                "author_detail": {
                    "name": "Eunhyeok Park"
                },
                "author": "Eunhyeok Park",
                "arxiv_comment": "EMNLP 2025 Main Conference, Long Paper (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12019v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12019v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18240v2",
                "updated": "2025-09-15T14:50:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    50,
                    39,
                    0,
                    258,
                    0
                ],
                "published": "2025-08-22T12:14:17Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    14,
                    17,
                    4,
                    234,
                    0
                ],
                "title": "MTalk-Bench: Evaluating Speech-to-Speech Models in Multi-Turn Dialogues\n  via Arena-style and Rubrics Protocols",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MTalk-Bench: Evaluating Speech-to-Speech Models in Multi-Turn Dialogues\n  via Arena-style and Rubrics Protocols"
                },
                "summary": "The rapid advancement of speech-to-speech (S2S) large language models (LLMs)\nhas significantly improved real-time spoken interaction. However, current\nevaluation frameworks remain inadequate for assessing performance in complex,\nmulti-turn dialogues. To address this, we introduce MTalk-Bench, a multi-turn\nS2S benchmark covering three core dimensions: Semantic Information,\nParalinguistic Information, and Ambient Sound. Each dimension includes nine\nrealistic scenarios, along with targeted tasks to assess specific capabilities\nsuch as reasoning. Our dual-method evaluation framework combines Arena-style\nevaluation (pairwise comparison) and Rubrics-based evaluation (absolute\nscoring) for relative and absolute assessment. The benchmark includes both\nmodel and human outputs, evaluated by human evaluators and LLMs. Experimental\nresults reveal two sets of findings. Overall performance of S2S LLMs: (1)\nmodels excel at semantic information processing yet underperform on\nparalinguistic information and ambient sounds perception; (2) models typically\nregain coherence by increasing response length, sacrificing efficiency in\nmulti-turn dialogues; (3) modality-aware, task-specific designs outperform\nbrute scaling. Evaluation framework and reliability: (1) Arena and Rubrics\nyield consistent, complementary rankings, but reliable distinctions emerge only\nwhen performance gaps are large; (2) LLM-as-a-judge aligns with humans when\ngaps are clear or criteria explicit, but exhibits position and length biases\nand is reliable on nonverbal evaluation only with text annotations. These\nresults highlight current limitations in S2S evaluation and the need for more\nrobust, speech-aware assessment frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of speech-to-speech (S2S) large language models (LLMs)\nhas significantly improved real-time spoken interaction. However, current\nevaluation frameworks remain inadequate for assessing performance in complex,\nmulti-turn dialogues. To address this, we introduce MTalk-Bench, a multi-turn\nS2S benchmark covering three core dimensions: Semantic Information,\nParalinguistic Information, and Ambient Sound. Each dimension includes nine\nrealistic scenarios, along with targeted tasks to assess specific capabilities\nsuch as reasoning. Our dual-method evaluation framework combines Arena-style\nevaluation (pairwise comparison) and Rubrics-based evaluation (absolute\nscoring) for relative and absolute assessment. The benchmark includes both\nmodel and human outputs, evaluated by human evaluators and LLMs. Experimental\nresults reveal two sets of findings. Overall performance of S2S LLMs: (1)\nmodels excel at semantic information processing yet underperform on\nparalinguistic information and ambient sounds perception; (2) models typically\nregain coherence by increasing response length, sacrificing efficiency in\nmulti-turn dialogues; (3) modality-aware, task-specific designs outperform\nbrute scaling. Evaluation framework and reliability: (1) Arena and Rubrics\nyield consistent, complementary rankings, but reliable distinctions emerge only\nwhen performance gaps are large; (2) LLM-as-a-judge aligns with humans when\ngaps are clear or criteria explicit, but exhibits position and length biases\nand is reliable on nonverbal evaluation only with text annotations. These\nresults highlight current limitations in S2S evaluation and the need for more\nrobust, speech-aware assessment frameworks."
                },
                "authors": [
                    {
                        "name": "Yuhao Du"
                    },
                    {
                        "name": "Qianwei Huang"
                    },
                    {
                        "name": "Guo Zhu"
                    },
                    {
                        "name": "Zhanchen Dai"
                    },
                    {
                        "name": "Shunian Chen"
                    },
                    {
                        "name": "Qiming Zhu"
                    },
                    {
                        "name": "Le Pan"
                    },
                    {
                        "name": "Minghao Chen"
                    },
                    {
                        "name": "Yuhao Zhang"
                    },
                    {
                        "name": "Li Zhou"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Haizhou Li"
                    }
                ],
                "author_detail": {
                    "name": "Haizhou Li"
                },
                "author": "Haizhou Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11990v1",
                "updated": "2025-09-15T14:42:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    42,
                    17,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T14:42:17Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    42,
                    17,
                    0,
                    258,
                    0
                ],
                "title": "Bootstrapping Liquidity in BTC-Denominated Prediction Markets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bootstrapping Liquidity in BTC-Denominated Prediction Markets"
                },
                "summary": "Prediction markets have gained adoption as on-chain mechanisms for\naggregating information, with platforms such as Polymarket demonstrating demand\nfor stablecoin-denominated markets. However, denominating in\nnon-interest-bearing stablecoins introduces inefficiencies: participants face\nopportunity costs relative to the fiat risk-free rate, and Bitcoin holders in\nparticular lose exposure to BTC appreciation when converting into stablecoins.\nThis paper explores the case for prediction markets denominated in Bitcoin,\ntreating BTC as a deflationary settlement asset analogous to gold under the\nclassical gold standard. We analyse three methods of supplying liquidity to a\nnewly created BTC-denominated prediction market: cross-market making against\nexisting stablecoin venues, automated market making, and DeFi-based redirection\nof user trades. For each approach we evaluate execution mechanics, risks\n(slippage, exchange-rate risk, and liquidation risk), and capital efficiency.\nOur analysis shows that cross-market making provides the most user-friendly\nrisk profile, though it requires active professional makers or\nplatform-subsidised liquidity. DeFi redirection offers rapid bootstrapping and\nreuse of existing USDC liquidity, but exposes users to liquidation thresholds\nand exchange-rate volatility, reducing capital efficiency. Automated market\nmaking is simple to deploy but capital-inefficient and exposes liquidity\nproviders to permanent loss. The results suggest that BTC-denominated\nprediction markets are feasible, but their success depends critically on the\nchoice of liquidity provisioning mechanism and the trade-off between user\nsafety and deployment convenience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prediction markets have gained adoption as on-chain mechanisms for\naggregating information, with platforms such as Polymarket demonstrating demand\nfor stablecoin-denominated markets. However, denominating in\nnon-interest-bearing stablecoins introduces inefficiencies: participants face\nopportunity costs relative to the fiat risk-free rate, and Bitcoin holders in\nparticular lose exposure to BTC appreciation when converting into stablecoins.\nThis paper explores the case for prediction markets denominated in Bitcoin,\ntreating BTC as a deflationary settlement asset analogous to gold under the\nclassical gold standard. We analyse three methods of supplying liquidity to a\nnewly created BTC-denominated prediction market: cross-market making against\nexisting stablecoin venues, automated market making, and DeFi-based redirection\nof user trades. For each approach we evaluate execution mechanics, risks\n(slippage, exchange-rate risk, and liquidation risk), and capital efficiency.\nOur analysis shows that cross-market making provides the most user-friendly\nrisk profile, though it requires active professional makers or\nplatform-subsidised liquidity. DeFi redirection offers rapid bootstrapping and\nreuse of existing USDC liquidity, but exposes users to liquidation thresholds\nand exchange-rate volatility, reducing capital efficiency. Automated market\nmaking is simple to deploy but capital-inefficient and exposes liquidity\nproviders to permanent loss. The results suggest that BTC-denominated\nprediction markets are feasible, but their success depends critically on the\nchoice of liquidity provisioning mechanism and the trade-off between user\nsafety and deployment convenience."
                },
                "authors": [
                    {
                        "name": "Fedor Shabashev"
                    }
                ],
                "author_detail": {
                    "name": "Fedor Shabashev"
                },
                "author": "Fedor Shabashev",
                "arxiv_comment": "LaTeX; pdfLaTeX; includes precompiled main.bbl. 2 figures (TikZ)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.TR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13231v2",
                "updated": "2025-09-15T14:40:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    40,
                    16,
                    0,
                    258,
                    0
                ],
                "published": "2025-08-17T19:07:08Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    19,
                    7,
                    8,
                    6,
                    229,
                    0
                ],
                "title": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System"
                },
                "summary": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference."
                },
                "authors": [
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Kaoutar El Maghraoui"
                    },
                    {
                        "name": "Naigang Wang"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "IEEE Computer Architecture Letter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09198v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09198v2",
                "updated": "2025-09-15T14:23:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    23,
                    51,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-11T07:13:44Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    7,
                    13,
                    44,
                    3,
                    254,
                    0
                ],
                "title": "GmSLM : Generative Marmoset Spoken Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GmSLM : Generative Marmoset Spoken Language Modeling"
                },
                "summary": "Marmoset monkeys exhibit complex vocal communication, challenging the view\nthat nonhuman primates vocal communication is entirely innate, and show similar\nfeatures of human speech, such as vocal labeling of others and turn-taking.\nStudying their vocal communication offers a unique opportunity to link it with\nbrain activity-especially given the difficulty of accessing the human brain in\nspeech and language research. Since Marmosets communicate primarily through\nvocalizations, applying standard LLM approaches is not straightforward. We\nintroduce Generative Marmoset Spoken Language Modeling (GmSLM), an optimized\nspoken language model pipeline for Marmoset vocal communication. We designed a\nnovel zero-shot evaluation metrics using unsupervised in-the-wild data,\nalongside weakly labeled conversational data, to assess GmSLM and demonstrate\nits advantage over a basic human-speech-based baseline. GmSLM generated\nvocalizations closely matched real resynthesized samples acoustically and\nperformed well on downstream tasks. Despite being fully unsupervised, GmSLM\neffectively distinguish real from artificial conversations and may support\nfurther investigations of the neural basis of vocal communication and provides\na practical framework linking vocalization and brain activity. We believe GmSLM\nstands to benefit future work in neuroscience, bioacoustics, and evolutionary\nbiology. Samples are provided under: pages.cs.huji.ac.il/adiyoss-lab/GmSLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marmoset monkeys exhibit complex vocal communication, challenging the view\nthat nonhuman primates vocal communication is entirely innate, and show similar\nfeatures of human speech, such as vocal labeling of others and turn-taking.\nStudying their vocal communication offers a unique opportunity to link it with\nbrain activity-especially given the difficulty of accessing the human brain in\nspeech and language research. Since Marmosets communicate primarily through\nvocalizations, applying standard LLM approaches is not straightforward. We\nintroduce Generative Marmoset Spoken Language Modeling (GmSLM), an optimized\nspoken language model pipeline for Marmoset vocal communication. We designed a\nnovel zero-shot evaluation metrics using unsupervised in-the-wild data,\nalongside weakly labeled conversational data, to assess GmSLM and demonstrate\nits advantage over a basic human-speech-based baseline. GmSLM generated\nvocalizations closely matched real resynthesized samples acoustically and\nperformed well on downstream tasks. Despite being fully unsupervised, GmSLM\neffectively distinguish real from artificial conversations and may support\nfurther investigations of the neural basis of vocal communication and provides\na practical framework linking vocalization and brain activity. We believe GmSLM\nstands to benefit future work in neuroscience, bioacoustics, and evolutionary\nbiology. Samples are provided under: pages.cs.huji.ac.il/adiyoss-lab/GmSLM."
                },
                "authors": [
                    {
                        "name": "Talia Sternberg"
                    },
                    {
                        "name": "Michael London"
                    },
                    {
                        "name": "David Omer"
                    },
                    {
                        "name": "Yossi Adi"
                    }
                ],
                "author_detail": {
                    "name": "Yossi Adi"
                },
                "author": "Yossi Adi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09198v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09198v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14403v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14403v4",
                "updated": "2025-09-15T14:23:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    23,
                    10,
                    0,
                    258,
                    0
                ],
                "published": "2025-05-20T14:16:49Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    16,
                    49,
                    1,
                    140,
                    0
                ],
                "title": "Unearthing Gems from Stones: Policy Optimization with Negative Sample\n  Augmentation for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unearthing Gems from Stones: Policy Optimization with Negative Sample\n  Augmentation for LLM Reasoning"
                },
                "summary": "Recent advances in reasoning language models have witnessed a paradigm shift\nfrom short to long CoT pattern. Given the substantial computational cost of\nrollouts in long CoT models, maximizing the utility of fixed training datasets\nbecomes crucial. Our analysis reveals that negative responses contain valuable\ncomponents such as self-reflection and error-correction steps, yet primary\nexisting methods either completely discard negative samples (RFT) or apply\nequal penalization across all tokens (RL), failing to leverage these potential\nlearning signals. In light of this, we propose Behavior Constrained Policy\nGradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline\nRL framework that encompasses three stages: 1) sample segmentation, 2)\nconsensus-based step correctness assessment combining LLM and PRM judgers, and\n3) policy optimization with NSA designed to effectively mine positive steps\nwithin negative samples. Experimental results show that BCPG-NSA outperforms\nbaselines on several challenging math/coding reasoning benchmarks using the\nsame training dataset, achieving improved sample efficiency and demonstrating\nrobustness and scalability when extended to multiple iterations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reasoning language models have witnessed a paradigm shift\nfrom short to long CoT pattern. Given the substantial computational cost of\nrollouts in long CoT models, maximizing the utility of fixed training datasets\nbecomes crucial. Our analysis reveals that negative responses contain valuable\ncomponents such as self-reflection and error-correction steps, yet primary\nexisting methods either completely discard negative samples (RFT) or apply\nequal penalization across all tokens (RL), failing to leverage these potential\nlearning signals. In light of this, we propose Behavior Constrained Policy\nGradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline\nRL framework that encompasses three stages: 1) sample segmentation, 2)\nconsensus-based step correctness assessment combining LLM and PRM judgers, and\n3) policy optimization with NSA designed to effectively mine positive steps\nwithin negative samples. Experimental results show that BCPG-NSA outperforms\nbaselines on several challenging math/coding reasoning benchmarks using the\nsame training dataset, achieving improved sample efficiency and demonstrating\nrobustness and scalability when extended to multiple iterations."
                },
                "authors": [
                    {
                        "name": "Zhaohui Yang"
                    },
                    {
                        "name": "Yuxiao Ye"
                    },
                    {
                        "name": "Shilei Jiang"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Linjing Li"
                    },
                    {
                        "name": "Shihong Deng"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14403v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14403v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11969v1",
                "updated": "2025-09-15T14:21:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    21,
                    10,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T14:21:10Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    21,
                    10,
                    0,
                    258,
                    0
                ],
                "title": "Optimization for Massive 3D-RIS Deployment: A Generative Diffusion\n  Model-Based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization for Massive 3D-RIS Deployment: A Generative Diffusion\n  Model-Based Approach"
                },
                "summary": "Reconfigurable Intelligent Surfaces (RISs) transform the wireless environment\nby modifying the amplitude, phase, and polarization of incoming waves,\nsignificantly improving coverage performance. Notably, optimizing the\ndeployment of RISs becomes vital, but existing optimization methods face\nchallenges such as high computational complexity, limited adaptability to\nchanging environments, and a tendency to converge on local optima. In this\npaper, we propose to optimize the deployment of large-scale 3D RISs using a\ndiffusion model based on probabilistic generative learning. We begin by\ndividing the target area into fixed grids, with each grid corresponding to a\npotential deployment location. Then, a multi-RIS deployment optimization\nproblem is formulated, which is difficult to solve directly. By treating RIS\ndeployment as a conditional generation task, the well-trained diffusion model\ncan generate the distribution of deployment strategies, and thus, the optimal\ndeployment strategy can be obtained by sampling from this distribution.\nSimulation results demonstrate that the proposed diffusion-based method\noutperforms traditional benchmark approaches in terms of exceed ratio and\ngeneralization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable Intelligent Surfaces (RISs) transform the wireless environment\nby modifying the amplitude, phase, and polarization of incoming waves,\nsignificantly improving coverage performance. Notably, optimizing the\ndeployment of RISs becomes vital, but existing optimization methods face\nchallenges such as high computational complexity, limited adaptability to\nchanging environments, and a tendency to converge on local optima. In this\npaper, we propose to optimize the deployment of large-scale 3D RISs using a\ndiffusion model based on probabilistic generative learning. We begin by\ndividing the target area into fixed grids, with each grid corresponding to a\npotential deployment location. Then, a multi-RIS deployment optimization\nproblem is formulated, which is difficult to solve directly. By treating RIS\ndeployment as a conditional generation task, the well-trained diffusion model\ncan generate the distribution of deployment strategies, and thus, the optimal\ndeployment strategy can be obtained by sampling from this distribution.\nSimulation results demonstrate that the proposed diffusion-based method\noutperforms traditional benchmark approaches in terms of exceed ratio and\ngeneralization."
                },
                "authors": [
                    {
                        "name": "Kaining Wang"
                    },
                    {
                        "name": "Bo Yang"
                    },
                    {
                        "name": "Zhiwen Yu"
                    },
                    {
                        "name": "Xuelin Cao"
                    },
                    {
                        "name": "Mérouane Debbah"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11967v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11967v2",
                "updated": "2025-09-16T02:36:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    2,
                    36,
                    5,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-15T14:18:51Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    18,
                    51,
                    0,
                    258,
                    0
                ],
                "title": "MillStone: How Open-Minded Are LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MillStone: How Open-Minded Are LLMs?"
                },
                "summary": "Large language models equipped with Web search, information retrieval tools,\nand other agentic capabilities are beginning to supplant traditional search\nengines. As users start to rely on LLMs for information on many topics,\nincluding controversial and debatable issues, it is important to understand how\nthe stances and opinions expressed in LLM outputs are influenced by the\ndocuments they use as their information sources.\n  In this paper, we present MillStone, the first benchmark that aims to\nsystematically measure the effect of external arguments on the stances that\nLLMs take on controversial issues (not all of them political). We apply\nMillStone to nine leading LLMs and measure how ``open-minded'' they are to\narguments supporting opposite sides of these issues, whether different LLMs\nagree with each other, which arguments LLMs find most persuasive, and whether\nthese arguments are the same for different LLMs.\n  In general, we find that LLMs are open-minded on most issues. An\nauthoritative source of information can easily sway an LLM's stance,\nhighlighting the importance of source selection and the risk that LLM-based\ninformation retrieval and search systems can be manipulated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models equipped with Web search, information retrieval tools,\nand other agentic capabilities are beginning to supplant traditional search\nengines. As users start to rely on LLMs for information on many topics,\nincluding controversial and debatable issues, it is important to understand how\nthe stances and opinions expressed in LLM outputs are influenced by the\ndocuments they use as their information sources.\n  In this paper, we present MillStone, the first benchmark that aims to\nsystematically measure the effect of external arguments on the stances that\nLLMs take on controversial issues (not all of them political). We apply\nMillStone to nine leading LLMs and measure how ``open-minded'' they are to\narguments supporting opposite sides of these issues, whether different LLMs\nagree with each other, which arguments LLMs find most persuasive, and whether\nthese arguments are the same for different LLMs.\n  In general, we find that LLMs are open-minded on most issues. An\nauthoritative source of information can easily sway an LLM's stance,\nhighlighting the importance of source selection and the risk that LLM-based\ninformation retrieval and search systems can be manipulated."
                },
                "authors": [
                    {
                        "name": "Harold Triedman"
                    },
                    {
                        "name": "Vitaly Shmatikov"
                    }
                ],
                "author_detail": {
                    "name": "Vitaly Shmatikov"
                },
                "author": "Vitaly Shmatikov",
                "arxiv_comment": "19 pages, 7 tables, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11967v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11967v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11963v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11963v1",
                "updated": "2025-09-15T14:17:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    17,
                    17,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T14:17:17Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    17,
                    17,
                    0,
                    258,
                    0
                ],
                "title": "ToolRM: Outcome Reward Models for Tool-Calling Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolRM: Outcome Reward Models for Tool-Calling Large Language Models"
                },
                "summary": "As large language models (LLMs) increasingly interact with external tools,\nreward modeling for tool use has become a critical yet underexplored area.\nExisting reward models, trained primarily on natural language outputs, struggle\nto evaluate tool-based reasoning and execution. To quantify this gap, we\nintroduce FC-RewardBench, the first benchmark designed to systematically assess\nreward models' performance in tool-calling scenarios. Our analysis shows that\ncurrent reward models often miss key signals of effective tool use,\nhighlighting the need for domain-specific modeling. To address this, we propose\na training framework for outcome-based reward models using data synthesized\nfrom permissively licensed, open-weight LLMs. We train models ranging from 1.7B\nto 14B parameters and evaluate them across seven out-of-domain benchmarks.\nThese models consistently outperform general-purpose baselines, achieving up to\n25\\% average improvement in downstream task performance and enabling\ndata-efficient fine-tuning through reward-guided filtering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly interact with external tools,\nreward modeling for tool use has become a critical yet underexplored area.\nExisting reward models, trained primarily on natural language outputs, struggle\nto evaluate tool-based reasoning and execution. To quantify this gap, we\nintroduce FC-RewardBench, the first benchmark designed to systematically assess\nreward models' performance in tool-calling scenarios. Our analysis shows that\ncurrent reward models often miss key signals of effective tool use,\nhighlighting the need for domain-specific modeling. To address this, we propose\na training framework for outcome-based reward models using data synthesized\nfrom permissively licensed, open-weight LLMs. We train models ranging from 1.7B\nto 14B parameters and evaluate them across seven out-of-domain benchmarks.\nThese models consistently outperform general-purpose baselines, achieving up to\n25\\% average improvement in downstream task performance and enabling\ndata-efficient fine-tuning through reward-guided filtering."
                },
                "authors": [
                    {
                        "name": "Mayank Agarwal"
                    },
                    {
                        "name": "Ibrahim Abdelaziz"
                    },
                    {
                        "name": "Kinjal Basu"
                    },
                    {
                        "name": "Merve Unuvar"
                    },
                    {
                        "name": "Luis A. Lastras"
                    },
                    {
                        "name": "Yara Rizk"
                    },
                    {
                        "name": "Pavan Kapanipathi"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Kapanipathi"
                },
                "author": "Pavan Kapanipathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11963v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11963v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11961v1",
                "updated": "2025-09-15T14:16:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    16,
                    51,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T14:16:51Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    16,
                    51,
                    0,
                    258,
                    0
                ],
                "title": "Spec-LLaVA: Accelerating Vision-Language Models with Dynamic Tree-Based\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spec-LLaVA: Accelerating Vision-Language Models with Dynamic Tree-Based\n  Speculative Decoding"
                },
                "summary": "Vision-Language Models (VLMs) enable powerful multimodal reasoning but suffer\nfrom slow autoregressive inference, limiting their deployment in real-time\napplications. We introduce Spec-LLaVA, a system that applies speculative\ndecoding to accelerate VLMs without sacrificing output quality. Spec-LLaVA\npairs a lightweight draft VLM with a large target model: the draft speculates\nfuture tokens, which the target verifies in parallel, allowing multiple tokens\nto be generated per step. To maximize efficiency, we design a dynamic\ntree-based verification algorithm that adaptively expands and prunes\nspeculative branches using draft model confidence. On MS COCO out-of-domain\nimages, Spec-LLaVA achieves up to 3.28$\\times$ faster decoding on LLaVA-1.5\n(7B, 13B) with no loss in generation quality. This work presents a lossless\nacceleration framework for VLMs using dynamic tree-structured speculative\ndecoding, opening a path toward practical real-time multimodal assistants.\nImportantly, the lightweight draft model design makes the framework amenable to\nresource-constrained or on-device deployment settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) enable powerful multimodal reasoning but suffer\nfrom slow autoregressive inference, limiting their deployment in real-time\napplications. We introduce Spec-LLaVA, a system that applies speculative\ndecoding to accelerate VLMs without sacrificing output quality. Spec-LLaVA\npairs a lightweight draft VLM with a large target model: the draft speculates\nfuture tokens, which the target verifies in parallel, allowing multiple tokens\nto be generated per step. To maximize efficiency, we design a dynamic\ntree-based verification algorithm that adaptively expands and prunes\nspeculative branches using draft model confidence. On MS COCO out-of-domain\nimages, Spec-LLaVA achieves up to 3.28$\\times$ faster decoding on LLaVA-1.5\n(7B, 13B) with no loss in generation quality. This work presents a lossless\nacceleration framework for VLMs using dynamic tree-structured speculative\ndecoding, opening a path toward practical real-time multimodal assistants.\nImportantly, the lightweight draft model design makes the framework amenable to\nresource-constrained or on-device deployment settings."
                },
                "authors": [
                    {
                        "name": "Mingxiao Huo"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Hewei Wang"
                    },
                    {
                        "name": "Jinfeng Xu"
                    },
                    {
                        "name": "Zheyu Chen"
                    },
                    {
                        "name": "Huilin Tai"
                    },
                    {
                        "name": "Yijun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yijun Chen"
                },
                "author": "Yijun Chen",
                "arxiv_comment": "7pages, accepted by ICML TTODLer-FM workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20344v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20344v2",
                "updated": "2025-09-15T14:09:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    9,
                    38,
                    0,
                    258,
                    0
                ],
                "published": "2025-02-27T18:16:47Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    16,
                    47,
                    3,
                    58,
                    0
                ],
                "title": "LinguaLens: Towards Interpreting Linguistic Mechanisms of Large Language\n  Models via Sparse Auto-Encoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LinguaLens: Towards Interpreting Linguistic Mechanisms of Large Language\n  Models via Sparse Auto-Encoder"
                },
                "summary": "Large language models (LLMs) demonstrate exceptional performance on tasks\nrequiring complex linguistic abilities, such as reference disambiguation and\nmetaphor recognition/generation. Although LLMs possess impressive capabilities,\ntheir internal mechanisms for processing and representing linguistic knowledge\nremain largely opaque. Prior research on linguistic mechanisms is limited by\ncoarse granularity, limited analysis scale, and narrow focus. In this study, we\npropose LinguaLens, a systematic and comprehensive framework for analyzing the\nlinguistic mechanisms of large language models, based on Sparse Auto-Encoders\n(SAEs). We extract a broad set of Chinese and English linguistic features\nacross four dimensions (morphology, syntax, semantics, and pragmatics). By\nemploying counterfactual methods, we construct a large-scale counterfactual\ndataset of linguistic features for mechanism analysis. Our findings reveal\nintrinsic representations of linguistic knowledge in LLMs, uncover patterns of\ncross-layer and cross-lingual distribution, and demonstrate the potential to\ncontrol model outputs. This work provides a systematic suite of resources and\nmethods for studying linguistic mechanisms, offers strong evidence that LLMs\npossess genuine linguistic knowledge, and lays the foundation for more\ninterpretable and controllable language modeling in future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate exceptional performance on tasks\nrequiring complex linguistic abilities, such as reference disambiguation and\nmetaphor recognition/generation. Although LLMs possess impressive capabilities,\ntheir internal mechanisms for processing and representing linguistic knowledge\nremain largely opaque. Prior research on linguistic mechanisms is limited by\ncoarse granularity, limited analysis scale, and narrow focus. In this study, we\npropose LinguaLens, a systematic and comprehensive framework for analyzing the\nlinguistic mechanisms of large language models, based on Sparse Auto-Encoders\n(SAEs). We extract a broad set of Chinese and English linguistic features\nacross four dimensions (morphology, syntax, semantics, and pragmatics). By\nemploying counterfactual methods, we construct a large-scale counterfactual\ndataset of linguistic features for mechanism analysis. Our findings reveal\nintrinsic representations of linguistic knowledge in LLMs, uncover patterns of\ncross-layer and cross-lingual distribution, and demonstrate the potential to\ncontrol model outputs. This work provides a systematic suite of resources and\nmethods for studying linguistic mechanisms, offers strong evidence that LLMs\npossess genuine linguistic knowledge, and lays the foundation for more\ninterpretable and controllable language modeling in future research."
                },
                "authors": [
                    {
                        "name": "Yi Jing"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Hongzhu Guo"
                    },
                    {
                        "name": "Lingxu Ran"
                    },
                    {
                        "name": "Xiaozhi Wang"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "Accepted by EMNLP 2025 MainConference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20344v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20344v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11947v1",
                "updated": "2025-09-15T14:06:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    6,
                    9,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T14:06:09Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    6,
                    9,
                    0,
                    258,
                    0
                ],
                "title": "A GPU-Accelerated RAG-Based Telegram Assistant for Supporting Parallel\n  Processing Students",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A GPU-Accelerated RAG-Based Telegram Assistant for Supporting Parallel\n  Processing Students"
                },
                "summary": "This project addresses a critical pedagogical need: offering students\ncontinuous, on-demand academic assistance beyond conventional reception hours.\nI present a domain-specific Retrieval-Augmented Generation (RAG) system powered\nby a quantized Mistral-7B Instruct model and deployed as a Telegram bot. The\nassistant enhances learning by delivering real-time, personalized responses\naligned with the \"Introduction to Parallel Processing\" course materials. GPU\nacceleration significantly improves inference latency, enabling practical\ndeployment on consumer hardware. This approach demonstrates how consumer GPUs\ncan enable affordable, private, and effective AI tutoring for HPC education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This project addresses a critical pedagogical need: offering students\ncontinuous, on-demand academic assistance beyond conventional reception hours.\nI present a domain-specific Retrieval-Augmented Generation (RAG) system powered\nby a quantized Mistral-7B Instruct model and deployed as a Telegram bot. The\nassistant enhances learning by delivering real-time, personalized responses\naligned with the \"Introduction to Parallel Processing\" course materials. GPU\nacceleration significantly improves inference latency, enabling practical\ndeployment on consumer hardware. This approach demonstrates how consumer GPUs\ncan enable affordable, private, and effective AI tutoring for HPC education."
                },
                "authors": [
                    {
                        "name": "Guy Tel-Zur"
                    }
                ],
                "author_detail": {
                    "name": "Guy Tel-Zur"
                },
                "author": "Guy Tel-Zur",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11942v1",
                "updated": "2025-09-15T14:02:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    2,
                    29,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T14:02:29Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    2,
                    29,
                    0,
                    258,
                    0
                ],
                "title": "VisDocSketcher: Towards Scalable Visual Documentation with Agentic\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisDocSketcher: Towards Scalable Visual Documentation with Agentic\n  Systems"
                },
                "summary": "Visual documentation is an effective tool for reducing the cognitive barrier\ndevelopers face when understanding unfamiliar code, enabling more intuitive\ncomprehension. Compared to textual documentation, it provides a higher-level\nunderstanding of the system structure and data flow. Developers usually prefer\nvisual representations over lengthy textual descriptions for large software\nsystems. Visual documentation is both difficult to produce and challenging to\nevaluate. Manually creating it is time-consuming, and currently, no existing\napproach can automatically generate high-level visual documentation directly\nfrom code. Its evaluation is often subjective, making it difficult to\nstandardize and automate. To address these challenges, this paper presents the\nfirst exploration of using agentic LLM systems to automatically generate visual\ndocumentation. We introduce VisDocSketcher, the first agent-based approach that\ncombines static analysis with LLM agents to identify key elements in the code\nand produce corresponding visual representations. We propose a novel evaluation\nframework, AutoSketchEval, for assessing the quality of generated visual\ndocumentation using code-level metrics. The experimental results show that our\napproach can valid visual documentation for 74.4% of the samples. It shows an\nimprovement of 26.7-39.8% over a simple template-based baseline. Our evaluation\nframework can reliably distinguish high-quality (code-aligned) visual\ndocumentation from low-quality (non-aligned) ones, achieving an AUC exceeding\n0.87. Our work lays the foundation for future research on automated visual\ndocumentation by introducing practical tools that not only generate valid\nvisual representations but also reliably assess their quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual documentation is an effective tool for reducing the cognitive barrier\ndevelopers face when understanding unfamiliar code, enabling more intuitive\ncomprehension. Compared to textual documentation, it provides a higher-level\nunderstanding of the system structure and data flow. Developers usually prefer\nvisual representations over lengthy textual descriptions for large software\nsystems. Visual documentation is both difficult to produce and challenging to\nevaluate. Manually creating it is time-consuming, and currently, no existing\napproach can automatically generate high-level visual documentation directly\nfrom code. Its evaluation is often subjective, making it difficult to\nstandardize and automate. To address these challenges, this paper presents the\nfirst exploration of using agentic LLM systems to automatically generate visual\ndocumentation. We introduce VisDocSketcher, the first agent-based approach that\ncombines static analysis with LLM agents to identify key elements in the code\nand produce corresponding visual representations. We propose a novel evaluation\nframework, AutoSketchEval, for assessing the quality of generated visual\ndocumentation using code-level metrics. The experimental results show that our\napproach can valid visual documentation for 74.4% of the samples. It shows an\nimprovement of 26.7-39.8% over a simple template-based baseline. Our evaluation\nframework can reliably distinguish high-quality (code-aligned) visual\ndocumentation from low-quality (non-aligned) ones, achieving an AUC exceeding\n0.87. Our work lays the foundation for future research on automated visual\ndocumentation by introducing practical tools that not only generate valid\nvisual representations but also reliably assess their quality."
                },
                "authors": [
                    {
                        "name": "Luís F. Gomes"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Rui Abreu"
                    }
                ],
                "author_detail": {
                    "name": "Rui Abreu"
                },
                "author": "Rui Abreu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11941v1",
                "updated": "2025-09-15T14:01:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    1,
                    22,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T14:01:22Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    1,
                    22,
                    0,
                    258,
                    0
                ],
                "title": "How to Evaluate Medical AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Evaluate Medical AI"
                },
                "summary": "The integration of artificial intelligence (AI) into medical diagnostic\nworkflows requires robust and consistent evaluation methods to ensure\nreliability, clinical relevance, and the inherent variability in expert\njudgments. Traditional metrics like precision and recall often fail to account\nfor the inherent variability in expert judgments, leading to inconsistent\nassessments of AI performance. Inter-rater agreement statistics like Cohen's\nKappa are more reliable but they lack interpretability. We introduce Relative\nPrecision and Recall of Algorithmic Diagnostics (RPAD and RRAD) - a new\nevaluation metrics that compare AI outputs against multiple expert opinions\nrather than a single reference. By normalizing performance against inter-expert\ndisagreement, these metrics provide a more stable and realistic measure of the\nquality of predicted diagnosis. In addition to the comprehensive analysis of\ndiagnostic quality measures, our study contains a very important side result.\nOur evaluation methodology allows us to avoid selecting diagnoses from a\nlimited list when evaluating a given case. Instead, both the models being\ntested and the examiners verifying them arrive at a free-form diagnosis. In\nthis automated methodology for establishing the identity of free-form clinical\ndiagnoses, a remarkable 98% accuracy becomes attainable. We evaluate our\napproach using 360 medical dialogues, comparing multiple large language models\n(LLMs) against a panel of physicians. Large-scale study shows that\ntop-performing models, such as DeepSeek-V3, achieve consistency on par with or\nexceeding expert consensus. Moreover, we demonstrate that expert judgments\nexhibit significant variability - often greater than that between AI and\nhumans. This finding underscores the limitations of any absolute metrics and\nsupports the need to adopt relative metrics in medical AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of artificial intelligence (AI) into medical diagnostic\nworkflows requires robust and consistent evaluation methods to ensure\nreliability, clinical relevance, and the inherent variability in expert\njudgments. Traditional metrics like precision and recall often fail to account\nfor the inherent variability in expert judgments, leading to inconsistent\nassessments of AI performance. Inter-rater agreement statistics like Cohen's\nKappa are more reliable but they lack interpretability. We introduce Relative\nPrecision and Recall of Algorithmic Diagnostics (RPAD and RRAD) - a new\nevaluation metrics that compare AI outputs against multiple expert opinions\nrather than a single reference. By normalizing performance against inter-expert\ndisagreement, these metrics provide a more stable and realistic measure of the\nquality of predicted diagnosis. In addition to the comprehensive analysis of\ndiagnostic quality measures, our study contains a very important side result.\nOur evaluation methodology allows us to avoid selecting diagnoses from a\nlimited list when evaluating a given case. Instead, both the models being\ntested and the examiners verifying them arrive at a free-form diagnosis. In\nthis automated methodology for establishing the identity of free-form clinical\ndiagnoses, a remarkable 98% accuracy becomes attainable. We evaluate our\napproach using 360 medical dialogues, comparing multiple large language models\n(LLMs) against a panel of physicians. Large-scale study shows that\ntop-performing models, such as DeepSeek-V3, achieve consistency on par with or\nexceeding expert consensus. Moreover, we demonstrate that expert judgments\nexhibit significant variability - often greater than that between AI and\nhumans. This finding underscores the limitations of any absolute metrics and\nsupports the need to adopt relative metrics in medical AI."
                },
                "authors": [
                    {
                        "name": "Ilia Kopanichuk"
                    },
                    {
                        "name": "Petr Anokhin"
                    },
                    {
                        "name": "Vladimir Shaposhnikov"
                    },
                    {
                        "name": "Vladimir Makharev"
                    },
                    {
                        "name": "Ekaterina Tsapieva"
                    },
                    {
                        "name": "Iaroslav Bespalov"
                    },
                    {
                        "name": "Dmitry V. Dylov"
                    },
                    {
                        "name": "Ivan Oseledets"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Oseledets"
                },
                "author": "Ivan Oseledets",
                "arxiv_comment": "10 pages, 7 fugures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11126v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11126v2",
                "updated": "2025-09-15T13:59:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    59,
                    49,
                    0,
                    258,
                    0
                ],
                "published": "2025-08-15T00:14:31Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    0,
                    14,
                    31,
                    4,
                    227,
                    0
                ],
                "title": "AI Agentic Programming: A Survey of Techniques, Challenges, and\n  Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Agentic Programming: A Survey of Techniques, Challenges, and\n  Opportunities"
                },
                "summary": "AI agentic programming is an emerging paradigm where large language model\n(LLM)-based coding agents autonomously plan, execute, and interact with tools\nsuch as compilers, debuggers, and version control systems. Unlike conventional\ncode generation, these agents decompose goals, coordinate multi-step processes,\nand adapt based on feedback, reshaping software development practices. This\nsurvey provides a timely review of the field, introducing a taxonomy of agent\nbehaviors and system architectures and examining relevant techniques for\nplanning, context management, tool integration, execution monitoring, and\nbenchmarking datasets. We highlight challenges of this fast-moving field and\ndiscuss opportunities for building reliable, transparent, and collaborative\ncoding agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agentic programming is an emerging paradigm where large language model\n(LLM)-based coding agents autonomously plan, execute, and interact with tools\nsuch as compilers, debuggers, and version control systems. Unlike conventional\ncode generation, these agents decompose goals, coordinate multi-step processes,\nand adapt based on feedback, reshaping software development practices. This\nsurvey provides a timely review of the field, introducing a taxonomy of agent\nbehaviors and system architectures and examining relevant techniques for\nplanning, context management, tool integration, execution monitoring, and\nbenchmarking datasets. We highlight challenges of this fast-moving field and\ndiscuss opportunities for building reliable, transparent, and collaborative\ncoding agents."
                },
                "authors": [
                    {
                        "name": "Huanting Wang"
                    },
                    {
                        "name": "Jingzhi Gong"
                    },
                    {
                        "name": "Huawei Zhang"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Zheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Wang"
                },
                "author": "Zheng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11126v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11126v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11939v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11939v1",
                "updated": "2025-09-15T13:58:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    58,
                    52,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T13:58:52Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    58,
                    52,
                    0,
                    258,
                    0
                ],
                "title": "PrivWeb: Unobtrusive and Content-aware Privacy Protection For Web Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrivWeb: Unobtrusive and Content-aware Privacy Protection For Web Agents"
                },
                "summary": "While web agents gained popularity by automating web interactions, their\nrequirement for interface access introduces significant privacy risks that are\nunderstudied, particularly from users' perspective. Through a formative study\n(N=15), we found users frequently misunderstand agents' data practices, and\ndesired unobtrusive, transparent data management. To achieve this, we designed\nand implemented PrivWeb, a trusted add-on on web agents that utilizes a\nlocalized LLM to anonymize private information on interfaces according to user\npreferences. It features privacy categorization schema and adaptive\nnotifications that selectively pauses tasks for user control over information\ncollection for highly sensitive information, while offering non-disruptive\noptions for less sensitive information, minimizing human oversight. The user\nstudy (N=14) across travel, information retrieval, shopping, and entertainment\ntasks compared PrivWeb with baselines without notification and without control\nfor private information access, where PrivWeb reduced perceived privacy risks\nwith no associated increase in cognitive effort, and resulted in higher overall\nsatisfaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While web agents gained popularity by automating web interactions, their\nrequirement for interface access introduces significant privacy risks that are\nunderstudied, particularly from users' perspective. Through a formative study\n(N=15), we found users frequently misunderstand agents' data practices, and\ndesired unobtrusive, transparent data management. To achieve this, we designed\nand implemented PrivWeb, a trusted add-on on web agents that utilizes a\nlocalized LLM to anonymize private information on interfaces according to user\npreferences. It features privacy categorization schema and adaptive\nnotifications that selectively pauses tasks for user control over information\ncollection for highly sensitive information, while offering non-disruptive\noptions for less sensitive information, minimizing human oversight. The user\nstudy (N=14) across travel, information retrieval, shopping, and entertainment\ntasks compared PrivWeb with baselines without notification and without control\nfor private information access, where PrivWeb reduced perceived privacy risks\nwith no associated increase in cognitive effort, and resulted in higher overall\nsatisfaction."
                },
                "authors": [
                    {
                        "name": "Shuning Zhang"
                    },
                    {
                        "name": "Yutong Jiang"
                    },
                    {
                        "name": "Rongjun Ma"
                    },
                    {
                        "name": "Yuting Yang"
                    },
                    {
                        "name": "Mingyao Xu"
                    },
                    {
                        "name": "Zhixin Huang"
                    },
                    {
                        "name": "Xin Yi"
                    },
                    {
                        "name": "Hewu Li"
                    }
                ],
                "author_detail": {
                    "name": "Hewu Li"
                },
                "author": "Hewu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11939v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11939v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11937v1",
                "updated": "2025-09-15T13:56:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    56,
                    6,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T13:56:06Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    56,
                    6,
                    0,
                    258,
                    0
                ],
                "title": "MMORE: Massive Multimodal Open RAG & Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMORE: Massive Multimodal Open RAG & Extraction"
                },
                "summary": "We introduce MMORE, an open-source pipeline for Massive Multimodal Open\nRetrievalAugmented Generation and Extraction, designed to ingest, transform,\nand retrieve knowledge from heterogeneous document formats at scale. MMORE\nsupports more than fifteen file types, including text, tables, images, emails,\naudio, and video, and processes them into a unified format to enable downstream\napplications for LLMs. The architecture offers modular, distributed processing,\nenabling scalable parallelization across CPUs and GPUs. On processing\nbenchmarks, MMORE demonstrates a 3.8-fold speedup over single-node baselines\nand 40% higher accuracy than Docling on scanned PDFs. The pipeline integrates\nhybrid dense-sparse retrieval and supports both interactive APIs and batch RAG\nendpoints. Evaluated on PubMedQA, MMORE-augmented medical LLMs improve\nbiomedical QA accuracy with increasing retrieval depth. MMORE provides a\nrobust, extensible foundation for deploying task-agnostic RAG systems on\ndiverse, real-world multimodal data. The codebase is available at\nhttps://github.com/swiss-ai/mmore.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MMORE, an open-source pipeline for Massive Multimodal Open\nRetrievalAugmented Generation and Extraction, designed to ingest, transform,\nand retrieve knowledge from heterogeneous document formats at scale. MMORE\nsupports more than fifteen file types, including text, tables, images, emails,\naudio, and video, and processes them into a unified format to enable downstream\napplications for LLMs. The architecture offers modular, distributed processing,\nenabling scalable parallelization across CPUs and GPUs. On processing\nbenchmarks, MMORE demonstrates a 3.8-fold speedup over single-node baselines\nand 40% higher accuracy than Docling on scanned PDFs. The pipeline integrates\nhybrid dense-sparse retrieval and supports both interactive APIs and batch RAG\nendpoints. Evaluated on PubMedQA, MMORE-augmented medical LLMs improve\nbiomedical QA accuracy with increasing retrieval depth. MMORE provides a\nrobust, extensible foundation for deploying task-agnostic RAG systems on\ndiverse, real-world multimodal data. The codebase is available at\nhttps://github.com/swiss-ai/mmore."
                },
                "authors": [
                    {
                        "name": "Alexandre Sallinen"
                    },
                    {
                        "name": "Stefan Krsteski"
                    },
                    {
                        "name": "Paul Teiletche"
                    },
                    {
                        "name": "Marc-Antoine Allard"
                    },
                    {
                        "name": "Baptiste Lecoeur"
                    },
                    {
                        "name": "Michael Zhang"
                    },
                    {
                        "name": "Fabrice Nemo"
                    },
                    {
                        "name": "David Kalajdzic"
                    },
                    {
                        "name": "Matthias Meyer"
                    },
                    {
                        "name": "Mary-Anne Hartley"
                    }
                ],
                "author_detail": {
                    "name": "Mary-Anne Hartley"
                },
                "author": "Mary-Anne Hartley",
                "arxiv_comment": "This paper was originally submitted to the CODEML workshop for ICML\n  2025. 9 pages (including references and appendices)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.0; E.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11928v1",
                "updated": "2025-09-15T13:45:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    45,
                    31,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T13:45:31Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    45,
                    31,
                    0,
                    258,
                    0
                ],
                "title": "Meta-Learning Neural Process for Implied Volatility Surfaces with\n  SABR-induced Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-Learning Neural Process for Implied Volatility Surfaces with\n  SABR-induced Priors"
                },
                "summary": "Constructing the implied volatility surface (IVS) is reframed as a\nmeta-learning problem training across trading days to learn a general process\nthat reconstructs a full IVS from few quotes, eliminating daily recalibration.\nWe introduce the Volatility Neural Process, an attention-based model that uses\na two-stage training: pre-training on SABR-generated surfaces to encode a\nfinancial prior, followed by fine-tuning on market data. On S&P 500 options\n(2006-2023; out-of-sample 2019-2023), our model outperforms SABR, SSVI,\nGaussian Process, and an ablation trained only on real data. Relative to the\nablation, the SABR-induced prior reduces RMSE by about 40% and dominates in\nmid- and long-maturity regions where quotes are sparse. The learned prior\nsuppresses large errors, providing a practical, data-efficient route to stable\nIVS construction with a single deployable model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing the implied volatility surface (IVS) is reframed as a\nmeta-learning problem training across trading days to learn a general process\nthat reconstructs a full IVS from few quotes, eliminating daily recalibration.\nWe introduce the Volatility Neural Process, an attention-based model that uses\na two-stage training: pre-training on SABR-generated surfaces to encode a\nfinancial prior, followed by fine-tuning on market data. On S&P 500 options\n(2006-2023; out-of-sample 2019-2023), our model outperforms SABR, SSVI,\nGaussian Process, and an ablation trained only on real data. Relative to the\nablation, the SABR-induced prior reduces RMSE by about 40% and dominates in\nmid- and long-maturity regions where quotes are sparse. The learned prior\nsuppresses large errors, providing a practical, data-efficient route to stable\nIVS construction with a single deployable model."
                },
                "authors": [
                    {
                        "name": "Jirong Zhuang"
                    },
                    {
                        "name": "Xuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xuan Wu"
                },
                "author": "Xuan Wu",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.CP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11923v1",
                "updated": "2025-09-15T13:38:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    38,
                    18,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T13:38:18Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    38,
                    18,
                    0,
                    258,
                    0
                ],
                "title": "Multi-Stage Location Optimization Through Power Delay Profile Alignment\n  Using Site-Specific Wireless Ray Tracing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Stage Location Optimization Through Power Delay Profile Alignment\n  Using Site-Specific Wireless Ray Tracing"
                },
                "summary": "Ray tracing (RT) simulations require accurate transmitter (TX) and receiver\n(RX) location information from real-world measurements to accurately\ncharacterize wireless propagation behavior in an environment. Such wireless\npropagation measurements typically employ GPS-based logging for TX/RX\nlocations, which can produce meter-level errors that lead to unreliable RT\ncalibration and validation. These location misalignments cause inaccurate\ninteractions between RT-generated multipath components (MPCs) and the modeled\n3D environment, which lead to erroneous channel predictions, and severe\ndiscrepancies between simulated and measured power delay profiles (PDPs) and\nchannel characteristics. Moreover, the same RT-generated PDPs using inaccurate\nlocations result in calibration errors when adjusting material properties such\nas conductivity and permittivity.\n  This paper presents a systematic multi-stage TX/RX location calibration\nframework to correct location errors and consequently align measured and\nsimulated omnidirectional PDPs.\n  Optimization is performed using a computationally efficient multi-stage grid\nsearch and the Powell method. Applying the location calibration framework to\nNYU WIRELESS urban-microcell (UMi) measurements at 6.75 GHz and 16.95 GHz\ncorrected TX/RX location errors of up to 7 m. The framework reduced the\ncomposite loss function by 42.3\\% for line-of-sight (LOS) and 13.5\\% for\nnon-line-of-sight (NLOS) scenarios. Furthermore, peak power prediction accuracy\nimproved by approximately 1 dB on average. Such improved geometric alignment\nenables accurate channel prediction, vital for beam management and\ninfrastructure deployment for next-generation wireless networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ray tracing (RT) simulations require accurate transmitter (TX) and receiver\n(RX) location information from real-world measurements to accurately\ncharacterize wireless propagation behavior in an environment. Such wireless\npropagation measurements typically employ GPS-based logging for TX/RX\nlocations, which can produce meter-level errors that lead to unreliable RT\ncalibration and validation. These location misalignments cause inaccurate\ninteractions between RT-generated multipath components (MPCs) and the modeled\n3D environment, which lead to erroneous channel predictions, and severe\ndiscrepancies between simulated and measured power delay profiles (PDPs) and\nchannel characteristics. Moreover, the same RT-generated PDPs using inaccurate\nlocations result in calibration errors when adjusting material properties such\nas conductivity and permittivity.\n  This paper presents a systematic multi-stage TX/RX location calibration\nframework to correct location errors and consequently align measured and\nsimulated omnidirectional PDPs.\n  Optimization is performed using a computationally efficient multi-stage grid\nsearch and the Powell method. Applying the location calibration framework to\nNYU WIRELESS urban-microcell (UMi) measurements at 6.75 GHz and 16.95 GHz\ncorrected TX/RX location errors of up to 7 m. The framework reduced the\ncomposite loss function by 42.3\\% for line-of-sight (LOS) and 13.5\\% for\nnon-line-of-sight (NLOS) scenarios. Furthermore, peak power prediction accuracy\nimproved by approximately 1 dB on average. Such improved geometric alignment\nenables accurate channel prediction, vital for beam management and\ninfrastructure deployment for next-generation wireless networks."
                },
                "authors": [
                    {
                        "name": "Mingjun Ying"
                    },
                    {
                        "name": "Peijie Ma"
                    },
                    {
                        "name": "Dipankar Shakya"
                    },
                    {
                        "name": "Theodore S. Rappaport"
                    }
                ],
                "author_detail": {
                    "name": "Theodore S. Rappaport"
                },
                "author": "Theodore S. Rappaport",
                "arxiv_comment": "6 pages, 3 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11921v1",
                "updated": "2025-09-15T13:37:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    37,
                    35,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T13:37:35Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    37,
                    35,
                    0,
                    258,
                    0
                ],
                "title": "Designing LLMs for cultural sensitivity: Evidence from English-Japanese\n  translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing LLMs for cultural sensitivity: Evidence from English-Japanese\n  translation"
                },
                "summary": "Large language models (LLMs) are increasingly used in everyday communication,\nincluding multilingual interactions across different cultural contexts. While\nLLMs can now generate near-perfect literal translations, it remains unclear\nwhether LLMs support culturally appropriate communication. In this paper, we\nanalyze the cultural sensitivity of different LLM designs when applied to\nEnglish-Japanese translations of workplace e-mails. Here, we vary the prompting\nstrategies: (1) naive \"just translate\" prompts, (2) audience-targeted prompts\nspecifying the recipient's cultural background, and (3) instructional prompts\nwith explicit guidance on Japanese communication norms. Using a mixed-methods\nstudy, we then analyze culture-specific language patterns to evaluate how well\ntranslations adapt to cultural norms. Further, we examine the appropriateness\nof the tone of the translations as perceived by native speakers. We find that\nculturally-tailored prompting can improve cultural fit, based on which we offer\nrecommendations for designing culturally inclusive LLMs in multilingual\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used in everyday communication,\nincluding multilingual interactions across different cultural contexts. While\nLLMs can now generate near-perfect literal translations, it remains unclear\nwhether LLMs support culturally appropriate communication. In this paper, we\nanalyze the cultural sensitivity of different LLM designs when applied to\nEnglish-Japanese translations of workplace e-mails. Here, we vary the prompting\nstrategies: (1) naive \"just translate\" prompts, (2) audience-targeted prompts\nspecifying the recipient's cultural background, and (3) instructional prompts\nwith explicit guidance on Japanese communication norms. Using a mixed-methods\nstudy, we then analyze culture-specific language patterns to evaluate how well\ntranslations adapt to cultural norms. Further, we examine the appropriateness\nof the tone of the translations as perceived by native speakers. We find that\nculturally-tailored prompting can improve cultural fit, based on which we offer\nrecommendations for designing culturally inclusive LLMs in multilingual\nsettings."
                },
                "authors": [
                    {
                        "name": "Helene Tenzer"
                    },
                    {
                        "name": "Oumnia Abidi"
                    },
                    {
                        "name": "Stefan Feuerriegel"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Feuerriegel"
                },
                "author": "Stefan Feuerriegel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10318v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10318v3",
                "updated": "2025-09-15T13:36:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    36,
                    53,
                    0,
                    258,
                    0
                ],
                "published": "2025-08-14T03:46:24Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    3,
                    46,
                    24,
                    3,
                    226,
                    0
                ],
                "title": "Quantifying the Value of Seismic Structural Health Monitoring for\n  post-earthquake recovery of electric power system in terms of resilience\n  enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying the Value of Seismic Structural Health Monitoring for\n  post-earthquake recovery of electric power system in terms of resilience\n  enhancement"
                },
                "summary": "Post-earthquake recovery of electric power networks (EPNs) is critical to\ncommunity resilience. Traditional recovery processes often rely on prolonged\nand imprecise manual inspections for damage diagnosis, leading to suboptimal\nrepair prioritization and extended service disruptions. Seismic Structural\nHealth Monitoring (SSHM) offers the potential to expedite recovery by enabling\nmore accurate and timely damage assessment. However, SSHM deployment incurs\ncosts, and its system-level resilience benefit remains underexplored. This\nstudy proposes a probabilistic simulation framework to quantify the value of\nSSHM for enhancing EPN resilience. The framework includes seismic damage\nmodeling based on network configuration, hazard intensity, fragility functions,\nand damage-functionality mappings, combined with recovery simulations\nincorporating resource constraints, repair and transfer durations. System\nfunctionality is evaluated using graph-based island detection and optimal power\nflow analysis. Resilience is quantified via the Lack of Resilience (LoR) metric\nderived from the functionality restoration curve. SSHM is incorporated by\naltering the quality of damage information used in repair scheduling. Different\nmonitoring scenarios (e.g., no-SSHM baseline, partial SSHM, full SSHM with\nvarious accuracies) are modeled using confusion matrices to simulate damage\nmisclassification. Results show that improved damage awareness via SSHM\nsignificantly accelerates recovery and reduces LoR by up to 21%. This work\nsupports evidence-based decisions for SSHM deployment in critical\ninfrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-earthquake recovery of electric power networks (EPNs) is critical to\ncommunity resilience. Traditional recovery processes often rely on prolonged\nand imprecise manual inspections for damage diagnosis, leading to suboptimal\nrepair prioritization and extended service disruptions. Seismic Structural\nHealth Monitoring (SSHM) offers the potential to expedite recovery by enabling\nmore accurate and timely damage assessment. However, SSHM deployment incurs\ncosts, and its system-level resilience benefit remains underexplored. This\nstudy proposes a probabilistic simulation framework to quantify the value of\nSSHM for enhancing EPN resilience. The framework includes seismic damage\nmodeling based on network configuration, hazard intensity, fragility functions,\nand damage-functionality mappings, combined with recovery simulations\nincorporating resource constraints, repair and transfer durations. System\nfunctionality is evaluated using graph-based island detection and optimal power\nflow analysis. Resilience is quantified via the Lack of Resilience (LoR) metric\nderived from the functionality restoration curve. SSHM is incorporated by\naltering the quality of damage information used in repair scheduling. Different\nmonitoring scenarios (e.g., no-SSHM baseline, partial SSHM, full SSHM with\nvarious accuracies) are modeled using confusion matrices to simulate damage\nmisclassification. Results show that improved damage awareness via SSHM\nsignificantly accelerates recovery and reduces LoR by up to 21%. This work\nsupports evidence-based decisions for SSHM deployment in critical\ninfrastructure."
                },
                "authors": [
                    {
                        "name": "Huangbin Liang"
                    },
                    {
                        "name": "Beatriz Moya"
                    },
                    {
                        "name": "Francisco Chinesta"
                    },
                    {
                        "name": "Eleni Chatzi"
                    }
                ],
                "author_detail": {
                    "name": "Eleni Chatzi"
                },
                "author": "Eleni Chatzi",
                "arxiv_comment": "21 pages. 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10318v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10318v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "90B25, 90C35, 62P30, 93C95",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11916v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11916v1",
                "updated": "2025-09-15T13:33:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    33,
                    54,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T13:33:54Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    33,
                    54,
                    0,
                    258,
                    0
                ],
                "title": "NeuroGaze-Distill: Brain-informed Distillation and Depression-Inspired\n  Geometric Priors for Robust Facial Emotion Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuroGaze-Distill: Brain-informed Distillation and Depression-Inspired\n  Geometric Priors for Robust Facial Emotion Recognition"
                },
                "summary": "Facial emotion recognition (FER) models trained only on pixels often fail to\ngeneralize across datasets because facial appearance is an indirect and biased\nproxy for underlying affect. We present NeuroGaze-Distill, a cross-modal\ndistillation framework that transfers brain-informed priors into an image-only\nFER student via static Valence/Arousal (V/A) prototypes and a\ndepression-inspired geometric prior (D-Geo). A teacher trained on EEG\ntopographic maps from DREAMER (with MAHNOB-HCI as unlabeled support) produces a\nconsolidated 5x5 V/A prototype grid that is frozen and reused; no EEG-face\npairing and no non-visual signals at deployment are required. The student\n(ResNet-18/50) is trained on FERPlus with conventional CE/KD and two\nlightweight regularizers: (i) Proto-KD (cosine) aligns student features to the\nstatic prototypes; (ii) D-Geo softly shapes the embedding geometry in line with\naffective findings often reported in depression research (e.g., anhedonia-like\ncontraction in high-valence regions). We evaluate both within-domain (FERPlus\nvalidation) and cross-dataset protocols (AffectNet-mini; optional CK+),\nreporting standard 8-way scores alongside present-only Macro-F1 and balanced\naccuracy to fairly handle label-set mismatch. Ablations attribute consistent\ngains to prototypes and D-Geo, and favor 5x5 over denser grids for stability.\nThe method is simple, deployable, and improves robustness without architectural\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facial emotion recognition (FER) models trained only on pixels often fail to\ngeneralize across datasets because facial appearance is an indirect and biased\nproxy for underlying affect. We present NeuroGaze-Distill, a cross-modal\ndistillation framework that transfers brain-informed priors into an image-only\nFER student via static Valence/Arousal (V/A) prototypes and a\ndepression-inspired geometric prior (D-Geo). A teacher trained on EEG\ntopographic maps from DREAMER (with MAHNOB-HCI as unlabeled support) produces a\nconsolidated 5x5 V/A prototype grid that is frozen and reused; no EEG-face\npairing and no non-visual signals at deployment are required. The student\n(ResNet-18/50) is trained on FERPlus with conventional CE/KD and two\nlightweight regularizers: (i) Proto-KD (cosine) aligns student features to the\nstatic prototypes; (ii) D-Geo softly shapes the embedding geometry in line with\naffective findings often reported in depression research (e.g., anhedonia-like\ncontraction in high-valence regions). We evaluate both within-domain (FERPlus\nvalidation) and cross-dataset protocols (AffectNet-mini; optional CK+),\nreporting standard 8-way scores alongside present-only Macro-F1 and balanced\naccuracy to fairly handle label-set mismatch. Ablations attribute consistent\ngains to prototypes and D-Geo, and favor 5x5 over denser grids for stability.\nThe method is simple, deployable, and improves robustness without architectural\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Zilin Li"
                    },
                    {
                        "name": "Weiwei Xu"
                    },
                    {
                        "name": "Xuanqi Zhao"
                    },
                    {
                        "name": "Yiran Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Zhu"
                },
                "author": "Yiran Zhu",
                "arxiv_comment": "Preprint. Vision-only deployment; EEG used only to form static\n  prototypes. Includes appendix, 7 figures and 3 tables. Considering submission\n  to the International Conference on Learning Representations (ICLR) 2026, Rio\n  de Janeiro, Brazil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11916v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.4.8; I.5.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11915v1",
                "updated": "2025-09-15T13:33:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    33,
                    32,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T13:33:32Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    33,
                    32,
                    0,
                    258,
                    0
                ],
                "title": "Uncertainty in Authorship: Why Perfect AI Detection Is Mathematically\n  Impossible",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty in Authorship: Why Perfect AI Detection Is Mathematically\n  Impossible"
                },
                "summary": "As large language models (LLMs) become more advanced, it is increasingly\ndifficult to distinguish between human-written and AI-generated text. This\npaper draws a conceptual parallel between quantum uncertainty and the limits of\nauthorship detection in natural language. We argue that there is a fundamental\ntrade-off: the more confidently one tries to identify whether a text was\nwritten by a human or an AI, the more one risks disrupting the text's natural\nflow and authenticity. This mirrors the tension between precision and\ndisturbance found in quantum systems. We explore how current detection\nmethods--such as stylometry, watermarking, and neural classifiers--face\ninherent limitations. Enhancing detection accuracy often leads to changes in\nthe AI's output, making other features less reliable. In effect, the very act\nof trying to detect AI authorship introduces uncertainty elsewhere in the text.\nOur analysis shows that when AI-generated text closely mimics human writing,\nperfect detection becomes not just technologically difficult but theoretically\nimpossible. We address counterarguments and discuss the broader implications\nfor authorship, ethics, and policy. Ultimately, we suggest that the challenge\nof AI-text detection is not just a matter of better tools--it reflects a\ndeeper, unavoidable tension in the nature of language itself.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become more advanced, it is increasingly\ndifficult to distinguish between human-written and AI-generated text. This\npaper draws a conceptual parallel between quantum uncertainty and the limits of\nauthorship detection in natural language. We argue that there is a fundamental\ntrade-off: the more confidently one tries to identify whether a text was\nwritten by a human or an AI, the more one risks disrupting the text's natural\nflow and authenticity. This mirrors the tension between precision and\ndisturbance found in quantum systems. We explore how current detection\nmethods--such as stylometry, watermarking, and neural classifiers--face\ninherent limitations. Enhancing detection accuracy often leads to changes in\nthe AI's output, making other features less reliable. In effect, the very act\nof trying to detect AI authorship introduces uncertainty elsewhere in the text.\nOur analysis shows that when AI-generated text closely mimics human writing,\nperfect detection becomes not just technologically difficult but theoretically\nimpossible. We address counterarguments and discuss the broader implications\nfor authorship, ethics, and policy. Ultimately, we suggest that the challenge\nof AI-text detection is not just a matter of better tools--it reflects a\ndeeper, unavoidable tension in the nature of language itself."
                },
                "authors": [
                    {
                        "name": "Aadil Gani Ganie"
                    }
                ],
                "author_detail": {
                    "name": "Aadil Gani Ganie"
                },
                "author": "Aadil Gani Ganie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11914v1",
                "updated": "2025-09-15T13:33:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    33,
                    29,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T13:33:29Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    33,
                    29,
                    0,
                    258,
                    0
                ],
                "title": "EgoMem: Lifelong Memory Agent for Full-duplex Omnimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EgoMem: Lifelong Memory Agent for Full-duplex Omnimodal Models"
                },
                "summary": "We introduce EgoMem, the first lifelong memory agent tailored for full-duplex\nmodels that process real-time omnimodal streams. EgoMem enables real-time\nmodels to recognize multiple users directly from raw audiovisual streams, to\nprovide personalized response, and to maintain long-term knowledge of users'\nfacts, preferences, and social relationships extracted from audiovisual\nhistory. EgoMem operates with three asynchronous processes: (i) a retrieval\nprocess that dynamically identifies user via face and voice, and gathers\nrelevant context from a long-term memory; (ii) an omnimodal dialog process that\ngenerates personalized audio responses based on the retrieved context; and\n(iii) a memory management process that automatically detects dialog boundaries\nfrom omnimodal streams, and extracts necessary information to update the\nlong-term memory. Unlike existing memory agents for LLMs, EgoMem relies\nentirely on raw audiovisual streams, making it especially suitable for\nlifelong, real-time, and embodied scenarios. Experimental results demonstrate\nthat EgoMem's retrieval and memory management modules achieve over 95% accuracy\non the test set. When integrated with a fine-tuned RoboEgo omnimodal chatbot,\nthe system achieves fact-consistency scores above 87% in real-time personalized\ndialogs, establishing a strong baseline for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce EgoMem, the first lifelong memory agent tailored for full-duplex\nmodels that process real-time omnimodal streams. EgoMem enables real-time\nmodels to recognize multiple users directly from raw audiovisual streams, to\nprovide personalized response, and to maintain long-term knowledge of users'\nfacts, preferences, and social relationships extracted from audiovisual\nhistory. EgoMem operates with three asynchronous processes: (i) a retrieval\nprocess that dynamically identifies user via face and voice, and gathers\nrelevant context from a long-term memory; (ii) an omnimodal dialog process that\ngenerates personalized audio responses based on the retrieved context; and\n(iii) a memory management process that automatically detects dialog boundaries\nfrom omnimodal streams, and extracts necessary information to update the\nlong-term memory. Unlike existing memory agents for LLMs, EgoMem relies\nentirely on raw audiovisual streams, making it especially suitable for\nlifelong, real-time, and embodied scenarios. Experimental results demonstrate\nthat EgoMem's retrieval and memory management modules achieve over 95% accuracy\non the test set. When integrated with a fine-tuned RoboEgo omnimodal chatbot,\nthe system achieves fact-consistency scores above 87% in real-time personalized\ndialogs, establishing a strong baseline for future research."
                },
                "authors": [
                    {
                        "name": "Yiqun Yao"
                    },
                    {
                        "name": "Naitong Yu"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Xuezhi Fang"
                    },
                    {
                        "name": "Wenjia Ma"
                    },
                    {
                        "name": "Xuying Meng"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Aixin Sun"
                    },
                    {
                        "name": "Yequan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yequan Wang"
                },
                "author": "Yequan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01791v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01791v2",
                "updated": "2025-09-15T13:13:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    13,
                    33,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-01T21:41:34Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    21,
                    41,
                    34,
                    0,
                    244,
                    0
                ],
                "title": "E-PhishGen: Unlocking Novel Research in Phishing Email Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-PhishGen: Unlocking Novel Research in Phishing Email Detection"
                },
                "summary": "Every day, our inboxes are flooded with unsolicited emails, ranging between\nannoying spam to more subtle phishing scams. Unfortunately, despite abundant\nprior efforts proposing solutions achieving near-perfect accuracy, the reality\nis that countering malicious emails still remains an unsolved dilemma.\n  This \"open problem\" paper carries out a critical assessment of scientific\nworks in the context of phishing email detection. First, we focus on the\nbenchmark datasets that have been used to assess the methods proposed in\nresearch. We find that most prior work relied on datasets containing emails\nthat -- we argue -- are not representative of current trends, and mostly\nencompass the English language. Based on this finding, we then re-implement and\nre-assess a variety of detection methods reliant on machine learning (ML),\nincluding large-language models (LLM), and release all of our codebase -- an\n(unfortunately) uncommon practice in related research. We show that most such\nmethods achieve near-perfect performance when trained and tested on the same\ndataset -- a result which intrinsically hinders development (how can future\nresearch outperform methods that are already near perfect?). To foster the\ncreation of \"more challenging benchmarks\" that reflect current phishing trends,\nwe propose E-PhishGEN, an LLM-based (and privacy-savvy) framework to generate\nnovel phishing-email datasets. We use our E-PhishGEN to create E-PhishLLM, a\nnovel phishing-email detection dataset containing 16616 emails in three\nlanguages. We use E-PhishLLM to test the detectors we considered, showing a\nmuch lower performance than that achieved on existing benchmarks -- indicating\na larger room for improvement. We also validate the quality of E-PhishLLM with\na user study (n=30). To sum up, we show that phishing email detection is still\nan open problem -- and provide the means to tackle such a problem by future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Every day, our inboxes are flooded with unsolicited emails, ranging between\nannoying spam to more subtle phishing scams. Unfortunately, despite abundant\nprior efforts proposing solutions achieving near-perfect accuracy, the reality\nis that countering malicious emails still remains an unsolved dilemma.\n  This \"open problem\" paper carries out a critical assessment of scientific\nworks in the context of phishing email detection. First, we focus on the\nbenchmark datasets that have been used to assess the methods proposed in\nresearch. We find that most prior work relied on datasets containing emails\nthat -- we argue -- are not representative of current trends, and mostly\nencompass the English language. Based on this finding, we then re-implement and\nre-assess a variety of detection methods reliant on machine learning (ML),\nincluding large-language models (LLM), and release all of our codebase -- an\n(unfortunately) uncommon practice in related research. We show that most such\nmethods achieve near-perfect performance when trained and tested on the same\ndataset -- a result which intrinsically hinders development (how can future\nresearch outperform methods that are already near perfect?). To foster the\ncreation of \"more challenging benchmarks\" that reflect current phishing trends,\nwe propose E-PhishGEN, an LLM-based (and privacy-savvy) framework to generate\nnovel phishing-email datasets. We use our E-PhishGEN to create E-PhishLLM, a\nnovel phishing-email detection dataset containing 16616 emails in three\nlanguages. We use E-PhishLLM to test the detectors we considered, showing a\nmuch lower performance than that achieved on existing benchmarks -- indicating\na larger room for improvement. We also validate the quality of E-PhishLLM with\na user study (n=30). To sum up, we show that phishing email detection is still\nan open problem -- and provide the means to tackle such a problem by future\nresearch."
                },
                "authors": [
                    {
                        "name": "Luca Pajola"
                    },
                    {
                        "name": "Eugenio Caripoti"
                    },
                    {
                        "name": "Stefan Banzer"
                    },
                    {
                        "name": "Simeone Pizzi"
                    },
                    {
                        "name": "Mauro Conti"
                    },
                    {
                        "name": "Giovanni Apruzzese"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Apruzzese"
                },
                "author": "Giovanni Apruzzese",
                "arxiv_doi": "10.1145/3733799.3762967",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3733799.3762967",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.01791v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01791v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to ACM AISec '25",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05755v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05755v2",
                "updated": "2025-09-15T13:11:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    11,
                    40,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-06T15:48:49Z",
                "published_parsed": [
                    2025,
                    9,
                    6,
                    15,
                    48,
                    49,
                    5,
                    249,
                    0
                ],
                "title": "Exploit Tool Invocation Prompt for Tool Behavior Hijacking in LLM-Based\n  Agentic System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploit Tool Invocation Prompt for Tool Behavior Hijacking in LLM-Based\n  Agentic System"
                },
                "summary": "LLM-based agentic systems leverage large language models to handle user\nqueries, make decisions, and execute external tools for complex tasks across\ndomains like chatbots, customer service, and software engineering. A critical\ncomponent of these systems is the Tool Invocation Prompt (TIP), which defines\ntool interaction protocols and guides LLMs to ensure the security and\ncorrectness of tool usage. Despite its importance, TIP security has been\nlargely overlooked. This work investigates TIP-related security risks,\nrevealing that major LLM-based systems like Cursor, Claude Code, and others are\nvulnerable to attacks such as remote code execution (RCE) and denial of service\n(DoS). Through a systematic TIP exploitation workflow (TEW), we demonstrate\nexternal tool behavior hijacking via manipulated tool invocations. We also\npropose defense mechanisms to enhance TIP security in LLM-based agentic\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agentic systems leverage large language models to handle user\nqueries, make decisions, and execute external tools for complex tasks across\ndomains like chatbots, customer service, and software engineering. A critical\ncomponent of these systems is the Tool Invocation Prompt (TIP), which defines\ntool interaction protocols and guides LLMs to ensure the security and\ncorrectness of tool usage. Despite its importance, TIP security has been\nlargely overlooked. This work investigates TIP-related security risks,\nrevealing that major LLM-based systems like Cursor, Claude Code, and others are\nvulnerable to attacks such as remote code execution (RCE) and denial of service\n(DoS). Through a systematic TIP exploitation workflow (TEW), we demonstrate\nexternal tool behavior hijacking via manipulated tool invocations. We also\npropose defense mechanisms to enhance TIP security in LLM-based agentic\nsystems."
                },
                "authors": [
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Yuchong Xie"
                    },
                    {
                        "name": "Mingyu Luo"
                    },
                    {
                        "name": "Zesen Liu"
                    },
                    {
                        "name": "Zhixiang Zhang"
                    },
                    {
                        "name": "Kaikai Zhang"
                    },
                    {
                        "name": "Zongjie Li"
                    },
                    {
                        "name": "Ping Chen"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Dongdong She"
                    }
                ],
                "author_detail": {
                    "name": "Dongdong She"
                },
                "author": "Dongdong She",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05755v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05755v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11878v1",
                "updated": "2025-09-15T12:58:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    58,
                    38,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T12:58:38Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    58,
                    38,
                    0,
                    258,
                    0
                ],
                "title": "Do It Yourself (DIY): Modifying Images for Poems in a Zero-Shot Setting\n  Using Weighted Prompt Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do It Yourself (DIY): Modifying Images for Poems in a Zero-Shot Setting\n  Using Weighted Prompt Manipulation"
                },
                "summary": "Poetry is an expressive form of art that invites multiple interpretations, as\nreaders often bring their own emotions, experiences, and cultural backgrounds\ninto their understanding of a poem. Recognizing this, we aim to generate images\nfor poems and improve these images in a zero-shot setting, enabling audiences\nto modify images as per their requirements. To achieve this, we introduce a\nnovel Weighted Prompt Manipulation (WPM) technique, which systematically\nmodifies attention weights and text embeddings within diffusion models. By\ndynamically adjusting the importance of specific words, WPM enhances or\nsuppresses their influence in the final generated image, leading to\nsemantically richer and more contextually accurate visualizations. Our approach\nexploits diffusion models and large language models (LLMs) such as GPT in\nconjunction with existing poetry datasets, ensuring a comprehensive and\nstructured methodology for improved image generation in the literary domain. To\nthe best of our knowledge, this is the first attempt at integrating weighted\nprompt manipulation for enhancing imagery in poetic language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Poetry is an expressive form of art that invites multiple interpretations, as\nreaders often bring their own emotions, experiences, and cultural backgrounds\ninto their understanding of a poem. Recognizing this, we aim to generate images\nfor poems and improve these images in a zero-shot setting, enabling audiences\nto modify images as per their requirements. To achieve this, we introduce a\nnovel Weighted Prompt Manipulation (WPM) technique, which systematically\nmodifies attention weights and text embeddings within diffusion models. By\ndynamically adjusting the importance of specific words, WPM enhances or\nsuppresses their influence in the final generated image, leading to\nsemantically richer and more contextually accurate visualizations. Our approach\nexploits diffusion models and large language models (LLMs) such as GPT in\nconjunction with existing poetry datasets, ensuring a comprehensive and\nstructured methodology for improved image generation in the literary domain. To\nthe best of our knowledge, this is the first attempt at integrating weighted\nprompt manipulation for enhancing imagery in poetic language."
                },
                "authors": [
                    {
                        "name": "Sofia Jamil"
                    },
                    {
                        "name": "Kotla Sai Charan"
                    },
                    {
                        "name": "Sriparna Saha"
                    },
                    {
                        "name": "Koustava Goswami"
                    },
                    {
                        "name": "K J Joseph"
                    }
                ],
                "author_detail": {
                    "name": "K J Joseph"
                },
                "author": "K J Joseph",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11868v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11868v1",
                "updated": "2025-09-15T12:39:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    39,
                    55,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T12:39:55Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    39,
                    55,
                    0,
                    258,
                    0
                ],
                "title": "Growing Perspectives: Modelling Embodied Perspective Taking and Inner\n  Narrative Development Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Growing Perspectives: Modelling Embodied Perspective Taking and Inner\n  Narrative Development Using Large Language Models"
                },
                "summary": "Language and embodied perspective taking are essential for human\ncollaboration, yet few computational models address both simultaneously. This\nwork investigates the PerspAct system [1], which integrates the ReAct (Reason\nand Act) paradigm with Large Language Models (LLMs) to simulate developmental\nstages of perspective taking, grounded in Selman's theory [2]. Using an\nextended director task, we evaluate GPT's ability to generate internal\nnarratives aligned with specified developmental stages, and assess how these\ninfluence collaborative performance both qualitatively (action selection) and\nquantitatively (task efficiency). Results show that GPT reliably produces\ndevelopmentally-consistent narratives before task execution but often shifts\ntowards more advanced stages during interaction, suggesting that language\nexchanges help refine internal representations. Higher developmental stages\ngenerally enhance collaborative effectiveness, while earlier stages yield more\nvariable outcomes in complex contexts. These findings highlight the potential\nof integrating embodied perspective taking and language in LLMs to better model\ndevelopmental dynamics and stress the importance of evaluating internal speech\nduring combined linguistic and embodied tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language and embodied perspective taking are essential for human\ncollaboration, yet few computational models address both simultaneously. This\nwork investigates the PerspAct system [1], which integrates the ReAct (Reason\nand Act) paradigm with Large Language Models (LLMs) to simulate developmental\nstages of perspective taking, grounded in Selman's theory [2]. Using an\nextended director task, we evaluate GPT's ability to generate internal\nnarratives aligned with specified developmental stages, and assess how these\ninfluence collaborative performance both qualitatively (action selection) and\nquantitatively (task efficiency). Results show that GPT reliably produces\ndevelopmentally-consistent narratives before task execution but often shifts\ntowards more advanced stages during interaction, suggesting that language\nexchanges help refine internal representations. Higher developmental stages\ngenerally enhance collaborative effectiveness, while earlier stages yield more\nvariable outcomes in complex contexts. These findings highlight the potential\nof integrating embodied perspective taking and language in LLMs to better model\ndevelopmental dynamics and stress the importance of evaluating internal speech\nduring combined linguistic and embodied tasks."
                },
                "authors": [
                    {
                        "name": "Sabrina Patania"
                    },
                    {
                        "name": "Luca Annese"
                    },
                    {
                        "name": "Anna Lambiase"
                    },
                    {
                        "name": "Anita Pellegrini"
                    },
                    {
                        "name": "Tom Foulsham"
                    },
                    {
                        "name": "Azzurra Ruggeri"
                    },
                    {
                        "name": "Silvia Rossi"
                    },
                    {
                        "name": "Silvia Serino"
                    },
                    {
                        "name": "Dimitri Ognibene"
                    }
                ],
                "author_detail": {
                    "name": "Dimitri Ognibene"
                },
                "author": "Dimitri Ognibene",
                "arxiv_comment": "Accepted at ICDL https://icdl2025.fel.cvut.cz/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11868v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11868v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.2.7; I.2.10; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11867v1",
                "updated": "2025-09-15T12:39:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    39,
                    51,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T12:39:51Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    39,
                    51,
                    0,
                    258,
                    0
                ],
                "title": "Letter of Intent: 100m Atom Interferometer Experiment at CERN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Letter of Intent: 100m Atom Interferometer Experiment at CERN"
                },
                "summary": "We propose an O(100)m Atom Interferometer (AI) experiment to be installed\nagainst a wall of the PX46 access shaft to the LHC. This experiment would probe\nunexplored ranges of the possible couplings of bosonic ultralight dark matter\n(ULDM) to atomic constituents and undertake a pioneering search for\ngravitational waves (GWs) at frequencies intermediate between those to which\nexisting and planned experiments are sensitive, among other fundamental physics\nstudies. A conceptual feasibility study showed that this AI experiment could be\nisolated from the LHC by installing a shielding wall in the TX46 gallery, and\nsurveyed issues related to the proximity of the LHC machine, finding no\ntechnical obstacles. A detailed technical implementation study has shown that\nthe preparatory civil-engineering work, installation of bespoke radiation\nshielding, deployment of access-control systems and safety alarms, and\ninstallation of an elevator platform could be carried out during LS3, allowing\ninstallation and operation of the detector to proceed during Run 4 without\nimpacting HL-LHC operation. These studies have established that PX46 is a\nuniquely promising location for an AI experiment. We foresee that, if the CERN\nmanagement encourages this Letter of Intent, a significant fraction of the\nTerrestrial Very Long Baseline Atom Interferometer (TVLBAI) Proto-Collaboration\nmay wish to contribute to such an AI experiment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an O(100)m Atom Interferometer (AI) experiment to be installed\nagainst a wall of the PX46 access shaft to the LHC. This experiment would probe\nunexplored ranges of the possible couplings of bosonic ultralight dark matter\n(ULDM) to atomic constituents and undertake a pioneering search for\ngravitational waves (GWs) at frequencies intermediate between those to which\nexisting and planned experiments are sensitive, among other fundamental physics\nstudies. A conceptual feasibility study showed that this AI experiment could be\nisolated from the LHC by installing a shielding wall in the TX46 gallery, and\nsurveyed issues related to the proximity of the LHC machine, finding no\ntechnical obstacles. A detailed technical implementation study has shown that\nthe preparatory civil-engineering work, installation of bespoke radiation\nshielding, deployment of access-control systems and safety alarms, and\ninstallation of an elevator platform could be carried out during LS3, allowing\ninstallation and operation of the detector to proceed during Run 4 without\nimpacting HL-LHC operation. These studies have established that PX46 is a\nuniquely promising location for an AI experiment. We foresee that, if the CERN\nmanagement encourages this Letter of Intent, a significant fraction of the\nTerrestrial Very Long Baseline Atom Interferometer (TVLBAI) Proto-Collaboration\nmay wish to contribute to such an AI experiment."
                },
                "authors": [
                    {
                        "name": "Charles Baynham"
                    },
                    {
                        "name": "Andrea Bertoldi"
                    },
                    {
                        "name": "Diego Blas"
                    },
                    {
                        "name": "Oliver Buchmueller"
                    },
                    {
                        "name": "Sergio Calatroni"
                    },
                    {
                        "name": "Vassilis Charmandaris"
                    },
                    {
                        "name": "Maria Luisa Chiofalo"
                    },
                    {
                        "name": "Pierre Cladé"
                    },
                    {
                        "name": "Jonathon Coleman"
                    },
                    {
                        "name": "Fabio Di Pumpo"
                    },
                    {
                        "name": "John Ellis"
                    },
                    {
                        "name": "Naceur Gaaloul"
                    },
                    {
                        "name": "Saïda Guellati-Khelifa"
                    },
                    {
                        "name": "Tiffany Harte"
                    },
                    {
                        "name": "Richard Hobson"
                    },
                    {
                        "name": "Michael Holynski"
                    },
                    {
                        "name": "Samuel Lellouch"
                    },
                    {
                        "name": "Lucas Lombriser"
                    },
                    {
                        "name": "Elias Lopez Asamar"
                    },
                    {
                        "name": "Michele Maggiore"
                    },
                    {
                        "name": "Christopher McCabe"
                    },
                    {
                        "name": "Jeremiah Mitchell"
                    },
                    {
                        "name": "Ernst M. Rasel"
                    },
                    {
                        "name": "Federico Sanchez Nieto"
                    },
                    {
                        "name": "Wolfgang Schleich"
                    },
                    {
                        "name": "Dennis Schlippert"
                    },
                    {
                        "name": "Ulrich Schneider"
                    },
                    {
                        "name": "Steven Schramm"
                    },
                    {
                        "name": "Marcelle Soares-Santos"
                    },
                    {
                        "name": "Guglielmo M. Tino"
                    },
                    {
                        "name": "Jonathan N. Tinsley"
                    },
                    {
                        "name": "Tristan Valenzuela"
                    },
                    {
                        "name": "Maurits van der Grinten"
                    },
                    {
                        "name": "Wolf von Klitzing"
                    }
                ],
                "author_detail": {
                    "name": "Wolf von Klitzing"
                },
                "author": "Wolf von Klitzing",
                "arxiv_comment": "16 pages, including figures and appendices. Submitted to CERN LHCC as\n  Letter of Intent",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04427v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04427v2",
                "updated": "2025-09-15T12:38:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    38,
                    51,
                    0,
                    258,
                    0
                ],
                "published": "2025-06-04T20:21:52Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    20,
                    21,
                    52,
                    2,
                    155,
                    0
                ],
                "title": "Plugging Schema Graph into Multi-Table QA: A Human-Guided Framework for\n  Reducing LLM Reliance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plugging Schema Graph into Multi-Table QA: A Human-Guided Framework for\n  Reducing LLM Reliance"
                },
                "summary": "Large language models (LLMs) have shown promise in table Question Answering\n(Table QA). However, extending these capabilities to multi-table QA remains\nchallenging due to unreliable schema linking across complex tables. Existing\nmethods based on semantic similarity work well only on simplified hand-crafted\ndatasets and struggle to handle complex, real-world scenarios with numerous and\ndiverse columns. To address this, we propose a graph-based framework that\nleverages human-curated relational knowledge to explicitly encode schema links\nand join paths. Given a natural language query, our method searches on graph to\nconstruct interpretable reasoning chains, aided by pruning and sub-path merging\nstrategies to enhance efficiency and coherence. Experiments on both standard\nbenchmarks and a realistic, large-scale dataset demonstrate the effectiveness\nof our approach. To our knowledge, this is the first multi-table QA system\napplied to truly complex industrial tabular data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promise in table Question Answering\n(Table QA). However, extending these capabilities to multi-table QA remains\nchallenging due to unreliable schema linking across complex tables. Existing\nmethods based on semantic similarity work well only on simplified hand-crafted\ndatasets and struggle to handle complex, real-world scenarios with numerous and\ndiverse columns. To address this, we propose a graph-based framework that\nleverages human-curated relational knowledge to explicitly encode schema links\nand join paths. Given a natural language query, our method searches on graph to\nconstruct interpretable reasoning chains, aided by pruning and sub-path merging\nstrategies to enhance efficiency and coherence. Experiments on both standard\nbenchmarks and a realistic, large-scale dataset demonstrate the effectiveness\nof our approach. To our knowledge, this is the first multi-table QA system\napplied to truly complex industrial tabular data."
                },
                "authors": [
                    {
                        "name": "Xixi Wang"
                    },
                    {
                        "name": "Miguel Costa"
                    },
                    {
                        "name": "Jordanka Kovaceva"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Francisco C. Pereira"
                    }
                ],
                "author_detail": {
                    "name": "Francisco C. Pereira"
                },
                "author": "Francisco C. Pereira",
                "arxiv_comment": "Accepted to EMNLP 2025 findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04427v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04427v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11864v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11864v1",
                "updated": "2025-09-15T12:38:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    38,
                    39,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T12:38:39Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    38,
                    39,
                    0,
                    258,
                    0
                ],
                "title": "NeuroStrike: Neuron-Level Attacks on Aligned LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuroStrike: Neuron-Level Attacks on Aligned LLMs"
                },
                "summary": "Safety alignment is critical for the ethical deployment of large language\nmodels (LLMs), guiding them to avoid generating harmful or unethical content.\nCurrent alignment techniques, such as supervised fine-tuning and reinforcement\nlearning from human feedback, remain fragile and can be bypassed by carefully\ncrafted adversarial prompts. Unfortunately, such attacks rely on trial and\nerror, lack generalizability across models, and are constrained by scalability\nand reliability.\n  This paper presents NeuroStrike, a novel and generalizable attack framework\nthat exploits a fundamental vulnerability introduced by alignment techniques:\nthe reliance on sparse, specialized safety neurons responsible for detecting\nand suppressing harmful inputs. We apply NeuroStrike to both white-box and\nblack-box settings: In the white-box setting, NeuroStrike identifies safety\nneurons through feedforward activation analysis and prunes them during\ninference to disable safety mechanisms. In the black-box setting, we propose\nthe first LLM profiling attack, which leverages safety neuron transferability\nby training adversarial prompt generators on open-weight surrogate models and\nthen deploying them against black-box and proprietary targets. We evaluate\nNeuroStrike on over 20 open-weight LLMs from major LLM developers. By removing\nless than 0.6% of neurons in targeted layers, NeuroStrike achieves an average\nattack success rate (ASR) of 76.9% using only vanilla malicious prompts.\nMoreover, Neurostrike generalizes to four multimodal LLMs with 100% ASR on\nunsafe image inputs. Safety neurons transfer effectively across architectures,\nraising ASR to 78.5% on 11 fine-tuned models and 77.7% on five distilled\nmodels. The black-box LLM profiling attack achieves an average ASR of 63.7%\nacross five black-box models, including the Google Gemini family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety alignment is critical for the ethical deployment of large language\nmodels (LLMs), guiding them to avoid generating harmful or unethical content.\nCurrent alignment techniques, such as supervised fine-tuning and reinforcement\nlearning from human feedback, remain fragile and can be bypassed by carefully\ncrafted adversarial prompts. Unfortunately, such attacks rely on trial and\nerror, lack generalizability across models, and are constrained by scalability\nand reliability.\n  This paper presents NeuroStrike, a novel and generalizable attack framework\nthat exploits a fundamental vulnerability introduced by alignment techniques:\nthe reliance on sparse, specialized safety neurons responsible for detecting\nand suppressing harmful inputs. We apply NeuroStrike to both white-box and\nblack-box settings: In the white-box setting, NeuroStrike identifies safety\nneurons through feedforward activation analysis and prunes them during\ninference to disable safety mechanisms. In the black-box setting, we propose\nthe first LLM profiling attack, which leverages safety neuron transferability\nby training adversarial prompt generators on open-weight surrogate models and\nthen deploying them against black-box and proprietary targets. We evaluate\nNeuroStrike on over 20 open-weight LLMs from major LLM developers. By removing\nless than 0.6% of neurons in targeted layers, NeuroStrike achieves an average\nattack success rate (ASR) of 76.9% using only vanilla malicious prompts.\nMoreover, Neurostrike generalizes to four multimodal LLMs with 100% ASR on\nunsafe image inputs. Safety neurons transfer effectively across architectures,\nraising ASR to 78.5% on 11 fine-tuned models and 77.7% on five distilled\nmodels. The black-box LLM profiling attack achieves an average ASR of 63.7%\nacross five black-box models, including the Google Gemini family."
                },
                "authors": [
                    {
                        "name": "Lichao Wu"
                    },
                    {
                        "name": "Sasha Behrouzi"
                    },
                    {
                        "name": "Mohamadreza Rostami"
                    },
                    {
                        "name": "Maximilian Thang"
                    },
                    {
                        "name": "Stjepan Picek"
                    },
                    {
                        "name": "Ahmad-Reza Sadeghi"
                    }
                ],
                "author_detail": {
                    "name": "Ahmad-Reza Sadeghi"
                },
                "author": "Ahmad-Reza Sadeghi",
                "arxiv_doi": "10.14722/ndss.2026.230660",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14722/ndss.2026.230660",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.11864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11864v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11851v1",
                "updated": "2025-09-15T12:31:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    31,
                    0,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T12:31:00Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    31,
                    0,
                    0,
                    258,
                    0
                ],
                "title": "The AI Memory Gap: Users Misremember What They Created With AI or\n  Without",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The AI Memory Gap: Users Misremember What They Created With AI or\n  Without"
                },
                "summary": "As large language models (LLMs) become embedded in interactive text\ngeneration, disclosure of AI as a source depends on people remembering which\nideas or texts came from themselves and which were created with AI. We\ninvestigate how accurately people remember the source of content when using AI.\nIn a pre-registered experiment, 184 participants generated and elaborated on\nideas both unaided and with an LLM-based chatbot. One week later, they were\nasked to identify the source (noAI vs withAI) of these ideas and texts. Our\nfindings reveal a significant gap in memory: After AI use, the odds of correct\nattribution dropped, with the steepest decline in mixed human-AI workflows,\nwhere either the idea or elaboration was created with AI. We validated our\nresults using a computational model of source memory. Discussing broader\nimplications, we highlight the importance of considering source confusion in\nthe design and use of interactive text generation technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become embedded in interactive text\ngeneration, disclosure of AI as a source depends on people remembering which\nideas or texts came from themselves and which were created with AI. We\ninvestigate how accurately people remember the source of content when using AI.\nIn a pre-registered experiment, 184 participants generated and elaborated on\nideas both unaided and with an LLM-based chatbot. One week later, they were\nasked to identify the source (noAI vs withAI) of these ideas and texts. Our\nfindings reveal a significant gap in memory: After AI use, the odds of correct\nattribution dropped, with the steepest decline in mixed human-AI workflows,\nwhere either the idea or elaboration was created with AI. We validated our\nresults using a computational model of source memory. Discussing broader\nimplications, we highlight the importance of considering source confusion in\nthe design and use of interactive text generation technologies."
                },
                "authors": [
                    {
                        "name": "Tim Zindulka"
                    },
                    {
                        "name": "Sven Goller"
                    },
                    {
                        "name": "Daniela Fernandes"
                    },
                    {
                        "name": "Robin Welsch"
                    },
                    {
                        "name": "Daniel Buschek"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Buschek"
                },
                "author": "Daniel Buschek",
                "arxiv_comment": "31 pages, 10 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07002v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07002v4",
                "updated": "2025-09-15T12:21:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    21,
                    58,
                    0,
                    258,
                    0
                ],
                "published": "2025-08-09T14:41:20Z",
                "published_parsed": [
                    2025,
                    8,
                    9,
                    14,
                    41,
                    20,
                    5,
                    221,
                    0
                ],
                "title": "Joint Transmit and Pinching Beamforming Design for Pinching\n  Antenna-assisted Symbiotic Radio",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Transmit and Pinching Beamforming Design for Pinching\n  Antenna-assisted Symbiotic Radio"
                },
                "summary": "This paper investigates a novel downlink symbiotic radio framework enabled by\nthe pinching antenna system (PASS), designed to enhance both primary and\nsecondary transmissions through reconfigurable antenna positioning. This\nreconfigurability introduces additional degrees of freedom for adaptive\npinching beamforming, thereby enabling constructive signal enhancement and\ninterference suppression tailored to the locations of the backscatter device,\nthe Internet of Things (IoT) receiver, and the primary receivers. To fully\nexploit these benefits, we formulate a joint transmit and pinching beamforming\noptimization problem that maximizes the achievable sum rate while satisfying\nthe IoT receiver's detection error probability constraint and feasible\ndeployment constraints for the pinching antennas. The resulting problem is\ninherently nonconvex and highly coupled. To address this challenge, we develop\ntwo complementary solution approaches. The first is a learning-aided gradient\ndescent method, where the constrained optimization is reformulated into a\ndifferentiable form and solved through end-to-end learning. In this approach,\nthe pinching antenna position matrix is reparameterized to automatically\nsatisfy minimum spacing constraints, while transmit power and waveguide length\nlimits are enforced via projection and normalization. The second approach is an\noptimization-based successive convex approximation-particle swarm optimization\nmethod, which first determines the transmit beamforming solution using\nsuccessive convex approximation and subsequently optimizes pinching beamforming\nvia a particle swarm optimization search over candidate pinching antenna\nplacements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates a novel downlink symbiotic radio framework enabled by\nthe pinching antenna system (PASS), designed to enhance both primary and\nsecondary transmissions through reconfigurable antenna positioning. This\nreconfigurability introduces additional degrees of freedom for adaptive\npinching beamforming, thereby enabling constructive signal enhancement and\ninterference suppression tailored to the locations of the backscatter device,\nthe Internet of Things (IoT) receiver, and the primary receivers. To fully\nexploit these benefits, we formulate a joint transmit and pinching beamforming\noptimization problem that maximizes the achievable sum rate while satisfying\nthe IoT receiver's detection error probability constraint and feasible\ndeployment constraints for the pinching antennas. The resulting problem is\ninherently nonconvex and highly coupled. To address this challenge, we develop\ntwo complementary solution approaches. The first is a learning-aided gradient\ndescent method, where the constrained optimization is reformulated into a\ndifferentiable form and solved through end-to-end learning. In this approach,\nthe pinching antenna position matrix is reparameterized to automatically\nsatisfy minimum spacing constraints, while transmit power and waveguide length\nlimits are enforced via projection and normalization. The second approach is an\noptimization-based successive convex approximation-particle swarm optimization\nmethod, which first determines the transmit beamforming solution using\nsuccessive convex approximation and subsequently optimizes pinching beamforming\nvia a particle swarm optimization search over candidate pinching antenna\nplacements."
                },
                "authors": [
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Guoping Zhang"
                    },
                    {
                        "name": "Hongbo Xu"
                    },
                    {
                        "name": "Ming Zeng"
                    },
                    {
                        "name": "Fang Fang"
                    },
                    {
                        "name": "Octavia A. Dobre"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07002v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07002v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11816v1",
                "updated": "2025-09-15T11:55:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    55,
                    10,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T11:55:10Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    55,
                    10,
                    0,
                    258,
                    0
                ],
                "title": "Collapse of Irrelevant Representations (CIR) Ensures Robust and\n  Non-Disruptive LLM Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collapse of Irrelevant Representations (CIR) Ensures Robust and\n  Non-Disruptive LLM Unlearning"
                },
                "summary": "Current unlearning techniques and safety training consistently fail to remove\ndangerous knowledge from language models. We analyze the root causes and\npropose a highly selective technique which unlearns robustly and without\ndisrupting general performance.\n  We perform PCA on activations and module output gradients to identify\nsubspaces containing common representations, and collapse them before\ncalculating unlearning updates. This way we avoid unlearning general\nrepresentations, and only target those specific to the unlearned facts.\n  When unlearning WMDP dataset facts from Llama-3.1-8B, we drop post-attack\naccuracy 80x more than our best baseline (Circuit Breakers) on biohazardous\nfacts and 30x more on cyberhazardous facts. Despite this, we disrupt general\nperformance 30x less (only 0.1% WikiText loss increase), while requiring less\nthan 3 GPU-seconds per fact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current unlearning techniques and safety training consistently fail to remove\ndangerous knowledge from language models. We analyze the root causes and\npropose a highly selective technique which unlearns robustly and without\ndisrupting general performance.\n  We perform PCA on activations and module output gradients to identify\nsubspaces containing common representations, and collapse them before\ncalculating unlearning updates. This way we avoid unlearning general\nrepresentations, and only target those specific to the unlearned facts.\n  When unlearning WMDP dataset facts from Llama-3.1-8B, we drop post-attack\naccuracy 80x more than our best baseline (Circuit Breakers) on biohazardous\nfacts and 30x more on cyberhazardous facts. Despite this, we disrupt general\nperformance 30x less (only 0.1% WikiText loss increase), while requiring less\nthan 3 GPU-seconds per fact."
                },
                "authors": [
                    {
                        "name": "Filip Sondej"
                    },
                    {
                        "name": "Yushi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yushi Yang"
                },
                "author": "Yushi Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11815v1",
                "updated": "2025-09-15T11:53:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    53,
                    56,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T11:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    53,
                    56,
                    0,
                    258,
                    0
                ],
                "title": "SpecVLM: Fast Speculative Decoding in Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecVLM: Fast Speculative Decoding in Vision-Language Models"
                },
                "summary": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM."
                },
                "authors": [
                    {
                        "name": "Haiduo Huang"
                    },
                    {
                        "name": "Fuwei Yang"
                    },
                    {
                        "name": "Zhenhua Liu"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Pengju Ren"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11803v1",
                "updated": "2025-09-15T11:34:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    34,
                    46,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T11:34:46Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    34,
                    46,
                    0,
                    258,
                    0
                ],
                "title": "From Fuzzy Speech to Medical Insight: Benchmarking LLMs on Noisy Patient\n  Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Fuzzy Speech to Medical Insight: Benchmarking LLMs on Noisy Patient\n  Narratives"
                },
                "summary": "The widespread adoption of large language models (LLMs) in healthcare raises\ncritical questions about their ability to interpret patient-generated\nnarratives, which are often informal, ambiguous, and noisy. Existing benchmarks\ntypically rely on clean, structured clinical text, offering limited insight\ninto model performance under realistic conditions. In this work, we present a\nnovel synthetic dataset designed to simulate patient self-descriptions\ncharacterized by varying levels of linguistic noise, fuzzy language, and\nlayperson terminology. Our dataset comprises clinically consistent scenarios\nannotated with ground-truth diagnoses, spanning a spectrum of communication\nclarity to reflect diverse real-world reporting styles. Using this benchmark,\nwe fine-tune and evaluate several state-of-the-art models (LLMs), including\nBERT-based and encoder-decoder T5 models. To support reproducibility and future\nresearch, we release the Noisy Diagnostic Benchmark (NDB), a structured dataset\nof noisy, synthetic patient descriptions designed to stress-test and compare\nthe diagnostic capabilities of large language models (LLMs) under realistic\nlinguistic conditions. We made the benchmark available for the community:\nhttps://github.com/lielsheri/PatientSignal",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of large language models (LLMs) in healthcare raises\ncritical questions about their ability to interpret patient-generated\nnarratives, which are often informal, ambiguous, and noisy. Existing benchmarks\ntypically rely on clean, structured clinical text, offering limited insight\ninto model performance under realistic conditions. In this work, we present a\nnovel synthetic dataset designed to simulate patient self-descriptions\ncharacterized by varying levels of linguistic noise, fuzzy language, and\nlayperson terminology. Our dataset comprises clinically consistent scenarios\nannotated with ground-truth diagnoses, spanning a spectrum of communication\nclarity to reflect diverse real-world reporting styles. Using this benchmark,\nwe fine-tune and evaluate several state-of-the-art models (LLMs), including\nBERT-based and encoder-decoder T5 models. To support reproducibility and future\nresearch, we release the Noisy Diagnostic Benchmark (NDB), a structured dataset\nof noisy, synthetic patient descriptions designed to stress-test and compare\nthe diagnostic capabilities of large language models (LLMs) under realistic\nlinguistic conditions. We made the benchmark available for the community:\nhttps://github.com/lielsheri/PatientSignal"
                },
                "authors": [
                    {
                        "name": "Eden Mama"
                    },
                    {
                        "name": "Liel Sheri"
                    },
                    {
                        "name": "Yehudit Aperstein"
                    },
                    {
                        "name": "Alexander Apartsin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Apartsin"
                },
                "author": "Alexander Apartsin",
                "arxiv_comment": "6 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11802v1",
                "updated": "2025-09-15T11:31:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    31,
                    25,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T11:31:25Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    31,
                    25,
                    0,
                    258,
                    0
                ],
                "title": "When Curiosity Signals Danger: Predicting Health Crises Through Online\n  Medication Inquiries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Curiosity Signals Danger: Predicting Health Crises Through Online\n  Medication Inquiries"
                },
                "summary": "Online medical forums are a rich and underutilized source of insight into\npatient concerns, especially regarding medication use. Some of the many\nquestions users pose may signal confusion, misuse, or even the early warning\nsigns of a developing health crisis. Detecting these critical questions that\nmay precede severe adverse events or life-threatening complications is vital\nfor timely intervention and improving patient safety. This study introduces a\nnovel annotated dataset of medication-related questions extracted from online\nforums. Each entry is manually labelled for criticality based on clinical risk\nfactors. We benchmark the performance of six traditional machine learning\nclassifiers using TF-IDF textual representations, alongside three\nstate-of-the-art large language model (LLM)-based classification approaches\nthat leverage deep contextual understanding. Our results highlight the\npotential of classical and modern methods to support real-time triage and alert\nsystems in digital health spaces. The curated dataset is made publicly\navailable to encourage further research at the intersection of\npatient-generated data, natural language processing, and early warning systems\nfor critical health events. The dataset and benchmark are available at:\nhttps://github.com/Dvora-coder/LLM-Medication-QA-Risk-Classifier-MediGuard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online medical forums are a rich and underutilized source of insight into\npatient concerns, especially regarding medication use. Some of the many\nquestions users pose may signal confusion, misuse, or even the early warning\nsigns of a developing health crisis. Detecting these critical questions that\nmay precede severe adverse events or life-threatening complications is vital\nfor timely intervention and improving patient safety. This study introduces a\nnovel annotated dataset of medication-related questions extracted from online\nforums. Each entry is manually labelled for criticality based on clinical risk\nfactors. We benchmark the performance of six traditional machine learning\nclassifiers using TF-IDF textual representations, alongside three\nstate-of-the-art large language model (LLM)-based classification approaches\nthat leverage deep contextual understanding. Our results highlight the\npotential of classical and modern methods to support real-time triage and alert\nsystems in digital health spaces. The curated dataset is made publicly\navailable to encourage further research at the intersection of\npatient-generated data, natural language processing, and early warning systems\nfor critical health events. The dataset and benchmark are available at:\nhttps://github.com/Dvora-coder/LLM-Medication-QA-Risk-Classifier-MediGuard."
                },
                "authors": [
                    {
                        "name": "Dvora Goncharok"
                    },
                    {
                        "name": "Arbel Shifman"
                    },
                    {
                        "name": "Alexander Apartsin"
                    },
                    {
                        "name": "Yehudit Aperstein"
                    }
                ],
                "author_detail": {
                    "name": "Yehudit Aperstein"
                },
                "author": "Yehudit Aperstein",
                "arxiv_comment": "5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11796v1",
                "updated": "2025-09-15T11:27:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    27,
                    23,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T11:27:23Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    27,
                    23,
                    0,
                    258,
                    0
                ],
                "title": "FineQuest: Adaptive Knowledge-Assisted Sports Video Understanding via\n  Agent-of-Thoughts Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineQuest: Adaptive Knowledge-Assisted Sports Video Understanding via\n  Agent-of-Thoughts Reasoning"
                },
                "summary": "Video Question Answering (VideoQA) based on Large Language Models (LLMs) has\nshown potential in general video understanding but faces significant challenges\nwhen applied to the inherently complex domain of sports videos. In this work,\nwe propose FineQuest, the first training-free framework that leverages\ndual-mode reasoning inspired by cognitive science: i) Reactive Reasoning for\nstraightforward sports queries and ii) Deliberative Reasoning for more complex\nones. To bridge the knowledge gap between general-purpose models and\ndomain-specific sports understanding, FineQuest incorporates SSGraph, a\nmultimodal sports knowledge scene graph spanning nine sports, which encodes\nboth visual instances and domain-specific terminology to enhance reasoning\naccuracy. Furthermore, we introduce two new sports VideoQA benchmarks, Gym-QA\nand Diving-QA, derived from the FineGym and FineDiving datasets, enabling\ndiverse and comprehensive evaluation. FineQuest achieves state-of-the-art\nperformance on these benchmarks as well as the existing SPORTU dataset, while\nmaintains strong general VideoQA capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Question Answering (VideoQA) based on Large Language Models (LLMs) has\nshown potential in general video understanding but faces significant challenges\nwhen applied to the inherently complex domain of sports videos. In this work,\nwe propose FineQuest, the first training-free framework that leverages\ndual-mode reasoning inspired by cognitive science: i) Reactive Reasoning for\nstraightforward sports queries and ii) Deliberative Reasoning for more complex\nones. To bridge the knowledge gap between general-purpose models and\ndomain-specific sports understanding, FineQuest incorporates SSGraph, a\nmultimodal sports knowledge scene graph spanning nine sports, which encodes\nboth visual instances and domain-specific terminology to enhance reasoning\naccuracy. Furthermore, we introduce two new sports VideoQA benchmarks, Gym-QA\nand Diving-QA, derived from the FineGym and FineDiving datasets, enabling\ndiverse and comprehensive evaluation. FineQuest achieves state-of-the-art\nperformance on these benchmarks as well as the existing SPORTU dataset, while\nmaintains strong general VideoQA capabilities."
                },
                "authors": [
                    {
                        "name": "Haodong Chen"
                    },
                    {
                        "name": "Haojian Huang"
                    },
                    {
                        "name": "XinXiang Yin"
                    },
                    {
                        "name": "Dian Shao"
                    }
                ],
                "author_detail": {
                    "name": "Dian Shao"
                },
                "author": "Dian Shao",
                "arxiv_comment": "ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11789v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11789v1",
                "updated": "2025-09-15T11:19:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    19,
                    42,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T11:19:42Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    19,
                    42,
                    0,
                    258,
                    0
                ],
                "title": "Watch Your Step: A Cost-Sensitive Framework for Accelerometer-Based Fall\n  Detection in Real-World Streaming Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watch Your Step: A Cost-Sensitive Framework for Accelerometer-Based Fall\n  Detection in Real-World Streaming Scenarios"
                },
                "summary": "Real-time fall detection is crucial for enabling timely interventions and\nmitigating the severe health consequences of falls, particularly in older\nadults. However, existing methods often rely on simulated data or assumptions\nsuch as prior knowledge of fall events, limiting their real-world\napplicability. Practical deployment also requires efficient computation and\nrobust evaluation metrics tailored to continuous monitoring. This paper\npresents a real-time fall detection framework for continuous monitoring without\nprior knowledge of fall events. Using over 60 hours of inertial measurement\nunit (IMU) data from the FARSEEING real-world falls dataset, we employ recent\nefficient classifiers to compute fall probabilities in streaming mode. To\nenhance robustness, we introduce a cost-sensitive learning strategy that tunes\nthe decision threshold using a cost function reflecting the higher risk of\nmissed falls compared to false alarms. Unlike many methods that achieve high\nrecall only at the cost of precision, our framework achieved Recall of 1.00,\nPrecision of 0.84, and an F1 score of 0.91 on FARSEEING, detecting all falls\nwhile keeping false alarms low, with average inference time below 5 ms per\nsample. These results demonstrate that cost-sensitive threshold tuning enhances\nthe robustness of accelerometer-based fall detection. They also highlight the\npotential of our computationally efficient framework for deployment in\nreal-time wearable sensor systems for continuous monitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time fall detection is crucial for enabling timely interventions and\nmitigating the severe health consequences of falls, particularly in older\nadults. However, existing methods often rely on simulated data or assumptions\nsuch as prior knowledge of fall events, limiting their real-world\napplicability. Practical deployment also requires efficient computation and\nrobust evaluation metrics tailored to continuous monitoring. This paper\npresents a real-time fall detection framework for continuous monitoring without\nprior knowledge of fall events. Using over 60 hours of inertial measurement\nunit (IMU) data from the FARSEEING real-world falls dataset, we employ recent\nefficient classifiers to compute fall probabilities in streaming mode. To\nenhance robustness, we introduce a cost-sensitive learning strategy that tunes\nthe decision threshold using a cost function reflecting the higher risk of\nmissed falls compared to false alarms. Unlike many methods that achieve high\nrecall only at the cost of precision, our framework achieved Recall of 1.00,\nPrecision of 0.84, and an F1 score of 0.91 on FARSEEING, detecting all falls\nwhile keeping false alarms low, with average inference time below 5 ms per\nsample. These results demonstrate that cost-sensitive threshold tuning enhances\nthe robustness of accelerometer-based fall detection. They also highlight the\npotential of our computationally efficient framework for deployment in\nreal-time wearable sensor systems for continuous monitoring."
                },
                "authors": [
                    {
                        "name": "Timilehin B. Aderinola"
                    },
                    {
                        "name": "Luca Palmerini"
                    },
                    {
                        "name": "Ilaria D'Ascanio"
                    },
                    {
                        "name": "Lorenzo Chiari"
                    },
                    {
                        "name": "Jochen Klenk"
                    },
                    {
                        "name": "Clemens Becker"
                    },
                    {
                        "name": "Brian Caulfield"
                    },
                    {
                        "name": "Georgiana Ifrim"
                    }
                ],
                "author_detail": {
                    "name": "Georgiana Ifrim"
                },
                "author": "Georgiana Ifrim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11789v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11789v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11787v1",
                "updated": "2025-09-15T11:16:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    16,
                    4,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T11:16:04Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    16,
                    4,
                    0,
                    258,
                    0
                ],
                "title": "CodeCureAgent: Automatic Classification and Repair of Static Analysis\n  Warnings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeCureAgent: Automatic Classification and Repair of Static Analysis\n  Warnings"
                },
                "summary": "Static analysis tools are widely used to detect bugs, vulnerabilities, and\ncode smells. Traditionally, developers must resolve these warnings manually.\nBecause this process is tedious, developers sometimes ignore warnings, leading\nto an accumulation of warnings and a degradation of code quality. This paper\npresents CodeCureAgent, an approach that harnesses LLM-based agents to\nautomatically analyze, classify, and repair static analysis warnings. Unlike\nprevious work, our method does not follow a predetermined algorithm. Instead,\nwe adopt an agentic framework that iteratively invokes tools to gather\nadditional information from the codebase (e.g., via code search) and edit the\ncodebase to resolve the warning. CodeCureAgent detects and suppresses false\npositives, while fixing true positives when identified. We equip CodeCureAgent\nwith a three-step heuristic to approve patches: (1) build the project, (2)\nverify that the warning disappears without introducing new warnings, and (3)\nrun the test suite. We evaluate CodeCureAgent on a dataset of 1,000 SonarQube\nwarnings found in 106 Java projects and covering 291 distinct rules. Our\napproach produces plausible fixes for 96.8% of the warnings, outperforming\nstate-of-the-art baseline approaches by 30.7% and 29.2% in plausible-fix rate,\nrespectively. Manual inspection of 291 cases reveals a correct-fix rate of\n86.3%, showing that CodeCureAgent can reliably repair static analysis warnings.\nThe approach incurs LLM costs of about 2.9 cents (USD) and an end-to-end\nprocessing time of about four minutes per warning. We envision CodeCureAgent\nhelping to clean existing codebases and being integrated into CI/CD pipelines\nto prevent the accumulation of static analysis warnings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static analysis tools are widely used to detect bugs, vulnerabilities, and\ncode smells. Traditionally, developers must resolve these warnings manually.\nBecause this process is tedious, developers sometimes ignore warnings, leading\nto an accumulation of warnings and a degradation of code quality. This paper\npresents CodeCureAgent, an approach that harnesses LLM-based agents to\nautomatically analyze, classify, and repair static analysis warnings. Unlike\nprevious work, our method does not follow a predetermined algorithm. Instead,\nwe adopt an agentic framework that iteratively invokes tools to gather\nadditional information from the codebase (e.g., via code search) and edit the\ncodebase to resolve the warning. CodeCureAgent detects and suppresses false\npositives, while fixing true positives when identified. We equip CodeCureAgent\nwith a three-step heuristic to approve patches: (1) build the project, (2)\nverify that the warning disappears without introducing new warnings, and (3)\nrun the test suite. We evaluate CodeCureAgent on a dataset of 1,000 SonarQube\nwarnings found in 106 Java projects and covering 291 distinct rules. Our\napproach produces plausible fixes for 96.8% of the warnings, outperforming\nstate-of-the-art baseline approaches by 30.7% and 29.2% in plausible-fix rate,\nrespectively. Manual inspection of 291 cases reveals a correct-fix rate of\n86.3%, showing that CodeCureAgent can reliably repair static analysis warnings.\nThe approach incurs LLM costs of about 2.9 cents (USD) and an end-to-end\nprocessing time of about four minutes per warning. We envision CodeCureAgent\nhelping to clean existing codebases and being integrated into CI/CD pipelines\nto prevent the accumulation of static analysis warnings."
                },
                "authors": [
                    {
                        "name": "Pascal Joos"
                    },
                    {
                        "name": "Islem Bouzenia"
                    },
                    {
                        "name": "Michael Pradel"
                    }
                ],
                "author_detail": {
                    "name": "Michael Pradel"
                },
                "author": "Michael Pradel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11777v1",
                "updated": "2025-09-15T10:58:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    10,
                    58,
                    41,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T10:58:41Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    10,
                    58,
                    41,
                    0,
                    258,
                    0
                ],
                "title": "User eXperience Perception Insights Dataset (UXPID): Synthetic User\n  Feedback from Public Industrial Forums",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User eXperience Perception Insights Dataset (UXPID): Synthetic User\n  Feedback from Public Industrial Forums"
                },
                "summary": "Customer feedback in industrial forums reflect a rich but underexplored\nsource of insight into real-world product experience. These publicly shared\ndiscussions offer an organic view of user expectations, frustrations, and\nsuccess stories shaped by the specific contexts of use. Yet, harnessing this\ninformation for systematic analysis remains challenging due to the unstructured\nand domain-specific nature of the content. The lack of structure and\nspecialized vocabulary makes it difficult for traditional data analysis\ntechniques to accurately interpret, categorize, and quantify the feedback,\nthereby limiting its potential to inform product development and support\nstrategies. To address these challenges, this paper presents the User\neXperience Perception Insights Dataset (UXPID), a collection of 7130\nartificially synthesized and anonymized user feedback branches extracted from a\npublic industrial automation forum. Each JavaScript object notation (JSON)\nrecord contains multi-post comments related to specific hardware and software\nproducts, enriched with metadata and contextual conversation data. Leveraging a\nlarge language model (LLM), each branch is systematically analyzed and\nannotated for UX insights, user expectations, severity and sentiment ratings,\nand topic classifications. The UXPID dataset is designed to facilitate research\nin user requirements, user experience (UX) analysis, and AI-driven feedback\nprocessing, particularly where privacy and licensing restrictions limit access\nto real-world data. UXPID supports the training and evaluation of\ntransformer-based models for tasks such as issue detection, sentiment analysis,\nand requirements extraction in the context of technical forums.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customer feedback in industrial forums reflect a rich but underexplored\nsource of insight into real-world product experience. These publicly shared\ndiscussions offer an organic view of user expectations, frustrations, and\nsuccess stories shaped by the specific contexts of use. Yet, harnessing this\ninformation for systematic analysis remains challenging due to the unstructured\nand domain-specific nature of the content. The lack of structure and\nspecialized vocabulary makes it difficult for traditional data analysis\ntechniques to accurately interpret, categorize, and quantify the feedback,\nthereby limiting its potential to inform product development and support\nstrategies. To address these challenges, this paper presents the User\neXperience Perception Insights Dataset (UXPID), a collection of 7130\nartificially synthesized and anonymized user feedback branches extracted from a\npublic industrial automation forum. Each JavaScript object notation (JSON)\nrecord contains multi-post comments related to specific hardware and software\nproducts, enriched with metadata and contextual conversation data. Leveraging a\nlarge language model (LLM), each branch is systematically analyzed and\nannotated for UX insights, user expectations, severity and sentiment ratings,\nand topic classifications. The UXPID dataset is designed to facilitate research\nin user requirements, user experience (UX) analysis, and AI-driven feedback\nprocessing, particularly where privacy and licensing restrictions limit access\nto real-world data. UXPID supports the training and evaluation of\ntransformer-based models for tasks such as issue detection, sentiment analysis,\nand requirements extraction in the context of technical forums."
                },
                "authors": [
                    {
                        "name": "Mikhail Kulyabin"
                    },
                    {
                        "name": "Jan Joosten"
                    },
                    {
                        "name": "Choro Ulan uulu"
                    },
                    {
                        "name": "Nuno Miguel Martins Pacheco"
                    },
                    {
                        "name": "Fabian Ries"
                    },
                    {
                        "name": "Filippos Petridis"
                    },
                    {
                        "name": "Jan Bosch"
                    },
                    {
                        "name": "Helena Holmström Olsson"
                    }
                ],
                "author_detail": {
                    "name": "Helena Holmström Olsson"
                },
                "author": "Helena Holmström Olsson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11774v1",
                "updated": "2025-09-15T10:53:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    10,
                    53,
                    28,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T10:53:28Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    10,
                    53,
                    28,
                    0,
                    258,
                    0
                ],
                "title": "SA-UNetv2: Rethinking Spatial Attention U-Net for Retinal Vessel\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SA-UNetv2: Rethinking Spatial Attention U-Net for Retinal Vessel\n  Segmentation"
                },
                "summary": "Retinal vessel segmentation is essential for early diagnosis of diseases such\nas diabetic retinopathy, hypertension, and neurodegenerative disorders.\nAlthough SA-UNet introduces spatial attention in the bottleneck, it underuses\nattention in skip connections and does not address the severe\nforeground-background imbalance. We propose SA-UNetv2, a lightweight model that\ninjects cross-scale spatial attention into all skip connections to strengthen\nmulti-scale feature fusion and adopts a weighted Binary Cross-Entropy (BCE)\nplus Matthews Correlation Coefficient (MCC) loss to improve robustness to class\nimbalance. On the public DRIVE and STARE datasets, SA-UNetv2 achieves\nstate-of-the-art performance with only 1.2MB memory and 0.26M parameters (less\nthan 50% of SA-UNet), and 1 second CPU inference on 592 x 592 x 3 images,\ndemonstrating strong efficiency and deployability in resource-constrained,\nCPU-only settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retinal vessel segmentation is essential for early diagnosis of diseases such\nas diabetic retinopathy, hypertension, and neurodegenerative disorders.\nAlthough SA-UNet introduces spatial attention in the bottleneck, it underuses\nattention in skip connections and does not address the severe\nforeground-background imbalance. We propose SA-UNetv2, a lightweight model that\ninjects cross-scale spatial attention into all skip connections to strengthen\nmulti-scale feature fusion and adopts a weighted Binary Cross-Entropy (BCE)\nplus Matthews Correlation Coefficient (MCC) loss to improve robustness to class\nimbalance. On the public DRIVE and STARE datasets, SA-UNetv2 achieves\nstate-of-the-art performance with only 1.2MB memory and 0.26M parameters (less\nthan 50% of SA-UNet), and 1 second CPU inference on 592 x 592 x 3 images,\ndemonstrating strong efficiency and deployability in resource-constrained,\nCPU-only settings."
                },
                "authors": [
                    {
                        "name": "Changlu Guo"
                    },
                    {
                        "name": "Anders Nymark Christensen"
                    },
                    {
                        "name": "Anders Bjorholm Dahl"
                    },
                    {
                        "name": "Yugen Yi"
                    },
                    {
                        "name": "Morten Rieger Hannemose"
                    }
                ],
                "author_detail": {
                    "name": "Morten Rieger Hannemose"
                },
                "author": "Morten Rieger Hannemose",
                "arxiv_comment": "The code is available at github.com/clguo/SA-UNetv2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11773v1",
                "updated": "2025-09-15T10:53:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    10,
                    53,
                    5,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T10:53:05Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    10,
                    53,
                    5,
                    0,
                    258,
                    0
                ],
                "title": "An Agentic Toolkit for Adaptive Information Extraction from Regulatory\n  Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Agentic Toolkit for Adaptive Information Extraction from Regulatory\n  Documents"
                },
                "summary": "Declaration of Performance (DoP) documents, mandated by EU regulation,\ncertify the performance of construction products. While some of their content\nis standardized, DoPs vary widely in layout, language, schema, and format,\nposing challenges for automated key-value pair extraction (KVP) and question\nanswering (QA). Existing static or LLM-only IE pipelines often hallucinate and\nfail to adapt to this structural diversity. Our domain-specific, stateful\nagentic system addresses these challenges through a planner-executor-responder\narchitecture. The system infers user intent, detects document modality, and\norchestrates tools dynamically for robust, traceable reasoning while avoiding\ntool misuse or execution loops. Evaluation on a curated DoP dataset\ndemonstrates improved robustness across formats and languages, offering a\nscalable solution for structured data extraction in regulated workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Declaration of Performance (DoP) documents, mandated by EU regulation,\ncertify the performance of construction products. While some of their content\nis standardized, DoPs vary widely in layout, language, schema, and format,\nposing challenges for automated key-value pair extraction (KVP) and question\nanswering (QA). Existing static or LLM-only IE pipelines often hallucinate and\nfail to adapt to this structural diversity. Our domain-specific, stateful\nagentic system addresses these challenges through a planner-executor-responder\narchitecture. The system infers user intent, detects document modality, and\norchestrates tools dynamically for robust, traceable reasoning while avoiding\ntool misuse or execution loops. Evaluation on a curated DoP dataset\ndemonstrates improved robustness across formats and languages, offering a\nscalable solution for structured data extraction in regulated workflows."
                },
                "authors": [
                    {
                        "name": "Gaye Colakoglu"
                    },
                    {
                        "name": "Gürkan Solmaz"
                    },
                    {
                        "name": "Jonathan Fürst"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Fürst"
                },
                "author": "Jonathan Fürst",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11772v1",
                "updated": "2025-09-15T10:52:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    10,
                    52,
                    27,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T10:52:27Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    10,
                    52,
                    27,
                    0,
                    258,
                    0
                ],
                "title": "Seg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for\n  Zero-shot Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for\n  Zero-shot Generalization"
                },
                "summary": "Autonomous systems require robust Multi-Object Tracking (MOT) capabilities to\noperate reliably in dynamic environments. MOT ensures consistent object\nidentity assignment and precise spatial delineation. Recent advances in\nfoundation models, such as SAM2, have demonstrated strong zero-shot\ngeneralization for video segmentation, but their direct application to MOTS\n(MOT+Segmentation) remains limited by insufficient identity management and\nmemory efficiency. This work introduces Seg2Track-SAM2, a framework that\nintegrates pre-trained object detectors with SAM2 and a novel Seg2Track module\nto address track initialization, track management, and reinforcement. The\nproposed approach requires no fine-tuning and remains detector-agnostic.\nExperimental results on KITTI MOT and KITTI MOTS benchmarks show that\nSeg2Track-SAM2 achieves state-of-the-art (SOTA) performance, ranking fourth\noverall in both car and pedestrian classes on KITTI MOTS, while establishing a\nnew benchmark in association accuracy (AssA). Furthermore, a sliding-window\nmemory strategy reduces memory usage by up to 75% with negligible performance\ndegradation, supporting deployment under resource constraints. These results\nconfirm that Seg2Track-SAM2 advances MOTS by combining robust zero-shot\ntracking, enhanced identity preservation, and efficient memory utilization. The\ncode is available at https://github.com/hcmr-lab/Seg2Track-SAM2",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous systems require robust Multi-Object Tracking (MOT) capabilities to\noperate reliably in dynamic environments. MOT ensures consistent object\nidentity assignment and precise spatial delineation. Recent advances in\nfoundation models, such as SAM2, have demonstrated strong zero-shot\ngeneralization for video segmentation, but their direct application to MOTS\n(MOT+Segmentation) remains limited by insufficient identity management and\nmemory efficiency. This work introduces Seg2Track-SAM2, a framework that\nintegrates pre-trained object detectors with SAM2 and a novel Seg2Track module\nto address track initialization, track management, and reinforcement. The\nproposed approach requires no fine-tuning and remains detector-agnostic.\nExperimental results on KITTI MOT and KITTI MOTS benchmarks show that\nSeg2Track-SAM2 achieves state-of-the-art (SOTA) performance, ranking fourth\noverall in both car and pedestrian classes on KITTI MOTS, while establishing a\nnew benchmark in association accuracy (AssA). Furthermore, a sliding-window\nmemory strategy reduces memory usage by up to 75% with negligible performance\ndegradation, supporting deployment under resource constraints. These results\nconfirm that Seg2Track-SAM2 advances MOTS by combining robust zero-shot\ntracking, enhanced identity preservation, and efficient memory utilization. The\ncode is available at https://github.com/hcmr-lab/Seg2Track-SAM2"
                },
                "authors": [
                    {
                        "name": "Diogo Mendonça"
                    },
                    {
                        "name": "Tiago Barros"
                    },
                    {
                        "name": "Cristiano Premebida"
                    },
                    {
                        "name": "Urbano J. Nunes"
                    }
                ],
                "author_detail": {
                    "name": "Urbano J. Nunes"
                },
                "author": "Urbano J. Nunes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15008v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15008v3",
                "updated": "2025-09-15T10:32:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    10,
                    32,
                    11,
                    0,
                    258,
                    0
                ],
                "published": "2025-08-20T18:56:26Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    18,
                    56,
                    26,
                    2,
                    232,
                    0
                ],
                "title": "Quantized Neural Networks for Microcontrollers: A Comprehensive Review\n  of Methods, Platforms, and Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantized Neural Networks for Microcontrollers: A Comprehensive Review\n  of Methods, Platforms, and Applications"
                },
                "summary": "The deployment of Quantized Neural Networks (QNNs) on resource-constrained\ndevices, such as microcontrollers, has introduced significant challenges in\nbalancing model performance, computational complexity, and memory constraints.\nTiny Machine Learning (TinyML) addresses these issues by integrating\nadvancements across machine learning algorithms, hardware acceleration, and\nsoftware optimization to efficiently run deep neural networks on embedded\nsystems. This survey presents a hardware-centric introduction to quantization,\nsystematically reviewing essential quantization techniques employed to\naccelerate deep learning models for embedded applications. In particular,\nfurther emphasis is placed on the critical trade-offs between model performance\nand hardware capabilities. The survey further evaluates existing software\nframeworks and hardware platforms designed specifically for supporting QNN\nexecution on microcontrollers. Moreover, we provide an analysis of the current\nchallenges and an outline of promising future directions in the rapidly\nevolving domain of QNN deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Quantized Neural Networks (QNNs) on resource-constrained\ndevices, such as microcontrollers, has introduced significant challenges in\nbalancing model performance, computational complexity, and memory constraints.\nTiny Machine Learning (TinyML) addresses these issues by integrating\nadvancements across machine learning algorithms, hardware acceleration, and\nsoftware optimization to efficiently run deep neural networks on embedded\nsystems. This survey presents a hardware-centric introduction to quantization,\nsystematically reviewing essential quantization techniques employed to\naccelerate deep learning models for embedded applications. In particular,\nfurther emphasis is placed on the critical trade-offs between model performance\nand hardware capabilities. The survey further evaluates existing software\nframeworks and hardware platforms designed specifically for supporting QNN\nexecution on microcontrollers. Moreover, we provide an analysis of the current\nchallenges and an outline of promising future directions in the rapidly\nevolving domain of QNN deployment."
                },
                "authors": [
                    {
                        "name": "Hamza A. Abushahla"
                    },
                    {
                        "name": "Dara Varam"
                    },
                    {
                        "name": "Ariel J. N. Panopio"
                    },
                    {
                        "name": "Mohamed I. AlHajri"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed I. AlHajri"
                },
                "author": "Mohamed I. AlHajri",
                "arxiv_comment": "39 pages, 16 figures, 8 Tables, submitted to the Proceedings of the\n  IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15008v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15008v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18337v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18337v5",
                "updated": "2025-09-15T10:05:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    10,
                    5,
                    37,
                    0,
                    258,
                    0
                ],
                "published": "2024-11-27T13:35:32Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    13,
                    35,
                    32,
                    2,
                    332,
                    0
                ],
                "title": "Can LLMs assist with Ambiguity? A Quantitative Evaluation of various\n  Large Language Models on Word Sense Disambiguation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs assist with Ambiguity? A Quantitative Evaluation of various\n  Large Language Models on Word Sense Disambiguation"
                },
                "summary": "Ambiguous words are often found in modern digital communications. Lexical\nambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due\nto limited data. Consequently, the efficiency of translation, information\nretrieval, and question-answering systems is hindered by these limitations.\nThis study investigates the use of Large Language Models (LLMs) to improve WSD\nusing a novel approach combining a systematic prompt augmentation mechanism\nwith a knowledge base (KB) consisting of different sense interpretations. The\nproposed method incorporates a human-in-loop approach for prompt augmentation\nwhere prompt is supported by Part-of-Speech (POS) tagging, synonyms of\nambiguous words, aspect-based sense filtering and few-shot prompting to guide\nthe LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based\napproach, this work demonstrates a substantial improvement in performance. The\nevaluation was conducted using FEWS test data and sense tags. This research\nadvances accurate word interpretation in social media and digital\ncommunication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ambiguous words are often found in modern digital communications. Lexical\nambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due\nto limited data. Consequently, the efficiency of translation, information\nretrieval, and question-answering systems is hindered by these limitations.\nThis study investigates the use of Large Language Models (LLMs) to improve WSD\nusing a novel approach combining a systematic prompt augmentation mechanism\nwith a knowledge base (KB) consisting of different sense interpretations. The\nproposed method incorporates a human-in-loop approach for prompt augmentation\nwhere prompt is supported by Part-of-Speech (POS) tagging, synonyms of\nambiguous words, aspect-based sense filtering and few-shot prompting to guide\nthe LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based\napproach, this work demonstrates a substantial improvement in performance. The\nevaluation was conducted using FEWS test data and sense tags. This research\nadvances accurate word interpretation in social media and digital\ncommunication."
                },
                "authors": [
                    {
                        "name": "T. G. D. K. Sumanathilaka"
                    },
                    {
                        "name": "Nicholas Micallef"
                    },
                    {
                        "name": "Julian Hough"
                    }
                ],
                "author_detail": {
                    "name": "Julian Hough"
                },
                "author": "Julian Hough",
                "arxiv_comment": "12 pages,6 tables, 1 figure, Proceedings of the 1st International\n  Conference on NLP & AI for Cyber Security",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18337v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18337v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00831v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00831v5",
                "updated": "2025-09-15T10:05:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    10,
                    5,
                    34,
                    0,
                    258,
                    0
                ],
                "published": "2025-05-01T19:44:36Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    44,
                    36,
                    3,
                    121,
                    0
                ],
                "title": "SmallPlan: Leverage Small Language Models for Sequential Path Planning\n  with Simulation-Powered, LLM-Guided Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmallPlan: Leverage Small Language Models for Sequential Path Planning\n  with Simulation-Powered, LLM-Guided Distillation"
                },
                "summary": "Efficient path planning in robotics, particularly within large-scale, complex\nenvironments, remains a significant hurdle. While Large Language Models (LLMs)\noffer strong reasoning capabilities, their high computational cost and limited\nadaptability hinder real-time deployment on edge devices. We present SmallPlan\n- a novel framework leveraging LLMs as teacher models to train lightweight\nSmall Language Models (SLMs) for high-level path planning tasks. In SmallPlan,\nthe SLMs provide optimal action sequences to navigate across scene graphs that\ncompactly represent full-scaled 3D scenes. The SLMs are trained in a\nsimulation-powered, interleaved manner with LLM-guided supervised fine-tuning\n(SFT) and reinforcement learning (RL). This strategy not only enables SLMs to\nsuccessfully complete navigation tasks but also makes them aware of important\nfactors like distance travel, providing more efficient path planning. Through\nexperiments, we demonstrate that the fine-tuned SLMs perform competitively with\nlarger models like GPT-4o on sequential path planning, without suffering from\nhallucination and overfitting. SmallPlan is resource-efficient, making it\nwell-suited for edge-device deployment and advancing practical autonomous\nrobotics. Our source code is available here:\nhttps://github.com/quangpham2006/SmallPlan",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient path planning in robotics, particularly within large-scale, complex\nenvironments, remains a significant hurdle. While Large Language Models (LLMs)\noffer strong reasoning capabilities, their high computational cost and limited\nadaptability hinder real-time deployment on edge devices. We present SmallPlan\n- a novel framework leveraging LLMs as teacher models to train lightweight\nSmall Language Models (SLMs) for high-level path planning tasks. In SmallPlan,\nthe SLMs provide optimal action sequences to navigate across scene graphs that\ncompactly represent full-scaled 3D scenes. The SLMs are trained in a\nsimulation-powered, interleaved manner with LLM-guided supervised fine-tuning\n(SFT) and reinforcement learning (RL). This strategy not only enables SLMs to\nsuccessfully complete navigation tasks but also makes them aware of important\nfactors like distance travel, providing more efficient path planning. Through\nexperiments, we demonstrate that the fine-tuned SLMs perform competitively with\nlarger models like GPT-4o on sequential path planning, without suffering from\nhallucination and overfitting. SmallPlan is resource-efficient, making it\nwell-suited for edge-device deployment and advancing practical autonomous\nrobotics. Our source code is available here:\nhttps://github.com/quangpham2006/SmallPlan"
                },
                "authors": [
                    {
                        "name": "Quang P. M. Pham"
                    },
                    {
                        "name": "Khoi T. N. Nguyen"
                    },
                    {
                        "name": "Nhi H. Doan"
                    },
                    {
                        "name": "Cuong A. Pham"
                    },
                    {
                        "name": "Qinbo Sun"
                    },
                    {
                        "name": "Weimin Qi"
                    },
                    {
                        "name": "Kentaro Inui"
                    },
                    {
                        "name": "Dezhen Song"
                    }
                ],
                "author_detail": {
                    "name": "Dezhen Song"
                },
                "author": "Dezhen Song",
                "arxiv_comment": "Paper is under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00831v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00831v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11742v1",
                "updated": "2025-09-15T09:44:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    9,
                    44,
                    57,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T09:44:57Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    9,
                    44,
                    57,
                    0,
                    258,
                    0
                ],
                "title": "Adaptive Motorized LiDAR Scanning Control for Robust Localization with\n  OpenStreetMap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Motorized LiDAR Scanning Control for Robust Localization with\n  OpenStreetMap"
                },
                "summary": "LiDAR-to-OpenStreetMap (OSM) localization has gained increasing attention, as\nOSM provides lightweight global priors such as building footprints. These\npriors enhance global consistency for robot navigation, but OSM is often\nincomplete or outdated, limiting its reliability in real-world deployment.\nMeanwhile, LiDAR itself suffers from a limited field of view (FoV), where\nmotorized rotation is commonly used to achieve panoramic coverage. Existing\nmotorized LiDAR systems, however, typically employ constant-speed scanning that\ndisregards both scene structure and map priors, leading to wasted effort in\nfeature-sparse regions and degraded localization accuracy. To address these\nchallenges, we propose Adaptive LiDAR Scanning with OSM guidance, a framework\nthat integrates global priors with local observability prediction to improve\nlocalization robustness. Specifically, we augment uncertainty-aware model\npredictive control with an OSM-aware term that adaptively allocates scanning\neffort according to both scene-dependent observability and the spatial\ndistribution of OSM features. The method is implemented in ROS with a motorized\nLiDAR odometry backend and evaluated in both simulation and real-world\nexperiments. Results on campus roads, indoor corridors, and urban environments\ndemonstrate significant reductions in trajectory error compared to\nconstant-speed baselines, while maintaining scan completeness. These findings\nhighlight the potential of coupling open-source maps with adaptive LiDAR\nscanning to achieve robust and efficient localization in complex environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiDAR-to-OpenStreetMap (OSM) localization has gained increasing attention, as\nOSM provides lightweight global priors such as building footprints. These\npriors enhance global consistency for robot navigation, but OSM is often\nincomplete or outdated, limiting its reliability in real-world deployment.\nMeanwhile, LiDAR itself suffers from a limited field of view (FoV), where\nmotorized rotation is commonly used to achieve panoramic coverage. Existing\nmotorized LiDAR systems, however, typically employ constant-speed scanning that\ndisregards both scene structure and map priors, leading to wasted effort in\nfeature-sparse regions and degraded localization accuracy. To address these\nchallenges, we propose Adaptive LiDAR Scanning with OSM guidance, a framework\nthat integrates global priors with local observability prediction to improve\nlocalization robustness. Specifically, we augment uncertainty-aware model\npredictive control with an OSM-aware term that adaptively allocates scanning\neffort according to both scene-dependent observability and the spatial\ndistribution of OSM features. The method is implemented in ROS with a motorized\nLiDAR odometry backend and evaluated in both simulation and real-world\nexperiments. Results on campus roads, indoor corridors, and urban environments\ndemonstrate significant reductions in trajectory error compared to\nconstant-speed baselines, while maintaining scan completeness. These findings\nhighlight the potential of coupling open-source maps with adaptive LiDAR\nscanning to achieve robust and efficient localization in complex environments."
                },
                "authors": [
                    {
                        "name": "Jianping Li"
                    },
                    {
                        "name": "Kaisong Zhu"
                    },
                    {
                        "name": "Zhongyuan Liu"
                    },
                    {
                        "name": "Rui Jin"
                    },
                    {
                        "name": "Xinhang Xu"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Lihua Xie"
                    }
                ],
                "author_detail": {
                    "name": "Lihua Xie"
                },
                "author": "Lihua Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20258v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20258v2",
                "updated": "2025-09-15T09:44:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    9,
                    44,
                    7,
                    0,
                    258,
                    0
                ],
                "published": "2025-02-27T16:46:23Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    46,
                    23,
                    3,
                    58,
                    0
                ],
                "title": "LLM as a Broken Telephone: Iterative Generation Distorts Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM as a Broken Telephone: Iterative Generation Distorts Information"
                },
                "summary": "As large language models are increasingly responsible for online content,\nconcerns arise about the impact of repeatedly processing their own outputs.\nInspired by the \"broken telephone\" effect in chained human communication, this\nstudy investigates whether LLMs similarly distort information through iterative\ngeneration. Through translation-based experiments, we find that distortion\naccumulates over time, influenced by language choice and chain complexity.\nWhile degradation is inevitable, it can be mitigated through strategic\nprompting techniques. These findings contribute to discussions on the long-term\neffects of AI-mediated information propagation, raising important questions\nabout the reliability of LLM-generated content in iterative workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models are increasingly responsible for online content,\nconcerns arise about the impact of repeatedly processing their own outputs.\nInspired by the \"broken telephone\" effect in chained human communication, this\nstudy investigates whether LLMs similarly distort information through iterative\ngeneration. Through translation-based experiments, we find that distortion\naccumulates over time, influenced by language choice and chain complexity.\nWhile degradation is inevitable, it can be mitigated through strategic\nprompting techniques. These findings contribute to discussions on the long-term\neffects of AI-mediated information propagation, raising important questions\nabout the reliability of LLM-generated content in iterative workflows."
                },
                "authors": [
                    {
                        "name": "Amr Mohamed"
                    },
                    {
                        "name": "Mingmeng Geng"
                    },
                    {
                        "name": "Michalis Vazirgiannis"
                    },
                    {
                        "name": "Guokan Shang"
                    }
                ],
                "author_detail": {
                    "name": "Guokan Shang"
                },
                "author": "Guokan Shang",
                "arxiv_comment": "Accepted to ACL 2025, Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20258v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20258v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05362v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05362v2",
                "updated": "2025-09-15T09:43:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    9,
                    43,
                    22,
                    0,
                    258,
                    0
                ],
                "published": "2025-03-07T12:07:59Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    12,
                    7,
                    59,
                    4,
                    66,
                    0
                ],
                "title": "Chain of Strategy Optimization Makes Large Language Models Better\n  Emotional Supporter",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Strategy Optimization Makes Large Language Models Better\n  Emotional Supporter"
                },
                "summary": "The growing emotional stress in modern society has increased the demand for\nEmotional Support Conversations (ESC). While Large Language Models (LLMs) show\npromise for ESC, they face two key challenges: (1) low strategy selection\naccuracy, and (2) preference bias, limiting their adaptability to emotional\nneeds of users. Existing supervised fine-tuning (SFT) struggles to address\nthese issues, as it rigidly trains models on single gold-standard responses\nwithout modeling nuanced strategy trade-offs. To overcome these limitations, we\npropose Chain-of-Strategy Optimization (CSO), a novel approach that optimizes\nstrategy selection preferences at each dialogue turn. We first leverage Monte\nCarlo Tree Search to construct ESC-Pro, a high-quality preference dataset with\nturn-level strategy-response pairs. Training on ESC-Pro with CSO improves both\nstrategy accuracy and bias mitigation, enabling LLMs to generate more\nempathetic and contextually appropriate responses. Experiments on LLaMA-3.1-8B,\nGemma-2-9B, and Qwen2.5-7B demonstrate that CSO outperforms standard SFT,\nhighlighting the efficacy of fine-grained, turn-level preference modeling in\nESC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing emotional stress in modern society has increased the demand for\nEmotional Support Conversations (ESC). While Large Language Models (LLMs) show\npromise for ESC, they face two key challenges: (1) low strategy selection\naccuracy, and (2) preference bias, limiting their adaptability to emotional\nneeds of users. Existing supervised fine-tuning (SFT) struggles to address\nthese issues, as it rigidly trains models on single gold-standard responses\nwithout modeling nuanced strategy trade-offs. To overcome these limitations, we\npropose Chain-of-Strategy Optimization (CSO), a novel approach that optimizes\nstrategy selection preferences at each dialogue turn. We first leverage Monte\nCarlo Tree Search to construct ESC-Pro, a high-quality preference dataset with\nturn-level strategy-response pairs. Training on ESC-Pro with CSO improves both\nstrategy accuracy and bias mitigation, enabling LLMs to generate more\nempathetic and contextually appropriate responses. Experiments on LLaMA-3.1-8B,\nGemma-2-9B, and Qwen2.5-7B demonstrate that CSO outperforms standard SFT,\nhighlighting the efficacy of fine-grained, turn-level preference modeling in\nESC."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhao"
                    },
                    {
                        "name": "Xingyu Sui"
                    },
                    {
                        "name": "Xinyang Han"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Yulin Hu"
                    },
                    {
                        "name": "Jiahe Guo"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Qianyun Du"
                    },
                    {
                        "name": "Shijin Wang"
                    },
                    {
                        "name": "Yanyan Zhao"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "arxiv_comment": "21 pages, 9 figures, 17 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05362v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05362v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11740v1",
                "updated": "2025-09-15T09:42:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    9,
                    42,
                    13,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T09:42:13Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    9,
                    42,
                    13,
                    0,
                    258,
                    0
                ],
                "title": "From Pixels to Shelf: End-to-End Algorithmic Control of a Mobile\n  Manipulator for Supermarket Stocking and Fronting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Pixels to Shelf: End-to-End Algorithmic Control of a Mobile\n  Manipulator for Supermarket Stocking and Fronting"
                },
                "summary": "Autonomous stocking in retail environments, particularly supermarkets,\npresents challenges due to dynamic human interactions, constrained spaces, and\ndiverse product geometries. This paper introduces an efficient end-to-end\nrobotic system for autonomous shelf stocking and fronting, integrating\ncommercially available hardware with a scalable algorithmic architecture. A\nmajor contribution of this work is the system integration of off-the-shelf\nhardware and ROS2-based perception, planning, and control into a single\ndeployable platform for retail environments. Our solution leverages Behavior\nTrees (BTs) for task planning, fine-tuned vision models for object detection,\nand a two-step Model Predictive Control (MPC) framework for precise shelf\nnavigation using ArUco markers. Laboratory experiments replicating realistic\nsupermarket conditions demonstrate reliable performance, achieving over 98%\nsuccess in pick-and-place operations across a total of more than 700 stocking\nevents. However, our comparative benchmarks indicate that the performance and\ncost-effectiveness of current autonomous systems remain inferior to that of\nhuman workers, which we use to highlight key improvement areas and quantify the\nprogress still required before widespread commercial deployment can\nrealistically be achieved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous stocking in retail environments, particularly supermarkets,\npresents challenges due to dynamic human interactions, constrained spaces, and\ndiverse product geometries. This paper introduces an efficient end-to-end\nrobotic system for autonomous shelf stocking and fronting, integrating\ncommercially available hardware with a scalable algorithmic architecture. A\nmajor contribution of this work is the system integration of off-the-shelf\nhardware and ROS2-based perception, planning, and control into a single\ndeployable platform for retail environments. Our solution leverages Behavior\nTrees (BTs) for task planning, fine-tuned vision models for object detection,\nand a two-step Model Predictive Control (MPC) framework for precise shelf\nnavigation using ArUco markers. Laboratory experiments replicating realistic\nsupermarket conditions demonstrate reliable performance, achieving over 98%\nsuccess in pick-and-place operations across a total of more than 700 stocking\nevents. However, our comparative benchmarks indicate that the performance and\ncost-effectiveness of current autonomous systems remain inferior to that of\nhuman workers, which we use to highlight key improvement areas and quantify the\nprogress still required before widespread commercial deployment can\nrealistically be achieved."
                },
                "authors": [
                    {
                        "name": "Davide Peron"
                    },
                    {
                        "name": "Victor Nan Fernandez-Ayala"
                    },
                    {
                        "name": "Lukas Segelmark"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Segelmark"
                },
                "author": "Lukas Segelmark",
                "arxiv_comment": "Submitted for publication at IEEE ICRA 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06165v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06165v2",
                "updated": "2025-09-15T09:23:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    9,
                    23,
                    58,
                    0,
                    258,
                    0
                ],
                "published": "2025-08-08T09:33:20Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    33,
                    20,
                    4,
                    220,
                    0
                ],
                "title": "UR$^2$: Unify RAG and Reasoning through Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UR$^2$: Unify RAG and Reasoning through Reinforcement Learning"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities through two\ncomplementary paradigms: Retrieval-Augmented Generation (RAG), which enhances\nknowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR),\nwhich optimizes complex reasoning abilities. However, these two capabilities\nare often developed in isolation, and existing efforts to unify them remain\nnarrow in scope -- typically limited to open-domain QA with fixed retrieval\nsettings and task-specific constraints. This lack of integration constrains\ngeneralization and limits the applicability of RAG-RL methods to broader\ndomains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a\ngeneral framework that unifies retrieval and reasoning through reinforcement\nlearning. UR2 introduces two key contributions: a difficulty-aware curriculum\ntraining that selectively invokes retrieval only for challenging problems, and\na hybrid knowledge access strategy combining domain-specific offline corpora\nwith LLM-generated summaries. These components are designed to enable dynamic\ncoordination between retrieval and reasoning, improving adaptability across a\ndiverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical,\nand mathematical reasoning tasks demonstrate that UR$^2$ (built on\nQwen-2.5-3/7B and LLaMA-3.1-8B) significantly outperforms existing RAG and RL\nmethods, achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on\nseveral benchmarks. We have released all code, models, and data at\nhttps://github.com/Tsinghua-dhy/UR2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities through two\ncomplementary paradigms: Retrieval-Augmented Generation (RAG), which enhances\nknowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR),\nwhich optimizes complex reasoning abilities. However, these two capabilities\nare often developed in isolation, and existing efforts to unify them remain\nnarrow in scope -- typically limited to open-domain QA with fixed retrieval\nsettings and task-specific constraints. This lack of integration constrains\ngeneralization and limits the applicability of RAG-RL methods to broader\ndomains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a\ngeneral framework that unifies retrieval and reasoning through reinforcement\nlearning. UR2 introduces two key contributions: a difficulty-aware curriculum\ntraining that selectively invokes retrieval only for challenging problems, and\na hybrid knowledge access strategy combining domain-specific offline corpora\nwith LLM-generated summaries. These components are designed to enable dynamic\ncoordination between retrieval and reasoning, improving adaptability across a\ndiverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical,\nand mathematical reasoning tasks demonstrate that UR$^2$ (built on\nQwen-2.5-3/7B and LLaMA-3.1-8B) significantly outperforms existing RAG and RL\nmethods, achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on\nseveral benchmarks. We have released all code, models, and data at\nhttps://github.com/Tsinghua-dhy/UR2."
                },
                "authors": [
                    {
                        "name": "Weitao Li"
                    },
                    {
                        "name": "Boran Xiang"
                    },
                    {
                        "name": "Xiaolong Wang"
                    },
                    {
                        "name": "Zhinan Gou"
                    },
                    {
                        "name": "Weizhi Ma"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06165v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06165v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11717v1",
                "updated": "2025-09-15T09:12:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    9,
                    12,
                    57,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T09:12:57Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    9,
                    12,
                    57,
                    0,
                    258,
                    0
                ],
                "title": "Neural Audio Codecs for Prompt-Driven Universal Source Separation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Audio Codecs for Prompt-Driven Universal Source Separation"
                },
                "summary": "Text-guided source separation supports flexible audio editing across media\nand assistive applications, but existing models like AudioSep are too\ncompute-heavy for edge deployment. Neural audio codec (NAC) models such as\nCodecFormer and SDCodec are compute-efficient but limited to fixed-class\nseparation. We introduce CodecSep, the first NAC-based model for on-device\nuniversal, text-driven separation. CodecSep combines DAC compression with a\nTransformer masker modulated by CLAP-derived FiLM parameters. Across six\nopen-domain benchmarks under matched training/prompt protocols,\n\\textbf{CodecSep} surpasses \\textbf{AudioSep} in separation fidelity (SI-SDR)\nwhile remaining competitive in perceptual quality (ViSQOL) and matching or\nexceeding fixed-stem baselines (TDANet, CodecFormer, SDCodec). In code-stream\ndeployments, it needs just 1.35~GMACs end-to-end -- approximately $54\\times$\nless compute ($25\\times$ architecture-only) than spectrogram-domain separators\nlike AudioSep -- while remaining fully bitstream-compatible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-guided source separation supports flexible audio editing across media\nand assistive applications, but existing models like AudioSep are too\ncompute-heavy for edge deployment. Neural audio codec (NAC) models such as\nCodecFormer and SDCodec are compute-efficient but limited to fixed-class\nseparation. We introduce CodecSep, the first NAC-based model for on-device\nuniversal, text-driven separation. CodecSep combines DAC compression with a\nTransformer masker modulated by CLAP-derived FiLM parameters. Across six\nopen-domain benchmarks under matched training/prompt protocols,\n\\textbf{CodecSep} surpasses \\textbf{AudioSep} in separation fidelity (SI-SDR)\nwhile remaining competitive in perceptual quality (ViSQOL) and matching or\nexceeding fixed-stem baselines (TDANet, CodecFormer, SDCodec). In code-stream\ndeployments, it needs just 1.35~GMACs end-to-end -- approximately $54\\times$\nless compute ($25\\times$ architecture-only) than spectrogram-domain separators\nlike AudioSep -- while remaining fully bitstream-compatible."
                },
                "authors": [
                    {
                        "name": "Adhiraj Banerjee"
                    },
                    {
                        "name": "Vipul Arora"
                    }
                ],
                "author_detail": {
                    "name": "Vipul Arora"
                },
                "author": "Vipul Arora",
                "arxiv_comment": "21 pages, 1 figure, pre-print, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17850v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17850v4",
                "updated": "2025-09-15T09:08:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    9,
                    8,
                    9,
                    0,
                    258,
                    0
                ],
                "published": "2025-08-25T09:57:35Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    9,
                    57,
                    35,
                    0,
                    237,
                    0
                ],
                "title": "Group Expectation Policy Optimization for Heterogeneous Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group Expectation Policy Optimization for Heterogeneous Reinforcement\n  Learning"
                },
                "summary": "As single-center computing approaches power constraints, decentralized\ntraining is becoming essential. Reinforcement Learning (RL) post-training\nenhances Large Language Models (LLMs) but faces challenges in heterogeneous\ndistributed environments due to its tightly-coupled sampling-learning\nalternation. We propose HeteroRL, an asynchronous RL architecture that\ndecouples rollout sampling from parameter learning, enabling robust deployment\nacross geographically distributed nodes under network delays. We identify that\nlatency-induced KL divergence causes importance sampling failure due to high\nvariance. To address this, we propose Group Expectation Policy Optimization\n(GEPO), which reduces importance weight variance through a refined sampling\nmechanism. Theoretically, GEPO achieves exponential variance reduction.\nExperiments show it maintains superior stability over methods like GRPO, with\nless than 3% performance degradation under 1800-second delays, demonstrating\nstrong potential for decentralized RL in heterogeneous networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As single-center computing approaches power constraints, decentralized\ntraining is becoming essential. Reinforcement Learning (RL) post-training\nenhances Large Language Models (LLMs) but faces challenges in heterogeneous\ndistributed environments due to its tightly-coupled sampling-learning\nalternation. We propose HeteroRL, an asynchronous RL architecture that\ndecouples rollout sampling from parameter learning, enabling robust deployment\nacross geographically distributed nodes under network delays. We identify that\nlatency-induced KL divergence causes importance sampling failure due to high\nvariance. To address this, we propose Group Expectation Policy Optimization\n(GEPO), which reduces importance weight variance through a refined sampling\nmechanism. Theoretically, GEPO achieves exponential variance reduction.\nExperiments show it maintains superior stability over methods like GRPO, with\nless than 3% performance degradation under 1800-second delays, demonstrating\nstrong potential for decentralized RL in heterogeneous networks."
                },
                "authors": [
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Ruibin Zheng"
                    },
                    {
                        "name": "Zexuan Yi"
                    },
                    {
                        "name": "Zhuo Zhang"
                    },
                    {
                        "name": "Hanyang Peng"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Zike Yuan"
                    },
                    {
                        "name": "Cai Ke"
                    },
                    {
                        "name": "Shiwei Chen"
                    },
                    {
                        "name": "Jiacheng Yang"
                    },
                    {
                        "name": "Yangning Li"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Jiangyue Yan"
                    },
                    {
                        "name": "Yaoqi Liu"
                    },
                    {
                        "name": "Liwen Jing"
                    },
                    {
                        "name": "Jiayin Qi"
                    },
                    {
                        "name": "Ruifeng Xu"
                    },
                    {
                        "name": "Binxing Fang"
                    },
                    {
                        "name": "Yue Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yue Yu"
                },
                "author": "Yue Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17850v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17850v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11708v1",
                "updated": "2025-09-15T09:07:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    9,
                    7,
                    52,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T09:07:52Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    9,
                    7,
                    52,
                    0,
                    258,
                    0
                ],
                "title": "From Evaluation to Enhancement: Large Language Models for Zero-Knowledge\n  Proof Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Evaluation to Enhancement: Large Language Models for Zero-Knowledge\n  Proof Code Generation"
                },
                "summary": "Zero-knowledge proofs (ZKPs) are increasingly deployed in domains such as\nprivacy-preserving authentication, blockchain scalability, and secure finance.\nHowever, authoring ZK programs remains challenging: unlike mainstream\nprogramming, ZK development requires reasoning about finite field arithmetic,\nconstraint systems, and gadgets, making it knowledge-intensive and error-prone.\nWhile large language models (LLMs) have demonstrated strong code generation\ncapabilities in general-purpose languages, their effectiveness for ZK\nprogramming, where correctness hinges on both language mastery and gadget-level\nreasoning, remains unexplored. To address this gap, we propose\n\\textsc{ZK-Eval}, a domain-specific evaluation pipeline that probes LLM\ncapabilities at three levels: language knowledge, gadget competence, and\nend-to-end program generation. Our evaluation of four state-of-the-art LLMs\nreveals that models excel at surface-level syntax but struggle with gadget\nusage and semantic correctness, often yielding incorrect programs. Based on\nthese insights, we introduce \\textsc{ZK-Coder}, an agentic framework that\naugments LLMs with constraint sketching, guided retrieval, and interactive\nrepair. Experiments on Circom and Noir show substantial gains, with success\nrates improving from 17.35\\% to 83.38\\% and from 32.21\\% to 90.05\\%,\nrespectively. With \\textsc{ZK-Eval} and \\textsc{ZK-Coder}, we establish a\nfoundation for systematically measuring and augmenting LLMs in ZK code\ngeneration to lower barriers for practitioners and advance trustworthy\ncomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-knowledge proofs (ZKPs) are increasingly deployed in domains such as\nprivacy-preserving authentication, blockchain scalability, and secure finance.\nHowever, authoring ZK programs remains challenging: unlike mainstream\nprogramming, ZK development requires reasoning about finite field arithmetic,\nconstraint systems, and gadgets, making it knowledge-intensive and error-prone.\nWhile large language models (LLMs) have demonstrated strong code generation\ncapabilities in general-purpose languages, their effectiveness for ZK\nprogramming, where correctness hinges on both language mastery and gadget-level\nreasoning, remains unexplored. To address this gap, we propose\n\\textsc{ZK-Eval}, a domain-specific evaluation pipeline that probes LLM\ncapabilities at three levels: language knowledge, gadget competence, and\nend-to-end program generation. Our evaluation of four state-of-the-art LLMs\nreveals that models excel at surface-level syntax but struggle with gadget\nusage and semantic correctness, often yielding incorrect programs. Based on\nthese insights, we introduce \\textsc{ZK-Coder}, an agentic framework that\naugments LLMs with constraint sketching, guided retrieval, and interactive\nrepair. Experiments on Circom and Noir show substantial gains, with success\nrates improving from 17.35\\% to 83.38\\% and from 32.21\\% to 90.05\\%,\nrespectively. With \\textsc{ZK-Eval} and \\textsc{ZK-Coder}, we establish a\nfoundation for systematically measuring and augmenting LLMs in ZK code\ngeneration to lower barriers for practitioners and advance trustworthy\ncomputation."
                },
                "authors": [
                    {
                        "name": "Zhantong Xue"
                    },
                    {
                        "name": "Pingchuan Ma"
                    },
                    {
                        "name": "Zhaoyu Wang"
                    },
                    {
                        "name": "Shuai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Wang"
                },
                "author": "Shuai Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11697v1",
                "updated": "2025-09-15T08:56:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    8,
                    56,
                    33,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T08:56:33Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    8,
                    56,
                    33,
                    0,
                    258,
                    0
                ],
                "title": "Towards the Distributed Large-scale k-NN Graph Construction by Graph\n  Merge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards the Distributed Large-scale k-NN Graph Construction by Graph\n  Merge"
                },
                "summary": "In order to support the real-time interaction with LLMs and the instant\nsearch or the instant recommendation on social media, it becomes an imminent\nproblem to build k-NN graph or indexing graph for the massive number of\nvectorized multimedia data. In such scenarios, the scale of the data or the\nscale of the graph may exceed the processing capacity of a single machine. This\npaper aims to address the graph construction problem of such scale via\nefficient graph merge. For the graph construction on a single node, two generic\nand highly parallelizable algorithms, namely Two-way Merge and Multi-way Merge\nare proposed to merge subgraphs into one. For the graph construction across\nmultiple nodes, a multi-node procedure based on Two-way Merge is presented. The\nprocedure makes it feasible to construct a large-scale k-NN graph/indexing\ngraph on either a single node or multiple nodes when the data size exceeds the\nmemory capacity of one node. Extensive experiments are conducted on both\nlarge-scale k-NN graph and indexing graph construction. For the k-NN graph\nconstruction, the large-scale and high-quality k-NN graphs are constructed by\ngraph merge in parallel. Typically, a billion-scale k-NN graph can be built in\napproximately 17h when only three nodes are employed. For the indexing graph\nconstruction, similar NN search performance as the original indexing graph is\nachieved with the merged indexing graphs while requiring much less time of\nconstruction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to support the real-time interaction with LLMs and the instant\nsearch or the instant recommendation on social media, it becomes an imminent\nproblem to build k-NN graph or indexing graph for the massive number of\nvectorized multimedia data. In such scenarios, the scale of the data or the\nscale of the graph may exceed the processing capacity of a single machine. This\npaper aims to address the graph construction problem of such scale via\nefficient graph merge. For the graph construction on a single node, two generic\nand highly parallelizable algorithms, namely Two-way Merge and Multi-way Merge\nare proposed to merge subgraphs into one. For the graph construction across\nmultiple nodes, a multi-node procedure based on Two-way Merge is presented. The\nprocedure makes it feasible to construct a large-scale k-NN graph/indexing\ngraph on either a single node or multiple nodes when the data size exceeds the\nmemory capacity of one node. Extensive experiments are conducted on both\nlarge-scale k-NN graph and indexing graph construction. For the k-NN graph\nconstruction, the large-scale and high-quality k-NN graphs are constructed by\ngraph merge in parallel. Typically, a billion-scale k-NN graph can be built in\napproximately 17h when only three nodes are employed. For the indexing graph\nconstruction, similar NN search performance as the original indexing graph is\nachieved with the merged indexing graphs while requiring much less time of\nconstruction."
                },
                "authors": [
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Wan-Lei Zhao"
                    },
                    {
                        "name": "Shihai Xiao"
                    },
                    {
                        "name": "Jiajie Yao"
                    },
                    {
                        "name": "Xuecang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xuecang Zhang"
                },
                "author": "Xuecang Zhang",
                "arxiv_comment": "14 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11686v1",
                "updated": "2025-09-15T08:38:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    8,
                    38,
                    1,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T08:38:01Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    8,
                    38,
                    1,
                    0,
                    258,
                    0
                ],
                "title": "Do Code Semantics Help? A Comprehensive Study on Execution Trace-Based\n  Information for Code Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Code Semantics Help? A Comprehensive Study on Execution Trace-Based\n  Information for Code Large Language Models"
                },
                "summary": "Code Large Language Models (Code LLMs) have opened a new era in programming\nwith their impressive capabilities. However, recent research has revealed\ncritical limitations in their ability to reason about runtime behavior and\nunderstand the actual functionality of programs, which poses significant\nchallenges for their post-training and practical deployment. Specifically, Code\nLLMs encounter two principal issues: (1) a lack of proficiency in reasoning\nabout program execution behavior, as they struggle to interpret what programs\nactually do during runtime, and (2) the inconsistent and fragmented\nrepresentation of semantic information, such as execution traces, across\nexisting methods, which hinders their ability to generalize and reason\neffectively. These challenges underscore the necessity for more systematic\napproaches to enhance the reasoning capabilities of Code LLMs. To address these\nissues, we introduce a generic framework to support integrating semantic\ninformation~(e.g., execution trace) to code task-relevant prompts, and conduct\na comprehensive study to explore the role of semantic information in enhancing\nthe reasoning ability of Code LLMs accordingly. Specifically, we focus on\ninvestigating the usefulness of trace-based semantic information in boosting\nsupervised fine-tuning~(SFT) and post-phase inference of Code LLMs. The\nexperimental results surprisingly disagree with previous works and demonstrate\nthat semantic information has limited usefulness for SFT and test time scaling\nof Code LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Large Language Models (Code LLMs) have opened a new era in programming\nwith their impressive capabilities. However, recent research has revealed\ncritical limitations in their ability to reason about runtime behavior and\nunderstand the actual functionality of programs, which poses significant\nchallenges for their post-training and practical deployment. Specifically, Code\nLLMs encounter two principal issues: (1) a lack of proficiency in reasoning\nabout program execution behavior, as they struggle to interpret what programs\nactually do during runtime, and (2) the inconsistent and fragmented\nrepresentation of semantic information, such as execution traces, across\nexisting methods, which hinders their ability to generalize and reason\neffectively. These challenges underscore the necessity for more systematic\napproaches to enhance the reasoning capabilities of Code LLMs. To address these\nissues, we introduce a generic framework to support integrating semantic\ninformation~(e.g., execution trace) to code task-relevant prompts, and conduct\na comprehensive study to explore the role of semantic information in enhancing\nthe reasoning ability of Code LLMs accordingly. Specifically, we focus on\ninvestigating the usefulness of trace-based semantic information in boosting\nsupervised fine-tuning~(SFT) and post-phase inference of Code LLMs. The\nexperimental results surprisingly disagree with previous works and demonstrate\nthat semantic information has limited usefulness for SFT and test time scaling\nof Code LLM."
                },
                "authors": [
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Xiaofei Xie"
                    },
                    {
                        "name": "Qiang Hu"
                    },
                    {
                        "name": "Shangqing Liu"
                    },
                    {
                        "name": "Yi Li"
                    }
                ],
                "author_detail": {
                    "name": "Yi Li"
                },
                "author": "Yi Li",
                "arxiv_comment": "EMNLP2025-findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12301v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12301v2",
                "updated": "2025-09-15T08:34:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    8,
                    34,
                    53,
                    0,
                    258,
                    0
                ],
                "published": "2025-03-16T00:22:00Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    0,
                    22,
                    0,
                    6,
                    75,
                    0
                ],
                "title": "One Goal, Many Challenges: Robust Preference Optimization Amid\n  Content-Aware and Multi-Source Noise",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Goal, Many Challenges: Robust Preference Optimization Amid\n  Content-Aware and Multi-Source Noise"
                },
                "summary": "Large Language Models (LLMs) have made significant strides in generating\nhuman-like responses, largely due to preference alignment techniques. However,\nthese methods often assume unbiased human feedback, which is rarely the case in\nreal-world scenarios. This paper introduces Content-Aware Noise-Resilient\nPreference Optimization (CNRPO), a novel framework that addresses multiple\nsources of content-dependent noise in preference learning. CNRPO employs a\nmulti-objective optimization approach to separate true preferences from\ncontent-aware noises, effectively mitigating their impact. We leverage backdoor\nattack mechanisms to efficiently learn and control various noise sources within\na single model. Theoretical analysis and extensive experiments on different\nsynthetic noisy datasets demonstrate that CNRPO significantly improves\nalignment with primary human preferences while controlling for secondary noises\nand biases, such as response length and harmfulness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant strides in generating\nhuman-like responses, largely due to preference alignment techniques. However,\nthese methods often assume unbiased human feedback, which is rarely the case in\nreal-world scenarios. This paper introduces Content-Aware Noise-Resilient\nPreference Optimization (CNRPO), a novel framework that addresses multiple\nsources of content-dependent noise in preference learning. CNRPO employs a\nmulti-objective optimization approach to separate true preferences from\ncontent-aware noises, effectively mitigating their impact. We leverage backdoor\nattack mechanisms to efficiently learn and control various noise sources within\na single model. Theoretical analysis and extensive experiments on different\nsynthetic noisy datasets demonstrate that CNRPO significantly improves\nalignment with primary human preferences while controlling for secondary noises\nand biases, such as response length and harmfulness."
                },
                "authors": [
                    {
                        "name": "Amirabbas Afzali"
                    },
                    {
                        "name": "Amirhossein Afsharrad"
                    },
                    {
                        "name": "Seyed Shahabeddin Mousavi"
                    },
                    {
                        "name": "Sanjay Lall"
                    }
                ],
                "author_detail": {
                    "name": "Sanjay Lall"
                },
                "author": "Sanjay Lall",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12301v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12301v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08401v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08401v3",
                "updated": "2025-09-15T08:24:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    8,
                    24,
                    50,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-10T08:45:08Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    8,
                    45,
                    8,
                    2,
                    253,
                    0
                ],
                "title": "Two Sides of the Same Optimization Coin: Model Degradation and\n  Representation Collapse in Graph Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two Sides of the Same Optimization Coin: Model Degradation and\n  Representation Collapse in Graph Foundation Models"
                },
                "summary": "Graph foundation models, inspired by the success of LLMs, are designed to\nlearn the optimal embedding from multi-domain TAGs for the downstream\ncross-task generalization capability. During our investigation, graph VQ-MAE\nstands out among the increasingly diverse landscape of GFM architectures. This\nis attributed to its ability to jointly encode topology and textual attributes\nfrom multiple domains into discrete embedding spaces with clear semantic\nboundaries. Despite its potential, domain generalization conflicts cause\nimperceptible pitfalls. In this paper, we instantiate two of them, and they are\njust like two sides of the same GFM optimization coin - Side 1 Model\nDegradation: The encoder and codebook fail to capture the diversity of inputs;\nSide 2 Representation Collapse: The hidden embedding and codebook vector fail\nto preserve semantic separability due to constraints from narrow representation\nsubspaces. These two pitfalls (sides) collectively impair the decoder and\ngenerate the low-quality reconstructed supervision, causing the GFM\noptimization dilemma during pre-training (coin). Through empirical\ninvestigation, we attribute the above challenges to Information Bottleneck and\nRegularization Deficit. To address them, we propose MoT (Mixture-of-Tinkers) -\n(1) Information Tinker for Two Pitfalls, which utilizes an edge-wise semantic\nfusion strategy and a mixture-of-codebooks with domain-aware routing to improve\ninformation capacity. (2) Regularization Tinker for Optimization Coin, which\nutilizes two additional regularizations to further improve gradient supervision\nin our proposed Information Tinker. Notably, as a flexible architecture, MoT\nadheres to the scaling laws of GFM, offering a controllable model scale.\nCompared to SOTA baselines, experiments on 22 datasets across 6 domains\ndemonstrate that MoT achieves significant improvements in supervised, few-shot,\nand zero-shot scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph foundation models, inspired by the success of LLMs, are designed to\nlearn the optimal embedding from multi-domain TAGs for the downstream\ncross-task generalization capability. During our investigation, graph VQ-MAE\nstands out among the increasingly diverse landscape of GFM architectures. This\nis attributed to its ability to jointly encode topology and textual attributes\nfrom multiple domains into discrete embedding spaces with clear semantic\nboundaries. Despite its potential, domain generalization conflicts cause\nimperceptible pitfalls. In this paper, we instantiate two of them, and they are\njust like two sides of the same GFM optimization coin - Side 1 Model\nDegradation: The encoder and codebook fail to capture the diversity of inputs;\nSide 2 Representation Collapse: The hidden embedding and codebook vector fail\nto preserve semantic separability due to constraints from narrow representation\nsubspaces. These two pitfalls (sides) collectively impair the decoder and\ngenerate the low-quality reconstructed supervision, causing the GFM\noptimization dilemma during pre-training (coin). Through empirical\ninvestigation, we attribute the above challenges to Information Bottleneck and\nRegularization Deficit. To address them, we propose MoT (Mixture-of-Tinkers) -\n(1) Information Tinker for Two Pitfalls, which utilizes an edge-wise semantic\nfusion strategy and a mixture-of-codebooks with domain-aware routing to improve\ninformation capacity. (2) Regularization Tinker for Optimization Coin, which\nutilizes two additional regularizations to further improve gradient supervision\nin our proposed Information Tinker. Notably, as a flexible architecture, MoT\nadheres to the scaling laws of GFM, offering a controllable model scale.\nCompared to SOTA baselines, experiments on 22 datasets across 6 domains\ndemonstrate that MoT achieves significant improvements in supervised, few-shot,\nand zero-shot scenarios."
                },
                "authors": [
                    {
                        "name": "Xunkai Li"
                    },
                    {
                        "name": "Daohan Su"
                    },
                    {
                        "name": "Sicheng Liu"
                    },
                    {
                        "name": "Ru Zhang"
                    },
                    {
                        "name": "Zhenjun Li"
                    },
                    {
                        "name": "Bing Zhou"
                    },
                    {
                        "name": "Rong-Hua Li"
                    },
                    {
                        "name": "Guoren Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guoren Wang"
                },
                "author": "Guoren Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08401v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08401v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09448v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09448v3",
                "updated": "2025-09-15T08:09:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    8,
                    9,
                    43,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-11T13:31:35Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    31,
                    35,
                    3,
                    254,
                    0
                ],
                "title": "TORSO: Template-Oriented Reasoning Towards General Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TORSO: Template-Oriented Reasoning Towards General Tasks"
                },
                "summary": "The approaches that guide Large Language Models (LLMs) to emulate human\nreasoning during response generation have emerged as an effective method for\nenabling them to solve complex problems in a step-by-step manner, thereby\nachieving superior performance. However, most existing approaches using\nfew-shot prompts to generate responses heavily depend on the provided examples,\nlimiting the utilization of the model's inherent reasoning capabilities.\nMoreover, constructing task-specific few-shot prompts is often costly and may\nlead to inconsistencies across different tasks. In this work, we introduce\nTemplate-Oriented Reasoning (TORSO), which elicits the model to utilize\ninternal reasoning abilities to generate proper responses across various tasks\nwithout the need for manually crafted few-shot examples. Our experimental\nresults demonstrate that TORSO achieves strong performance on diverse LLMs\nbenchmarks with reasonable rationales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The approaches that guide Large Language Models (LLMs) to emulate human\nreasoning during response generation have emerged as an effective method for\nenabling them to solve complex problems in a step-by-step manner, thereby\nachieving superior performance. However, most existing approaches using\nfew-shot prompts to generate responses heavily depend on the provided examples,\nlimiting the utilization of the model's inherent reasoning capabilities.\nMoreover, constructing task-specific few-shot prompts is often costly and may\nlead to inconsistencies across different tasks. In this work, we introduce\nTemplate-Oriented Reasoning (TORSO), which elicits the model to utilize\ninternal reasoning abilities to generate proper responses across various tasks\nwithout the need for manually crafted few-shot examples. Our experimental\nresults demonstrate that TORSO achieves strong performance on diverse LLMs\nbenchmarks with reasonable rationales."
                },
                "authors": [
                    {
                        "name": "Minhyuk Kim"
                    },
                    {
                        "name": "Seungyoon Lee"
                    },
                    {
                        "name": "Heuiseok Lim"
                    }
                ],
                "author_detail": {
                    "name": "Heuiseok Lim"
                },
                "author": "Heuiseok Lim",
                "arxiv_comment": "Accepted to EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09448v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09448v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11663v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11663v1",
                "updated": "2025-09-15T08:02:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    8,
                    2,
                    55,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T08:02:55Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    8,
                    2,
                    55,
                    0,
                    258,
                    0
                ],
                "title": "ParaEQsA: Parallel and Asynchronous Embodied Questions Scheduling and\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParaEQsA: Parallel and Asynchronous Embodied Questions Scheduling and\n  Answering"
                },
                "summary": "This paper formulates the Embodied Questions Answering (EQsA) problem,\nintroduces a corresponding benchmark, and proposes a system to tackle the\nproblem. Classical Embodied Question Answering (EQA) is typically formulated as\nanswering one single question by actively exploring a 3D environment. Real\ndeployments, however, often demand handling multiple questions that may arrive\nasynchronously and carry different urgencies. We formalize this setting as\nEmbodied Questions Answering (EQsA) and present ParaEQsA, a framework for\nparallel, urgency-aware scheduling and answering. ParaEQsA leverages a group\nmemory module shared among questions to reduce redundant exploration, and a\npriority-planning module to dynamically schedule questions. To evaluate this\nsetting, we contribute the Parallel Asynchronous Embodied Questions (PAEQs)\nbenchmark containing 40 indoor scenes and five questions per scene (200 in\ntotal), featuring asynchronous follow-up questions and urgency labels. We\nfurther propose metrics for EQsA performance: Direct Answer Rate (DAR), and\nNormalized Urgency-Weighted Latency (NUWL), which jointly measure efficiency\nand responsiveness of this system. ParaEQsA consistently outperforms strong\nsequential baselines adapted from recent EQA systems, while reducing\nexploration and delay. Empirical evaluations investigate the relative\ncontributions of priority, urgency modeling, spatial scope, reward estimation,\nand dependency reasoning within our framework. Together, these results\ndemonstrate that urgency-aware, parallel scheduling is key to making embodied\nagents responsive and efficient under realistic, multi-question workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper formulates the Embodied Questions Answering (EQsA) problem,\nintroduces a corresponding benchmark, and proposes a system to tackle the\nproblem. Classical Embodied Question Answering (EQA) is typically formulated as\nanswering one single question by actively exploring a 3D environment. Real\ndeployments, however, often demand handling multiple questions that may arrive\nasynchronously and carry different urgencies. We formalize this setting as\nEmbodied Questions Answering (EQsA) and present ParaEQsA, a framework for\nparallel, urgency-aware scheduling and answering. ParaEQsA leverages a group\nmemory module shared among questions to reduce redundant exploration, and a\npriority-planning module to dynamically schedule questions. To evaluate this\nsetting, we contribute the Parallel Asynchronous Embodied Questions (PAEQs)\nbenchmark containing 40 indoor scenes and five questions per scene (200 in\ntotal), featuring asynchronous follow-up questions and urgency labels. We\nfurther propose metrics for EQsA performance: Direct Answer Rate (DAR), and\nNormalized Urgency-Weighted Latency (NUWL), which jointly measure efficiency\nand responsiveness of this system. ParaEQsA consistently outperforms strong\nsequential baselines adapted from recent EQA systems, while reducing\nexploration and delay. Empirical evaluations investigate the relative\ncontributions of priority, urgency modeling, spatial scope, reward estimation,\nand dependency reasoning within our framework. Together, these results\ndemonstrate that urgency-aware, parallel scheduling is key to making embodied\nagents responsive and efficient under realistic, multi-question workloads."
                },
                "authors": [
                    {
                        "name": "Haisheng Wang"
                    },
                    {
                        "name": "Weiming Zhi"
                    }
                ],
                "author_detail": {
                    "name": "Weiming Zhi"
                },
                "author": "Weiming Zhi",
                "arxiv_comment": "8 pages, 6 figures, 2026 IEEE Conference on Robotics and Automation\n  (ICRA 2026)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11663v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11663v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11661v1",
                "updated": "2025-09-15T07:59:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    7,
                    59,
                    34,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T07:59:34Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    7,
                    59,
                    34,
                    0,
                    258,
                    0
                ],
                "title": "DTGen: Generative Diffusion-Based Few-Shot Data Augmentation for\n  Fine-Grained Dirty Tableware Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DTGen: Generative Diffusion-Based Few-Shot Data Augmentation for\n  Fine-Grained Dirty Tableware Recognition"
                },
                "summary": "Intelligent tableware cleaning is a critical application in food safety and\nsmart homes, but existing methods are limited by coarse-grained classification\nand scarcity of few-shot data, making it difficult to meet industrialization\nrequirements. We propose DTGen, a few-shot data augmentation scheme based on\ngenerative diffusion models, specifically designed for fine-grained dirty\ntableware recognition. DTGen achieves efficient domain specialization through\nLoRA, generates diverse dirty images via structured prompts, and ensures data\nquality through CLIP-based cross-modal filtering. Under extremely limited real\nfew-shot conditions, DTGen can synthesize virtually unlimited high-quality\nsamples, significantly improving classifier performance and supporting\nfine-grained dirty tableware recognition. We further elaborate on lightweight\ndeployment strategies, promising to transfer DTGen's benefits to embedded\ndishwashers and integrate with cleaning programs to intelligently regulate\nenergy consumption and detergent usage. Research results demonstrate that DTGen\nnot only validates the value of generative AI in few-shot industrial vision but\nalso provides a feasible deployment path for automated tableware cleaning and\nfood safety monitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent tableware cleaning is a critical application in food safety and\nsmart homes, but existing methods are limited by coarse-grained classification\nand scarcity of few-shot data, making it difficult to meet industrialization\nrequirements. We propose DTGen, a few-shot data augmentation scheme based on\ngenerative diffusion models, specifically designed for fine-grained dirty\ntableware recognition. DTGen achieves efficient domain specialization through\nLoRA, generates diverse dirty images via structured prompts, and ensures data\nquality through CLIP-based cross-modal filtering. Under extremely limited real\nfew-shot conditions, DTGen can synthesize virtually unlimited high-quality\nsamples, significantly improving classifier performance and supporting\nfine-grained dirty tableware recognition. We further elaborate on lightweight\ndeployment strategies, promising to transfer DTGen's benefits to embedded\ndishwashers and integrate with cleaning programs to intelligently regulate\nenergy consumption and detergent usage. Research results demonstrate that DTGen\nnot only validates the value of generative AI in few-shot industrial vision but\nalso provides a feasible deployment path for automated tableware cleaning and\nfood safety monitoring."
                },
                "authors": [
                    {
                        "name": "Lifei Hao"
                    },
                    {
                        "name": "Yue Cheng"
                    },
                    {
                        "name": "Baoqi Huang"
                    },
                    {
                        "name": "Bing Jia"
                    },
                    {
                        "name": "Xuandong Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xuandong Zhao"
                },
                "author": "Xuandong Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]